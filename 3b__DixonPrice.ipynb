{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xxx3b__DixonPrice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Dixon-Price synthetic function:\r\n",
        "\r\n",
        "GP EI: Newton-CG (exact GP EI gradients + exact GP EI Hessian) vs. L-BFGS-B (exact GP EI gradients, without Hessian)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/dixonpr.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "3d513663-c50e-464b-8154-f8741005dc6e"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyGPGO in /usr/local/lib/python3.6/dist-packages (0.4.0.dev1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "\r\n",
        "rc('text', usetex=False)\r\n",
        "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\r\n",
        "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "n_start_AcqFunc = 100\r\n",
        "\r\n",
        "obj_func = 'DixonPrice'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dEI_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'DixonPrice': # 2-D\r\n",
        "            \r\n",
        "    def f_syn_polarity(x1_training, x2_training):\r\n",
        "        return  operator * ((x1_training - 1)**2\r\n",
        "                            + 2 * (2 * x2_training ** 2 - x1_training)**2            \r\n",
        "                           )\r\n",
        "\r\n",
        "# Constraints:\r\n",
        "    lb = -10 \r\n",
        "    ub = +10\r\n",
        "    \r\n",
        "# Input array dimension(s):\r\n",
        "    dim = 2\r\n",
        "    max_iter = 100\r\n",
        "\r\n",
        "# 2-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub]),\r\n",
        "             }\r\n",
        "\r\n",
        "    \r\n",
        "# True y bounds:\r\n",
        "    y_lb = 0.00000\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "\r\n",
        "# Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, max_iter) \r\n",
        "    x2_test = np.linspace(lb, ub, max_iter)\r\n",
        "    Xstar_d = np.column_stack((x1_test,x2_test))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 99\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1e-4\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r **2 - 1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "### Acquisition function derivatives:\r\n",
        "\r\n",
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1e-4\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(v.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: Exact Hessian\r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    p = np.full((n_start,1),1)*0 + 1\r\n",
        "    eps = 1e-08\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "\r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        d2f = (z * norm.cdf(z) + norm.pdf(z)[0]) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx\r\n",
        "\r\n",
        "        return d2f\r\n",
        "\r\n",
        "    def hessp_nonzero1(self, xnew, p, *args):\r\n",
        "      new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "      new_std = np.sqrt(new_var + 1e-6)\r\n",
        "      ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "      df2 = np.empty((self.n_start,))\r\n",
        "      df2 = -self.dEI_GP(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0] * p\r\n",
        "      return df2\r\n",
        "\r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,                  \r\n",
        "                                                                 hessp = self.hessp_nonzero1,                      \r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "\r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):    \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "60e31021-3e29-471d-ba8f-704d882d3b13"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1612257881.7192826"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "601ba57f-d91c-479f-8f5b-5e9070e01fb0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_1 = d2GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.24340351 -6.61860491]. \t  -14577.093458365041 \t -540.0726783817402\n",
            "init   \t [-1.27881961  5.38524945]. \t  -7033.582264098692 \t -540.0726783817402\n",
            "init   \t [-4.09349391 -7.01674086]. \t  -21064.19892235138 \t -540.0726783817402\n",
            "init   \t [-9.55043351 -1.59551015]. \t  -540.0726783817402 \t -540.0726783817402\n",
            "init   \t [-5.22635718 -3.24687616]. \t  -1423.2804229542423 \t -540.0726783817402\n",
            "1      \t [8.37416725 6.27688756]. \t  -9973.589615047737 \t -540.0726783817402\n",
            "2      \t [-9.76713775  9.96269922]. \t  -86875.28945668103 \t -540.0726783817402\n",
            "3      \t [ 9.98926049 -9.3463478 ]. \t  -54345.61921377557 \t -540.0726783817402\n",
            "4      \t [ 9.44490266 -0.94685252]. \t  \u001b[92m-188.41779181630653\u001b[0m \t -188.41779181630653\n",
            "5      \t [  1.16221783 -13.60510856]. \t  -272374.7814404298 \t -188.41779181630653\n",
            "6      \t [ -9.2043966  -19.15693124]. \t  -1104738.4209002384 \t -188.41779181630653\n",
            "7      \t [-10.63161987  -9.89577976]. \t  -85407.02013506749 \t -188.41779181630653\n",
            "8      \t [3.44076809 0.27907271]. \t  \u001b[92m-27.539863698698085\u001b[0m \t -27.539863698698085\n",
            "9      \t [-7.29911972  3.709041  ]. \t  -2492.7778644557866 \t -27.539863698698085\n",
            "10     \t [-0.73495703 -0.81851356]. \t  \u001b[92m-11.620379147224739\u001b[0m \t -11.620379147224739\n",
            "11     \t [-38.28592447 -39.15456041]. \t  -19276702.470083196 \t -11.620379147224739\n",
            "12     \t [3.50322486 8.13226875]. \t  -33166.789630439715 \t -11.620379147224739\n",
            "13     \t [-3.37870255  9.74368565]. \t  -74716.11309951566 \t -11.620379147224739\n",
            "14     \t [ 6.92233961 -4.46391467]. \t  -2203.9398923755048 \t -11.620379147224739\n",
            "15     \t [6.81718644 1.51240727]. \t  -43.896686830439606 \t -11.620379147224739\n",
            "16     \t [-2.98981467  0.6455539 ]. \t  -45.15378047333768 \t -11.620379147224739\n",
            "17     \t [-28.28506102 -29.58126635]. \t  -6326183.228001587 \t -11.620379147224739\n",
            "18     \t [4.43684566 3.20846261]. \t  -533.5615016805237 \t -11.620379147224739\n",
            "19     \t [0.56454101 1.24982823]. \t  -13.292730763065736 \t -11.620379147224739\n",
            "20     \t [ 1.70727734 -2.11067474]. \t  -104.25566489729175 \t -11.620379147224739\n",
            "21     \t [  5.08785534 -10.11925712]. \t  -79785.57693995879 \t -11.620379147224739\n",
            "22     \t [-9.30368701 -5.09329536]. \t  -7593.864457902248 \t -11.620379147224739\n",
            "23     \t [-1.12365849 -3.98071064]. \t  -2158.2598313702983 \t -11.620379147224739\n",
            "24     \t [-0.24582787  0.26776761]. \t  \u001b[92m-1.8550821588236268\u001b[0m \t -1.8550821588236268\n",
            "25     \t [9.1256098  1.93348714]. \t  -71.46304475831397 \t -1.8550821588236268\n",
            "26     \t [ 5.68254314 -0.60375224]. \t  -71.00072505971642 \t -1.8550821588236268\n",
            "27     \t [-0.8115279  -0.25321924]. \t  -5.047960650296771 \t -1.8550821588236268\n",
            "28     \t [-1.34483863  0.86349999]. \t  -21.585229676053643 \t -1.8550821588236268\n",
            "29     \t [-6.71666219 -0.28074889]. \t  -154.05893299924375 \t -1.8550821588236268\n",
            "30     \t [-4.11871937  2.47284065]. \t  -560.7548030799861 \t -1.8550821588236268\n",
            "31     \t [2.4148974  1.18348577]. \t  -2.300513233876282 \t -1.8550821588236268\n",
            "32     \t [-0.22755242  0.00198212]. \t  \u001b[92m-1.6104522891095416\u001b[0m \t -1.6104522891095416\n",
            "33     \t [7.43336651 9.91035573]. \t  -71481.07096276061 \t -1.6104522891095416\n",
            "34     \t [-2.37414951 -0.03322398]. \t  -22.679031692215098 \t -1.6104522891095416\n",
            "35     \t [2.45926948 0.31759795]. \t  -12.322373998548077 \t -1.6104522891095416\n",
            "36     \t [2.21035347 1.51165454]. \t  -12.602696571555187 \t -1.6104522891095416\n",
            "37     \t [-3.36962251 -0.18173171]. \t  -42.70133096956171 \t -1.6104522891095416\n",
            "38     \t [-9.8545825   1.34674201]. \t  -481.3511805040524 \t -1.6104522891095416\n",
            "39     \t [ 3.71977635 -1.84206982]. \t  -26.206065699800472 \t -1.6104522891095416\n",
            "40     \t [2.66663212 0.76131084]. \t  -7.3224357229285335 \t -1.6104522891095416\n",
            "41     \t [1.91296055 0.55388432]. \t  -4.210299085552981 \t -1.6104522891095416\n",
            "42     \t [ 9.78778828 -3.13610806]. \t  -272.55517948900217 \t -1.6104522891095416\n",
            "43     \t [-0.74934068 -8.84508107]. \t  -49439.47984660758 \t -1.6104522891095416\n",
            "44     \t [0.23726056 0.96381082]. \t  -5.834473280457192 \t -1.6104522891095416\n",
            "45     \t [-0.54100142  1.39211057]. \t  -41.393493185191666 \t -1.6104522891095416\n",
            "46     \t [-7.02565385  6.94581249]. \t  -21494.82184061763 \t -1.6104522891095416\n",
            "47     \t [-5.91223484  8.2221967 ]. \t  -39878.20582274619 \t -1.6104522891095416\n",
            "48     \t [-1.29426966 -0.67473396]. \t  -14.98597242488901 \t -1.6104522891095416\n",
            "49     \t [-1.12767355 -7.87485051]. \t  -31331.681813730527 \t -1.6104522891095416\n",
            "50     \t [1.75542957 0.92587339]. \t  \u001b[92m-0.5740270696740744\u001b[0m \t -0.5740270696740744\n",
            "51     \t [0.10739441 0.27421282]. \t  -0.8004411893953339 \t -0.5740270696740744\n",
            "52     \t [5.96478258 5.29373115]. \t  -5041.14190295565 \t -0.5740270696740744\n",
            "53     \t [1.04414074 0.77201658]. \t  \u001b[92m-0.04568447350163922\u001b[0m \t -0.04568447350163922\n",
            "54     \t [1.89890345 0.92989741]. \t  -0.8654777762145416 \t -0.04568447350163922\n",
            "55     \t [-2.53389748 -8.241353  ]. \t  -38307.04787877346 \t -0.04568447350163922\n",
            "56     \t [-4.64210491  7.23811639]. \t  -23978.51530931304 \t -0.04568447350163922\n",
            "57     \t [ 2.65547766 -4.35937835]. \t  -2502.3930702570133 \t -0.04568447350163922\n",
            "58     \t [3.93131005 2.43882584]. \t  -135.4569623453088 \t -0.04568447350163922\n",
            "59     \t [1.99889829 5.79626343]. \t  -8501.628130104542 \t -0.04568447350163922\n",
            "60     \t [0.66436043 0.79101033]. \t  -0.8018723891737554 \t -0.04568447350163922\n",
            "61     \t [3.5649309  1.43362308]. \t  -7.174271551447151 \t -0.04568447350163922\n",
            "62     \t [ 3.55810801 -1.24698549]. \t  -6.94561566183089 \t -0.04568447350163922\n",
            "63     \t [3.08580586 1.47958825]. \t  -7.691992944882913 \t -0.04568447350163922\n",
            "64     \t [ 2.17849603 -0.87463367]. \t  -2.230029811969959 \t -0.04568447350163922\n",
            "65     \t [ 1.73437093 -0.61154979]. \t  -2.485210035388694 \t -0.04568447350163922\n",
            "66     \t [-0.28079526 -1.18962417]. \t  -20.999649416284967 \t -0.04568447350163922\n",
            "67     \t [ 4.04868908 -1.29093973]. \t  -10.318781515064941 \t -0.04568447350163922\n",
            "68     \t [4.91664008 1.20679108]. \t  -23.37170588352303 \t -0.04568447350163922\n",
            "69     \t [ 0.66852413 -0.85686487]. \t  -1.38959045539405 \t -0.04568447350163922\n",
            "70     \t [ 0.48527097 -0.77956298]. \t  -1.3312304645847437 \t -0.04568447350163922\n",
            "71     \t [ 4.61162552 -1.7932563 ]. \t  -19.667989550885103 \t -0.04568447350163922\n",
            "72     \t [-1.36766416  6.02301355]. \t  -10934.248704746018 \t -0.04568447350163922\n",
            "73     \t [-1.08564396  3.5247215 ]. \t  -1349.3874904645695 \t -0.04568447350163922\n",
            "74     \t [-1.34219568 -9.43450538]. \t  -64346.925071400525 \t -0.04568447350163922\n",
            "75     \t [-1.21685943  1.94656391]. \t  -159.62138470584105 \t -0.04568447350163922\n",
            "76     \t [-2.35087264  1.36952208]. \t  -85.69847514120576 \t -0.04568447350163922\n",
            "77     \t [ 1.05409182 -2.81709776]. \t  -439.1484486340524 \t -0.04568447350163922\n",
            "78     \t [ 1.19633367 -1.04157599]. \t  -1.9336687717627659 \t -0.04568447350163922\n",
            "79     \t [-2.11542918 -0.94077489]. \t  -39.90080283256739 \t -0.04568447350163922\n",
            "80     \t [ 1.70330964 -1.22908663]. \t  -3.968883279134737 \t -0.04568447350163922\n",
            "81     \t [ 3.2481393  -1.14003583]. \t  -5.895950675392666 \t -0.04568447350163922\n",
            "82     \t [4.40914038 2.89442519]. \t  -316.4822145066299 \t -0.04568447350163922\n",
            "83     \t [4.45233326 1.97971428]. \t  -34.85135964807685 \t -0.04568447350163922\n",
            "84     \t [0.62649811 0.57865358]. \t  -0.14323300422446833 \t -0.04568447350163922\n",
            "85     \t [-8.89096135  3.26803045]. \t  -1928.0780127427365 \t -0.04568447350163922\n",
            "86     \t [ 3.62652117 -8.97584202]. \t  -49622.52268386854 \t -0.04568447350163922\n",
            "87     \t [0.96696844 0.43880532]. \t  -0.6782323510923229 \t -0.04568447350163922\n",
            "88     \t [-5.28205775  0.69650269]. \t  -117.64650369098928 \t -0.04568447350163922\n",
            "89     \t [4.32847395 3.52225331]. \t  -850.2723996448592 \t -0.04568447350163922\n",
            "90     \t [ 5.95507955 -0.58533502]. \t  -80.09535385780426 \t -0.04568447350163922\n",
            "91     \t [1.27144605 1.61000405]. \t  -30.693378009561993 \t -0.04568447350163922\n",
            "92     \t [ 0.20537001 -0.90481193]. \t  -4.732680496469586 \t -0.04568447350163922\n",
            "93     \t [ 5.26403367 -6.36853727]. \t  -11525.385601354039 \t -0.04568447350163922\n",
            "94     \t [-5.71866905  3.75727751]. \t  -2350.7444695066465 \t -0.04568447350163922\n",
            "95     \t [ 8.07429825 -8.26933835]. \t  -33172.07978023832 \t -0.04568447350163922\n",
            "96     \t [-8.07480962 -7.10576937]. \t  -23869.963676107633 \t -0.04568447350163922\n",
            "97     \t [-1.0387751   5.84805472]. \t  -9647.50139297433 \t -0.04568447350163922\n",
            "98     \t [ 8.40402256 -4.17626348]. \t  -1457.0235509451786 \t -0.04568447350163922\n",
            "99     \t [1.67328896 1.86655206]. \t  -56.52195071756115 \t -0.04568447350163922\n",
            "100    \t [-5.34906297  1.65464129]. \t  -274.660530146399 \t -0.04568447350163922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "e344a730-e146-41d6-c01b-673c871012ed"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_2 = d2GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.03970985 -8.52192802]. \t  -38238.719775621794 \t -57.443300302345605\n",
            "init   \t [ 7.89863525 -1.27012905]. \t  -91.24969381804144 \t -57.443300302345605\n",
            "init   \t [-7.44644532  1.51715749]. \t  -361.74642651372136 \t -57.443300302345605\n",
            "init   \t [ 6.80941839 -1.29758898]. \t  -57.443300302345605 \t -57.443300302345605\n",
            "init   \t [3.91821121 3.69276194]. \t  -1099.406544633519 \t -57.443300302345605\n",
            "1      \t [-3.35617937 -7.20418892]. \t  -22984.1595726879 \t -57.443300302345605\n",
            "2      \t [-2.87227721  9.90072912]. \t  -79154.24352053565 \t -57.443300302345605\n",
            "3      \t [9.76713458 9.30216525]. \t  -53406.34593257343 \t -57.443300302345605\n",
            "4      \t [-11.24064619  -4.90092752]. \t  -7177.792158743019 \t -57.443300302345605\n",
            "5      \t [-30.90087714 -40.93040187]. \t  -22870054.44621162 \t -57.443300302345605\n",
            "6      \t [-9.85635154  8.2084324 ]. \t  -41943.744160795184 \t -57.443300302345605\n",
            "7      \t [-11.11753331 -11.81255655]. \t  -168567.90799378618 \t -57.443300302345605\n",
            "8      \t [-0.96925372 -0.61007646]. \t  \u001b[92m-9.751086339135874\u001b[0m \t -9.751086339135874\n",
            "9      \t [-30.46353784 -28.91096169]. \t  -5795625.661534864 \t -9.751086339135874\n",
            "10     \t [ -1.10899828 -13.89886158]. \t  -300263.79289118276 \t -9.751086339135874\n",
            "11     \t [ 2.41083847 -3.97223991]. \t  -1701.0322145252344 \t -9.751086339135874\n",
            "12     \t [-62.54006766 -61.7876113 ]. \t  -118521148.65703183 \t -9.751086339135874\n",
            "13     \t [-2.50531529  3.66357468]. \t  -1735.0000676863613 \t -9.751086339135874\n",
            "14     \t [2.97258899 9.37888261]. \t  -59830.263246461305 \t -9.751086339135874\n",
            "15     \t [9.93364434 3.1610923 ]. \t  -281.86986791399926 \t -9.751086339135874\n",
            "16     \t [-16.61839949 -16.7288864 ]. \t  -664622.2072911619 \t -9.751086339135874\n",
            "17     \t [-4.3648278  -1.91290446]. \t  -301.77732452187024 \t -9.751086339135874\n",
            "18     \t [-12.9240923    0.89021404]. \t  -614.9056642376906 \t -9.751086339135874\n",
            "19     \t [ 2.36215539 -0.0343243 ]. \t  -12.992770650367074 \t -9.751086339135874\n",
            "20     \t [ 1.6965072  -8.90268805]. \t  -49185.009340831726 \t -9.751086339135874\n",
            "21     \t [ 9.78932172 -3.79351762]. \t  -798.6617852489943 \t -9.751086339135874\n",
            "22     \t [-7.84119665 -7.857502  ]. \t  -34569.034888886876 \t -9.751086339135874\n",
            "23     \t [ 4.50080923 -0.03237134]. \t  -52.73251027235888 \t -9.751086339135874\n",
            "24     \t [-9.07185692 -1.47569719]. \t  -462.02284720524085 \t -9.751086339135874\n",
            "25     \t [-1.19848282 -2.91446412]. \t  -666.344355168578 \t -9.751086339135874\n",
            "26     \t [0.84064087 1.01076477]. \t  \u001b[92m-2.9181292315735634\u001b[0m \t -2.9181292315735634\n",
            "27     \t [7.4322608 5.624308 ]. \t  -6276.115572689569 \t -2.9181292315735634\n",
            "28     \t [-1.14439597  0.35568025]. \t  -8.503959360211185 \t -2.9181292315735634\n",
            "29     \t [0.1609796  0.09078924]. \t  \u001b[92m-0.7457123985537077\u001b[0m \t -0.7457123985537077\n",
            "30     \t [-5.92526949  5.27255267]. \t  -7618.58423470874 \t -0.7457123985537077\n",
            "31     \t [9.66708751 0.40316363]. \t  -249.6645469005997 \t -0.7457123985537077\n",
            "32     \t [ 0.0508366  -0.03585975]. \t  -0.905570134789214 \t -0.7457123985537077\n",
            "33     \t [-0.62673533 -0.21716182]. \t  -3.6861051043431754 \t -0.7457123985537077\n",
            "34     \t [0.23272385 0.11518293]. \t  \u001b[92m-0.6737410248369122\u001b[0m \t -0.6737410248369122\n",
            "35     \t [ 5.8550793  -4.45603839]. \t  -2316.2292802856286 \t -0.6737410248369122\n",
            "36     \t [-0.49446034  6.17490108]. \t  -11784.361449314862 \t -0.6737410248369122\n",
            "37     \t [6.34963568 0.53972791]. \t  -95.1357160247394 \t -0.6737410248369122\n",
            "38     \t [-1.83422304 -0.20584179]. \t  -15.397671516899365 \t -0.6737410248369122\n",
            "39     \t [0.98387592 1.40484486]. \t  -17.562580023112382 \t -0.6737410248369122\n",
            "40     \t [0.16308031 2.00231073]. \t  -124.1155613071918 \t -0.6737410248369122\n",
            "41     \t [1.70144483 0.29624375]. \t  -5.148913665659565 \t -0.6737410248369122\n",
            "42     \t [1.30859837 0.91852788]. \t  \u001b[92m-0.3821945218466912\u001b[0m \t -0.3821945218466912\n",
            "43     \t [-0.09649101 -0.46175283]. \t  -1.749188122894419 \t -0.3821945218466912\n",
            "44     \t [0.8390783  0.64851188]. \t  \u001b[92m-0.025904257113482494\u001b[0m \t -0.025904257113482494\n",
            "45     \t [-7.01332457 -3.91169723]. \t  -2894.1515587274944 \t -0.025904257113482494\n",
            "46     \t [-9.12987395  3.04892046]. \t  -1639.600308085574 \t -0.025904257113482494\n",
            "47     \t [-9.39681078  9.86068833]. \t  -83228.4691757223 \t -0.025904257113482494\n",
            "48     \t [ 0.90766099 -0.37734296]. \t  -0.784499367355668 \t -0.025904257113482494\n",
            "49     \t [0.7963069  0.68398708]. \t  -0.08033873734877423 \t -0.025904257113482494\n",
            "50     \t [-8.28999214 -2.10020643]. \t  -671.9263016290016 \t -0.025904257113482494\n",
            "51     \t [-4.15084462  0.46437327]. \t  -68.52302543305701 \t -0.025904257113482494\n",
            "52     \t [ 0.33046167 -0.6171681 ]. \t  -0.8203748906392512 \t -0.025904257113482494\n",
            "53     \t [9.76705851 8.21340072]. \t  -31403.330122896183 \t -0.025904257113482494\n",
            "54     \t [-2.80091004  1.81608432]. \t  -191.06295292016443 \t -0.025904257113482494\n",
            "55     \t [ 4.31762819 -1.1680969 ]. \t  -16.054766647850514 \t -0.025904257113482494\n",
            "56     \t [ 5.02745548 -1.27737766]. \t  -22.44427013260116 \t -0.025904257113482494\n",
            "57     \t [ 1.86157517 -0.69254877]. \t  -2.370701852213603 \t -0.025904257113482494\n",
            "58     \t [-9.42952584 -4.1515827 ]. \t  -3963.336127203735 \t -0.025904257113482494\n",
            "59     \t [1.25303374 0.89425685]. \t  -0.3039522795476428 \t -0.025904257113482494\n",
            "60     \t [2.28011225 0.71455782]. \t  -4.808479206333333 \t -0.025904257113482494\n",
            "61     \t [-3.19560488  1.14343779]. \t  -85.12703375483325 \t -0.025904257113482494\n",
            "62     \t [-5.28721486  0.7090003 ]. \t  -118.72213931421999 \t -0.025904257113482494\n",
            "63     \t [-2.83471316 -2.43994193]. \t  -449.31961196714644 \t -0.025904257113482494\n",
            "64     \t [ 2.63471392 -0.81827083]. \t  -6.029342620978083 \t -0.025904257113482494\n",
            "65     \t [-0.54379441  6.45592031]. \t  -14081.343119217587 \t -0.025904257113482494\n",
            "66     \t [-0.1896273   4.22494727]. \t  -2577.597424934544 \t -0.025904257113482494\n",
            "67     \t [-0.83396895  1.60450346]. \t  -74.9519910178512 \t -0.025904257113482494\n",
            "68     \t [1.47063782 0.60320376]. \t  -1.3253848093746392 \t -0.025904257113482494\n",
            "69     \t [2.49795694 1.41860388]. \t  -6.906825862421291 \t -0.025904257113482494\n",
            "70     \t [0.72006032 0.58500891]. \t  -0.08089944663980621 \t -0.025904257113482494\n",
            "71     \t [2.1130402 1.1241049]. \t  -1.581954353975373 \t -0.025904257113482494\n",
            "72     \t [3.33488104 0.87917915]. \t  -11.85249019561287 \t -0.025904257113482494\n",
            "73     \t [ 1.25910626 -0.77446087]. \t  -0.07422297869913888 \t -0.025904257113482494\n",
            "74     \t [ 3.45629411 -1.35216389]. \t  -6.1137012746390464 \t -0.025904257113482494\n",
            "75     \t [ 0.60394876 -0.53768646]. \t  -0.15818119664587996 \t -0.025904257113482494\n",
            "76     \t [8.63905631 5.4974074 ]. \t  -5425.6474319473755 \t -0.025904257113482494\n",
            "77     \t [-5.7912885   4.92505611]. \t  -5943.893075288378 \t -0.025904257113482494\n",
            "78     \t [-5.55150189  2.82055068]. \t  -964.2012006171227 \t -0.025904257113482494\n",
            "79     \t [-5.04643942  6.02138366]. \t  -12067.840893946795 \t -0.025904257113482494\n",
            "80     \t [-5.05007529  3.03266977]. \t  -1135.8688586114838 \t -0.025904257113482494\n",
            "81     \t [2.6698593  8.31714593]. \t  -36820.89887926603 \t -0.025904257113482494\n",
            "82     \t [-5.20499438  9.18626805]. \t  -60576.60751066257 \t -0.025904257113482494\n",
            "83     \t [9.81821777 7.46230331]. \t  -20704.07848571376 \t -0.025904257113482494\n",
            "84     \t [ 3.44669397 -0.93923385]. \t  -11.647072744110712 \t -0.025904257113482494\n",
            "85     \t [-0.42287358 -3.42135285]. \t  -1138.1612419423348 \t -0.025904257113482494\n",
            "86     \t [7.17441931 5.68570548]. \t  -6646.041337880619 \t -0.025904257113482494\n",
            "87     \t [9.22419433 1.02904472]. \t  -168.63717468394677 \t -0.025904257113482494\n",
            "88     \t [0.12358243 1.07867932]. \t  -10.479070644808722 \t -0.025904257113482494\n",
            "89     \t [1.3270714  0.74694865]. \t  -0.19619236803955004 \t -0.025904257113482494\n",
            "90     \t [8.17555757 7.6307228 ]. \t  -23500.736860312965 \t -0.025904257113482494\n",
            "91     \t [7.61007925 2.30212772]. \t  -61.56742566188194 \t -0.025904257113482494\n",
            "92     \t [-1.39136264 -7.11112906]. \t  -21029.567136461766 \t -0.025904257113482494\n",
            "93     \t [-1.75019318  1.96533379]. \t  -187.12493048878747 \t -0.025904257113482494\n",
            "94     \t [ 1.25097145 -0.54566878]. \t  -0.9222491649755977 \t -0.025904257113482494\n",
            "95     \t [ 6.33238924 -5.55543991]. \t  -6165.2998444628665 \t -0.025904257113482494\n",
            "96     \t [5.71707915 4.2191094 ]. \t  -1808.44012269075 \t -0.025904257113482494\n",
            "97     \t [-1.89617604  0.76884797]. \t  -27.341303541816906 \t -0.025904257113482494\n",
            "98     \t [ 5.28574755 -2.76259469]. \t  -217.4930416817421 \t -0.025904257113482494\n",
            "99     \t [ 5.087094   -4.05750602]. \t  -1566.7927954166332 \t -0.025904257113482494\n",
            "100    \t [-3.29500331  4.82568198]. \t  -4992.364448817138 \t -0.025904257113482494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "9225b17b-b7c9-4f43-d5e6-9e6414580a4b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_3 = d2GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.03151214 -7.81497449]. \t  -27448.6338140421 \t -1.7863168283775226\n",
            "init   \t [-0.27757442 -0.00337645]. \t  -1.7863168283775226 \t -1.7863168283775226\n",
            "init   \t [ 3.14219998 -5.28230578]. \t  -5551.437171353593 \t -1.7863168283775226\n",
            "init   \t [ 2.25589781 -7.60695193]. \t  -25754.9725373955 \t -1.7863168283775226\n",
            "init   \t [ 4.24404593 -2.54878916]. \t  -163.60006264419064 \t -1.7863168283775226\n",
            "1      \t [-5.61872775  6.96371276]. \t  -21099.51033227109 \t -1.7863168283775226\n",
            "2      \t [-9.6736302  -6.79831075]. \t  -20965.88636571433 \t -1.7863168283775226\n",
            "3      \t [5.95256263 7.26277879]. \t  -19842.284445028028 \t -1.7863168283775226\n",
            "4      \t [-9.5655754   1.04731115]. \t  -388.1935005848966 \t -1.7863168283775226\n",
            "5      \t [ -3.39252592 -11.90986646]. \t  -164851.86162540514 \t -1.7863168283775226\n",
            "6      \t [-14.83455298 -15.44187818]. \t  -483862.7756741295 \t -1.7863168283775226\n",
            "7      \t [-3.77790647 -3.78287836]. \t  -2122.1208017688878 \t -1.7863168283775226\n",
            "8      \t [-15.75961712  -0.86138631]. \t  -875.5675495590881 \t -1.7863168283775226\n",
            "9      \t [ 9.8823633  -0.26074619]. \t  -268.8804640347504 \t -1.7863168283775226\n",
            "10     \t [0.18037714 9.89102847]. \t  -76429.06005447076 \t -1.7863168283775226\n",
            "11     \t [2.07178033 2.78701362]. \t  -363.659350568178 \t -1.7863168283775226\n",
            "12     \t [-4.2951643   1.51896721]. \t  -186.80381168241567 \t -1.7863168283775226\n",
            "13     \t [ 9.97923216 -7.68882125]. \t  -23519.65817380143 \t -1.7863168283775226\n",
            "14     \t [9.78265058 3.98023095]. \t  -1036.5149354350756 \t -1.7863168283775226\n",
            "15     \t [ 7.94053879 -3.16817154]. \t  -342.6430186434637 \t -1.7863168283775226\n",
            "16     \t [-12.62139779   6.53417729]. \t  -19398.381438197117 \t -1.7863168283775226\n",
            "17     \t [9.91784296 9.46154543]. \t  -57285.27507142265 \t -1.7863168283775226\n",
            "18     \t [-1.70705706  4.50067886]. \t  -3572.262941138695 \t -1.7863168283775226\n",
            "19     \t [6.41675827 0.63169142]. \t  -92.48062835900797 \t -1.7863168283775226\n",
            "20     \t [-7.67941964 -2.54382223]. \t  -925.8234206649154 \t -1.7863168283775226\n",
            "21     \t [ 0.47355157 -1.40361623]. \t  -24.31351928671875 \t -1.7863168283775226\n",
            "22     \t [ 2.14451171 -0.3649435 ]. \t  -8.364758292372253 \t -1.7863168283775226\n",
            "23     \t [-2.13757202  0.61526108]. \t  -26.602522470674447 \t -1.7863168283775226\n",
            "24     \t [-4.81939566 -7.051957  ]. \t  -21782.3314983724 \t -1.7863168283775226\n",
            "25     \t [-8.70096922  3.83707114]. \t  -3004.5264021678117 \t -1.7863168283775226\n",
            "26     \t [-9.63444314  9.87604258]. \t  -83922.90247770057 \t -1.7863168283775226\n",
            "27     \t [0.5609216  0.28871852]. \t  \u001b[92m-0.5035847435912526\u001b[0m \t -0.5035847435912526\n",
            "28     \t [-0.73479683  0.16673361]. \t  -4.258974926798736 \t -0.5035847435912526\n",
            "29     \t [-0.93593511 -1.04544144]. \t  -23.23949904556323 \t -0.5035847435912526\n",
            "30     \t [6.74584776 3.04907814]. \t  -313.76058030577246 \t -0.5035847435912526\n",
            "31     \t [4.25025825 0.67153897]. \t  -32.986793741507334 \t -0.5035847435912526\n",
            "32     \t [-0.22070749  0.91804348]. \t  -8.758201656501045 \t -0.5035847435912526\n",
            "33     \t [ 1.29456625 -0.41412349]. \t  -1.8977391875340102 \t -0.5035847435912526\n",
            "34     \t [ 1.20166387 -2.27978692]. \t  -169.07026119089664 \t -0.5035847435912526\n",
            "35     \t [-0.95235046 -4.94390255]. \t  -4971.204065033471 \t -0.5035847435912526\n",
            "36     \t [1.9097893  0.06171165]. \t  -8.064238090568919 \t -0.5035847435912526\n",
            "37     \t [1.82114459 5.62065139]. \t  -7531.337902276627 \t -0.5035847435912526\n",
            "38     \t [ 3.10151141 -0.22910713]. \t  -22.374749363394404 \t -0.5035847435912526\n",
            "39     \t [-0.72016443  1.01610053]. \t  -18.47236253557247 \t -0.5035847435912526\n",
            "40     \t [-6.94525525  0.77577085]. \t  -195.93609584558035 \t -0.5035847435912526\n",
            "41     \t [ 0.41295727 -0.47829522]. \t  \u001b[92m-0.34859309677135714\u001b[0m \t -0.34859309677135714\n",
            "42     \t [-2.97543308 -0.30258771]. \t  -35.756966117306234 \t -0.34859309677135714\n",
            "43     \t [ 6.26686273 -1.28309778]. \t  -45.43137086547496 \t -0.34859309677135714\n",
            "44     \t [0.85667398 0.53954777]. \t  \u001b[92m-0.17118838213653775\u001b[0m \t -0.17118838213653775\n",
            "45     \t [ 1.27053041 -0.33185326]. \t  -2.2793512727729617 \t -0.17118838213653775\n",
            "46     \t [ 1.66295637 -0.82982573]. \t  -0.6028000162250124 \t -0.17118838213653775\n",
            "47     \t [-5.58540288  4.39491028]. \t  -3953.4548543088213 \t -0.17118838213653775\n",
            "48     \t [ 0.20636065 -1.03118292]. \t  -8.005088743736575 \t -0.17118838213653775\n",
            "49     \t [ 1.03584294 -0.62554944]. \t  \u001b[92m-0.12952418440825247\u001b[0m \t -0.12952418440825247\n",
            "50     \t [1.03450793 0.42992548]. \t  -0.8852048554706723 \t -0.12952418440825247\n",
            "51     \t [-7.55261153 -6.98049741]. \t  -22126.213147022947 \t -0.12952418440825247\n",
            "52     \t [-4.79538118 -0.90116801]. \t  -116.00868270749399 \t -0.12952418440825247\n",
            "53     \t [-7.65280328  8.39240761]. \t  -44189.95329155957 \t -0.12952418440825247\n",
            "54     \t [0.69268672 0.57984457]. \t  \u001b[92m-0.09526135668747171\u001b[0m \t -0.09526135668747171\n",
            "55     \t [-7.74309729  4.8701817 ]. \t  -6166.202986000307 \t -0.09526135668747171\n",
            "56     \t [ 1.83415236 -0.81424352]. \t  -1.212278243855153 \t -0.09526135668747171\n",
            "57     \t [-6.79058801  3.04631228]. \t  -1346.001643784961 \t -0.09526135668747171\n",
            "58     \t [4.27113026 9.60126228]. \t  -64880.80724541189 \t -0.09526135668747171\n",
            "59     \t [0.4912342  1.29054362]. \t  -16.387446906871514 \t -0.09526135668747171\n",
            "60     \t [-1.84316155 -0.84936544]. \t  -29.679223317333147 \t -0.09526135668747171\n",
            "61     \t [-2.59300614 -9.39339568]. \t  -64141.29417985735 \t -0.09526135668747171\n",
            "62     \t [-1.97006732 -0.32444149]. \t  -18.331261772302526 \t -0.09526135668747171\n",
            "63     \t [-8.44339496  0.08126659]. \t  -232.20599304231348 \t -0.09526135668747171\n",
            "64     \t [ 0.68569177 -0.501645  ]. \t  -0.1653265279530442 \t -0.09526135668747171\n",
            "65     \t [-5.92098836 -9.43033498]. \t  -67600.59403596063 \t -0.09526135668747171\n",
            "66     \t [8.13498007 1.93468773]. \t  -51.750204667839355 \t -0.09526135668747171\n",
            "67     \t [-6.30677823  1.02322762]. \t  -194.53480580590767 \t -0.09526135668747171\n",
            "68     \t [ 0.97628404 -6.80955698]. \t  -16841.20832557118 \t -0.09526135668747171\n",
            "69     \t [4.73898245 3.74417699]. \t  -1099.64257608463 \t -0.09526135668747171\n",
            "70     \t [8.62350834 1.41865155]. \t  -100.40778028227365 \t -0.09526135668747171\n",
            "71     \t [-1.62358223 -1.94224028]. \t  -174.9941159860788 \t -0.09526135668747171\n",
            "72     \t [0.70236421 0.28894428]. \t  -0.6618647306546054 \t -0.09526135668747171\n",
            "73     \t [ 5.26578534 -6.19497163]. \t  -10239.711748289283 \t -0.09526135668747171\n",
            "74     \t [ 0.25318592 -3.02381372]. \t  -650.9873297211229 \t -0.09526135668747171\n",
            "75     \t [ 6.79081091 -9.25907725]. \t  -54266.09092018643 \t -0.09526135668747171\n",
            "76     \t [ 3.20228902 -1.32563536]. \t  -5.045175958816867 \t -0.09526135668747171\n",
            "77     \t [-5.53208922 -3.06693246]. \t  -1227.9521036431554 \t -0.09526135668747171\n",
            "78     \t [-0.7973397 -0.612444 ]. \t  -8.020035398703488 \t -0.09526135668747171\n",
            "79     \t [ 4.04697172 -0.84087746]. \t  -23.147539190339764 \t -0.09526135668747171\n",
            "80     \t [ 2.85167   -1.4620323]. \t  -7.480856050483563 \t -0.09526135668747171\n",
            "81     \t [-1.38890174 -0.55778568]. \t  -13.796311972686956 \t -0.09526135668747171\n",
            "82     \t [ 2.764416   -1.07770781]. \t  -3.5030220232318805 \t -0.09526135668747171\n",
            "83     \t [-6.70974456  6.86359402]. \t  -20432.19709372185 \t -0.09526135668747171\n",
            "84     \t [-3.50885816  4.46413975]. \t  -3781.5402603850184 \t -0.09526135668747171\n",
            "85     \t [-8.65102928  9.910253  ]. \t  -84206.5061584383 \t -0.09526135668747171\n",
            "86     \t [1.82850804 4.73345635]. \t  -3695.7131544669023 \t -0.09526135668747171\n",
            "87     \t [ 0.65459451 -1.51311626]. \t  -30.92187508731874 \t -0.09526135668747171\n",
            "88     \t [ 1.02735708 -0.84406543]. \t  -0.31681785501756504 \t -0.09526135668747171\n",
            "89     \t [-0.21351966 -3.97757932]. \t  -2031.0558726958338 \t -0.09526135668747171\n",
            "90     \t [-4.66950881 -2.21553784]. \t  -451.87371405420083 \t -0.09526135668747171\n",
            "91     \t [2.49362315 1.53912676]. \t  -12.30377062266507 \t -0.09526135668747171\n",
            "92     \t [ 2.78182392 -1.27265521]. \t  -3.593469907187097 \t -0.09526135668747171\n",
            "93     \t [ 3.47587984 -4.77822747]. \t  -3565.6219455686582 \t -0.09526135668747171\n",
            "94     \t [-3.33938631 -3.56105429]. \t  -1666.394147176159 \t -0.09526135668747171\n",
            "95     \t [3.73924683 5.81216801]. \t  -8154.342748064939 \t -0.09526135668747171\n",
            "96     \t [2.02714413 1.0559903 ]. \t  -1.1375136443397926 \t -0.09526135668747171\n",
            "97     \t [-6.11691153  5.8954905 ]. \t  -11490.603962595726 \t -0.09526135668747171\n",
            "98     \t [4.0798654 0.869062 ]. \t  -22.688462380370442 \t -0.09526135668747171\n",
            "99     \t [2.98462554 0.88524282]. \t  -7.956306810331252 \t -0.09526135668747171\n",
            "100    \t [3.38451542 1.30086674]. \t  -5.6859137757815175 \t -0.09526135668747171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "96d7becb-39c9-4ae7-e7d6-919f3a25f781"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_4 = d2GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.78722095 6.4258993 ]. \t  -11523.905614569041 \t -765.9755148718126\n",
            "init   \t [2.8441433  3.34445237]. \t  -765.9755148718126 \t -765.9755148718126\n",
            "init   \t [-9.21720189 -8.68655223]. \t  -51387.35679532384 \t -765.9755148718126\n",
            "init   \t [-4.4699872  -3.67278993]. \t  -2007.9696367442386 \t -765.9755148718126\n",
            "init   \t [-6.12815176 -6.05814236]. \t  -12700.957148627067 \t -765.9755148718126\n",
            "1      \t [ 5.15020416 -5.71066603]. \t  -7234.8050437839365 \t -765.9755148718126\n",
            "2      \t [-3.97305893  9.21389704]. \t  -60413.181901672484 \t -765.9755148718126\n",
            "3      \t [-8.51899843  2.33131307]. \t  -842.4810529068852 \t -765.9755148718126\n",
            "4      \t [  1.17903038 -11.46989784]. \t  -137223.15007446162 \t -765.9755148718126\n",
            "5      \t [-69.7258764  -67.86198413]. \t  -172250104.40819478 \t -765.9755148718126\n",
            "6      \t [-13.74774616  -1.80552776]. \t  -1039.0478168843993 \t -765.9755148718126\n",
            "7      \t [-2.83803863  2.67646344]. \t  \u001b[92m-604.0016793287856\u001b[0m \t -604.0016793287856\n",
            "8      \t [-15.77348487 -11.7170745 ]. \t  -168891.2586491704 \t -604.0016793287856\n",
            "9      \t [ 8.97539946 -0.27131946]. \t  \u001b[92m-219.4802014122361\u001b[0m \t -219.4802014122361\n",
            "10     \t [ 0.87945553 -1.72052983]. \t  \u001b[92m-50.837696295977\u001b[0m \t -50.837696295977\n",
            "11     \t [  7.58466069 -10.92633473]. \t  -106936.29278058367 \t -50.837696295977\n",
            "12     \t [ -9.01243446 -15.81971997]. \t  -519361.2820907303 \t -50.837696295977\n",
            "13     \t [-10.15357019   7.64051398]. \t  -32335.92564358089 \t -50.837696295977\n",
            "14     \t [-0.03159313 -5.64249699]. \t  -8118.262987347491 \t -50.837696295977\n",
            "15     \t [1.85127105 8.41605129]. \t  -39093.59571546353 \t -50.837696295977\n",
            "16     \t [ 4.93600484 -0.91887052]. \t  \u001b[92m-36.582812320051914\u001b[0m \t -36.582812320051914\n",
            "17     \t [ -4.40638926 -10.93728664]. \t  -118764.61764225809 \t -36.582812320051914\n",
            "18     \t [-9.18107617 -1.91958135]. \t  -651.5030928985935 \t -36.582812320051914\n",
            "19     \t [-5.56313523  0.2387673 ]. \t  -107.53491890629383 \t -36.582812320051914\n",
            "20     \t [-1.33383904 -0.79170411]. \t  \u001b[92m-18.8363908155254\u001b[0m \t -18.8363908155254\n",
            "21     \t [ 8.8265108  -4.11307361]. \t  -1312.078224120276 \t -18.8363908155254\n",
            "22     \t [6.13145361 1.66761882]. \t  -26.98058724585679 \t -18.8363908155254\n",
            "23     \t [9.53651703 3.34482853]. \t  -402.5642253885253 \t -18.8363908155254\n",
            "24     \t [0.79218882 0.56683283]. \t  \u001b[92m-0.08793976629197425\u001b[0m \t -0.08793976629197425\n",
            "25     \t [8.57973321 9.98456116]. \t  -72869.16695759939 \t -0.08793976629197425\n",
            "26     \t [-6.53689488  4.90865836]. \t  -6046.838833143219 \t -0.08793976629197425\n",
            "27     \t [ 2.9466678  -2.06007388]. \t  -65.19800169086778 \t -0.08793976629197425\n",
            "28     \t [0.10783759 0.16183711]. \t  -0.8021042933517469 \t -0.08793976629197425\n",
            "29     \t [0.28729995 0.16287836]. \t  -0.6176792684089881 \t -0.08793976629197425\n",
            "30     \t [-3.40545753 -0.31622067]. \t  -45.40657458059367 \t -0.08793976629197425\n",
            "31     \t [1.34693447 0.09164139]. \t  -3.658898676206393 \t -0.08793976629197425\n",
            "32     \t [-0.07270396  1.62557158]. \t  -58.55994698080651 \t -0.08793976629197425\n",
            "33     \t [-0.05847882  4.26708431]. \t  -2661.8980750668748 \t -0.08793976629197425\n",
            "34     \t [0.18189607 0.28696296]. \t  -0.669885756144594 \t -0.08793976629197425\n",
            "35     \t [1.25592922 0.62408964]. \t  -0.5204689821455685 \t -0.08793976629197425\n",
            "36     \t [ 6.62303047 -1.72727961]. \t  -32.47925059699761 \t -0.08793976629197425\n",
            "37     \t [4.85582567 0.64597757]. \t  -47.20832103208613 \t -0.08793976629197425\n",
            "38     \t [1.01942292 0.83118324]. \t  -0.2629117530741221 \t -0.08793976629197425\n",
            "39     \t [0.8463342  0.28743519]. \t  -0.9513973057137283 \t -0.08793976629197425\n",
            "40     \t [6.09202015 2.78638739]. \t  -204.00067974463605 \t -0.08793976629197425\n",
            "41     \t [0.07732238 0.95647137]. \t  -6.9928132061596635 \t -0.08793976629197425\n",
            "42     \t [6.18028785 0.87625937]. \t  -69.98050000839145 \t -0.08793976629197425\n",
            "43     \t [-0.84424308 -2.28743273]. \t  -259.1854820136321 \t -0.08793976629197425\n",
            "44     \t [ 5.57734152 -1.78229998]. \t  -22.155925992111435 \t -0.08793976629197425\n",
            "45     \t [-2.35515747 -0.67036938]. \t  -32.4334326974584 \t -0.08793976629197425\n",
            "46     \t [ 9.60576125 -7.20033915]. \t  -17777.659722130218 \t -0.08793976629197425\n",
            "47     \t [1.91299589 0.49054712]. \t  -4.933222636279261 \t -0.08793976629197425\n",
            "48     \t [-0.66042172 -0.08984019]. \t  -3.6724786176249067 \t -0.08793976629197425\n",
            "49     \t [3.56905532 0.54776267]. \t  -24.2295804521989 \t -0.08793976629197425\n",
            "50     \t [-9.91387411 -4.76925574]. \t  -6258.645914932864 \t -0.08793976629197425\n",
            "51     \t [8.81606978 0.28697194]. \t  -210.7831447781824 \t -0.08793976629197425\n",
            "52     \t [9.61072306 2.50904558]. \t  -91.90411608464652 \t -0.08793976629197425\n",
            "53     \t [9.34620855 1.62215972]. \t  -103.00757773285565 \t -0.08793976629197425\n",
            "54     \t [ 6.49196718 -7.21609816]. \t  -19102.05751112033 \t -0.08793976629197425\n",
            "55     \t [ 3.85389757 -5.01233296]. \t  -4312.777861907882 \t -0.08793976629197425\n",
            "56     \t [-5.33888765 -4.42680939]. \t  -4006.4098859951646 \t -0.08793976629197425\n",
            "57     \t [3.20122798 0.02553411]. \t  -25.324431859395908 \t -0.08793976629197425\n",
            "58     \t [-5.54828522 -2.83642507]. \t  -979.3639882971961 \t -0.08793976629197425\n",
            "59     \t [-9.31011341  2.2251477 ]. \t  -844.5524321080262 \t -0.08793976629197425\n",
            "60     \t [ 1.49127003 -0.86876078]. \t  -0.242010220498908 \t -0.08793976629197425\n",
            "61     \t [ 1.51529009 -1.02225423]. \t  -0.9261239403920316 \t -0.08793976629197425\n",
            "62     \t [ 5.10272363 -8.37402313]. \t  -36545.6085694846 \t -0.08793976629197425\n",
            "63     \t [-3.36428627 -4.54178341]. \t  -4000.9142702923155 \t -0.08793976629197425\n",
            "64     \t [-9.52691162  7.81135867]. \t  -34727.70400980378 \t -0.08793976629197425\n",
            "65     \t [0.70007031 2.81598637]. \t  -459.71012085825157 \t -0.08793976629197425\n",
            "66     \t [-3.66699095 -1.76817563]. \t  -218.58888550221934 \t -0.08793976629197425\n",
            "67     \t [0.72490275 0.50496981]. \t  -0.1680543182939337 \t -0.08793976629197425\n",
            "68     \t [0.56436426 4.51406788]. \t  -3230.541957069111 \t -0.08793976629197425\n",
            "69     \t [1.06649519 1.58133135]. \t  -30.9685033007486 \t -0.08793976629197425\n",
            "70     \t [-7.19007984  2.48036784]. \t  -827.1507365802629 \t -0.08793976629197425\n",
            "71     \t [-2.95393482  4.31677071]. \t  -3251.4055782804458 \t -0.08793976629197425\n",
            "72     \t [-2.52117737  1.34526022]. \t  -87.81330172882122 \t -0.08793976629197425\n",
            "73     \t [-1.43321797 -0.44400218]. \t  -12.60001720740824 \t -0.08793976629197425\n",
            "74     \t [0.29994861 0.71606543]. \t  -1.5429198503183588 \t -0.08793976629197425\n",
            "75     \t [ 0.31874756 -0.1894839 ]. \t  -0.5860628912546688 \t -0.08793976629197425\n",
            "76     \t [ 2.01578475 -0.26943551]. \t  -8.030060629979012 \t -0.08793976629197425\n",
            "77     \t [7.03050385 8.26918204]. \t  -33695.19910931697 \t -0.08793976629197425\n",
            "78     \t [8.53890693 8.20984019]. \t  -31942.053167629954 \t -0.08793976629197425\n",
            "79     \t [ 2.12312966 -8.41193406]. \t  -38864.9440071836 \t -0.08793976629197425\n",
            "80     \t [-8.01751757 -8.31904695]. \t  -42965.158021264026 \t -0.08793976629197425\n",
            "81     \t [-0.34299285  0.37725207]. \t  -2.591471405136864 \t -0.08793976629197425\n",
            "82     \t [9.83375544 1.97039514]. \t  -86.59544505113597 \t -0.08793976629197425\n",
            "83     \t [7.47771772 1.93812294]. \t  -41.963266179237756 \t -0.08793976629197425\n",
            "84     \t [7.23246528 9.31484792]. \t  -55350.43524845048 \t -0.08793976629197425\n",
            "85     \t [8.73485241 4.70702164]. \t  -2591.309797284475 \t -0.08793976629197425\n",
            "86     \t [ 1.36357184 -0.56019412]. \t  -1.2153908256735835 \t -0.08793976629197425\n",
            "87     \t [-9.74162944  5.69879592]. \t  -11273.823476747046 \t -0.08793976629197425\n",
            "88     \t [ 9.77528004 -4.79792497]. \t  -2707.289737859233 \t -0.08793976629197425\n",
            "89     \t [-3.70262294 -8.69285128]. \t  -47969.21868999177 \t -0.08793976629197425\n",
            "90     \t [ 1.78014597 -0.74883894]. \t  -1.4762053760300258 \t -0.08793976629197425\n",
            "91     \t [-3.93501848  1.12177016]. \t  -107.60469387945847 \t -0.08793976629197425\n",
            "92     \t [ 5.37857082 -2.58028248]. \t  -145.16840990953483 \t -0.08793976629197425\n",
            "93     \t [5.45426873 5.76928074]. \t  -7489.910291650112 \t -0.08793976629197425\n",
            "94     \t [0.08161118 1.76828527]. \t  -77.03194911698674 \t -0.08793976629197425\n",
            "95     \t [-1.29255613 -2.49606527]. \t  -383.55902441051194 \t -0.08793976629197425\n",
            "96     \t [ 6.62625188 -0.43495995]. \t  -109.72650818812865 \t -0.08793976629197425\n",
            "97     \t [3.86956944 0.02692883]. \t  -38.15911986727106 \t -0.08793976629197425\n",
            "98     \t [-5.98245003  7.30174925]. \t  -25412.370855398018 \t -0.08793976629197425\n",
            "99     \t [-0.24093743 -0.70878098]. \t  -4.643355030458981 \t -0.08793976629197425\n",
            "100    \t [ 6.29792024 -4.77555238]. \t  -3119.230411100237 \t -0.08793976629197425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "c3e8ecf1-f693-4af2-eb99-8204a9b1bb5e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_5 = d2GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.14706549 0.83521255]. \t  -0.14473002518929054 \t -0.14473002518929054\n",
            "init   \t [ 1.20847567 -2.35903813]. \t  -196.92158721117866 \t -0.14473002518929054\n",
            "init   \t [-7.61811642  6.91998231]. \t  -21453.432632023236 \t -0.14473002518929054\n",
            "init   \t [4.79483225 8.64715129]. \t  -41920.46290057033 \t -0.14473002518929054\n",
            "init   \t [7.18656013 4.16209252]. \t  -1546.3183076435794 \t -0.14473002518929054\n",
            "1      \t [-1.28443039  2.56483835]. \t  -422.31640133461167 \t -0.14473002518929054\n",
            "2      \t [2.20317475 0.72550179]. \t  -4.094787565175626 \t -0.14473002518929054\n",
            "3      \t [ 9.14614046 -9.26496158]. \t  -52900.23738128742 \t -0.14473002518929054\n",
            "4      \t [-8.79367401 -8.68508685]. \t  -51075.444313405846 \t -0.14473002518929054\n",
            "5      \t [-6.8891595  -1.20388848]. \t  -253.8430259429188 \t -0.14473002518929054\n",
            "6      \t [  1.74505468 -11.71884734]. \t  -148968.7092896172 \t -0.14473002518929054\n",
            "7      \t [ 8.14153168 -2.29392053]. \t  -62.355146555882165 \t -0.14473002518929054\n",
            "8      \t [-13.127044    -0.63775431]. \t  -588.2487382065306 \t -0.14473002518929054\n",
            "9      \t [-3.20221523 -5.27215518]. \t  -6930.999472348657 \t -0.14473002518929054\n",
            "10     \t [-1.84291699  8.9668659 ]. \t  -52919.61385971303 \t -0.14473002518929054\n",
            "11     \t [ 4.10901338 -6.66589508]. \t  -14377.944182595398 \t -0.14473002518929054\n",
            "12     \t [-9.81401238  1.90982282]. \t  -702.3689615573313 \t -0.14473002518929054\n",
            "13     \t [ 5.23862478 -0.33235927]. \t  -68.32055478306292 \t -0.14473002518929054\n",
            "14     \t [-17.14618532  -9.36747823]. \t  -74553.84760149506 \t -0.14473002518929054\n",
            "15     \t [ -3.80217191 -10.64494137]. \t  -106220.62119938868 \t -0.14473002518929054\n",
            "16     \t [9.8423579  8.93336417]. \t  -44938.875045174995 \t -0.14473002518929054\n",
            "17     \t [9.98706881 0.45188222]. \t  -264.2693865262832 \t -0.14473002518929054\n",
            "18     \t [2.62293186 3.57953752]. \t  -1060.9325450801755 \t -0.14473002518929054\n",
            "19     \t [-4.82790206  1.44681468]. \t  -196.4849690295227 \t -0.14473002518929054\n",
            "20     \t [2.84013853 0.29764507]. \t  -17.568750935389215 \t -0.14473002518929054\n",
            "21     \t [-3.13659983 -0.8973509 ]. \t  -62.18094052921577 \t -0.14473002518929054\n",
            "22     \t [ 9.71329142 -4.8427496 ]. \t  -2842.285613658635 \t -0.14473002518929054\n",
            "23     \t [-12.8925848  -5.6573371]. \t  -12021.303929249883 \t -0.14473002518929054\n",
            "24     \t [1.55599861 0.3698541 ]. \t  -3.5983083914245286 \t -0.14473002518929054\n",
            "25     \t [ 4.45406992 -2.4730354 ]. \t  -132.91702384913037 \t -0.14473002518929054\n",
            "26     \t [1.29197572 0.60157165]. \t  -0.7309496076958966 \t -0.14473002518929054\n",
            "27     \t [1.55685076 0.70052217]. \t  -0.9722257568793669 \t -0.14473002518929054\n",
            "28     \t [-8.49169949 -3.47181353]. \t  -2215.4399590369885 \t -0.14473002518929054\n",
            "29     \t [-1.43298827 -1.38114447]. \t  -61.00471510119176 \t -0.14473002518929054\n",
            "30     \t [0.6911461  0.41591498]. \t  -0.333683065651094 \t -0.14473002518929054\n",
            "31     \t [-3.97942325  4.13866992]. \t  -2948.869911722159 \t -0.14473002518929054\n",
            "32     \t [6.75277291 1.05588498]. \t  -74.00921392624667 \t -0.14473002518929054\n",
            "33     \t [ 7.00076435 -0.89459341]. \t  -94.33283657327536 \t -0.14473002518929054\n",
            "34     \t [ 0.45730514 -5.35960526]. \t  -6496.797064097895 \t -0.14473002518929054\n",
            "35     \t [0.44948625 0.34367089]. \t  -0.39403092200703094 \t -0.14473002518929054\n",
            "36     \t [-7.08024673  2.05958194]. \t  -549.7667216159996 \t -0.14473002518929054\n",
            "37     \t [0.03776338 0.52974466]. \t  -1.473994249019223 \t -0.14473002518929054\n",
            "38     \t [4.41059298 1.23477726]. \t  -15.338110603825989 \t -0.14473002518929054\n",
            "39     \t [-4.04270974 -8.11068269]. \t  -36805.06171120061 \t -0.14473002518929054\n",
            "40     \t [-0.37664613  5.33264668]. \t  -6557.223340876488 \t -0.14473002518929054\n",
            "41     \t [0.259762   0.73613382]. \t  -1.9059834191978755 \t -0.14473002518929054\n",
            "42     \t [1.21505183 0.67809029]. \t  -0.22081563759203085 \t -0.14473002518929054\n",
            "43     \t [-0.5537127   0.08392089]. \t  -3.0588125779110795 \t -0.14473002518929054\n",
            "44     \t [-7.13427636  8.61588551]. \t  -48489.65974397891 \t -0.14473002518929054\n",
            "45     \t [9.99091957 3.82021444]. \t  -817.8983239354657 \t -0.14473002518929054\n",
            "46     \t [-4.41761244 -1.65350377]. \t  -224.80709949856373 \t -0.14473002518929054\n",
            "47     \t [0.22649024 0.50535806]. \t  -0.7599513350914169 \t -0.14473002518929054\n",
            "48     \t [-5.00223563  4.62773213]. \t  -4612.218532478034 \t -0.14473002518929054\n",
            "49     \t [-0.5993517  5.5202509]. \t  -7578.302277915097 \t -0.14473002518929054\n",
            "50     \t [5.50806031 6.92633429]. \t  -16379.165169186766 \t -0.14473002518929054\n",
            "51     \t [-2.21384766  0.20785693]. \t  -20.91117833670712 \t -0.14473002518929054\n",
            "52     \t [ 0.99725083 -0.43475213]. \t  -0.7669040832315646 \t -0.14473002518929054\n",
            "53     \t [-0.00500506  0.02017868]. \t  -1.0101028960365688 \t -0.14473002518929054\n",
            "54     \t [5.09791047 1.46819062]. \t  -18.03079954411408 \t -0.14473002518929054\n",
            "55     \t [ 6.40466661 -3.12495411]. \t  -373.79468694818684 \t -0.14473002518929054\n",
            "56     \t [ 8.70979338 -1.79778323]. \t  -69.52764849066111 \t -0.14473002518929054\n",
            "57     \t [-9.12550271 -8.71878829]. \t  -52047.64348572759 \t -0.14473002518929054\n",
            "58     \t [-7.09555014 -1.87065862]. \t  -462.83524980013703 \t -0.14473002518929054\n",
            "59     \t [ 0.75868219 -0.73814022]. \t  -0.2773824700882223 \t -0.14473002518929054\n",
            "60     \t [ 7.55197615 -4.15741169]. \t  -1502.675032654747 \t -0.14473002518929054\n",
            "61     \t [ 8.19411257 -1.87895633]. \t  -54.32335346758279 \t -0.14473002518929054\n",
            "62     \t [0.58488066 0.581116  ]. \t  -0.1887085283562825 \t -0.14473002518929054\n",
            "63     \t [-2.98420544 -5.26783978]. \t  -6856.742089050018 \t -0.14473002518929054\n",
            "64     \t [ 0.92926639 -0.42745446]. \t  -0.64081573713701 \t -0.14473002518929054\n",
            "65     \t [6.80905509 0.11733726]. \t  -125.72312142648643 \t -0.14473002518929054\n",
            "66     \t [4.83577875 1.65862836]. \t  -15.601156239351546 \t -0.14473002518929054\n",
            "67     \t [-2.01667832  4.6241505 ]. \t  -4019.993725988372 \t -0.14473002518929054\n",
            "68     \t [ 0.18942375 -0.7103234 ]. \t  -2.000833360687899 \t -0.14473002518929054\n",
            "69     \t [-6.24072439  0.690922  ]. \t  -155.97769002463605 \t -0.14473002518929054\n",
            "70     \t [-7.98074325 -9.01144947]. \t  -58148.32966529846 \t -0.14473002518929054\n",
            "71     \t [ 7.19505941 -2.2019923 ]. \t  -50.90358099568845 \t -0.14473002518929054\n",
            "72     \t [ 2.49053743 -3.98211312]. \t  -1710.2958083542276 \t -0.14473002518929054\n",
            "73     \t [-8.33248565  0.93367549]. \t  -290.14625559474524 \t -0.14473002518929054\n",
            "74     \t [ 7.03740317 -5.13546999]. \t  -4215.020903821437 \t -0.14473002518929054\n",
            "75     \t [-7.43781506 -0.14536251]. \t  -183.0997841788702 \t -0.14473002518929054\n",
            "76     \t [ 1.25417438 -8.4556558 ]. \t  -40181.68519036071 \t -0.14473002518929054\n",
            "77     \t [9.54647128 9.218395  ]. \t  -51536.510783866055 \t -0.14473002518929054\n",
            "78     \t [ 1.64567548 -0.79931802]. \t  -0.6875341970480675 \t -0.14473002518929054\n",
            "79     \t [-5.68758126  9.14632012]. \t  -59901.28760036906 \t -0.14473002518929054\n",
            "80     \t [-8.24874276 -6.05901984]. \t  -13426.231068029243 \t -0.14473002518929054\n",
            "81     \t [ 4.51812308 -2.96445056]. \t  -353.3900566599988 \t -0.14473002518929054\n",
            "82     \t [ 3.78276595 -0.85424074]. \t  -18.53933857539038 \t -0.14473002518929054\n",
            "83     \t [ 3.5932631  -1.31636857]. \t  -6.757582461633976 \t -0.14473002518929054\n",
            "84     \t [1.25943139 0.81670423]. \t  \u001b[92m-0.0784290628392909\u001b[0m \t -0.0784290628392909\n",
            "85     \t [ 4.36878359 -1.39793882]. \t  -11.772487649462366 \t -0.0784290628392909\n",
            "86     \t [2.56789062 4.34321879]. \t  -2474.798140262368 \t -0.0784290628392909\n",
            "87     \t [1.01842622 1.40811759]. \t  -17.37189189256229 \t -0.0784290628392909\n",
            "88     \t [ 1.57698761 -7.3710349 ]. \t  -22935.722571382918 \t -0.0784290628392909\n",
            "89     \t [2.22115608 2.44442886]. \t  -190.8101235879158 \t -0.0784290628392909\n",
            "90     \t [-7.14545072 -6.97458179]. \t  -21879.700625109097 \t -0.0784290628392909\n",
            "91     \t [ 3.03407927 -0.93957637]. \t  -7.355519630724508 \t -0.0784290628392909\n",
            "92     \t [ 8.64980484 -6.85356859]. \t  -14608.311187269232 \t -0.0784290628392909\n",
            "93     \t [4.34752965 2.30528011]. \t  -90.1104686486231 \t -0.0784290628392909\n",
            "94     \t [-5.34944394  6.9938714 ]. \t  -21331.681049501938 \t -0.0784290628392909\n",
            "95     \t [ 8.44278079 -7.19997955]. \t  -18195.42650662692 \t -0.0784290628392909\n",
            "96     \t [7.94956547 1.96494096]. \t  -48.40004505770033 \t -0.0784290628392909\n",
            "97     \t [ 9.17998145 -4.28924253]. \t  -1592.1130148597142 \t -0.0784290628392909\n",
            "98     \t [-8.86077216  7.52949317]. \t  -29986.042979117836 \t -0.0784290628392909\n",
            "99     \t [-3.64129716  4.08824078]. \t  -2769.7220190796297 \t -0.0784290628392909\n",
            "100    \t [1.16722373 0.74175764]. \t  \u001b[92m-0.036892250700436766\u001b[0m \t -0.036892250700436766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "2ebd7290-f6c2-4325-b1cd-fd4525512831"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_6 = d2GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.85720303 -3.36040389]. \t  -480.81536490368427 \t -43.91981950896418\n",
            "init   \t [ 6.42458246 -9.16606749]. \t  -52264.36633669738 \t -43.91981950896418\n",
            "init   \t [-7.8468664   1.90104128]. \t  -532.7651696660866 \t -43.91981950896418\n",
            "init   \t [ 0.59634724 -1.62385143]. \t  -43.91981950896418 \t -43.91981950896418\n",
            "init   \t [-3.29184301  2.45038864]. \t  -486.6398213674611 \t -43.91981950896418\n",
            "1      \t [2.90710192 9.80448543]. \t  -71709.54822485936 \t -43.91981950896418\n",
            "2      \t [-9.22684845 -9.49630667]. \t  -71990.69239039435 \t -43.91981950896418\n",
            "3      \t [-7.69566798  9.86414506]. \t  -81924.88656432132 \t -43.91981950896418\n",
            "4      \t [-14.49088274  -1.61342767]. \t  -1015.9252832120238 \t -43.91981950896418\n",
            "5      \t [-1.42589422 -8.37210292]. \t  -40112.73076912865 \t -43.91981950896418\n",
            "6      \t [6.91801255 2.83109097]. \t  -201.085049944819 \t -43.91981950896418\n",
            "7      \t [-19.59825205 -20.37813451]. \t  -1445883.2720486857 \t -43.91981950896418\n",
            "8      \t [9.25715772 8.0152434 ]. \t  -28500.28911684498 \t -43.91981950896418\n",
            "9      \t [-4.90896655 -3.1291349 ]. \t  -1234.6258372479238 \t -43.91981950896418\n",
            "10     \t [2.02914485 3.51885659]. \t  -1034.8703896618576 \t -43.91981950896418\n",
            "11     \t [-1.82649604  6.25302689]. \t  -12816.690262854694 \t -43.91981950896418\n",
            "12     \t [-53.93648943 -56.45967316]. \t  -82675480.45747563 \t -43.91981950896418\n",
            "13     \t [-9.69959496 -3.6432752 ]. \t  -2742.099343251739 \t -43.91981950896418\n",
            "14     \t [ 2.95077853 -4.59137484]. \t  -3078.7583923360176 \t -43.91981950896418\n",
            "15     \t [-9.73766146  5.06611173]. \t  -7574.054347195249 \t -43.91981950896418\n",
            "16     \t [ 4.07982375 -0.20500782]. \t  \u001b[92m-41.4176276003607\u001b[0m \t -41.4176276003607\n",
            "17     \t [9.96878176 0.33386517]. \t  -270.402225297586 \t -41.4176276003607\n",
            "18     \t [9.88907131 3.42383547]. \t  -446.5581873408777 \t -41.4176276003607\n",
            "19     \t [-0.3085534   0.37295929]. \t  \u001b[92m-2.400864691916189\u001b[0m \t -2.400864691916189\n",
            "20     \t [ 9.94339114 -5.83893736]. \t  -6864.475500501584 \t -2.400864691916189\n",
            "21     \t [-1.40789736 -3.87381463]. \t  -1980.3284163693127 \t -2.400864691916189\n",
            "22     \t [-5.52268268 -6.94329763]. \t  -20826.662449252213 \t -2.400864691916189\n",
            "23     \t [ 6.47365283 -0.10186634]. \t  -113.24069486906149 \t -2.400864691916189\n",
            "24     \t [-0.92096538  0.71057555]. \t  -11.146089933675487 \t -2.400864691916189\n",
            "25     \t [5.88507079 5.6322615 ]. \t  -6650.094762645206 \t -2.400864691916189\n",
            "26     \t [-5.61427526  0.08836312]. \t  -107.13999011846055 \t -2.400864691916189\n",
            "27     \t [-0.52729332 -0.33376025]. \t  -3.45788031331116 \t -2.400864691916189\n",
            "28     \t [-0.02112416 -0.0644817 ]. \t  \u001b[92m-1.0444279594933907\u001b[0m \t -1.0444279594933907\n",
            "29     \t [ 0.67886926 -0.48958899]. \t  \u001b[92m-0.18270511013132607\u001b[0m \t -0.18270511013132607\n",
            "30     \t [ 1.95165392 -0.92110338]. \t  -1.0354821205920615 \t -0.18270511013132607\n",
            "31     \t [2.22604554 0.04247199]. \t  -11.381647173006462 \t -0.18270511013132607\n",
            "32     \t [0.33569442 0.67160449]. \t  -1.0829442104911082 \t -0.18270511013132607\n",
            "33     \t [0.37921514 0.11178818]. \t  -0.6363202139709243 \t -0.18270511013132607\n",
            "34     \t [ 0.50455892 -0.37492265]. \t  -0.3452992605154044 \t -0.18270511013132607\n",
            "35     \t [-2.23936006 -0.75611641]. \t  -33.379910466131996 \t -0.18270511013132607\n",
            "36     \t [ 1.1367388  -0.27255539]. \t  -1.9716412782339923 \t -0.18270511013132607\n",
            "37     \t [ 1.49794773 -1.30191666]. \t  -7.407478668549623 \t -0.18270511013132607\n",
            "38     \t [-5.4178733  5.785831 ]. \t  -10515.893810317048 \t -0.18270511013132607\n",
            "39     \t [-0.44859786  1.71351337]. \t  -82.00482592882513 \t -0.18270511013132607\n",
            "40     \t [ 1.58969771 -0.77234865]. \t  -0.6624103433081502 \t -0.18270511013132607\n",
            "41     \t [-1.37569669 -0.11997504]. \t  -9.58908936230584 \t -0.18270511013132607\n",
            "42     \t [ 4.67892651 -1.93448782]. \t  -29.276831331822052 \t -0.18270511013132607\n",
            "43     \t [-2.06431944  2.74454778]. \t  -596.2230506718907 \t -0.18270511013132607\n",
            "44     \t [0.33505793 0.28785187]. \t  -0.4995003844371341 \t -0.18270511013132607\n",
            "45     \t [-0.3398488   1.92037519]. \t  -120.85401780007317 \t -0.18270511013132607\n",
            "46     \t [-9.70531014 -0.41807056]. \t  -316.8047327360364 \t -0.18270511013132607\n",
            "47     \t [5.79106982 3.37893875]. \t  -603.908252005958 \t -0.18270511013132607\n",
            "48     \t [-3.6744557   9.65116286]. \t  -72194.75521572115 \t -0.18270511013132607\n",
            "49     \t [ 3.36906149 -0.59688918]. \t  -19.726523072711384 \t -0.18270511013132607\n",
            "50     \t [-5.19297945 -4.5061829 ]. \t  -4234.429616036886 \t -0.18270511013132607\n",
            "51     \t [-3.79388167  7.61725185]. \t  -28745.721149029167 \t -0.18270511013132607\n",
            "52     \t [-4.54112471 -3.23566852]. \t  -1329.1879235966794 \t -0.18270511013132607\n",
            "53     \t [-5.9226666  -1.74626943]. \t  -336.9601059669967 \t -0.18270511013132607\n",
            "54     \t [ 8.02164712 -2.67265804]. \t  -127.7928233851124 \t -0.18270511013132607\n",
            "55     \t [ 6.92146635 -2.07353916]. \t  -40.69286965924218 \t -0.18270511013132607\n",
            "56     \t [ 9.24987096 -1.60625186]. \t  -101.51298654315019 \t -0.18270511013132607\n",
            "57     \t [ 3.2188219  -1.39082617]. \t  -5.768100386170028 \t -0.18270511013132607\n",
            "58     \t [4.55449115 1.57638214]. \t  -12.979638220860418 \t -0.18270511013132607\n",
            "59     \t [ 1.48185466 -0.86420782]. \t  -0.23246502420954696 \t -0.18270511013132607\n",
            "60     \t [3.04362669 0.72259344]. \t  -12.171163950885502 \t -0.18270511013132607\n",
            "61     \t [ 7.48668909 -2.11199074]. \t  -46.19168681964074 \t -0.18270511013132607\n",
            "62     \t [ 2.37201578 -1.0664734 ]. \t  -1.9013559416280712 \t -0.18270511013132607\n",
            "63     \t [ 6.00309693 -2.05836679]. \t  -37.239208909389816 \t -0.18270511013132607\n",
            "64     \t [0.02035451 0.68780627]. \t  -2.673918048711197 \t -0.18270511013132607\n",
            "65     \t [-9.37022401 -5.04390326]. \t  -7368.183318605593 \t -0.18270511013132607\n",
            "66     \t [ 8.08797761 -1.96182666]. \t  -50.5443289507511 \t -0.18270511013132607\n",
            "67     \t [2.37092546 2.83696544]. \t  -378.6757268110483 \t -0.18270511013132607\n",
            "68     \t [3.4968339  1.94921555]. \t  -39.88778567959616 \t -0.18270511013132607\n",
            "69     \t [3.09460965 1.09727333]. \t  -5.330207074463902 \t -0.18270511013132607\n",
            "70     \t [2.69417797 1.2836781 ]. \t  -3.5937977274026682 \t -0.18270511013132607\n",
            "71     \t [3.62721474 1.02349319]. \t  -11.597151798153263 \t -0.18270511013132607\n",
            "72     \t [4.80084814 1.37714294]. \t  -16.477779581300187 \t -0.18270511013132607\n",
            "73     \t [ 1.31820967 -0.04303052]. \t  -3.5571116527695414 \t -0.18270511013132607\n",
            "74     \t [ 1.05122142 -0.74109144]. \t  \u001b[92m-0.00708151029528118\u001b[0m \t -0.00708151029528118\n",
            "75     \t [5.15337841 1.83999554]. \t  -22.485033051864832 \t -0.00708151029528118\n",
            "76     \t [7.78650154 3.89916615]. \t  -1069.4298990570135 \t -0.00708151029528118\n",
            "77     \t [4.66219434 9.80236653]. \t  -70333.85057301067 \t -0.00708151029528118\n",
            "78     \t [7.56543594 1.86411422]. \t  -43.86285685114696 \t -0.00708151029528118\n",
            "79     \t [-2.66403716 -9.86890604]. \t  -77990.1026981319 \t -0.00708151029528118\n",
            "80     \t [6.61863326 1.90365174]. \t  -32.36069057782426 \t -0.00708151029528118\n",
            "81     \t [5.70377261 4.76196462]. \t  -3166.186738470946 \t -0.00708151029528118\n",
            "82     \t [1.9622965  1.62543049]. \t  -22.99408782945656 \t -0.00708151029528118\n",
            "83     \t [ 3.7895797  -1.06406981]. \t  -12.433557590511274 \t -0.00708151029528118\n",
            "84     \t [-8.6698848  -1.14240882]. \t  -347.9871088865486 \t -0.00708151029528118\n",
            "85     \t [-1.97214442 -6.10811556]. \t  -11740.98045602341 \t -0.00708151029528118\n",
            "86     \t [7.2842428  5.64106982]. \t  -6392.186790655198 \t -0.00708151029528118\n",
            "87     \t [ 2.5136994  -9.24678397]. \t  -56781.620496337455 \t -0.00708151029528118\n",
            "88     \t [ 1.11231044 -0.80776528]. \t  -0.08684866822332754 \t -0.00708151029528118\n",
            "89     \t [2.65789245 1.5900288 ]. \t  -14.25412310740024 \t -0.00708151029528118\n",
            "90     \t [2.01909092 1.19344839]. \t  -2.4148434104055925 \t -0.00708151029528118\n",
            "91     \t [-8.21078768 -9.43408712]. \t  -69436.73252782355 \t -0.00708151029528118\n",
            "92     \t [0.86294258 0.43450981]. \t  -0.4899043306348182 \t -0.00708151029528118\n",
            "93     \t [-3.60856824  0.66276031]. \t  -61.50650783442721 \t -0.00708151029528118\n",
            "94     \t [-0.05444533  5.13783057]. \t  -5587.156302415063 \t -0.00708151029528118\n",
            "95     \t [6.57010489 8.77670648]. \t  -43538.24471272127 \t -0.00708151029528118\n",
            "96     \t [-8.81909582 -6.38636263]. \t  -16437.245746249155 \t -0.00708151029528118\n",
            "97     \t [-7.38079581 -5.17206868]. \t  -7483.323761270523 \t -0.00708151029528118\n",
            "98     \t [-5.93568273 -5.75394603]. \t  -10459.773688473073 \t -0.00708151029528118\n",
            "99     \t [-3.06810703  6.60137068]. \t  -16297.396964435744 \t -0.00708151029528118\n",
            "100    \t [-7.12382562 -8.06620731]. \t  -37741.79479062148 \t -0.00708151029528118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "4fbf7941-ba4c-4337-f385-2d226cba809e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_7 = d2GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.9467253  -3.95286781]. \t  -2981.1801714787616 \t -297.17705346549894\n",
            "init   \t [-8.75927171 -0.80279317]. \t  -297.17705346549894 \t -297.17705346549894\n",
            "init   \t [6.70506769 8.53994095]. \t  -38761.39667089972 \t -297.17705346549894\n",
            "init   \t [4.53977969 5.36992444]. \t  -5658.63331787327 \t -297.17705346549894\n",
            "init   \t [-4.61589867  2.88058584]. \t  -931.3893916140455 \t -297.17705346549894\n",
            "1      \t [ 2.44677649 -4.93762112]. \t  -4291.960403637314 \t -297.17705346549894\n",
            "2      \t [-16.04657854  -7.48838337]. \t  -33160.21944419802 \t -297.17705346549894\n",
            "3      \t [ 9.83790456 -9.57130198]. \t  -60200.563023412855 \t -297.17705346549894\n",
            "4      \t [-7.89992531  9.8439324 ]. \t  -81449.7775076618 \t -297.17705346549894\n",
            "5      \t [ 9.0936247  -0.78669084]. \t  \u001b[92m-188.93582542124247\u001b[0m \t -188.93582542124247\n",
            "6      \t [-28.95405166 -20.20256856]. \t  -1429763.9842124362 \t -188.93582542124247\n",
            "7      \t [ -0.25902569 -11.33544159]. \t  -132350.01950972754 \t -188.93582542124247\n",
            "8      \t [ -7.37402228 -13.29224224]. \t  -260338.88237096596 \t -188.93582542124247\n",
            "9      \t [0.42210597 0.91487768]. \t  \u001b[92m-3.4684505922616853\u001b[0m \t -3.4684505922616853\n",
            "10     \t [-1.254296   8.6667351]. \t  -45896.790383750355 \t -3.4684505922616853\n",
            "11     \t [-17.41873762 -15.57915233]. \t  -506032.7907630059 \t -3.4684505922616853\n",
            "12     \t [-1.90916005 -2.01863838]. \t  -210.82864095331672 \t -3.4684505922616853\n",
            "13     \t [-9.80709078  3.90771135]. \t  -3372.6358064271258 \t -3.4684505922616853\n",
            "14     \t [ 4.7633995  -0.28747927]. \t  -56.44842240043229 \t -3.4684505922616853\n",
            "15     \t [-10.32476231  -8.04503864]. \t  -39199.58715678419 \t -3.4684505922616853\n",
            "16     \t [-2.27051388 -6.22187753]. \t  -12712.97430038411 \t -3.4684505922616853\n",
            "17     \t [ 6.98259028 -4.18475787]. \t  -1608.4772167805138 \t -3.4684505922616853\n",
            "18     \t [9.2308028  3.85823094]. \t  -911.6187989663807 \t -3.4684505922616853\n",
            "19     \t [  4.88996125 -10.50734508]. \t  -93256.85000031456 \t -3.4684505922616853\n",
            "20     \t [-0.77337085  1.7188456 ]. \t  -92.44927319836273 \t -3.4684505922616853\n",
            "21     \t [1.17449585 1.72972581]. \t  -46.29123898808672 \t -3.4684505922616853\n",
            "22     \t [-0.28270835  0.18519308]. \t  \u001b[92m-1.8921659111897258\u001b[0m \t -1.8921659111897258\n",
            "23     \t [0.50977244 0.19575997]. \t  \u001b[92m-0.6155236750725124\u001b[0m \t -0.6155236750725124\n",
            "24     \t [0.85400843 4.63704081]. \t  -3553.3146330598224 \t -0.6155236750725124\n",
            "25     \t [2.82119489 1.02867157]. \t  -4.310418725611653 \t -0.6155236750725124\n",
            "26     \t [-6.11819398 -8.83966819]. \t  -52796.669499556876 \t -0.6155236750725124\n",
            "27     \t [2.08254843 0.19597094]. \t  -9.217890656245293 \t -0.6155236750725124\n",
            "28     \t [1.68721746 0.87680454]. \t  \u001b[92m-0.5170551277107062\u001b[0m \t -0.5170551277107062\n",
            "29     \t [5.56335407 1.57810364]. \t  -21.502887077306394 \t -0.5170551277107062\n",
            "30     \t [-5.56266665 -0.91724788]. \t  -148.0589022561295 \t -0.5170551277107062\n",
            "31     \t [ 1.81277462 -1.25244666]. \t  -4.1690475702031025 \t -0.5170551277107062\n",
            "32     \t [0.95496895 0.80786529]. \t  \u001b[92m-0.24748118177909306\u001b[0m \t -0.24748118177909306\n",
            "33     \t [2.19819211 1.00044477]. \t  -1.5128201715471539 \t -0.24748118177909306\n",
            "34     \t [ 2.39509947 -1.37738335]. \t  -5.8622174390034285 \t -0.24748118177909306\n",
            "35     \t [3.57759234 1.45400274]. \t  -7.49068759718551 \t -0.24748118177909306\n",
            "36     \t [6.85960443 0.39748222]. \t  -119.97289465896506 \t -0.24748118177909306\n",
            "37     \t [-5.02562184  6.32598603]. \t  -14507.334718664019 \t -0.24748118177909306\n",
            "38     \t [ 0.32522402 -0.20048571]. \t  -0.5752109587661227 \t -0.24748118177909306\n",
            "39     \t [ 1.09714402 -0.98277209]. \t  -1.4023440549030388 \t -0.24748118177909306\n",
            "40     \t [2.27386429 1.33669435]. \t  -5.000854887381533 \t -0.24748118177909306\n",
            "41     \t [ 0.3420016  -1.82713251]. \t  -80.69297238271565 \t -0.24748118177909306\n",
            "42     \t [ 1.25759336 -0.79583816]. \t  \u001b[92m-0.06652081148694637\u001b[0m \t -0.06652081148694637\n",
            "43     \t [ 3.0514052  -1.40033879]. \t  -5.723776894473894 \t -0.06652081148694637\n",
            "44     \t [ 2.55097521 -0.7044754 ]. \t  -7.262770347372372 \t -0.06652081148694637\n",
            "45     \t [ 1.05224331 -0.49900088]. \t  -0.6170923306322798 \t -0.06652081148694637\n",
            "46     \t [4.33353588 1.53290015]. \t  -11.380417121805811 \t -0.06652081148694637\n",
            "47     \t [ 4.09436519 -1.66270756]. \t  -13.692556912772787 \t -0.06652081148694637\n",
            "48     \t [ 0.54012536 -0.22009734]. \t  -0.6044075147402664 \t -0.06652081148694637\n",
            "49     \t [ 9.94343064 -3.91729509]. \t  -940.8585646067031 \t -0.06652081148694637\n",
            "50     \t [-1.68077183  6.07669278]. \t  -11417.703688746778 \t -0.06652081148694637\n",
            "51     \t [0.09078931 0.49741316]. \t  -1.1531775245016371 \t -0.06652081148694637\n",
            "52     \t [ 0.70468157 -0.497099  ]. \t  -0.1758054681011272 \t -0.06652081148694637\n",
            "53     \t [-2.04184856  0.11436612]. \t  -17.806154984796667 \t -0.06652081148694637\n",
            "54     \t [ 1.21790954 -0.79675347]. \t  \u001b[92m-0.052835028882927494\u001b[0m \t -0.052835028882927494\n",
            "55     \t [-7.12713076  0.98027303]. \t  -229.81909990190945 \t -0.052835028882927494\n",
            "56     \t [ 4.72560139 -3.81075416]. \t  -1196.619410042751 \t -0.052835028882927494\n",
            "57     \t [ 3.35692785 -1.09024844]. \t  -7.4745157423161075 \t -0.052835028882927494\n",
            "58     \t [ 3.71157744 -1.16664213]. \t  -9.310752949004538 \t -0.052835028882927494\n",
            "59     \t [0.07662458 0.33608454]. \t  -0.897191825365439 \t -0.052835028882927494\n",
            "60     \t [-2.37426724  7.82643393]. \t  -31201.6159004122 \t -0.052835028882927494\n",
            "61     \t [2.52161427 9.518892  ]. \t  -63867.547790540775 \t -0.052835028882927494\n",
            "62     \t [9.45672564 4.78440042]. \t  -2710.4183504109697 \t -0.052835028882927494\n",
            "63     \t [ 9.65168305 -3.44245725]. \t  -469.61957102685653 \t -0.052835028882927494\n",
            "64     \t [8.95999671 2.72421107]. \t  -132.57281263578255 \t -0.052835028882927494\n",
            "65     \t [ 9.65399849 -2.64245281]. \t  -112.06311789377975 \t -0.052835028882927494\n",
            "66     \t [ 4.72524061 -1.4670669 ]. \t  -14.231344181027087 \t -0.052835028882927494\n",
            "67     \t [-9.98911439 -3.7319014 ]. \t  -2984.9899047190547 \t -0.052835028882927494\n",
            "68     \t [-1.73689293 -2.46917106]. \t  -395.6084880472298 \t -0.052835028882927494\n",
            "69     \t [-3.79301943 -0.67752154]. \t  -67.36177895576463 \t -0.052835028882927494\n",
            "70     \t [-2.92670956 -0.51002889]. \t  -39.18223157807998 \t -0.052835028882927494\n",
            "71     \t [7.5867087  0.21594955]. \t  -155.68802635951303 \t -0.052835028882927494\n",
            "72     \t [-3.6391784  -1.64017452]. \t  -184.22557669005312 \t -0.052835028882927494\n",
            "73     \t [-4.54631698  7.84714993]. \t  -32646.281973896577 \t -0.052835028882927494\n",
            "74     \t [ 0.87445829 -0.61735948]. \t  \u001b[92m-0.04093518822293769\u001b[0m \t -0.04093518822293769\n",
            "75     \t [-1.29114681 -0.66673266]. \t  -14.755999619255512 \t -0.04093518822293769\n",
            "76     \t [0.71879392 0.64516382]. \t  -0.10492259551610922 \t -0.04093518822293769\n",
            "77     \t [1.28463349 0.65014381]. \t  -0.4669141093128888 \t -0.04093518822293769\n",
            "78     \t [1.02795401 0.69871993]. \t  \u001b[92m-0.0060931225801696826\u001b[0m \t -0.0060931225801696826\n",
            "79     \t [5.61940086 1.89146486]. \t  -26.05670548141057 \t -0.0060931225801696826\n",
            "80     \t [ 5.13145893 -6.98450545]. \t  -17105.591663517112 \t -0.0060931225801696826\n",
            "81     \t [-3.9326075   0.09442642]. \t  -55.542572245751245 \t -0.0060931225801696826\n",
            "82     \t [-6.86179769 -6.00034947]. \t  -12502.820114758297 \t -0.0060931225801696826\n",
            "83     \t [9.01949008 0.36589373]. \t  -217.49790444167454 \t -0.0060931225801696826\n",
            "84     \t [-1.87594435 -0.71639782]. \t  -25.118861073358822 \t -0.0060931225801696826\n",
            "85     \t [4.32363851 5.40727383]. \t  -5876.267471595092 \t -0.0060931225801696826\n",
            "86     \t [-5.64153068 -1.58224571]. \t  -270.8924671331364 \t -0.0060931225801696826\n",
            "87     \t [-2.10379631 -5.07196877]. \t  -5745.594840224066 \t -0.0060931225801696826\n",
            "88     \t [7.14575166 3.60301174]. \t  -745.9770755526986 \t -0.0060931225801696826\n",
            "89     \t [-2.20172319  0.51132802]. \t  -25.09831279186061 \t -0.0060931225801696826\n",
            "90     \t [4.07593377 1.83815865]. \t  -23.844619983493963 \t -0.0060931225801696826\n",
            "91     \t [ 8.99863834 -2.64479191]. \t  -113.80257243977942 \t -0.0060931225801696826\n",
            "92     \t [ 0.71210475 -0.51965567]. \t  -0.14206593561364175 \t -0.0060931225801696826\n",
            "93     \t [3.60505136 0.91041019]. \t  -14.370698529723422 \t -0.0060931225801696826\n",
            "94     \t [ 0.75676517 -4.31883246]. \t  -2671.551655887892 \t -0.0060931225801696826\n",
            "95     \t [-2.55040769 -1.69504259]. \t  -150.27739847390453 \t -0.0060931225801696826\n",
            "96     \t [-5.90528167 -5.65835373]. \t  -9830.670664961111 \t -0.0060931225801696826\n",
            "97     \t [ 1.37175544 -1.3771187 ]. \t  -11.862198666920797 \t -0.0060931225801696826\n",
            "98     \t [ 5.61987587 -7.25406049]. \t  -19870.792210242922 \t -0.0060931225801696826\n",
            "99     \t [-6.33179118  5.3289402 ]. \t  -8023.791023469346 \t -0.0060931225801696826\n",
            "100    \t [-2.64019465  8.15832871]. \t  -36873.09521062235 \t -0.0060931225801696826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "2f6e8252-584c-4d4e-c41e-84d345187550"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_8 = d2GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-7.8100025  -9.15569971]. \t  -61652.61989589963 \t -123.0699213709516\n",
            "init   \t [ 1.9938517 -4.0025859]. \t  -1806.696586806874 \t -123.0699213709516\n",
            "init   \t [ 5.93223564 -2.6760775 ]. \t  -165.12946982275756 \t -123.0699213709516\n",
            "init   \t [ 7.61216688 -4.04316733]. \t  -1301.9580077172761 \t -123.0699213709516\n",
            "init   \t [-3.41793472  1.37433832]. \t  -123.0699213709516 \t -123.0699213709516\n",
            "1      \t [-98.81463622 -92.74238158]. \t  -598667007.0549785 \t -123.0699213709516\n",
            "2      \t [7.33725011 6.34690223]. \t  -10765.158736519235 \t -123.0699213709516\n",
            "3      \t [-6.99148152  9.31430956]. \t  -65227.40102136099 \t -123.0699213709516\n",
            "4      \t [0.47559161 8.55656073]. \t  -42605.34024544054 \t -123.0699213709516\n",
            "5      \t [  6.32829992 -10.63150345]. \t  -96590.43670836445 \t -123.0699213709516\n",
            "6      \t [-9.721135    0.63451534]. \t  -336.55101039217925 \t -123.0699213709516\n",
            "7      \t [-0.87928697 -9.68256642]. \t  -70980.19675164264 \t -123.0699213709516\n",
            "8      \t [-15.0664549   -9.84504932]. \t  -87550.29468968815 \t -123.0699213709516\n",
            "9      \t [1.71839717 2.01235802]. \t  \u001b[92m-81.94460722993999\u001b[0m \t -81.94460722993999\n",
            "10     \t [-3.88378169 -3.83766151]. \t  -2246.838956820363 \t -81.94460722993999\n",
            "11     \t [-273.81968155 -270.35190156]. \t  -42897694660.24339 \t -81.94460722993999\n",
            "12     \t [-11.8145267   -4.35727399]. \t  -5121.543518617534 \t -81.94460722993999\n",
            "13     \t [9.31972392 1.10093927]. \t  -164.31611155750096 \t -81.94460722993999\n",
            "14     \t [-10.93490529   5.15056324]. \t  -8332.268338055514 \t -81.94460722993999\n",
            "15     \t [-5.93291056  4.05033388]. \t  -3050.1548899646777 \t -81.94460722993999\n",
            "16     \t [5.32767498 1.63956256]. \t  \u001b[92m-18.733505529849037\u001b[0m \t -18.733505529849037\n",
            "17     \t [-6.06268961 -0.46779705]. \t  -134.39088683243835 \t -18.733505529849037\n",
            "18     \t [-0.59616993 -0.55884963]. \t  \u001b[92m-5.528442910625369\u001b[0m \t -5.528442910625369\n",
            "19     \t [-1.88533287  4.0016384 ]. \t  -2308.3119424538177 \t -5.528442910625369\n",
            "20     \t [ 3.16848501 -0.57093923]. \t  -17.36829254745442 \t -5.528442910625369\n",
            "21     \t [5.28988091 9.87566309]. \t  -72041.8584116926 \t -5.528442910625369\n",
            "22     \t [-0.86780242 -1.67731768]. \t  -87.84812617121251 \t -5.528442910625369\n",
            "23     \t [-0.7618266   0.62863647]. \t  -7.922646799711739 \t -5.528442910625369\n",
            "24     \t [3.76807405 3.71911116]. \t  -1149.652233457039 \t -5.528442910625369\n",
            "25     \t [9.96127562 9.88691337]. \t  -68931.10822656454 \t -5.528442910625369\n",
            "26     \t [-0.4692321   0.01873558]. \t  \u001b[92m-2.600319184299668\u001b[0m \t -2.600319184299668\n",
            "27     \t [5.82963287 0.94926854]. \t  -55.765438025321686 \t -2.600319184299668\n",
            "28     \t [-1.31380838 -0.22895807]. \t  -9.378856105964044 \t -2.600319184299668\n",
            "29     \t [-0.29097675 -0.14014602]. \t  \u001b[92m-1.884762398950114\u001b[0m \t -1.884762398950114\n",
            "30     \t [-0.58515343 -0.39823144]. \t  -4.141110238649375 \t -1.884762398950114\n",
            "31     \t [-8.19914041 -2.52280994]. \t  -960.6100235096055 \t -1.884762398950114\n",
            "32     \t [ 3.89522534 -0.66467605]. \t  -26.52224262288524 \t -1.884762398950114\n",
            "33     \t [ 9.50806253 -7.22407849]. \t  -18071.71465657068 \t -1.884762398950114\n",
            "34     \t [ 3.45858874 -7.09050671]. \t  -18859.750598967257 \t -1.884762398950114\n",
            "35     \t [9.8799349  3.53873474]. \t  -538.8290587321862 \t -1.884762398950114\n",
            "36     \t [0.25150889 0.48670672]. \t  \u001b[92m-0.6590361533667979\u001b[0m \t -0.6590361533667979\n",
            "37     \t [0.14501329 1.18393584]. \t  -14.865128493631627 \t -0.6590361533667979\n",
            "38     \t [ 9.58787667 -1.49820867]. \t  -125.74344140498607 \t -0.6590361533667979\n",
            "39     \t [ 1.63582599 -0.160976  ]. \t  -5.422383185662185 \t -0.6590361533667979\n",
            "40     \t [7.11397195 2.16611578]. \t  -47.687753371531926 \t -0.6590361533667979\n",
            "41     \t [ 0.29386152 -0.66716229]. \t  -1.2098970272340082 \t -0.6590361533667979\n",
            "42     \t [-0.78605697 -4.58063386]. \t  -3658.395380638124 \t -0.6590361533667979\n",
            "43     \t [ 0.93529965 -0.25051774]. \t  -1.3156778895830032 \t -0.6590361533667979\n",
            "44     \t [0.14288923 0.32305559]. \t  -0.7433088496091277 \t -0.6590361533667979\n",
            "45     \t [0.19549425 0.15967268]. \t  -0.6889920372295295 \t -0.6590361533667979\n",
            "46     \t [3.01524324 0.56180564]. \t  -15.428041943388571 \t -0.6590361533667979\n",
            "47     \t [ 0.33371736 -0.56558523]. \t  \u001b[92m-0.631273038861805\u001b[0m \t -0.631273038861805\n",
            "48     \t [4.60578657 0.88861974]. \t  -31.321058547676735 \t -0.631273038861805\n",
            "49     \t [1.23873191 0.05738459]. \t  -3.093360097824159 \t -0.631273038861805\n",
            "50     \t [-3.23428689 -0.57425663]. \t  -48.25298214641066 \t -0.631273038861805\n",
            "51     \t [5.06834609 5.96825386]. \t  -8773.955635900966 \t -0.631273038861805\n",
            "52     \t [-2.7579393  -5.49723944]. \t  -8001.8998847583525 \t -0.631273038861805\n",
            "53     \t [-3.2883097   6.75676595]. \t  -17915.227050376325 \t -0.631273038861805\n",
            "54     \t [0.56668251 0.2643545 ]. \t  \u001b[92m-0.5522784182582866\u001b[0m \t -0.5522784182582866\n",
            "55     \t [ 6.13223941 -8.32988771]. \t  -35214.04422492559 \t -0.5522784182582866\n",
            "56     \t [-6.13893734 -1.59182807]. \t  -302.14783823643666 \t -0.5522784182582866\n",
            "57     \t [ 0.46456789 -0.3258581 ]. \t  \u001b[92m-0.41389811919616726\u001b[0m \t -0.41389811919616726\n",
            "58     \t [0.39177332 0.47859436]. \t  \u001b[92m-0.37873951816249485\u001b[0m \t -0.37873951816249485\n",
            "59     \t [ 7.37959442 -0.75197014]. \t  -118.79113438831111 \t -0.37873951816249485\n",
            "60     \t [-5.26021872  0.57031597]. \t  -109.06401744077854 \t -0.37873951816249485\n",
            "61     \t [-1.31521669 -1.20963095]. \t  -41.343091088664025 \t -0.37873951816249485\n",
            "62     \t [-7.01656824  4.39875358]. \t  -4243.920950143667 \t -0.37873951816249485\n",
            "63     \t [-6.95847747 -5.23601194]. \t  -7699.375094852092 \t -0.37873951816249485\n",
            "64     \t [ 0.55010921 -0.52669059]. \t  \u001b[92m-0.20244584003205648\u001b[0m \t -0.20244584003205648\n",
            "65     \t [9.06425658 7.71589085]. \t  -24267.53443867601 \t -0.20244584003205648\n",
            "66     \t [ 4.50698232 -1.72569522]. \t  -16.498507588212938 \t -0.20244584003205648\n",
            "67     \t [-6.69536845  2.63555047]. \t  -906.9189730934511 \t -0.20244584003205648\n",
            "68     \t [-5.39998576 -6.84540931]. \t  -19690.208465023108 \t -0.20244584003205648\n",
            "69     \t [-6.5677788   0.64094374]. \t  -166.47763911216762 \t -0.20244584003205648\n",
            "70     \t [ 1.0094345  -0.76651103]. \t  \u001b[92m-0.05496475915320519\u001b[0m \t -0.05496475915320519\n",
            "71     \t [ 9.42253238 -0.3995322 ]. \t  -236.67848034395837 \t -0.05496475915320519\n",
            "72     \t [ 1.08284567 -0.76123167]. \t  \u001b[92m-0.018446319864058494\u001b[0m \t -0.018446319864058494\n",
            "73     \t [ 2.78685118 -0.15378793]. \t  -18.203102605247672 \t -0.018446319864058494\n",
            "74     \t [-8.84729879 -5.24014396]. \t  -8229.051424410012 \t -0.018446319864058494\n",
            "75     \t [-4.09492747 -2.35923518]. \t  -489.6757853579257 \t -0.018446319864058494\n",
            "76     \t [ 2.04095505 -0.91198357]. \t  -1.3686406872156445 \t -0.018446319864058494\n",
            "77     \t [-5.91462627e+00 -2.10471570e-03]. \t  -117.77787374168585 \t -0.018446319864058494\n",
            "78     \t [ 9.87354977 -8.45660868]. \t  -35539.20050149259 \t -0.018446319864058494\n",
            "79     \t [ 1.4836496  -0.83448976]. \t  -0.250443754783901 \t -0.018446319864058494\n",
            "80     \t [1.67849088 7.00937803]. \t  -18657.502736448616 \t -0.018446319864058494\n",
            "81     \t [ 6.164922   -2.46170931]. \t  -97.60293384974274 \t -0.018446319864058494\n",
            "82     \t [-5.28630706  7.33387952]. \t  -25513.321947840155 \t -0.018446319864058494\n",
            "83     \t [ 7.09723595 -3.04258965]. \t  -297.89342045477264 \t -0.018446319864058494\n",
            "84     \t [ 7.18884261 -1.81663248]. \t  -38.99452072330875 \t -0.018446319864058494\n",
            "85     \t [-1.53502454 -5.06789235]. \t  -5603.688678125964 \t -0.018446319864058494\n",
            "86     \t [ 0.93232909 -0.2382274 ]. \t  -1.3455264914714893 \t -0.018446319864058494\n",
            "87     \t [1.27632162 2.44985546]. \t  -230.22465187294836 \t -0.018446319864058494\n",
            "88     \t [1.96352415 0.98137928]. \t  -0.9311633886251407 \t -0.018446319864058494\n",
            "89     \t [ 1.52017907 -1.78452586]. \t  -47.29397676328524 \t -0.018446319864058494\n",
            "90     \t [ 0.66501211 -3.19880746]. \t  -784.1705345699172 \t -0.018446319864058494\n",
            "91     \t [-6.54058379  8.44174   ]. \t  -44498.522389758466 \t -0.018446319864058494\n",
            "92     \t [ 6.84795183 -2.08385802]. \t  -40.947507437165875 \t -0.018446319864058494\n",
            "93     \t [ 2.97050752 -1.4050746 ]. \t  -5.795718183824131 \t -0.018446319864058494\n",
            "94     \t [3.22035406 7.00700377]. \t  -18045.75399055018 \t -0.018446319864058494\n",
            "95     \t [-4.18805224  5.0527217 ]. \t  -6131.608945360351 \t -0.018446319864058494\n",
            "96     \t [-9.842156   -3.27093426]. \t  -2069.4497773451208 \t -0.018446319864058494\n",
            "97     \t [ 9.86423471 -7.70892022]. \t  -23836.50570445349 \t -0.018446319864058494\n",
            "98     \t [ 2.08086811 -1.21197818]. \t  -2.636879467086902 \t -0.018446319864058494\n",
            "99     \t [-0.2734736  -5.50592384]. \t  -7420.184072100605 \t -0.018446319864058494\n",
            "100    \t [-9.9626663  -1.12497805]. \t  -432.37100613122556 \t -0.018446319864058494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "1ecdd52b-3849-49fa-9454-417e49b1ce63"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_9 = d2GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.44557117 -0.23843202]. \t  -28.18355402366269 \t -28.18355402366269\n",
            "init   \t [ 6.50990348 -9.37107225]. \t  -57236.27240949214 \t -28.18355402366269\n",
            "init   \t [6.16099927 1.31234839]. \t  -41.39446954533859 \t -28.18355402366269\n",
            "init   \t [-4.04755003 -9.06608559]. \t  -56766.42271365435 \t -28.18355402366269\n",
            "init   \t [ 9.81254799 -9.86348534]. \t  -68353.23444364112 \t -28.18355402366269\n",
            "1      \t [-8.12773813  6.26616826]. \t  -15102.348898507695 \t -28.18355402366269\n",
            "2      \t [3.58702664 9.74831565]. \t  -69550.52203005194 \t -28.18355402366269\n",
            "3      \t [-9.43477705 -1.75785015]. \t  -596.5317353689182 \t -28.18355402366269\n",
            "4      \t [-10.92255157 -14.64605117]. \t  -387229.8348439033 \t -28.18355402366269\n",
            "5      \t [-25.12605661 -28.9641451 ]. \t  -5800892.5280988915 \t -28.18355402366269\n",
            "6      \t [-2.43872117 -1.89096121]. \t  -195.76828369601628 \t -28.18355402366269\n",
            "7      \t [-19.88461718 -22.42353782]. \t  -2103797.024655435 \t -28.18355402366269\n",
            "8      \t [-1.40064521  4.59220821]. \t  -3803.742289943509 \t -28.18355402366269\n",
            "9      \t [-12.6090683   -7.46854011]. \t  -31020.21927129518 \t -28.18355402366269\n",
            "10     \t [ 9.75128696 -3.02170619]. \t  -221.4296353439725 \t -28.18355402366269\n",
            "11     \t [ 1.72952825 -5.08127741]. \t  -4982.395090000365 \t -28.18355402366269\n",
            "12     \t [9.42812131 5.7332353 ]. \t  -6413.082723731119 \t -28.18355402366269\n",
            "13     \t [-3.07662798  9.01497684]. \t  -54874.09859199814 \t -28.18355402366269\n",
            "14     \t [-5.76761681  1.41660455]. \t  -237.14257436249596 \t -28.18355402366269\n",
            "15     \t [ 6.09645188 -2.90109981]. \t  -256.5104572450423 \t -28.18355402366269\n",
            "16     \t [3.50553214 4.1686209 ]. \t  -1959.311226556791 \t -28.18355402366269\n",
            "17     \t [  1.0756976  -10.65697613]. \t  -102212.20551325676 \t -28.18355402366269\n",
            "18     \t [9.31253201 0.86788036]. \t  -190.96856354894862 \t -28.18355402366269\n",
            "19     \t [-5.79459431 -4.05059995]. \t  -3027.524798278105 \t -28.18355402366269\n",
            "20     \t [ 0.33480144 -0.28157673]. \t  \u001b[92m-0.5046035264236911\u001b[0m \t -0.5046035264236911\n",
            "21     \t [1.5959279  0.65238208]. \t  -1.4643551669147068 \t -0.5046035264236911\n",
            "22     \t [ 0.84744213 -0.11952521]. \t  -1.3643686930170043 \t -0.5046035264236911\n",
            "23     \t [0.83672598 0.72697099]. \t  \u001b[92m-0.12367647103295268\u001b[0m \t -0.12367647103295268\n",
            "24     \t [0.92549943 0.88121676]. \t  -0.7932800473282012 \t -0.12367647103295268\n",
            "25     \t [-8.44816764 -7.43875505]. \t  -28467.61674079713 \t -0.12367647103295268\n",
            "26     \t [-9.60473065  2.21670449]. \t  -867.6879617370829 \t -0.12367647103295268\n",
            "27     \t [-0.60469858  0.65526898]. \t  -6.8584495678459145 \t -0.12367647103295268\n",
            "28     \t [0.61681276 0.28766579]. \t  -0.5541930688438318 \t -0.12367647103295268\n",
            "29     \t [-9.61089534  9.75523019]. \t  -80064.54022108462 \t -0.12367647103295268\n",
            "30     \t [-0.06111703 -1.11626952]. \t  -14.163959731580778 \t -0.12367647103295268\n",
            "31     \t [8.57398277 9.82735492]. \t  -68196.78992464022 \t -0.12367647103295268\n",
            "32     \t [0.38439037 0.83173806]. \t  -2.3757206652801393 \t -0.12367647103295268\n",
            "33     \t [ 8.56162199 -5.49826575]. \t  -5444.445976499021 \t -0.12367647103295268\n",
            "34     \t [-2.23039425 -4.57833756]. \t  -3909.365044106635 \t -0.12367647103295268\n",
            "35     \t [-2.81636826  1.21568924]. \t  -81.2005340807094 \t -0.12367647103295268\n",
            "36     \t [-0.90360361 -0.01672471]. \t  -5.258728282497571 \t -0.12367647103295268\n",
            "37     \t [ 2.09538573 -1.23213011]. \t  -2.970468638494177 \t -0.12367647103295268\n",
            "38     \t [-4.97477693  3.81780617]. \t  -2364.8745670262233 \t -0.12367647103295268\n",
            "39     \t [1.49774087 0.24524924]. \t  -4.04246367363041 \t -0.12367647103295268\n",
            "40     \t [-4.3517255  -0.64133564]. \t  -82.18872652515466 \t -0.12367647103295268\n",
            "41     \t [7.19295645 2.43318338]. \t  -81.55691561412576 \t -0.12367647103295268\n",
            "42     \t [0.92075275 0.52342196]. \t  -0.28425718885340884 \t -0.12367647103295268\n",
            "43     \t [-0.11073318 -0.01054058]. \t  -1.258350394263938 \t -0.12367647103295268\n",
            "44     \t [ 6.44537307 -0.8122507 ]. \t  -82.20118824925245 \t -0.12367647103295268\n",
            "45     \t [ 8.38677758 -1.51761913]. \t  -83.14796579960742 \t -0.12367647103295268\n",
            "46     \t [3.20804675 0.45212507]. \t  -20.546652767415697 \t -0.12367647103295268\n",
            "47     \t [ 3.52470214 -1.92862941]. \t  -37.021064453977985 \t -0.12367647103295268\n",
            "48     \t [4.7869921  1.40198461]. \t  -15.806337438195051 \t -0.12367647103295268\n",
            "49     \t [4.26101471 0.67896651]. \t  -32.93237505004037 \t -0.12367647103295268\n",
            "50     \t [ 1.9443366  -0.92170442]. \t  -1.012075098827998 \t -0.12367647103295268\n",
            "51     \t [ 2.40196759 -1.11106934]. \t  -1.9744864412390422 \t -0.12367647103295268\n",
            "52     \t [2.41840128 0.0186894 ]. \t  -13.702434761301584 \t -0.12367647103295268\n",
            "53     \t [2.41099584 0.90713834]. \t  -3.1619588578595237 \t -0.12367647103295268\n",
            "54     \t [-6.75598933 -0.2864638 ]. \t  -155.93128028367303 \t -0.12367647103295268\n",
            "55     \t [-8.55978671 -9.9309001 ]. \t  -84803.06583085285 \t -0.12367647103295268\n",
            "56     \t [ 7.98605608 -0.48736107]. \t  -161.63565597696189 \t -0.12367647103295268\n",
            "57     \t [-2.64801284  0.01406951]. \t  -27.33613532821893 \t -0.12367647103295268\n",
            "58     \t [-7.30446077 -4.45298308]. \t  -4479.931410554043 \t -0.12367647103295268\n",
            "59     \t [3.19764279 4.65963946]. \t  -3241.2264667529867 \t -0.12367647103295268\n",
            "60     \t [4.16229759 2.73146187]. \t  -241.5325267734987 \t -0.12367647103295268\n",
            "61     \t [5.3565544 1.0560678]. \t  -38.52326848420439 \t -0.12367647103295268\n",
            "62     \t [ 7.01098383 -5.27149603]. \t  -4753.513997951199 \t -0.12367647103295268\n",
            "63     \t [2.28876159 9.60659356]. \t  -66456.95186784181 \t -0.12367647103295268\n",
            "64     \t [6.02882431 4.49178304]. \t  -2381.481385870719 \t -0.12367647103295268\n",
            "65     \t [7.82567942 8.46717967]. \t  -36799.93362102439 \t -0.12367647103295268\n",
            "66     \t [ 1.02551281 -0.37702712]. \t  -1.0994470205442355 \t -0.12367647103295268\n",
            "67     \t [ 1.32890142 -1.14151099]. \t  -3.3706212636673056 \t -0.12367647103295268\n",
            "68     \t [-8.75020178 -5.68207233]. \t  -10847.325259422847 \t -0.12367647103295268\n",
            "69     \t [1.51869859 1.15084469]. \t  -2.8236999928478896 \t -0.12367647103295268\n",
            "70     \t [ 6.33822363 -6.12437893]. \t  -9461.780082440946 \t -0.12367647103295268\n",
            "71     \t [ 0.39286627 -0.5332549 ]. \t  -0.43046154222307037 \t -0.12367647103295268\n",
            "72     \t [1.72739661 2.71616151]. \t  -339.96948038768585 \t -0.12367647103295268\n",
            "73     \t [ 1.15990097 -2.97987577]. \t  -551.1069438955279 \t -0.12367647103295268\n",
            "74     \t [0.3050817  0.53268541]. \t  -0.6206460200499715 \t -0.12367647103295268\n",
            "75     \t [ 0.67615036 -0.89514612]. \t  -1.8213969141604853 \t -0.12367647103295268\n",
            "76     \t [8.27723762 2.05711569]. \t  -53.02753743044997 \t -0.12367647103295268\n",
            "77     \t [0.5090909  9.01203222]. \t  -52439.236761347966 \t -0.12367647103295268\n",
            "78     \t [7.06162477 1.79316917]. \t  -37.53889373198835 \t -0.12367647103295268\n",
            "79     \t [-2.28071149 -6.41422133]. \t  -14313.303102479305 \t -0.12367647103295268\n",
            "80     \t [ 0.267685   -6.28865747]. \t  -12427.84636356615 \t -0.12367647103295268\n",
            "81     \t [ 1.05834657 -0.66554439]. \t  \u001b[92m-0.06288087702130596\u001b[0m \t -0.06288087702130596\n",
            "82     \t [-9.04863361  1.2583201 ]. \t  -399.40569125181196 \t -0.06288087702130596\n",
            "83     \t [ 8.26840316 -2.99018816]. \t  -237.68949699227193 \t -0.06288087702130596\n",
            "84     \t [-3.18441793 -4.2986316 ]. \t  -3240.090681636895 \t -0.06288087702130596\n",
            "85     \t [-2.56593163  1.55682023]. \t  -122.63020144000545 \t -0.06288087702130596\n",
            "86     \t [8.73188655 3.77820009]. \t  -845.2649612317308 \t -0.06288087702130596\n",
            "87     \t [-1.13781692 -3.94984118]. \t  -2096.3612522192184 \t -0.06288087702130596\n",
            "88     \t [-6.99691523  3.70409887]. \t  -2435.848917227575 \t -0.06288087702130596\n",
            "89     \t [-4.54889154 -3.53520937]. \t  -1776.5217755378185 \t -0.06288087702130596\n",
            "90     \t [-0.75210659 -0.73949869]. \t  -9.884004191222136 \t -0.06288087702130596\n",
            "91     \t [-5.39542752 -4.86496958]. \t  -5602.078430337968 \t -0.06288087702130596\n",
            "92     \t [-5.34625143 -7.38849223]. \t  -26272.63213176849 \t -0.06288087702130596\n",
            "93     \t [ 4.34379926 -2.21067563]. \t  -70.158921569656 \t -0.06288087702130596\n",
            "94     \t [-6.10772408 -4.44348195]. \t  -4208.653841123318 \t -0.06288087702130596\n",
            "95     \t [-5.83204081 -5.12141147]. \t  -6842.067098723277 \t -0.06288087702130596\n",
            "96     \t [ 9.69587589 -7.31644498]. \t  -19035.44955913957 \t -0.06288087702130596\n",
            "97     \t [-5.07031387 -4.65553508]. \t  -4725.5175973484165 \t -0.06288087702130596\n",
            "98     \t [-3.20025038  3.94954699]. \t  -2384.099734581828 \t -0.06288087702130596\n",
            "99     \t [-2.21002255  4.40918558]. \t  -3387.3857635547797 \t -0.06288087702130596\n",
            "100    \t [ 5.85237815 -0.64502844]. \t  -73.95148943738262 \t -0.06288087702130596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "95b2b916-92e6-488d-85a2-e43d658bfd30"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_10 = d2GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.07179171 -7.69986114]. \t  -26686.60864226137 \t -217.28346804397978\n",
            "init   \t [ 9.00565729 -0.35617197]. \t  -217.28346804397978 \t -217.28346804397978\n",
            "init   \t [ 7.4494907  -5.75334638]. \t  -6945.3080829060855 \t -217.28346804397978\n",
            "init   \t [-9.1858075  -2.05611077]. \t  -726.1597680698058 \t -217.28346804397978\n",
            "init   \t [-5.33735605  6.83481449]. \t  -19549.897628464034 \t -217.28346804397978\n",
            "1      \t [7.70674409 9.05288798]. \t  -48843.620887164754 \t -217.28346804397978\n",
            "2      \t [-15.25611937 -23.0565236 ]. \t  -2326427.7693236805 \t -217.28346804397978\n",
            "3      \t [1.36988239 1.21701407]. \t  \u001b[92m-5.2080597150791546\u001b[0m \t -5.2080597150791546\n",
            "4      \t [-6.09332056 -9.94318576]. \t  -83141.37562406182 \t -5.2080597150791546\n",
            "5      \t [-13.32530843 -10.93196558]. \t  -127557.22670848254 \t -5.2080597150791546\n",
            "6      \t [-12.32578431   5.3295012 ]. \t  -9736.306173458132 \t -5.2080597150791546\n",
            "7      \t [-2.54602045 -3.07105467]. \t  -929.2454679402563 \t -5.2080597150791546\n",
            "8      \t [1.24965186 7.03532041]. \t  -19106.98703985305 \t -5.2080597150791546\n",
            "9      \t [  3.61922964 -14.7191843 ]. \t  -369273.0672026678 \t -5.2080597150791546\n",
            "10     \t [-5.23561052  1.25324361]. \t  -179.22605724377087 \t -5.2080597150791546\n",
            "11     \t [-25.39368359 -24.39695399]. \t  -2957115.7182881986 \t -5.2080597150791546\n",
            "12     \t [ 4.22877893 -1.43038947]. \t  -10.462415016810537 \t -5.2080597150791546\n",
            "13     \t [5.51544221 3.75025185]. \t  -1043.1151205791166 \t -5.2080597150791546\n",
            "14     \t [9.54236899 3.4799621 ]. \t  -503.85377059445585 \t -5.2080597150791546\n",
            "15     \t [  8.34072629 -10.77666103]. \t  -100345.07456433238 \t -5.2080597150791546\n",
            "16     \t [-10.58726456  -6.27195589]. \t  -16069.718939375893 \t -5.2080597150791546\n",
            "17     \t [-9.94821397  9.81261826]. \t  -82151.13071974964 \t -5.2080597150791546\n",
            "18     \t [ 1.68384244 -2.42476227]. \t  -203.4829907619261 \t -5.2080597150791546\n",
            "19     \t [-1.61008725 -8.12689283]. \t  -35759.722865654956 \t -5.2080597150791546\n",
            "20     \t [-1.08368964  1.76534359]. \t  -111.40602671137675 \t -5.2080597150791546\n",
            "21     \t [-8.86272635  1.69591828]. \t  -524.4700543909037 \t -5.2080597150791546\n",
            "22     \t [1.47436007 2.08092455]. \t  -103.50605803636999 \t -5.2080597150791546\n",
            "23     \t [1.47706662 0.10136807]. \t  \u001b[92m-4.470468244373279\u001b[0m \t -4.470468244373279\n",
            "24     \t [1.31647724 0.60330654]. \t  \u001b[92m-0.7928686392780588\u001b[0m \t -0.7928686392780588\n",
            "25     \t [5.92758943 0.03200877]. \t  -94.50519339523548 \t -0.7928686392780588\n",
            "26     \t [2.86290871 0.61574858]. \t  -12.329246552545051 \t -0.7928686392780588\n",
            "27     \t [1.83509705 0.48542584]. \t  -4.417400061870247 \t -0.7928686392780588\n",
            "28     \t [-5.93047362 -4.08043286]. \t  -3126.0703549650893 \t -0.7928686392780588\n",
            "29     \t [-2.4411987   9.96635689]. \t  -80892.44248158827 \t -0.7928686392780588\n",
            "30     \t [ 4.37461865 -3.04172198]. \t  -410.67509251468584 \t -0.7928686392780588\n",
            "31     \t [ 4.10201641 -0.5376862 ]. \t  -34.45688821842011 \t -0.7928686392780588\n",
            "32     \t [-1.55242068  4.02672546]. \t  -2315.993619530783 \t -0.7928686392780588\n",
            "33     \t [-2.11133023  0.7162611 ]. \t  -29.366809797451445 \t -0.7928686392780588\n",
            "34     \t [0.72452244 0.78664481]. \t  \u001b[92m-0.6024263243262422\u001b[0m \t -0.6024263243262422\n",
            "35     \t [-0.50780535 -0.26290934]. \t  -3.1082329756069855 \t -0.6024263243262422\n",
            "36     \t [0.95364766 0.99033197]. \t  -2.0337409940063993 \t -0.6024263243262422\n",
            "37     \t [-0.54113752  0.57541392]. \t  -5.271158655725608 \t -0.6024263243262422\n",
            "38     \t [2.49153428 0.99111253]. \t  -2.7799769411989397 \t -0.6024263243262422\n",
            "39     \t [0.03548771 0.70092975]. \t  -2.724346381624781 \t -0.6024263243262422\n",
            "40     \t [0.21509749 0.58204977]. \t  -1.0438222628255382 \t -0.6024263243262422\n",
            "41     \t [2.13801526 1.14123847]. \t  -1.7309489981875694 \t -0.6024263243262422\n",
            "42     \t [0.25932599 0.20052508]. \t  -0.6126122540927241 \t -0.6024263243262422\n",
            "43     \t [ 6.54051464 -1.86523814]. \t  -31.046269103610765 \t -0.6024263243262422\n",
            "44     \t [ 0.37173045 -0.33724089]. \t  \u001b[92m-0.4363489168057352\u001b[0m \t -0.4363489168057352\n",
            "45     \t [ 0.60710758 -0.22506819]. \t  -0.6660240443850638 \t -0.4363489168057352\n",
            "46     \t [ 1.95843325 -0.2285301 ]. \t  -7.793087083203086 \t -0.4363489168057352\n",
            "47     \t [0.51247083 0.05040804]. \t  -0.7525716583239963 \t -0.4363489168057352\n",
            "48     \t [1.70092861 5.94903049]. \t  -9544.860152662548 \t -0.4363489168057352\n",
            "49     \t [ 0.34604756 -4.53726842]. \t  -3334.207274896928 \t -0.4363489168057352\n",
            "50     \t [ 0.36116911 -0.15596855]. \t  -0.6034383259726894 \t -0.4363489168057352\n",
            "51     \t [ 1.398109   -0.47845757]. \t  -1.9266900332463273 \t -0.4363489168057352\n",
            "52     \t [-0.42015344 -0.79166638]. \t  -7.618875369096507 \t -0.4363489168057352\n",
            "53     \t [ 8.88357088 -2.73246838]. \t  -135.3362342838957 \t -0.4363489168057352\n",
            "54     \t [ 3.32467102 -1.34256785]. \t  -5.561238073206129 \t -0.4363489168057352\n",
            "55     \t [-8.01789792  4.06432841]. \t  -3452.4189418073884 \t -0.4363489168057352\n",
            "56     \t [-3.76757267  0.39468033]. \t  -56.00815295654134 \t -0.4363489168057352\n",
            "57     \t [ 6.76050866 -2.05206792]. \t  -38.70433769107767 \t -0.4363489168057352\n",
            "58     \t [-4.24352113  0.81128115]. \t  -89.31894166189932 \t -0.4363489168057352\n",
            "59     \t [2.10544765 0.86560528]. \t  -1.958676158100908 \t -0.4363489168057352\n",
            "60     \t [3.77713632 1.13686871]. \t  -10.555145804630532 \t -0.4363489168057352\n",
            "61     \t [-8.11865691 -7.58792165]. \t  -30475.006856139753 \t -0.4363489168057352\n",
            "62     \t [1.38354861 0.75315245]. \t  \u001b[92m-0.2711826470158853\u001b[0m \t -0.2711826470158853\n",
            "63     \t [ 1.51171163 -0.78200951]. \t  -0.4284678478953032 \t -0.2711826470158853\n",
            "64     \t [2.30669218 5.53075925]. \t  -6933.509486673679 \t -0.2711826470158853\n",
            "65     \t [3.61861458 1.533298  ]. \t  -9.204614157764116 \t -0.2711826470158853\n",
            "66     \t [1.33133264 0.81257982]. \t  \u001b[92m-0.11001290622835179\u001b[0m \t -0.11001290622835179\n",
            "67     \t [ 1.36710345 -0.84054689]. \t  -0.13898493565453612 \t -0.11001290622835179\n",
            "68     \t [ 2.3753554  -1.17126276]. \t  -2.1629769994058146 \t -0.11001290622835179\n",
            "69     \t [ 7.38047154 -9.25721189]. \t  -53840.23648129998 \t -0.11001290622835179\n",
            "70     \t [-6.86319983 -9.33844524]. \t  -65784.03751976759 \t -0.11001290622835179\n",
            "71     \t [1.27292744 0.82576303]. \t  \u001b[92m-0.09099382248244976\u001b[0m \t -0.09099382248244976\n",
            "72     \t [ 1.50812225 -1.30782685]. \t  -7.575030105411535 \t -0.09099382248244976\n",
            "73     \t [ 4.79010638 -1.18871613]. \t  -22.079610760707066 \t -0.09099382248244976\n",
            "74     \t [-6.32997512 -0.8376083 ]. \t  -173.33176649959933 \t -0.09099382248244976\n",
            "75     \t [-8.46680573  8.44266127]. \t  -45706.03566664887 \t -0.09099382248244976\n",
            "76     \t [ 3.25293074 -5.22260964]. \t  -5268.121662610138 \t -0.09099382248244976\n",
            "77     \t [-9.60702549  8.65089734]. \t  -50854.704943960656 \t -0.09099382248244976\n",
            "78     \t [ 1.43472543 -8.73045771]. \t  -45606.45220107105 \t -0.09099382248244976\n",
            "79     \t [3.60329986 9.95542526]. \t  -75758.86341568134 \t -0.09099382248244976\n",
            "80     \t [-9.36547476  2.02581118]. \t  -725.0845732068685 \t -0.09099382248244976\n",
            "81     \t [ 9.86970464 -8.11441696]. \t  -29757.834494110662 \t -0.09099382248244976\n",
            "82     \t [6.82596773 5.53661569]. \t  -5970.574953084083 \t -0.09099382248244976\n",
            "83     \t [-5.71077353  6.87092161]. \t  -20097.02860416015 \t -0.09099382248244976\n",
            "84     \t [-9.84186936 -0.80085221]. \t  -365.05948714640203 \t -0.09099382248244976\n",
            "85     \t [ 1.22662057 -0.69554102]. \t  -0.18558720608477844 \t -0.09099382248244976\n",
            "86     \t [ 1.09998098 -8.34432714]. \t  -38173.95512799832 \t -0.09099382248244976\n",
            "87     \t [-5.05129396  0.33177034]. \t  -92.1942562736133 \t -0.09099382248244976\n",
            "88     \t [ 7.11878012 -1.20910938]. \t  -72.63365938492225 \t -0.09099382248244976\n",
            "89     \t [7.22334221 1.58254436]. \t  -48.53755648241006 \t -0.09099382248244976\n",
            "90     \t [-0.06526875  2.98658386]. \t  -642.2867100258273 \t -0.09099382248244976\n",
            "91     \t [ 2.25504466 -3.58192906]. \t  -1097.1994125105232 \t -0.09099382248244976\n",
            "92     \t [3.84302816 0.29679479]. \t  -34.97444363718047 \t -0.09099382248244976\n",
            "93     \t [5.81368138 7.99447649]. \t  -29795.869878511236 \t -0.09099382248244976\n",
            "94     \t [ 2.1995583  -9.43301258]. \t  -61787.33215146634 \t -0.09099382248244976\n",
            "95     \t [-1.41958965 -0.15751026]. \t  -10.171562020910802 \t -0.09099382248244976\n",
            "96     \t [-3.95788396  5.24743984]. \t  -6993.457889775188 \t -0.09099382248244976\n",
            "97     \t [-3.87570309  8.8357695 ]. \t  -51234.876069122845 \t -0.09099382248244976\n",
            "98     \t [ 2.75545322 -0.94314159]. \t  -4.988412364616376 \t -0.09099382248244976\n",
            "99     \t [ 9.63310453 -6.07846482]. \t  -8333.833502275556 \t -0.09099382248244976\n",
            "100    \t [9.84274302 3.50032441]. \t  -508.1308030874569 \t -0.09099382248244976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "5bb06a93-e0d0-48e2-c3e6-b2bafa25ef8c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_11 = d2GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.62071325 6.42495699]. \t  -10714.638013792866 \t -9.740000060755142\n",
            "init   \t [ 2.91787228 -1.52623973]. \t  -9.740000060755142 \t -9.740000060755142\n",
            "init   \t [-5.95378042 -0.16186459]. \t  -120.5034756901465 \t -9.740000060755142\n",
            "init   \t [-7.22883337 -0.94957929]. \t  -230.87623811313406 \t -9.740000060755142\n",
            "init   \t [-7.72530745 -9.94151023]. \t  -84448.33746856163 \t -9.740000060755142\n",
            "1      \t [ 9.71879818 -6.86089495]. \t  -14331.165445255861 \t -9.740000060755142\n",
            "2      \t [-0.41525252  9.69422331]. \t  -70969.40865155176 \t -9.740000060755142\n",
            "3      \t [-9.5194484  9.1420568]. \t  -62537.95985021731 \t -9.740000060755142\n",
            "4      \t [ 0.94955298 -9.31626642]. \t  -59606.43954696483 \t -9.740000060755142\n",
            "5      \t [-13.3741993   -2.66088109]. \t  -1722.9453420353345 \t -9.740000060755142\n",
            "6      \t [-30.42616549 -34.97054119]. \t  -12265147.31843398 \t -9.740000060755142\n",
            "7      \t [ 8.49048764 -0.19399542]. \t  -197.73923329838738 \t -9.740000060755142\n",
            "8      \t [-0.93566644  2.80806315]. \t  -561.9346078326985 \t -9.740000060755142\n",
            "9      \t [-11.44103918   2.84793135]. \t  -1685.2034065022042 \t -9.740000060755142\n",
            "10     \t [-1.69852026 -3.22680082]. \t  -1021.8537270515295 \t -9.740000060755142\n",
            "11     \t [3.80479903 5.14337171]. \t  -4830.2226656981575 \t -9.740000060755142\n",
            "12     \t [ 4.74954489 -5.39539928]. \t  -5732.380823223212 \t -9.740000060755142\n",
            "13     \t [-4.90683956  5.94357573]. \t  -11453.223121939483 \t -9.740000060755142\n",
            "14     \t [-5.71181301 -5.03499417]. \t  -6410.160071356608 \t -9.740000060755142\n",
            "15     \t [-16.69783021 -20.02609489]. \t  -1341136.8052692392 \t -9.740000060755142\n",
            "16     \t [4.9374318 0.4799142]. \t  -55.586783304498965 \t -9.740000060755142\n",
            "17     \t [5.24640175 9.84084534]. \t  -71035.84612395246 \t -9.740000060755142\n",
            "18     \t [1.37913114 0.06006558]. \t  \u001b[92m-3.9080441130165298\u001b[0m \t -3.9080441130165298\n",
            "19     \t [  6.0577244  -10.41262233]. \t  -88888.47519942328 \t -3.9080441130165298\n",
            "20     \t [1.58716443 0.52747698]. \t  \u001b[92m-2.4694490967054668\u001b[0m \t -2.4694490967054668\n",
            "21     \t [1.69220352 0.61311038]. \t  \u001b[92m-2.247830625816612\u001b[0m \t -2.247830625816612\n",
            "22     \t [-15.38969791 -10.32594167]. \t  -104820.97564956297 \t -2.247830625816612\n",
            "23     \t [6.73920919 2.57971789]. \t  -119.2861815459801 \t -2.247830625816612\n",
            "24     \t [-9.98008727 -5.83498296]. \t  -12311.704842974184 \t -2.247830625816612\n",
            "25     \t [1.80824395 0.86910591]. \t  \u001b[92m-0.830334799634687\u001b[0m \t -0.830334799634687\n",
            "26     \t [1.87079499 1.19411038]. \t  -2.683022385928261 \t -0.830334799634687\n",
            "27     \t [1.02866697 0.00959294]. \t  -2.116376038151717 \t -0.830334799634687\n",
            "28     \t [ 2.11103866 -1.799357  ]. \t  -39.329204245967176 \t -0.830334799634687\n",
            "29     \t [-3.14668206 -0.23746722]. \t  -38.44317559228578 \t -0.830334799634687\n",
            "30     \t [ 3.57771275 -0.87993874]. \t  -14.87932702029648 \t -0.830334799634687\n",
            "31     \t [-9.94522097 -0.65158699]. \t  -352.83394027924084 \t -0.830334799634687\n",
            "32     \t [ 3.33727079 -1.36661083]. \t  -5.779610152409278 \t -0.830334799634687\n",
            "33     \t [-4.41086062  1.67943638]. \t  -231.3577428847595 \t -0.830334799634687\n",
            "34     \t [9.88847254 2.36546456]. \t  -82.3972933037139 \t -0.830334799634687\n",
            "35     \t [ 7.11321687 -2.07580494]. \t  -41.89975772758573 \t -0.830334799634687\n",
            "36     \t [-3.42738622 -8.83248316]. \t  -50870.05885384415 \t -0.830334799634687\n",
            "37     \t [-7.54775557  2.89077632]. \t  -1250.2496755986429 \t -0.830334799634687\n",
            "38     \t [2.25145657 1.19414518]. \t  -2.2873652842691294 \t -0.830334799634687\n",
            "39     \t [-5.25358867  9.68630861]. \t  -74462.0396231826 \t -0.830334799634687\n",
            "40     \t [ 4.07954436 -1.59769352]. \t  -11.58773412206113 \t -0.830334799634687\n",
            "41     \t [ 5.99506769 -7.48021104]. \t  -22459.674007112142 \t -0.830334799634687\n",
            "42     \t [-4.54756724 -1.76204974]. \t  -262.2104568959321 \t -0.830334799634687\n",
            "43     \t [1.54713351 0.75732063]. \t  \u001b[92m-0.6194582100546269\u001b[0m \t -0.6194582100546269\n",
            "44     \t [ 0.8021311  -0.33731344]. \t  -0.6994143618833732 \t -0.6194582100546269\n",
            "45     \t [-1.7687581  0.5638483]. \t  -19.230299812700427 \t -0.6194582100546269\n",
            "46     \t [ 0.38019515 -0.75350278]. \t  -1.5252282444465597 \t -0.6194582100546269\n",
            "47     \t [ 0.67758561 -0.50797742]. \t  \u001b[92m-0.15611780076293735\u001b[0m \t -0.15611780076293735\n",
            "48     \t [ 9.04785777 -2.50800448]. \t  -89.72251510204066 \t -0.15611780076293735\n",
            "49     \t [ 1.34721806 -4.55786193]. \t  -3232.360194582748 \t -0.15611780076293735\n",
            "50     \t [-0.38202811 -0.08194416]. \t  -2.222775432715206 \t -0.15611780076293735\n",
            "51     \t [2.47707147 1.03401369]. \t  -2.41117936187056 \t -0.15611780076293735\n",
            "52     \t [ 1.53432015 -1.01035666]. \t  -0.8002472121981175 \t -0.15611780076293735\n",
            "53     \t [-1.19893666 -0.4561245 ]. \t  -10.052003671872445 \t -0.15611780076293735\n",
            "54     \t [ 6.06431389 -2.19502167]. \t  -51.16459155626648 \t -0.15611780076293735\n",
            "55     \t [ 5.53284339 -8.2539833 ]. \t  -34197.885286768586 \t -0.15611780076293735\n",
            "56     \t [-0.23000497 -8.42508835]. \t  -40439.91226306904 \t -0.15611780076293735\n",
            "57     \t [-1.17872656 -0.17323914]. \t  -7.815853382713165 \t -0.15611780076293735\n",
            "58     \t [4.09868526 1.58938879]. \t  -11.420663851959533 \t -0.15611780076293735\n",
            "59     \t [8.37035745 7.22947534]. \t  -18547.91722967999 \t -0.15611780076293735\n",
            "60     \t [-2.05464406  0.28973249]. \t  -19.21016419962615 \t -0.15611780076293735\n",
            "61     \t [-0.08663677 -0.17069868]. \t  -1.2227789384973677 \t -0.15611780076293735\n",
            "62     \t [-8.01354218 -9.25776384]. \t  -64468.56887560746 \t -0.15611780076293735\n",
            "63     \t [-0.89017331  5.30630213]. \t  -6548.135452820438 \t -0.15611780076293735\n",
            "64     \t [0.40478553 0.87510017]. \t  -2.8937047231174846 \t -0.15611780076293735\n",
            "65     \t [-0.17762626  6.75297554]. \t  -16703.08617415138 \t -0.15611780076293735\n",
            "66     \t [1.97432265 2.53632665]. \t  -238.20247058643878 \t -0.15611780076293735\n",
            "67     \t [-2.91957718 -1.81146054]. \t  -195.19322620271217 \t -0.15611780076293735\n",
            "68     \t [ 0.1314036  -0.18215547]. \t  -0.7629207235064388 \t -0.15611780076293735\n",
            "69     \t [ 2.90253086 -0.99784194]. \t  -5.280026110322272 \t -0.15611780076293735\n",
            "70     \t [1.0003115  0.90210071]. \t  -0.7869100675756836 \t -0.15611780076293735\n",
            "71     \t [-8.16673039  1.17899306]. \t  -323.69291488217124 \t -0.15611780076293735\n",
            "72     \t [1.26688474 0.82203192]. \t  \u001b[92m-0.08553779603923299\u001b[0m \t -0.08553779603923299\n",
            "73     \t [ 1.50802348 -1.11088819]. \t  -2.1017550660244897 \t -0.08553779603923299\n",
            "74     \t [4.63518173 0.35127548]. \t  -51.730529059530966 \t -0.08553779603923299\n",
            "75     \t [3.50274631 1.23262595]. \t  -6.694354945898021 \t -0.08553779603923299\n",
            "76     \t [-0.05913795  0.47833897]. \t  -1.6558431734022214 \t -0.08553779603923299\n",
            "77     \t [ 9.67852138 -2.76893952]. \t  -139.28678989564332 \t -0.08553779603923299\n",
            "78     \t [ 1.05483998 -0.73713101]. \t  \u001b[92m-0.005040638158741799\u001b[0m \t -0.005040638158741799\n",
            "79     \t [-0.69810267 -7.36598808]. \t  -23858.13215841738 \t -0.005040638158741799\n",
            "80     \t [ 2.06016029 -0.68500232]. \t  -3.6403793024960556 \t -0.005040638158741799\n",
            "81     \t [ 7.82674079 -3.45484648]. \t  -561.5004818861966 \t -0.005040638158741799\n",
            "82     \t [ 6.43035272 -1.87325391]. \t  -30.179766529857964 \t -0.005040638158741799\n",
            "83     \t [ 8.58539429 -1.83522509]. \t  -64.37796873021168 \t -0.005040638158741799\n",
            "84     \t [ 8.32326456 -5.33167961]. \t  -4764.020014189273 \t -0.005040638158741799\n",
            "85     \t [0.38153504 0.26450757]. \t  -0.4992463373231544 \t -0.005040638158741799\n",
            "86     \t [-7.98137333  0.2164104 ]. \t  -211.07761670808316 \t -0.005040638158741799\n",
            "87     \t [-9.81419467  4.16506142]. \t  -4079.167887251405 \t -0.005040638158741799\n",
            "88     \t [-4.3145491  -3.64523851]. \t  -1936.638108318239 \t -0.005040638158741799\n",
            "89     \t [-8.72172539  6.95274767]. \t  -22314.149777184477 \t -0.005040638158741799\n",
            "90     \t [-2.07678822  8.62162833]. \t  -45455.60988660681 \t -0.005040638158741799\n",
            "91     \t [-1.13533016 -4.2962465 ]. \t  -2900.286107739129 \t -0.005040638158741799\n",
            "92     \t [-1.36834628  4.22089697]. \t  -2743.651788682231 \t -0.005040638158741799\n",
            "93     \t [ 3.08294269 -8.08402201]. \t  -32578.006703472063 \t -0.005040638158741799\n",
            "94     \t [7.61503009 3.48124151]. \t  -596.4105271155338 \t -0.005040638158741799\n",
            "95     \t [5.53489992 2.18684781]. \t  -53.042390952442354 \t -0.005040638158741799\n",
            "96     \t [0.54794103 0.86379893]. \t  -1.9879743804767294 \t -0.005040638158741799\n",
            "97     \t [ 7.47631853 -0.76898239]. \t  -121.16277987722734 \t -0.005040638158741799\n",
            "98     \t [-0.21372332  5.24303817]. \t  -6093.924112011358 \t -0.005040638158741799\n",
            "99     \t [6.82188993 1.44005846]. \t  -48.198732357543356 \t -0.005040638158741799\n",
            "100    \t [ 9.75240598 -3.36757363]. \t  -410.9070903804599 \t -0.005040638158741799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "6ed67637-44f0-4b89-8898-02c8fcadc2c5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_12 = d2GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.16961099  2.44217542]. \t  -706.482335023616 \t -706.482335023616\n",
            "init   \t [-1.24544522  5.70717167]. \t  -8820.05719953671 \t -706.482335023616\n",
            "init   \t [ 5.59951616 -4.54814789]. \t  -2580.3963775443385 \t -706.482335023616\n",
            "init   \t [-4.4707149   6.03744355]. \t  -12002.832292900235 \t -706.482335023616\n",
            "init   \t [9.16278707 7.51865269]. \t  -21656.00779050222 \t -706.482335023616\n",
            "1      \t [-8.80381554 -6.31425832]. \t  -15776.028882530063 \t -706.482335023616\n",
            "2      \t [-2.10622833 -9.37900803]. \t  -63404.573438939275 \t -706.482335023616\n",
            "3      \t [-83.52566462 -73.64076674]. \t  -238912930.57758468 \t -706.482335023616\n",
            "4      \t [-21.64989468 -14.38768079]. \t  -380113.35474365565 \t -706.482335023616\n",
            "5      \t [-12.58826088   0.7487888 ]. \t  \u001b[92m-560.5487556946173\u001b[0m \t -560.5487556946173\n",
            "6      \t [-0.91777906 -1.83475346]. \t  \u001b[92m-120.73571628803775\u001b[0m \t -120.73571628803775\n",
            "7      \t [-10.57911942 -13.20813166]. \t  -258598.18772360333 \t -120.73571628803775\n",
            "8      \t [  7.37951907 -10.98703361]. \t  -109599.78036880196 \t -120.73571628803775\n",
            "9      \t [-21.27263264 -22.73633954]. \t  -2227199.8127936763 \t -120.73571628803775\n",
            "10     \t [-11.19506264   7.67043837]. \t  -33361.77236576706 \t -120.73571628803775\n",
            "11     \t [4.56993223 2.33208275]. \t  \u001b[92m-92.30817136454681\u001b[0m \t -92.30817136454681\n",
            "12     \t [8.90595546 0.41689246]. \t  -208.99507419761974 \t -92.30817136454681\n",
            "13     \t [3.81060731 7.05653591]. \t  -18355.05300382024 \t -92.30817136454681\n",
            "14     \t [-5.0196228  -2.86264407]. \t  -952.9336974964183 \t -92.30817136454681\n",
            "15     \t [-34.25431048 -35.56909838]. \t  -13155341.237176966 \t -92.30817136454681\n",
            "16     \t [ 1.27577154 -5.09241647]. \t  -5118.699300360021 \t -92.30817136454681\n",
            "17     \t [-16.27477225 -16.85481572]. \t  -683448.9265978442 \t -92.30817136454681\n",
            "18     \t [-15.88818224  -6.44193821]. \t  -19841.82290791101 \t -92.30817136454681\n",
            "19     \t [1.41330909 1.05143309]. \t  \u001b[92m-1.4435196840971511\u001b[0m \t -1.4435196840971511\n",
            "20     \t [-9.23494356 -1.0720927 ]. \t  -370.8069600145044 \t -1.4435196840971511\n",
            "21     \t [ 9.80385012 -3.66204039]. \t  -656.6799799228362 \t -1.4435196840971511\n",
            "22     \t [ 2.55590189 -0.47272001]. \t  -11.316368908641596 \t -1.4435196840971511\n",
            "23     \t [ 3.02736634 -8.60907648]. \t  -42173.11120502375 \t -1.4435196840971511\n",
            "24     \t [-2.5838163   1.91582498]. \t  -209.83859301184586 \t -1.4435196840971511\n",
            "25     \t [1.38949384 1.87270182]. \t  -63.42238958859889 \t -1.4435196840971511\n",
            "26     \t [2.24974855 0.16948288]. \t  -11.17422696633679 \t -1.4435196840971511\n",
            "27     \t [ 3.84635456 -0.75894838]. \t  -22.62077042786802 \t -1.4435196840971511\n",
            "28     \t [-2.1550919   9.79142082]. \t  -75203.54889502253 \t -1.4435196840971511\n",
            "29     \t [0.08812375 0.7081129 ]. \t  -2.5049583463967444 \t -1.4435196840971511\n",
            "30     \t [8.30726276 3.35485216]. \t  -456.835330498602 \t -1.4435196840971511\n",
            "31     \t [ 0.85062771 -0.37427065]. \t  \u001b[92m-0.6731856539483387\u001b[0m \t -0.6731856539483387\n",
            "32     \t [0.47081769 0.28791741]. \t  \u001b[92m-0.4661142043144742\u001b[0m \t -0.4661142043144742\n",
            "33     \t [0.60618198 0.72970687]. \t  -0.5760182398531768 \t -0.4661142043144742\n",
            "34     \t [ 3.10693607 -1.75061302]. \t  -22.708448923303692 \t -0.4661142043144742\n",
            "35     \t [5.216341   0.28249496]. \t  -68.9186528803798 \t -0.4661142043144742\n",
            "36     \t [1.58725335 0.13192095]. \t  -5.1650503462682344 \t -0.4661142043144742\n",
            "37     \t [ 1.52383102 -1.58905131]. \t  -25.144505920891817 \t -0.4661142043144742\n",
            "38     \t [-9.61044485  4.09647699]. \t  -3840.3442743550877 \t -0.4661142043144742\n",
            "39     \t [-6.86405958  9.58526159]. \t  -72732.69823547796 \t -0.4661142043144742\n",
            "40     \t [1.48151554 0.57567554]. \t  -1.5724322597848341 \t -0.4661142043144742\n",
            "41     \t [ 2.03023734 -1.24810203]. \t  -3.4170544277389445 \t -0.4661142043144742\n",
            "42     \t [-6.82062611 -9.65198533]. \t  -74669.03325393294 \t -0.4661142043144742\n",
            "43     \t [ 1.84291153 -1.00102599]. \t  -0.7624671935053678 \t -0.4661142043144742\n",
            "44     \t [0.95039019 0.77536587]. \t  \u001b[92m-0.1294633715226449\u001b[0m \t -0.1294633715226449\n",
            "45     \t [2.09894055 0.87938011]. \t  -1.8177890670091892 \t -0.1294633715226449\n",
            "46     \t [0.90253678 0.59756994]. \t  \u001b[92m-0.08045588790891017\u001b[0m \t -0.08045588790891017\n",
            "47     \t [-3.96627776  0.11880661]. \t  -56.57609958536352 \t -0.08045588790891017\n",
            "48     \t [ 1.23966921 -0.6307238 ]. \t  -0.45179180458815715 \t -0.08045588790891017\n",
            "49     \t [-2.03486663 -4.27082771]. \t  -2975.990873540327 \t -0.08045588790891017\n",
            "50     \t [ 6.74833959 -1.5631733 ]. \t  -39.97241772429487 \t -0.08045588790891017\n",
            "51     \t [-2.30560633 -0.39849111]. \t  -24.689354688555653 \t -0.08045588790891017\n",
            "52     \t [ 8.27291248 -6.8084937 ]. \t  -14312.534873613535 \t -0.08045588790891017\n",
            "53     \t [ 0.22461997 -0.26212155]. \t  -0.6164234566238834 \t -0.08045588790891017\n",
            "54     \t [-1.01347514 -0.601566  ]. \t  -10.090076934669163 \t -0.08045588790891017\n",
            "55     \t [-0.13351617 -0.48952988]. \t  -2.0358947353323895 \t -0.08045588790891017\n",
            "56     \t [-0.01698643 -0.23657069]. \t  -1.0675010056413885 \t -0.08045588790891017\n",
            "57     \t [ 4.78475612 -1.20325003]. \t  -21.46203986918882 \t -0.08045588790891017\n",
            "58     \t [ 1.5065289  -0.76439206]. \t  -0.4849763393952129 \t -0.08045588790891017\n",
            "59     \t [-5.13610495  4.50565333]. \t  -4221.568063430106 \t -0.08045588790891017\n",
            "60     \t [ 1.70740218 -2.41744087]. \t  -199.72671044738223 \t -0.08045588790891017\n",
            "61     \t [-3.1389655   2.30059096]. \t  -393.84951075171017 \t -0.08045588790891017\n",
            "62     \t [-1.4169869   0.05183581]. \t  -9.888046207225466 \t -0.08045588790891017\n",
            "63     \t [-1.31959817  1.6932061 ]. \t  -104.88403347710107 \t -0.08045588790891017\n",
            "64     \t [ 0.09183108 -0.94850725]. \t  -6.655889552968279 \t -0.08045588790891017\n",
            "65     \t [ 6.8431608  -0.20728075]. \t  -125.46284750646188 \t -0.08045588790891017\n",
            "66     \t [ 0.65681684 -0.64620393]. \t  -0.1813865615295248 \t -0.08045588790891017\n",
            "67     \t [0.54709803 0.5190313 ]. \t  -0.20525834478270583 \t -0.08045588790891017\n",
            "68     \t [2.38666161 1.39486376]. \t  -6.45064255222184 \t -0.08045588790891017\n",
            "69     \t [-5.86891714 -9.7963886 ]. \t  -78302.69821855053 \t -0.08045588790891017\n",
            "70     \t [3.28012838 3.6359738 ]. \t  -1078.0148941175532 \t -0.08045588790891017\n",
            "71     \t [2.81059409 0.81276303]. \t  -7.715034212118235 \t -0.08045588790891017\n",
            "72     \t [6.21096425 1.06901135]. \t  -57.97158022380407 \t -0.08045588790891017\n",
            "73     \t [5.48138469 1.6134936 ]. \t  -20.233686594525135 \t -0.08045588790891017\n",
            "74     \t [4.38671998 1.52971845]. \t  -11.641989004998031 \t -0.08045588790891017\n",
            "75     \t [2.51591946 1.19644262]. \t  -2.538872029722632 \t -0.08045588790891017\n",
            "76     \t [-6.69310523 -0.1318563 ]. \t  -149.71253668810692 \t -0.08045588790891017\n",
            "77     \t [ 2.01271661 -6.01062813]. \t  -9869.066925274641 \t -0.08045588790891017\n",
            "78     \t [ 4.62301327 -5.15861912]. \t  -4736.985788552286 \t -0.08045588790891017\n",
            "79     \t [ 1.07596146 -0.63551883]. \t  -0.1496252200584844 \t -0.08045588790891017\n",
            "80     \t [6.26398562 3.76637446]. \t  -1005.1632549001689 \t -0.08045588790891017\n",
            "81     \t [-0.28452549 -7.4087819 ]. \t  -24230.092896333244 \t -0.08045588790891017\n",
            "82     \t [ 5.6169356  -1.61557999]. \t  -21.630896642011002 \t -0.08045588790891017\n",
            "83     \t [7.43594702 1.60305864]. \t  -51.967888692075356 \t -0.08045588790891017\n",
            "84     \t [ 1.17286731 -8.54177696]. \t  -41905.756991193644 \t -0.08045588790891017\n",
            "85     \t [-1.8420648   5.55248144]. \t  -8073.127249019639 \t -0.08045588790891017\n",
            "86     \t [9.2733085  8.70085541]. \t  -40473.99489624034 \t -0.08045588790891017\n",
            "87     \t [ 1.41526084 -1.06956786]. \t  -1.6956171227961934 \t -0.08045588790891017\n",
            "88     \t [-9.54284769 -3.55890536]. \t  -2543.6078971587644 \t -0.08045588790891017\n",
            "89     \t [ 6.00106142 -6.84294708]. \t  -15390.331879948177 \t -0.08045588790891017\n",
            "90     \t [-7.64447184  4.58158647]. \t  -5000.274323967172 \t -0.08045588790891017\n",
            "91     \t [-3.17673625 -2.51350093]. \t  -517.4907760011075 \t -0.08045588790891017\n",
            "92     \t [ 6.01484388 -3.34460012]. \t  -560.3075707518249 \t -0.08045588790891017\n",
            "93     \t [-2.84232082 -7.60664852]. \t  -28129.85622966966 \t -0.08045588790891017\n",
            "94     \t [-9.46905503  2.44046659]. \t  -1023.8791326775374 \t -0.08045588790891017\n",
            "95     \t [-6.84857657  9.85863994]. \t  -81051.95808320583 \t -0.08045588790891017\n",
            "96     \t [ 1.04779365 -7.94495041]. \t  -31348.41990580627 \t -0.08045588790891017\n",
            "97     \t [4.90338108 9.29475615]. \t  -56383.70145988153 \t -0.08045588790891017\n",
            "98     \t [-6.1461776   5.99112053]. \t  -12198.24388708779 \t -0.08045588790891017\n",
            "99     \t [-4.9205641  -9.43264269]. \t  -66917.97104986853 \t -0.08045588790891017\n",
            "100    \t [-2.47749687  0.37710979]. \t  -27.34939371594831 \t -0.08045588790891017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "9a40ca99-ea7b-47af-ee87-099f130be754"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_13 = d2GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.21288618 -1.72798696]. \t  -20.120935450808645 \t -20.120935450808645\n",
            "init   \t [ 3.36205116 -5.40113156]. \t  -6051.706374022101 \t -20.120935450808645\n",
            "init   \t [6.15356672 2.73636929]. \t  -182.2099268583363 \t -20.120935450808645\n",
            "init   \t [-6.55612296 -4.79228264]. \t  -5567.089701718418 \t -20.120935450808645\n",
            "init   \t [ 8.30639981 -0.74368981]. \t  -157.07069920616067 \t -20.120935450808645\n",
            "1      \t [-7.5961809  7.6177101]. \t  -30655.10899619148 \t -20.120935450808645\n",
            "2      \t [9.21200838 9.90573549]. \t  -70031.75643065354 \t -20.120935450808645\n",
            "3      \t [-0.35482891  6.84457911]. \t  -17693.150381619864 \t -20.120935450808645\n",
            "4      \t [  1.95769685 -12.19506591]. \t  -174619.6316143126 \t -20.120935450808645\n",
            "5      \t [-12.04474969 -15.67374386]. \t  -506947.46291128395 \t -20.120935450808645\n",
            "6      \t [-22.89619255 -26.54552861]. \t  -4103115.7810918004 \t -20.120935450808645\n",
            "7      \t [ 9.52302423 -8.71140556]. \t  -40545.127961554834 \t -20.120935450808645\n",
            "8      \t [-10.69102118   1.07360333]. \t  -474.48601337183226 \t -20.120935450808645\n",
            "9      \t [-13.35917951  -7.01077311]. \t  -25142.570065543998 \t -20.120935450808645\n",
            "10     \t [-2.31619218  0.30040277]. \t  -23.463911009305214 \t -20.120935450808645\n",
            "11     \t [ -5.53244139 -11.66781268]. \t  -154397.4058351455 \t -20.120935450808645\n",
            "12     \t [-5.93720474  2.21572622]. \t  -544.6335577762063 \t -20.120935450808645\n",
            "13     \t [-1.8336242  -6.66359555]. \t  -16439.479053179624 \t -20.120935450808645\n",
            "14     \t [1.97383931 1.54258181]. \t  \u001b[92m-16.46390988383897\u001b[0m \t -16.46390988383897\n",
            "15     \t [4.71657566 7.04019803]. \t  -17841.14028366028 \t -16.46390988383897\n",
            "16     \t [-10.49835702  -2.96818772]. \t  -1713.527334044442 \t -16.46390988383897\n",
            "17     \t [ 0.19753514 -1.15056144]. \t  \u001b[92m-12.649421222072922\u001b[0m \t -12.649421222072922\n",
            "18     \t [-49.50902474 -48.64722214]. \t  -45749334.34987929 \t -12.649421222072922\n",
            "19     \t [-2.97157523 -2.20497082]. \t  -338.118141979489 \t -12.649421222072922\n",
            "20     \t [9.32525675 4.27502511]. \t  -1551.8660816935121 \t -12.649421222072922\n",
            "21     \t [ 7.34447525 -3.7297532 ]. \t  -878.9200554564745 \t -12.649421222072922\n",
            "22     \t [-0.96855926  2.20092769]. \t  -231.0067876537607 \t -12.649421222072922\n",
            "23     \t [ 3.28985042 -0.06806035]. \t  -26.7679039328027 \t -12.649421222072922\n",
            "24     \t [-10.41805634 -10.32138589]. \t  -100017.05530965516 \t -12.649421222072922\n",
            "25     \t [-7.74567573 -0.99745382]. \t  -266.0469191386042 \t -12.649421222072922\n",
            "26     \t [2.75540488 2.97639351]. \t  -450.83017591408566 \t -12.649421222072922\n",
            "27     \t [2.1790425  0.48346071]. \t  \u001b[92m-7.249112268910095\u001b[0m \t -7.249112268910095\n",
            "28     \t [2.14363366 0.69161179]. \t  \u001b[92m-4.125740599805442\u001b[0m \t -4.125740599805442\n",
            "29     \t [ 1.68647015 -0.45780126]. \t  \u001b[92m-3.6833710497920507\u001b[0m \t -3.6833710497920507\n",
            "30     \t [1.82032848 0.06161529]. \t  -7.244959504795094 \t -3.6833710497920507\n",
            "31     \t [ 1.92571164 -1.05933929]. \t  \u001b[92m-1.060065904215028\u001b[0m \t -1.060065904215028\n",
            "32     \t [1.96575651 0.7240668 ]. \t  -2.6152379244249984 \t -1.060065904215028\n",
            "33     \t [ 5.34822395 -0.19269434]. \t  -74.53639617323107 \t -1.060065904215028\n",
            "34     \t [ 1.95772897 -1.83851694]. \t  -47.04641230011599 \t -1.060065904215028\n",
            "35     \t [ 2.09821487 -0.52862491]. \t  -5.945126698886531 \t -1.060065904215028\n",
            "36     \t [ 5.63294844 -7.94344199]. \t  -29092.624121161647 \t -1.060065904215028\n",
            "37     \t [-9.64241522  3.6722238 ]. \t  -2794.2655835943 \t -1.060065904215028\n",
            "38     \t [-3.75706325  9.77731556]. \t  -76032.74761131288 \t -1.060065904215028\n",
            "39     \t [ 1.88658467 -0.97527152]. \t  \u001b[92m-0.7865268919557042\u001b[0m \t -0.7865268919557042\n",
            "40     \t [-4.33086907  5.03113454]. \t  -6068.631810719255 \t -0.7865268919557042\n",
            "41     \t [ 0.65204757 -0.0127086 ]. \t  -0.97056067804169 \t -0.7865268919557042\n",
            "42     \t [ 0.45253534 -0.0195731 ]. \t  \u001b[92m-0.7079082445025824\u001b[0m \t -0.7079082445025824\n",
            "43     \t [1.40756781 1.4314015 ]. \t  -14.641030777048478 \t -0.7079082445025824\n",
            "44     \t [-1.63547466 -0.22646562]. \t  -12.987348530105791 \t -0.7079082445025824\n",
            "45     \t [ 2.3717882  -1.06308583]. \t  -1.9066607910579674 \t -0.7079082445025824\n",
            "46     \t [-1.08757468 -0.90209152]. \t  -19.101644026633945 \t -0.7079082445025824\n",
            "47     \t [-5.47159851 -0.85838843]. \t  -138.35485774646648 \t -0.7079082445025824\n",
            "48     \t [9.3826568  0.90601465]. \t  -190.1129819863197 \t -0.7079082445025824\n",
            "49     \t [1.77968609 1.222657  ]. \t  -3.536566264934 \t -0.7079082445025824\n",
            "50     \t [ 4.53744394 -2.24404405]. \t  -73.76434039861847 \t -0.7079082445025824\n",
            "51     \t [-0.43319345 -2.54083424]. \t  -358.22519397895184 \t -0.7079082445025824\n",
            "52     \t [-1.06968479 -9.60467806]. \t  -68876.2628835377 \t -0.7079082445025824\n",
            "53     \t [ 0.09651822 -0.53466522]. \t  -1.2679389879889027 \t -0.7079082445025824\n",
            "54     \t [-5.64174875 -3.92160871]. \t  -2694.0005780827005 \t -0.7079082445025824\n",
            "55     \t [-3.14733153  0.9734921 ]. \t  -68.05811179083469 \t -0.7079082445025824\n",
            "56     \t [ 1.0539022  -1.02644367]. \t  -2.2216650609076076 \t -0.7079082445025824\n",
            "57     \t [-2.41865885 -4.25492326]. \t  -2995.8397045771317 \t -0.7079082445025824\n",
            "58     \t [3.64003973 6.97317961]. \t  -17532.794323167178 \t -0.7079082445025824\n",
            "59     \t [ 0.62636285 -0.52987832]. \t  \u001b[92m-0.14800819098751414\u001b[0m \t -0.14800819098751414\n",
            "60     \t [7.40763866 5.51959415]. \t  -5770.737914270966 \t -0.14800819098751414\n",
            "61     \t [-9.9903605  -0.01517048]. \t  -320.42102403555316 \t -0.14800819098751414\n",
            "62     \t [ 4.79246814 -6.16510827]. \t  -10160.28483560334 \t -0.14800819098751414\n",
            "63     \t [0.26448304 0.34802002]. \t  -0.5419750656223005 \t -0.14800819098751414\n",
            "64     \t [-4.09931234  1.97253809]. \t  -308.3252665364161 \t -0.14800819098751414\n",
            "65     \t [ 0.77876368 -0.82187594]. \t  -0.7037630616891044 \t -0.14800819098751414\n",
            "66     \t [0.40466002 0.28197741]. \t  -0.4751052495294833 \t -0.14800819098751414\n",
            "67     \t [4.08981309 0.7279362 ]. \t  -27.909119138463204 \t -0.14800819098751414\n",
            "68     \t [4.80742649 1.22440502]. \t  -21.042118328385396 \t -0.14800819098751414\n",
            "69     \t [ 3.57649453 -8.66059476]. \t  -42893.23753008888 \t -0.14800819098751414\n",
            "70     \t [ 9.99087551 -4.86280159]. \t  -2863.8340680637184 \t -0.14800819098751414\n",
            "71     \t [3.86563751 8.40336539]. \t  -37747.851242460165 \t -0.14800819098751414\n",
            "72     \t [-3.98859714  0.66553368]. \t  -72.40696445989835 \t -0.14800819098751414\n",
            "73     \t [7.28700357 7.52424658]. \t  -22486.75882202814 \t -0.14800819098751414\n",
            "74     \t [3.76861117 1.50622246]. \t  -8.847317828411901 \t -0.14800819098751414\n",
            "75     \t [3.9413838  1.26896375]. \t  -9.690976059323212 \t -0.14800819098751414\n",
            "76     \t [-3.64442665 -0.16789657]. \t  -48.96261661685847 \t -0.14800819098751414\n",
            "77     \t [0.70073434 4.37042696]. \t  -2812.6689642216243 \t -0.14800819098751414\n",
            "78     \t [ 4.00702346 -4.21814416]. \t  -2003.4400713894202 \t -0.14800819098751414\n",
            "79     \t [ 1.46228444 -6.62887934]. \t  -14937.668100017143 \t -0.14800819098751414\n",
            "80     \t [2.01928411 9.20045794]. \t  -55964.60885655887 \t -0.14800819098751414\n",
            "81     \t [ 5.67102364 -3.82147844]. \t  -1129.740002070439 \t -0.14800819098751414\n",
            "82     \t [2.139813  4.7225701]. \t  -3607.940565359596 \t -0.14800819098751414\n",
            "83     \t [ 0.83746726 -0.59958682]. \t  \u001b[92m-0.0544817487459128\u001b[0m \t -0.0544817487459128\n",
            "84     \t [-3.2540645  -8.91447672]. \t  -52629.19515382502 \t -0.0544817487459128\n",
            "85     \t [-6.34781947  4.59890045]. \t  -4787.166495711154 \t -0.0544817487459128\n",
            "86     \t [ 4.30284567 -1.03718804]. \t  -20.16521060777707 \t -0.0544817487459128\n",
            "87     \t [-5.72642209 -1.03491284]. \t  -169.07169236201335 \t -0.0544817487459128\n",
            "88     \t [-1.84270439  9.92074067]. \t  -78959.45602882205 \t -0.0544817487459128\n",
            "89     \t [ 5.35086292 -4.73006518]. \t  -3123.046485571771 \t -0.0544817487459128\n",
            "90     \t [ 3.02741216 -1.17590633]. \t  -4.247584090191389 \t -0.0544817487459128\n",
            "91     \t [6.15302721 9.81655461]. \t  -69648.1057042643 \t -0.0544817487459128\n",
            "92     \t [ 8.76963296 -9.60728067]. \t  -61892.78078871723 \t -0.0544817487459128\n",
            "93     \t [-2.97216071  7.09633929]. \t  -21518.27341684187 \t -0.0544817487459128\n",
            "94     \t [-1.85301802 -9.63884415]. \t  -70446.43417333013 \t -0.0544817487459128\n",
            "95     \t [-0.17249246  1.00137998]. \t  -10.862247659516049 \t -0.0544817487459128\n",
            "96     \t [-7.06018259  3.18491234]. \t  -1560.7397815242407 \t -0.0544817487459128\n",
            "97     \t [0.6471761  0.71404779]. \t  -0.40207525925128895 \t -0.0544817487459128\n",
            "98     \t [0.39621054 0.82366194]. \t  -2.210171948225899 \t -0.0544817487459128\n",
            "99     \t [ 5.99666688 -0.69597377]. \t  -75.526395702937 \t -0.0544817487459128\n",
            "100    \t [9.84428029 6.27909718]. \t  -9602.938005260863 \t -0.0544817487459128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "796a8b69-e2bf-4810-b976-c781c47707ce"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_14 = d2GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.95102099 0.14299376]. \t  -20.744156010505872 \t -20.744156010505872\n",
            "init   \t [0.5668276  7.92570408]. \t  -31283.56791285067 \t -20.744156010505872\n",
            "init   \t [3.99982381 4.28594204]. \t  -2152.65365220131 \t -20.744156010505872\n",
            "init   \t [ 4.34676764 -5.54361076]. \t  -6535.783782469756 \t -20.744156010505872\n",
            "init   \t [-6.49690953 -0.86317023]. \t  -183.78911629570374 \t -20.744156010505872\n",
            "1      \t [-3.82909823 -9.9518425 ]. \t  -81556.54609796016 \t -20.744156010505872\n",
            "2      \t [-26.06767037 -28.69463689]. \t  -5597466.742767671 \t -20.744156010505872\n",
            "3      \t [-8.75957512  8.21832577]. \t  -41475.906975640784 \t -20.744156010505872\n",
            "4      \t [-10.16430805  -6.44754238]. \t  -17536.628817035937 \t -20.744156010505872\n",
            "5      \t [-12.4450934    1.11792525]. \t  -627.4531003750728 \t -20.744156010505872\n",
            "6      \t [8.42986383 9.7389967 ]. \t  -65770.10932310983 \t -20.744156010505872\n",
            "7      \t [  2.82968182 -12.22372239]. \t  -175246.11812803123 \t -20.744156010505872\n",
            "8      \t [8.47629598 0.03320735]. \t  -199.51542183217234 \t -20.744156010505872\n",
            "9      \t [ 9.93412348 -8.73957701]. \t  -40878.51622921913 \t -20.744156010505872\n",
            "10     \t [-1.52054017 -3.40177109]. \t  -1223.0413937129836 \t -20.744156010505872\n",
            "11     \t [-4.04855107  3.73708648]. \t  -2070.951930272472 \t -20.744156010505872\n",
            "12     \t [ 9.17290364 -3.71506726]. \t  -746.1663961663916 \t -20.744156010505872\n",
            "13     \t [9.29078075 4.26739481]. \t  -1540.869037274191 \t -20.744156010505872\n",
            "14     \t [-8.4739893  2.5291642]. \t  -994.3553314265954 \t -20.744156010505872\n",
            "15     \t [-0.27698467  1.07452841]. \t  \u001b[92m-15.007626072495281\u001b[0m \t -15.007626072495281\n",
            "16     \t [-9.68372627 -1.77694309]. \t  -626.063916390295 \t -15.007626072495281\n",
            "17     \t [-5.74654396 -4.48886757]. \t  -4286.05925990515 \t -15.007626072495281\n",
            "18     \t [-13.61671515 -12.68326838]. \t  -225129.2835969503 \t -15.007626072495281\n",
            "19     \t [ 4.93462122 -0.58925448]. \t  -51.43948921692635 \t -15.007626072495281\n",
            "20     \t [ 0.30405837 -7.54815389]. \t  -25830.945472488853 \t -15.007626072495281\n",
            "21     \t [0.11384825 3.0448303 ]. \t  -679.9775698182057 \t -15.007626072495281\n",
            "22     \t [ 0.96106515 -0.01807234]. \t  \u001b[92m-1.846298086854463\u001b[0m \t -1.846298086854463\n",
            "23     \t [-0.7228222   0.40180527]. \t  -5.155165035395682 \t -1.846298086854463\n",
            "24     \t [-4.12397981  7.70998486]. \t  -30290.018244203686 \t -1.846298086854463\n",
            "25     \t [ 2.09086457 -2.11002244]. \t  -94.03822670346919 \t -1.846298086854463\n",
            "26     \t [-2.86990943  0.33672408]. \t  -34.15499915685047 \t -1.846298086854463\n",
            "27     \t [1.27794013 0.21785427]. \t  -2.8763193968011858 \t -1.846298086854463\n",
            "28     \t [ 0.70801262 -0.05226983]. \t  \u001b[92m-1.072404996191551\u001b[0m \t -1.072404996191551\n",
            "29     \t [0.57152431 0.08355322]. \t  \u001b[92m-0.8053422534459305\u001b[0m \t -0.8053422534459305\n",
            "30     \t [5.50075624 1.37938407]. \t  -26.00526667623034 \t -0.8053422534459305\n",
            "31     \t [ 0.58951108 -0.70364728]. \t  \u001b[92m-0.4896668791270167\u001b[0m \t -0.4896668791270167\n",
            "32     \t [1.95635856 0.95226182]. \t  -0.9553787638360353 \t -0.4896668791270167\n",
            "33     \t [ 5.71998605 -2.02471248]. \t  -34.56850782683551 \t -0.4896668791270167\n",
            "34     \t [-4.12909203 -0.52282149]. \t  -70.03335985327645 \t -0.4896668791270167\n",
            "35     \t [4.93029513 8.00034005]. \t  -30313.10888632124 \t -0.4896668791270167\n",
            "36     \t [ 0.62923186 -0.36561634]. \t  \u001b[92m-0.39938507686093977\u001b[0m \t -0.39938507686093977\n",
            "37     \t [3.08008045 0.84306213]. \t  -9.828463091071466 \t -0.39938507686093977\n",
            "38     \t [-0.84947208  0.86570661]. \t  -14.450210936954667 \t -0.39938507686093977\n",
            "39     \t [2.21243124 0.62390212]. \t  -5.582262981047484 \t -0.39938507686093977\n",
            "40     \t [ 0.51091779 -0.51984862]. \t  \u001b[92m-0.24094987484770602\u001b[0m \t -0.24094987484770602\n",
            "41     \t [6.25715606 1.6561574 ]. \t  -28.827933546297718 \t -0.24094987484770602\n",
            "42     \t [7.00037581 1.56548397]. \t  -44.81523591915629 \t -0.24094987484770602\n",
            "43     \t [ 5.53478283 -8.8347775 ]. \t  -45364.30057150477 \t -0.24094987484770602\n",
            "44     \t [-7.41313869  3.95445077]. \t  -3064.3810391838615 \t -0.24094987484770602\n",
            "45     \t [-0.11122611  0.46634171]. \t  -1.831438347502695 \t -0.24094987484770602\n",
            "46     \t [-2.11094633 -0.45188439]. \t  -22.372188798572076 \t -0.24094987484770602\n",
            "47     \t [0.14857367 0.66297728]. \t  -1.792199209007368 \t -0.24094987484770602\n",
            "48     \t [-7.94882915 -9.79888739]. \t  -80068.25586658534 \t -0.24094987484770602\n",
            "49     \t [-0.96862353  0.06849528]. \t  -5.788472934535372 \t -0.24094987484770602\n",
            "50     \t [0.77994164 0.56111878]. \t  \u001b[92m-0.09356562668531226\u001b[0m \t -0.09356562668531226\n",
            "51     \t [-9.78985189  3.04528552]. \t  -1722.435073549983 \t -0.09356562668531226\n",
            "52     \t [ 0.89472625 -0.61788589]. \t  \u001b[92m-0.045488607899963125\u001b[0m \t -0.045488607899963125\n",
            "53     \t [0.58653982 0.49130538]. \t  -0.1924890091359318 \t -0.045488607899963125\n",
            "54     \t [0.25171495 1.82199399]. \t  -82.16303464478517 \t -0.045488607899963125\n",
            "55     \t [-7.84198973 -5.30413911]. \t  -8298.306829346673 \t -0.045488607899963125\n",
            "56     \t [2.75543878 1.19495299]. \t  -3.1017202041493457 \t -0.045488607899963125\n",
            "57     \t [ 1.91693084 -0.15184754]. \t  -7.840663390457358 \t -0.045488607899963125\n",
            "58     \t [1.19847245 1.23690276]. \t  -6.968895203993746 \t -0.045488607899963125\n",
            "59     \t [ 0.3671347  -0.01319853]. \t  -0.6695828646335406 \t -0.045488607899963125\n",
            "60     \t [-6.44961741  0.89599703]. \t  -185.27054250434546 \t -0.045488607899963125\n",
            "61     \t [-4.80405407 -9.43966596]. \t  -67025.33281313878 \t -0.045488607899963125\n",
            "62     \t [0.81006753 0.62896389]. \t  \u001b[92m-0.036786978518254514\u001b[0m \t -0.036786978518254514\n",
            "63     \t [-8.14075251  2.79073766]. \t  -1208.563170401845 \t -0.036786978518254514\n",
            "64     \t [ 0.74047536 -0.6718284 ]. \t  -0.11999111469854173 \t -0.036786978518254514\n",
            "65     \t [8.5032172  2.33748767]. \t  -68.05447515116728 \t -0.036786978518254514\n",
            "66     \t [ 2.51530742 -0.89063939]. \t  -4.021608227469106 \t -0.036786978518254514\n",
            "67     \t [ 6.84639051 -1.67780196]. \t  -37.13930494852526 \t -0.036786978518254514\n",
            "68     \t [ 9.95090493 -5.4944436 ]. \t  -5165.866424489724 \t -0.036786978518254514\n",
            "69     \t [8.93950362 1.5297959 ]. \t  -99.31307298132879 \t -0.036786978518254514\n",
            "70     \t [7.55974379 2.20276318]. \t  -52.228749290404686 \t -0.036786978518254514\n",
            "71     \t [ 0.66987384 -0.62645711]. \t  -0.13544394405799282 \t -0.036786978518254514\n",
            "72     \t [ 7.04396952 -2.75382248]. \t  -168.49930000271314 \t -0.036786978518254514\n",
            "73     \t [-1.74409519  0.99945557]. \t  -35.53396008361331 \t -0.036786978518254514\n",
            "74     \t [ 6.27941249 -1.2557734 ]. \t  -47.40943208722048 \t -0.036786978518254514\n",
            "75     \t [-0.50617677 -0.03006774]. \t  -2.7846658102813815 \t -0.036786978518254514\n",
            "76     \t [ 8.24492868 -8.27259918]. \t  -33142.230176738776 \t -0.036786978518254514\n",
            "77     \t [ 3.56308061 -0.85287917]. \t  -15.459027792068735 \t -0.036786978518254514\n",
            "78     \t [-8.28717958 -9.40295244]. \t  -68623.7541473987 \t -0.036786978518254514\n",
            "79     \t [ 6.74504647 -8.46425389]. \t  -37320.518206723325 \t -0.036786978518254514\n",
            "80     \t [0.65252111 0.70065927]. \t  -0.337652446580217 \t -0.036786978518254514\n",
            "81     \t [-7.29670082  2.55242434]. \t  -895.163642157233 \t -0.036786978518254514\n",
            "82     \t [ 4.09203371 -2.19582166]. \t  -71.19302119095501 \t -0.036786978518254514\n",
            "83     \t [ 9.12786332 -1.9961089 ]. \t  -68.7485470765412 \t -0.036786978518254514\n",
            "84     \t [6.09712833 0.76096319]. \t  -74.76812741203328 \t -0.036786978518254514\n",
            "85     \t [4.34873193 1.18204326]. \t  -16.045574395703774 \t -0.036786978518254514\n",
            "86     \t [4.96304012 2.96040326]. \t  -331.4608594387051 \t -0.036786978518254514\n",
            "87     \t [5.32045751 2.20607571]. \t  -57.61694887657228 \t -0.036786978518254514\n",
            "88     \t [8.16694511 2.96724987]. \t  -229.67532676975765 \t -0.036786978518254514\n",
            "89     \t [-0.52126876  7.08258155]. \t  -20342.626382327842 \t -0.036786978518254514\n",
            "90     \t [ 8.15236323 -4.41999009]. \t  -1963.2814553179464 \t -0.036786978518254514\n",
            "91     \t [-6.10918752  7.5731273 ]. \t  -29242.441528680607 \t -0.036786978518254514\n",
            "92     \t [-1.61079141  2.53916057]. \t  -427.6332137066193 \t -0.036786978518254514\n",
            "93     \t [ 4.88716679 -3.65327652]. \t  -966.0864564035858 \t -0.036786978518254514\n",
            "94     \t [-1.54270808  1.49471755]. \t  -78.73127016339454 \t -0.036786978518254514\n",
            "95     \t [2.50158433 4.22111784]. \t  -2197.9902345163655 \t -0.036786978518254514\n",
            "96     \t [ 5.84761504 -3.69315877]. \t  -942.0944713662909 \t -0.036786978518254514\n",
            "97     \t [ 6.34341167 -5.35530953]. \t  -5233.667329281431 \t -0.036786978518254514\n",
            "98     \t [ 9.7014008  -1.18928938]. \t  -170.17915159284428 \t -0.036786978518254514\n",
            "99     \t [ 8.99303271 -1.29363549]. \t  -127.64426897721775 \t -0.036786978518254514\n",
            "100    \t [-3.29970925 -9.55423711]. \t  -69111.27565440805 \t -0.036786978518254514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "56d2c7d9-ccac-4ef4-f986-709946391350"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_15 = d2GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.55763717 -4.88997339]. \t  -4281.404311682752 \t -3267.0472724202245\n",
            "init   \t [-4.94946261  4.19808709]. \t  -3267.0472724202245 \t -3267.0472724202245\n",
            "init   \t [-1.04895274 -5.46114084]. \t  -7372.467127638806 \t -3267.0472724202245\n",
            "init   \t [-1.96580869  7.6451547 ]. \t  -28265.42637457811 \t -3267.0472724202245\n",
            "init   \t [-1.25573066  7.56850362]. \t  -26833.737231776184 \t -3267.0472724202245\n",
            "1      \t [-9.76533982 -4.68487047]. \t  -5874.976174155925 \t -3267.0472724202245\n",
            "2      \t [8.28335803 6.88744078]. \t  -15048.8154067911 \t -3267.0472724202245\n",
            "3      \t [ 9.8362843  -6.65190316]. \t  -12452.665647642647 \t -3267.0472724202245\n",
            "4      \t [ -7.5132263  -15.06660863]. \t  -426071.34368116705 \t -3267.0472724202245\n",
            "5      \t [  6.30126693 -12.74130618]. \t  -202760.38799761262 \t -3267.0472724202245\n",
            "6      \t [-11.60558022   2.50188631]. \t  \u001b[92m-1322.8788683251835\u001b[0m \t -1322.8788683251835\n",
            "7      \t [3.11662658 2.0478742 ]. \t  \u001b[92m-60.04595490711753\u001b[0m \t -60.04595490711753\n",
            "8      \t [ -1.00161832 -12.02068203]. \t  -168198.4515443904 \t -60.04595490711753\n",
            "9      \t [-401.46303377 -401.55736809]. \t  -208526531512.6663 \t -60.04595490711753\n",
            "10     \t [9.33869523 0.13305796]. \t  -242.63611319534775 \t -60.04595490711753\n",
            "11     \t [-1.31327693  0.26187273]. \t  \u001b[92m-9.558753663536542\u001b[0m \t -9.558753663536542\n",
            "12     \t [-9.43319146  9.39489306]. \t  -69272.00418704003 \t -9.558753663536542\n",
            "13     \t [-18.81528821 -17.96913752]. \t  -883765.9252260703 \t -9.558753663536542\n",
            "14     \t [-5.13541416 -1.68790362]. \t  -272.3706473981328 \t -9.558753663536542\n",
            "15     \t [-6.21236802 -9.18113137]. \t  -61161.19930509341 \t -9.558753663536542\n",
            "16     \t [ 5.92511739 -2.82220197]. \t  -224.43804183091714 \t -9.558753663536542\n",
            "17     \t [3.41292332 6.92282708]. \t  -17095.443910178932 \t -9.558753663536542\n",
            "18     \t [ 5.01310713 -7.60769602]. \t  -24543.230092565893 \t -9.558753663536542\n",
            "19     \t [-0.42302743  2.88510736]. \t  -584.8437263443703 \t -9.558753663536542\n",
            "20     \t [-0.16470989 -1.10145798]. \t  -14.784450893194698 \t -9.558753663536542\n",
            "21     \t [6.05794994 1.05514713]. \t  -54.940255254322 \t -9.558753663536542\n",
            "22     \t [-2.53333895  0.22522385]. \t  -26.368726242966403 \t -9.558753663536542\n",
            "23     \t [-1.2302025  -0.28339211]. \t  \u001b[92m-8.84258979830038\u001b[0m \t -8.84258979830038\n",
            "24     \t [-8.65061276  0.06284463]. \t  -243.07397492319214 \t -8.84258979830038\n",
            "25     \t [ 2.12406673 -0.93845658]. \t  \u001b[92m-1.5265781348713974\u001b[0m \t -1.5265781348713974\n",
            "26     \t [ 9.1944841  -2.44941832]. \t  -82.88355600745071 \t -1.5265781348713974\n",
            "27     \t [ 2.55959379 -0.87484459]. \t  -4.5495524754281735 \t -1.5265781348713974\n",
            "28     \t [9.73745205 2.94436258]. \t  -191.89620547833889 \t -1.5265781348713974\n",
            "29     \t [ 0.31754563 -0.33152858]. \t  \u001b[92m-0.48484363007750664\u001b[0m \t -0.48484363007750664\n",
            "30     \t [ 2.50588227 -0.02542472]. \t  -14.81361791877858 \t -0.48484363007750664\n",
            "31     \t [ 1.77231288 -0.66430961]. \t  -2.1795935255634977 \t -0.48484363007750664\n",
            "32     \t [ 0.2690213  -0.08259833]. \t  -0.6647640008862287 \t -0.48484363007750664\n",
            "33     \t [ 1.72066799 -0.88136658]. \t  -0.5751763577586422 \t -0.48484363007750664\n",
            "34     \t [-8.85259951  3.98877067]. \t  -3405.689249257801 \t -0.48484363007750664\n",
            "35     \t [-4.82410634 -4.19458087]. \t  -3236.0203477487757 \t -0.48484363007750664\n",
            "36     \t [ 0.15773305 -0.2597102 ]. \t  -0.7104564240007516 \t -0.48484363007750664\n",
            "37     \t [5.79338369 3.74915229]. \t  -1019.243308431512 \t -0.48484363007750664\n",
            "38     \t [ 1.19764072 -1.00125792]. \t  -1.3428323483178457 \t -0.48484363007750664\n",
            "39     \t [-4.29579632  0.40079165]. \t  -70.68002249426108 \t -0.48484363007750664\n",
            "40     \t [ 2.38815579 -1.26345171]. \t  -3.22130319108295 \t -0.48484363007750664\n",
            "41     \t [0.72932316 0.01992999]. \t  -1.1347742429880943 \t -0.48484363007750664\n",
            "42     \t [3.34364731 0.40012984]. \t  -23.775055968914742 \t -0.48484363007750664\n",
            "43     \t [ 0.91848763 -0.65606032]. \t  \u001b[92m-0.013293005193297873\u001b[0m \t -0.013293005193297873\n",
            "44     \t [ 5.95424587 -0.3960496 ]. \t  -88.17582882496595 \t -0.013293005193297873\n",
            "45     \t [-6.47782027  0.17585729]. \t  -141.45241180245344 \t -0.013293005193297873\n",
            "46     \t [5.50934152 9.97080643]. \t  -74769.15171027063 \t -0.013293005193297873\n",
            "47     \t [ 0.3794039  -0.42480457]. \t  -0.3858229843734759 \t -0.013293005193297873\n",
            "48     \t [-3.12020309 -1.06913238]. \t  -75.4320424103498 \t -0.013293005193297873\n",
            "49     \t [-5.49428234 -6.18945936]. \t  -13527.297032795253 \t -0.013293005193297873\n",
            "50     \t [-5.04355946  9.7782733 ]. \t  -77082.56208637712 \t -0.013293005193297873\n",
            "51     \t [ 0.91774604 -0.55424122]. \t  -0.1908438035179484 \t -0.013293005193297873\n",
            "52     \t [1.64248406 1.28484655]. \t  -5.918524108348353 \t -0.013293005193297873\n",
            "53     \t [1.26374272 0.38186474]. \t  -1.959522322902396 \t -0.013293005193297873\n",
            "54     \t [ 3.80932269 -1.69945529]. \t  -15.630266518663795 \t -0.013293005193297873\n",
            "55     \t [ 0.66427154 -9.57772303]. \t  -66832.7325070629 \t -0.013293005193297873\n",
            "56     \t [2.43116934 5.18720372]. \t  -5282.4727117377015 \t -0.013293005193297873\n",
            "57     \t [-8.41074348 -8.70603717]. \t  -51289.14438183774 \t -0.013293005193297873\n",
            "58     \t [ 3.70757859 -0.59093285]. \t  -25.44125389780309 \t -0.013293005193297873\n",
            "59     \t [-4.38452908 -0.14070639]. \t  -68.13892913132634 \t -0.013293005193297873\n",
            "60     \t [0.6782251  0.33823138]. \t  -0.507503248292467 \t -0.013293005193297873\n",
            "61     \t [2.42559477 1.49682716]. \t  -10.481562535705507 \t -0.013293005193297873\n",
            "62     \t [ 3.64445271 -4.63530097]. \t  -3100.30982698243 \t -0.013293005193297873\n",
            "63     \t [1.36356184 0.61125589]. \t  -0.891814564927714 \t -0.013293005193297873\n",
            "64     \t [ 2.89586453 -3.25500391]. \t  -672.952472255595 \t -0.013293005193297873\n",
            "65     \t [-5.25129345  2.12510715]. \t  -447.11280209366765 \t -0.013293005193297873\n",
            "66     \t [3.33598621 2.31905593]. \t  -115.57125351130023 \t -0.013293005193297873\n",
            "67     \t [0.29177252 3.76797388]. \t  -1580.112963263459 \t -0.013293005193297873\n",
            "68     \t [-0.84374514  1.32104148]. \t  -40.967311074381485 \t -0.013293005193297873\n",
            "69     \t [ 0.12848401 -7.7581422 ]. \t  -28920.430898846866 \t -0.013293005193297873\n",
            "70     \t [ 2.20001352 -6.36777374]. \t  -12450.935743667296 \t -0.013293005193297873\n",
            "71     \t [-2.03787126  1.4285247 ]. \t  -84.118782606141 \t -0.013293005193297873\n",
            "72     \t [4.93270146 4.87445417]. \t  -3642.9255103394166 \t -0.013293005193297873\n",
            "73     \t [7.34644442 2.37431617]. \t  -71.1405977914262 \t -0.013293005193297873\n",
            "74     \t [-9.32629639  3.54705748]. \t  -2485.687738481979 \t -0.013293005193297873\n",
            "75     \t [ 2.75759941 -7.14404541]. \t  -19730.88974639189 \t -0.013293005193297873\n",
            "76     \t [ 4.01917162 -7.50134124]. \t  -23562.76031559984 \t -0.013293005193297873\n",
            "77     \t [-2.44688676  3.52765592]. \t  -1506.3510075462314 \t -0.013293005193297873\n",
            "78     \t [ 9.83384425 -0.69132109]. \t  -235.67437894663698 \t -0.013293005193297873\n",
            "79     \t [1.37856597 1.06985139]. \t  -1.8016897699415142 \t -0.013293005193297873\n",
            "80     \t [ 3.1859508  -2.08578183]. \t  -65.60929090224818 \t -0.013293005193297873\n",
            "81     \t [7.74797399 9.81281296]. \t  -68373.2105317395 \t -0.013293005193297873\n",
            "82     \t [0.519504   5.28291595]. \t  -6116.167104381529 \t -0.013293005193297873\n",
            "83     \t [2.79745241 1.07069193]. \t  -3.7402591249771278 \t -0.013293005193297873\n",
            "84     \t [ 3.80644068 -1.4545047 ]. \t  -8.236895645000224 \t -0.013293005193297873\n",
            "85     \t [ 4.84000247 -1.61495898]. \t  -15.028645552024376 \t -0.013293005193297873\n",
            "86     \t [-4.91063715  3.2103253 ]. \t  -1337.7841449282078 \t -0.013293005193297873\n",
            "87     \t [-8.26180241 -0.01564831]. \t  -222.31192709882077 \t -0.013293005193297873\n",
            "88     \t [-3.00665798  0.68913278]. \t  -47.36055381313222 \t -0.013293005193297873\n",
            "89     \t [-1.71217764 -0.83311535]. \t  -26.580124084270754 \t -0.013293005193297873\n",
            "90     \t [-2.89804816 -1.01231984]. \t  -64.15288597645016 \t -0.013293005193297873\n",
            "91     \t [6.47232451 0.25390567]. \t  -110.42348666866121 \t -0.013293005193297873\n",
            "92     \t [4.75370955 1.02145183]. \t  -28.31592011183271 \t -0.013293005193297873\n",
            "93     \t [-9.03410237  6.33762444]. \t  -16072.914496692341 \t -0.013293005193297873\n",
            "94     \t [7.47848886 4.32620845]. \t  -1836.4155354062034 \t -0.013293005193297873\n",
            "95     \t [ 1.90123312 -1.63750941]. \t  -24.778138253256913 \t -0.013293005193297873\n",
            "96     \t [6.19233357 2.00555006]. \t  -33.821087852470455 \t -0.013293005193297873\n",
            "97     \t [-0.75975625  0.48501702]. \t  -6.123719374525755 \t -0.013293005193297873\n",
            "98     \t [ 0.9744419  -0.64215307]. \t  -0.045485839241017643 \t -0.013293005193297873\n",
            "99     \t [-4.26426686 -2.52050317]. \t  -603.6837600478647 \t -0.013293005193297873\n",
            "100    \t [-1.35835707  0.83005507]. \t  -20.53696028083418 \t -0.013293005193297873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "f010cf21-e661-4ec2-e3bc-47489700073f"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_16 = d2GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.27896664 -3.67267997]. \t  -815.4627708918207 \t -28.992408538517264\n",
            "init   \t [3.44301557 0.15823335]. \t  -28.992408538517264 \t -28.992408538517264\n",
            "init   \t [ 5.63320853 -4.32636643]. \t  -2044.16089755496 \t -28.992408538517264\n",
            "init   \t [-5.32582432  1.27599373]. \t  -187.32245577722023 \t -28.992408538517264\n",
            "init   \t [7.50048729 4.37885841]. \t  -1945.4933626881252 \t -28.992408538517264\n",
            "1      \t [-2.44633796 -8.71755625]. \t  -47714.014485289874 \t -28.992408538517264\n",
            "2      \t [-0.94408028  9.32615359]. \t  -61182.65342174163 \t -28.992408538517264\n",
            "3      \t [-9.58291173  9.55394704]. \t  -73946.57346618871 \t -28.992408538517264\n",
            "4      \t [-9.96784472 -3.20585558]. \t  -1983.5841560879148 \t -28.992408538517264\n",
            "5      \t [  5.15023648 -11.07635602]. \t  -115429.56768716991 \t -28.992408538517264\n",
            "6      \t [-12.31760846  -9.53861255]. \t  -75672.90949663433 \t -28.992408538517264\n",
            "7      \t [-1.00187218 -2.41726176]. \t  -325.98736853159295 \t -28.992408538517264\n",
            "8      \t [-12.47791271 -16.79180699]. \t  -664672.8118333581 \t -28.992408538517264\n",
            "9      \t [-10.26422788   2.53137753]. \t  -1192.2530713133217 \t -28.992408538517264\n",
            "10     \t [2.61475510e-03 3.14594745e+00]. \t  -784.3903237365029 \t -28.992408538517264\n",
            "11     \t [8.87535711 9.17115881]. \t  -50843.647954551314 \t -28.992408538517264\n",
            "12     \t [ 9.83171549 -7.92230725]. \t  -26848.289891580538 \t -28.992408538517264\n",
            "13     \t [-5.41933399  5.7120616 ]. \t  -10031.014327157933 \t -28.992408538517264\n",
            "14     \t [-4.98143951 -3.92257847]. \t  -2592.572373034905 \t -28.992408538517264\n",
            "15     \t [-7.28963584 -9.92971163]. \t  -83699.38760514265 \t -28.992408538517264\n",
            "16     \t [ 9.29652411 -0.35907685]. \t  -232.22676289251507 \t -28.992408538517264\n",
            "17     \t [3.88862691 7.7632648 ]. \t  -27221.82512077568 \t -28.992408538517264\n",
            "18     \t [ 1.96098032 -6.68393273]. \t  -15274.574757862449 \t -28.992408538517264\n",
            "19     \t [ 2.46334558 -2.30520798]. \t  -135.46348811935638 \t -28.992408538517264\n",
            "20     \t [6.15393724 0.11551196]. \t  -101.64948390800413 \t -28.992408538517264\n",
            "21     \t [-2.8637721   1.23897574]. \t  -85.35092801391481 \t -28.992408538517264\n",
            "22     \t [4.00665987 2.59155578]. \t  -186.72624446466122 \t -28.992408538517264\n",
            "23     \t [-7.43667103 -0.69237447]. \t  -212.14410394843082 \t -28.992408538517264\n",
            "24     \t [ 9.94693951 -2.70869861]. \t  -124.73974959393753 \t -28.992408538517264\n",
            "25     \t [9.80181867 2.28416514]. \t  -78.27339515715359 \t -28.992408538517264\n",
            "26     \t [ 0.03095551 -0.10316497]. \t  \u001b[92m-0.9392342300294892\u001b[0m \t -0.9392342300294892\n",
            "27     \t [ 3.31573685 -7.15251266]. \t  -19607.813279788614 \t -0.9392342300294892\n",
            "28     \t [1.72486654 1.0790149 ]. \t  -1.254290081412941 \t -0.9392342300294892\n",
            "29     \t [1.39946598 0.37499563]. \t  -2.6604163245842853 \t -0.9392342300294892\n",
            "30     \t [-3.37457042 -1.12149797]. \t  -88.52308828853533 \t -0.9392342300294892\n",
            "31     \t [ 4.29381873 -0.67409973]. \t  -33.76566262212742 \t -0.9392342300294892\n",
            "32     \t [-0.73243173  0.40927741]. \t  -5.280209007045091 \t -0.9392342300294892\n",
            "33     \t [2.13274792 0.64859147]. \t  -4.618577423210709 \t -0.9392342300294892\n",
            "34     \t [-5.99232396 -1.89380901]. \t  -395.54542886528327 \t -0.9392342300294892\n",
            "35     \t [-0.7866281  -0.22956565]. \t  -4.78347098327445 \t -0.9392342300294892\n",
            "36     \t [0.8963526  0.46992399]. \t  \u001b[92m-0.42423876496488916\u001b[0m \t -0.42423876496488916\n",
            "37     \t [1.23288468 0.60747076]. \t  -0.5439749133013851 \t -0.42423876496488916\n",
            "38     \t [-5.57935559  9.92177697]. \t  -82025.55497900158 \t -0.42423876496488916\n",
            "39     \t [0.68323943 0.44123089]. \t  \u001b[92m-0.27305644178590766\u001b[0m \t -0.27305644178590766\n",
            "40     \t [-9.67901655  5.26295808]. \t  -8583.935849294823 \t -0.27305644178590766\n",
            "41     \t [-1.2770428   0.27378714]. \t  -9.25736274573276 \t -0.27305644178590766\n",
            "42     \t [7.39423722 9.80328097]. \t  -68353.61004999385 \t -0.27305644178590766\n",
            "43     \t [0.00810804 0.10082911]. \t  -0.984148561613974 \t -0.27305644178590766\n",
            "44     \t [-9.40136635 -6.39482722]. \t  -16739.054696567113 \t -0.27305644178590766\n",
            "45     \t [-2.99252903 -8.06234898]. \t  -35391.53099516104 \t -0.27305644178590766\n",
            "46     \t [0.82950982 0.01096577]. \t  -1.4044421387685726 \t -0.27305644178590766\n",
            "47     \t [ 0.8217099  -1.08503861]. \t  -4.7313991629964836 \t -0.27305644178590766\n",
            "48     \t [1.53002406 0.66713064]. \t  -1.0998630421277484 \t -0.27305644178590766\n",
            "49     \t [4.67432013 1.10826181]. \t  -23.338183015419787 \t -0.27305644178590766\n",
            "50     \t [-0.15033532 -0.49878295]. \t  -2.162831006135211 \t -0.27305644178590766\n",
            "51     \t [-8.03251586  4.12578886]. \t  -3622.4977333168395 \t -0.27305644178590766\n",
            "52     \t [-3.29673452 -4.93157514]. \t  -5413.4907693308 \t -0.27305644178590766\n",
            "53     \t [1.45497367 0.86576101]. \t  \u001b[92m-0.21089253083478177\u001b[0m \t -0.21089253083478177\n",
            "54     \t [ 1.09708419 -0.20178053]. \t  -2.072529108057234 \t -0.21089253083478177\n",
            "55     \t [ 0.71733069 -0.46317606]. \t  -0.24609716901289924 \t -0.21089253083478177\n",
            "56     \t [ 7.04365201 -4.0924781 ]. \t  -1436.0589353566659 \t -0.21089253083478177\n",
            "57     \t [-5.87599396  3.20777726]. \t  -1447.0835971801562 \t -0.21089253083478177\n",
            "58     \t [ 8.52377976 -9.26055523]. \t  -53189.38260389485 \t -0.21089253083478177\n",
            "59     \t [ 6.69722723 -1.72533586]. \t  -33.56445716067143 \t -0.21089253083478177\n",
            "60     \t [ 8.19433769 -2.12959435]. \t  -53.2932694898483 \t -0.21089253083478177\n",
            "61     \t [ 7.91063794 -2.65513478]. \t  -124.36048262371241 \t -0.21089253083478177\n",
            "62     \t [ 6.61537771 -1.9690889 ]. \t  -34.12822279685744 \t -0.21089253083478177\n",
            "63     \t [-0.94734154 -2.76240333]. \t  -529.2610645527533 \t -0.21089253083478177\n",
            "64     \t [-1.92418304 -0.8987766 ]. \t  -33.61095675223377 \t -0.21089253083478177\n",
            "65     \t [ 8.47139825 -1.67110709]. \t  -72.48209795439618 \t -0.21089253083478177\n",
            "66     \t [-1.30997513 -1.34809735]. \t  -54.236260248653295 \t -0.21089253083478177\n",
            "67     \t [ 0.45837197 -0.9992417 ]. \t  -5.0279160260922495 \t -0.21089253083478177\n",
            "68     \t [ 7.17931782 -0.57421152]. \t  -123.20164106679526 \t -0.21089253083478177\n",
            "69     \t [6.33797836 5.1826123 ]. \t  -4518.405148097174 \t -0.21089253083478177\n",
            "70     \t [7.72731257 1.62788751]. \t  -57.04008227305787 \t -0.21089253083478177\n",
            "71     \t [-0.0710583   7.88508369]. \t  -30961.893643346364 \t -0.21089253083478177\n",
            "72     \t [-2.11605301 -0.3652028 ]. \t  -21.06525019650725 \t -0.21089253083478177\n",
            "73     \t [3.25089107 3.65040972]. \t  -1100.1950384901293 \t -0.21089253083478177\n",
            "74     \t [3.73819659 1.41966354]. \t  -7.66905843336126 \t -0.21089253083478177\n",
            "75     \t [-6.52094594  0.82226348]. \t  -180.5385669132019 \t -0.21089253083478177\n",
            "76     \t [-5.53701779  5.0656976 ]. \t  -6508.76130733897 \t -0.21089253083478177\n",
            "77     \t [9.95284226 4.56920198]. \t  -2102.9350150547402 \t -0.21089253083478177\n",
            "78     \t [-0.10241599  0.11671667]. \t  -1.2489452582909175 \t -0.21089253083478177\n",
            "79     \t [9.81712769 0.91766775]. \t  -210.0298487638273 \t -0.21089253083478177\n",
            "80     \t [4.66460779 1.9894999 ]. \t  -34.575310771987304 \t -0.21089253083478177\n",
            "81     \t [-9.64089881 -9.7796186 ]. \t  -80853.1547843824 \t -0.21089253083478177\n",
            "82     \t [ 6.88349109 -5.5330375 ]. \t  -5941.485893656064 \t -0.21089253083478177\n",
            "83     \t [-4.22580767  0.60387942]. \t  -76.41605660863972 \t -0.21089253083478177\n",
            "84     \t [ 0.45225865 -0.46125052]. \t  -0.30145219496280545 \t -0.21089253083478177\n",
            "85     \t [ 5.4012867  -1.23645833]. \t  -30.356511785948996 \t -0.21089253083478177\n",
            "86     \t [1.15789466 5.79949124]. \t  -8741.169154549583 \t -0.21089253083478177\n",
            "87     \t [ 2.54599126 -8.46775776]. \t  -39685.38564442981 \t -0.21089253083478177\n",
            "88     \t [-8.10679451 -3.97384878]. \t  -3233.485078639014 \t -0.21089253083478177\n",
            "89     \t [0.61124987 4.40812793]. \t  -2926.5720044905806 \t -0.21089253083478177\n",
            "90     \t [ 1.33338474 -6.7834835 ]. \t  -16452.33364704236 \t -0.21089253083478177\n",
            "91     \t [-2.91648563  2.8480816 ]. \t  -747.9892165632863 \t -0.21089253083478177\n",
            "92     \t [-0.34321087 -7.74449847]. \t  -28944.888935127223 \t -0.21089253083478177\n",
            "93     \t [ 1.30104999 -0.87709612]. \t  \u001b[92m-0.2034865506675208\u001b[0m \t -0.2034865506675208\n",
            "94     \t [ 1.77727635 -0.29097903]. \t  -5.77509259372026 \t -0.2034865506675208\n",
            "95     \t [-3.30205712  8.57469757]. \t  -45230.517092884154 \t -0.2034865506675208\n",
            "96     \t [ 3.00188445 -1.29398595]. \t  -4.248241124859602 \t -0.2034865506675208\n",
            "97     \t [-5.68991653  6.92891146]. \t  -20734.417945070247 \t -0.2034865506675208\n",
            "98     \t [5.05108621 5.33718012]. \t  -5407.76273916 \t -0.2034865506675208\n",
            "99     \t [ 5.88009937 -1.63139406]. \t  -24.43632747351645 \t -0.2034865506675208\n",
            "100    \t [-6.40214562  9.5152672 ]. \t  -70354.36649607628 \t -0.2034865506675208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "5b11d3ff-8480-4be0-9c3d-137abe991142"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_17 = d2GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.03648056 4.61039249]. \t  -3120.691750998894 \t -133.8839693070206\n",
            "init   \t [-6.13265275  0.39354476]. \t  -133.8839693070206 \t -133.8839693070206\n",
            "init   \t [3.52766533 6.08889746]. \t  -9981.232217710998 \t -133.8839693070206\n",
            "init   \t [0.67025729 8.16068359]. \t  -35124.935571397844 \t -133.8839693070206\n",
            "init   \t [-1.13037742 -4.8805558 ]. \t  -4761.569630397088 \t -133.8839693070206\n",
            "1      \t [ 9.47756828 -1.18144623]. \t  -161.27269462973322 \t -133.8839693070206\n",
            "2      \t [-9.85290042 -8.55285713]. \t  -48886.9473889917 \t -133.8839693070206\n",
            "3      \t [-9.20794452  7.56905046]. \t  -30751.632331903027 \t -133.8839693070206\n",
            "4      \t [ 9.29095451 -9.9002349 ]. \t  -69811.16084951065 \t -133.8839693070206\n",
            "5      \t [9.65812984 9.94541442]. \t  -70886.65075415972 \t -133.8839693070206\n",
            "6      \t [  2.32802534 -12.12842359]. \t  -170377.13124064243 \t -133.8839693070206\n",
            "7      \t [-12.27451776  -1.41143953]. \t  -704.9126336870589 \t -133.8839693070206\n",
            "8      \t [ -4.41759174 -12.02174558]. \t  -172269.6232871119 \t -133.8839693070206\n",
            "9      \t [-24.69576913 -18.96019982]. \t  -1106762.6761782975 \t -133.8839693070206\n",
            "10     \t [ 3.76611465 -0.97520272]. \t  \u001b[92m-14.600933763827676\u001b[0m \t -14.600933763827676\n",
            "11     \t [-1.77917988  2.7276728 ]. \t  -562.8068971189019 \t -14.600933763827676\n",
            "12     \t [-15.17385241 -18.48549719]. \t  -976348.49002539 \t -14.600933763827676\n",
            "13     \t [-28.14418228 -31.88826779]. \t  -8503443.957428852 \t -14.600933763827676\n",
            "14     \t [ 4.36638703 -5.04689088]. \t  -4349.948075297572 \t -14.600933763827676\n",
            "15     \t [9.10259575 3.76068642]. \t  -801.6215813273823 \t -14.600933763827676\n",
            "16     \t [-6.80280046 -3.91495075]. \t  -2866.8593667113573 \t -14.600933763827676\n",
            "17     \t [-4.14762975  6.09236623]. \t  -12313.8116109825 \t -14.600933763827676\n",
            "18     \t [-9.41194106  2.25634125]. \t  -876.2654584452982 \t -14.600933763827676\n",
            "19     \t [5.97673344 0.84166637]. \t  -66.35377890437091 \t -14.600933763827676\n",
            "20     \t [ 1.21595857 -0.57304888]. \t  \u001b[92m-0.6720217232245321\u001b[0m \t -0.6720217232245321\n",
            "21     \t [1.81615087 0.60771996]. \t  -2.988131008351363 \t -0.6720217232245321\n",
            "22     \t [ 8.29291133 -4.14642715]. \t  -1414.8554883662055 \t -0.6720217232245321\n",
            "23     \t [1.44577925 1.67789363]. \t  -35.225073284875805 \t -0.6720217232245321\n",
            "24     \t [ 1.66809502 -0.35033549]. \t  -4.494075401198297 \t -0.6720217232245321\n",
            "25     \t [-2.95416191 -1.14883584]. \t  -78.21680514925553 \t -0.6720217232245321\n",
            "26     \t [-5.14676227  9.79389388]. \t  -77645.91488116814 \t -0.6720217232245321\n",
            "27     \t [ 0.1216153 -0.3818232]. \t  -0.8293342721975749 \t -0.6720217232245321\n",
            "28     \t [-4.70679914 -7.03119119]. \t  -21491.06968409885 \t -0.6720217232245321\n",
            "29     \t [ 6.54257869 -1.27232828]. \t  -52.56543751486487 \t -0.6720217232245321\n",
            "30     \t [ 0.78085351 -1.11597829]. \t  -5.89596239273167 \t -0.6720217232245321\n",
            "31     \t [ 1.75450306 -2.22842517]. \t  -134.30425328859306 \t -0.6720217232245321\n",
            "32     \t [ 1.43319463 -0.14279686]. \t  -4.065284073207399 \t -0.6720217232245321\n",
            "33     \t [2.57308957 1.12777627]. \t  -2.4763314142857045 \t -0.6720217232245321\n",
            "34     \t [4.53817702 9.84643969]. \t  -71731.91582202556 \t -0.6720217232245321\n",
            "35     \t [-0.48462041 -8.5446659 ]. \t  -42930.953323449256 \t -0.6720217232245321\n",
            "36     \t [2.80349425 0.47333055]. \t  -14.348509999428376 \t -0.6720217232245321\n",
            "37     \t [2.93988987 1.72636381]. \t  -22.013325690583063 \t -0.6720217232245321\n",
            "38     \t [-6.77409895  3.22254106]. \t  -1577.7409058667745 \t -0.6720217232245321\n",
            "39     \t [-0.68256731 -0.58645986]. \t  -6.587231388977374 \t -0.6720217232245321\n",
            "40     \t [ 0.79948955 -0.20138059]. \t  -1.07234771750924 \t -0.6720217232245321\n",
            "41     \t [ 1.05559519 -0.79244797]. \t  \u001b[92m-0.08337296773828692\u001b[0m \t -0.08337296773828692\n",
            "42     \t [ 4.74722308 -1.59289788]. \t  -14.256094079871044 \t -0.08337296773828692\n",
            "43     \t [-1.69644698 -0.24942248]. \t  -13.901962360533801 \t -0.08337296773828692\n",
            "44     \t [-8.53620819 -0.85337201]. \t  -290.6472041032862 \t -0.08337296773828692\n",
            "45     \t [6.31147133 3.49509339]. \t  -684.8721446397576 \t -0.08337296773828692\n",
            "46     \t [-0.74658915 -0.01139226]. \t  -4.166139677603834 \t -0.08337296773828692\n",
            "47     \t [ 4.79508243 -8.60485142]. \t  -41079.5131057878 \t -0.08337296773828692\n",
            "48     \t [2.10301427 1.18099331]. \t  -2.159139369332243 \t -0.08337296773828692\n",
            "49     \t [-4.4109564   0.92122091]. \t  -103.89994245760413 \t -0.08337296773828692\n",
            "50     \t [0.19352757 0.2834139 ]. \t  -0.6525600541627694 \t -0.08337296773828692\n",
            "51     \t [ 0.06351415 -0.01884392]. \t  -0.8848944278618075 \t -0.08337296773828692\n",
            "52     \t [-8.87122825 -6.11949031]. \t  -14131.446577500716 \t -0.08337296773828692\n",
            "53     \t [-0.82920668  6.66522266]. \t  -16088.204723007937 \t -0.08337296773828692\n",
            "54     \t [ 4.99273018 -0.61206811]. \t  -51.956062011842626 \t -0.08337296773828692\n",
            "55     \t [ 0.8033026  -0.91421729]. \t  -1.5465237091756487 \t -0.08337296773828692\n",
            "56     \t [ 1.92185207 -6.41993558]. \t  -12964.342570814926 \t -0.08337296773828692\n",
            "57     \t [7.35570631 0.57277674]. \t  -130.1632091411202 \t -0.08337296773828692\n",
            "58     \t [1.29916854 0.4499114 ]. \t  -1.689146936362883 \t -0.08337296773828692\n",
            "59     \t [ 0.03828497 -4.77979984]. \t  -4169.625990517897 \t -0.08337296773828692\n",
            "60     \t [-5.50634066  4.88851737]. \t  -5724.441837423282 \t -0.08337296773828692\n",
            "61     \t [8.76679784 4.29391193]. \t  -1640.5056011511847 \t -0.08337296773828692\n",
            "62     \t [-8.98890867  0.60653298]. \t  -288.9168339602197 \t -0.08337296773828692\n",
            "63     \t [4.67105362 7.62456185]. \t  -24921.18303646411 \t -0.08337296773828692\n",
            "64     \t [ 2.45485905 -8.43743202]. \t  -39160.49674376253 \t -0.08337296773828692\n",
            "65     \t [0.34576108 0.29031239]. \t  -0.4908271938161843 \t -0.08337296773828692\n",
            "66     \t [9.39201907 2.42110985]. \t  -81.29801818090317 \t -0.08337296773828692\n",
            "67     \t [-9.3030702   9.35362925]. \t  -68027.21372148508 \t -0.08337296773828692\n",
            "68     \t [-9.0776245  -0.18304074]. \t  -268.8071160142377 \t -0.08337296773828692\n",
            "69     \t [4.25934126 0.98271259]. \t  -21.461478847839793 \t -0.08337296773828692\n",
            "70     \t [ 2.84368001 -0.61791472]. \t  -12.05231203727452 \t -0.08337296773828692\n",
            "71     \t [1.25853039 7.04421599]. \t  -19201.570925244465 \t -0.08337296773828692\n",
            "72     \t [ 1.34013451 -0.6688151 ]. \t  -0.5126449020287062 \t -0.08337296773828692\n",
            "73     \t [9.89242458 2.94817077]. \t  -191.30529176924784 \t -0.08337296773828692\n",
            "74     \t [-3.02927854 -1.38291775]. \t  -110.19524358375693 \t -0.08337296773828692\n",
            "75     \t [7.71939719 1.3140332 ]. \t  -81.54833457870807 \t -0.08337296773828692\n",
            "76     \t [-2.74705742  0.466683  ]. \t  -34.298878200472714 \t -0.08337296773828692\n",
            "77     \t [-5.41577237  1.00350708]. \t  -151.56674301338865 \t -0.08337296773828692\n",
            "78     \t [-4.97692702  8.28742049]. \t  -40556.84917917206 \t -0.08337296773828692\n",
            "79     \t [9.37697898 0.9479332 ]. \t  -185.08124840416218 \t -0.08337296773828692\n",
            "80     \t [4.92905531 1.29978043]. \t  -20.243697047984963 \t -0.08337296773828692\n",
            "81     \t [8.58357118 2.3323771 ]. \t  -68.05740874267343 \t -0.08337296773828692\n",
            "82     \t [9.50807662 9.54940402]. \t  -59843.33626052231 \t -0.08337296773828692\n",
            "83     \t [ 3.67276286 -6.86781614]. \t  -16445.989871299862 \t -0.08337296773828692\n",
            "84     \t [-7.85460815  5.72821771]. \t  -10876.896890461632 \t -0.08337296773828692\n",
            "85     \t [9.48298339 8.28908303]. \t  -32806.59307027537 \t -0.08337296773828692\n",
            "86     \t [-3.61736309 -8.07696956]. \t  -35982.776385716104 \t -0.08337296773828692\n",
            "87     \t [-2.22392311  8.04670637]. \t  -34712.23316395252 \t -0.08337296773828692\n",
            "88     \t [-2.77815484 -3.57040748]. \t  -1613.0870410767286 \t -0.08337296773828692\n",
            "89     \t [6.22206129 1.86430453]. \t  -28.333393614984992 \t -0.08337296773828692\n",
            "90     \t [0.07531789 6.78340765]. \t  -16911.902094659115 \t -0.08337296773828692\n",
            "91     \t [-4.38835143 -1.15190316]. \t  -128.21704825640293 \t -0.08337296773828692\n",
            "92     \t [-4.91703696 -6.17035295]. \t  -13177.608925696788 \t -0.08337296773828692\n",
            "93     \t [ 1.20769166 -0.79984243]. \t  \u001b[92m-0.053447504305350696\u001b[0m \t -0.053447504305350696\n",
            "94     \t [-1.44728277  3.12767244]. \t  -888.9934981790411 \t -0.053447504305350696\n",
            "95     \t [ 3.71376938 -3.98894139]. \t  -1587.6567265140147 \t -0.053447504305350696\n",
            "96     \t [-2.85792593  1.3685304 ]. \t  -102.10068794850362 \t -0.053447504305350696\n",
            "97     \t [-1.48768052  0.89607536]. \t  -25.329061398933813 \t -0.053447504305350696\n",
            "98     \t [6.82112242 0.20006335]. \t  -124.76956260580201 \t -0.053447504305350696\n",
            "99     \t [8.1847803  9.48906865]. \t  -59150.8875324524 \t -0.053447504305350696\n",
            "100    \t [2.62822905 7.69920127]. \t  -26880.869212000907 \t -0.053447504305350696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "2d41215b-4d88-4546-fd93-b1a49f74ed20"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_18 = d2GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [7.46858806 9.37081326]. \t  -56594.51755971677 \t -0.3114572247523004\n",
            "init   \t [7.3838908  0.61711383]. \t  -128.46197097057788 \t -0.3114572247523004\n",
            "init   \t [-5.34543344 -9.77202391]. \t  -77131.46635795479 \t -0.3114572247523004\n",
            "init   \t [-1.39062363 -1.9529728 ]. \t  -168.39363959555394 \t -0.3114572247523004\n",
            "init   \t [ 0.45349343 -0.43216408]. \t  -0.3114572247523004 \t -0.3114572247523004\n",
            "1      \t [1.13550865 0.60170966]. \t  -0.3568618976266401 \t -0.3114572247523004\n",
            "2      \t [ 3.31968838 -1.02798371]. \t  -8.290730231475042 \t -0.3114572247523004\n",
            "3      \t [ 1.55677101 -0.46918709]. \t  -2.8031293078482102 \t -0.3114572247523004\n",
            "4      \t [0.21033772 1.30032928]. \t  -20.73880231778645 \t -0.3114572247523004\n",
            "5      \t [ 1.16777959 -0.15129036]. \t  -2.5459272005116174 \t -0.3114572247523004\n",
            "6      \t [2.47418429 2.01618701]. \t  -66.15017672068348 \t -0.3114572247523004\n",
            "7      \t [ 7.75378082 -9.0667772 ]. \t  -49129.77128708541 \t -0.3114572247523004\n",
            "8      \t [ 1.71731576 -1.58989539]. \t  -22.801953280491965 \t -0.3114572247523004\n",
            "9      \t [-7.62617408  6.75933541]. \t  -19677.75907934677 \t -0.3114572247523004\n",
            "10     \t [1.80849446 0.14477759]. \t  -6.895226115446554 \t -0.3114572247523004\n",
            "11     \t [-8.8857121  -1.85868522]. \t  -596.6997945649014 \t -0.3114572247523004\n",
            "12     \t [-12.56344999  -9.79237344]. \t  -83697.40258176033 \t -0.3114572247523004\n",
            "13     \t [-0.89233545  8.68263527]. \t  -46010.33718604979 \t -0.3114572247523004\n",
            "14     \t [ 1.2686685  -8.17219803]. \t  -35007.16694788161 \t -0.3114572247523004\n",
            "15     \t [-4.97314339  1.13487337]. \t  -149.65380277158954 \t -0.3114572247523004\n",
            "16     \t [ 9.64534257 -3.2686765 ]. \t  -349.6064176863371 \t -0.3114572247523004\n",
            "17     \t [8.9182694  4.38311782]. \t  -1803.8095957980202 \t -0.3114572247523004\n",
            "18     \t [ 5.5629867  -4.38022066]. \t  -2173.7709569319045 \t -0.3114572247523004\n",
            "19     \t [-15.66957913  -2.56339552]. \t  -1938.088267031291 \t -0.3114572247523004\n",
            "20     \t [-10.6587515    2.19812292]. \t  -961.9133799100691 \t -0.3114572247523004\n",
            "21     \t [0.58763415 0.08320862]. \t  -0.8285081806874215 \t -0.3114572247523004\n",
            "22     \t [-5.27154561 -4.0510081 ]. \t  -2941.4671141840217 \t -0.3114572247523004\n",
            "23     \t [-2.34801684  3.71911537]. \t  -1812.6080989225643 \t -0.3114572247523004\n",
            "24     \t [4.12617506 0.42047136]. \t  -38.23773138618567 \t -0.3114572247523004\n",
            "25     \t [3.81677572 5.39317514]. \t  -5917.062066422699 \t -0.3114572247523004\n",
            "26     \t [-9.32224177 -5.02565783]. \t  -7267.413617078394 \t -0.3114572247523004\n",
            "27     \t [-1.10997155 -4.81512089]. \t  -4513.295194941723 \t -0.3114572247523004\n",
            "28     \t [ 2.47176408 -3.200645  ]. \t  -651.3541138582052 \t -0.3114572247523004\n",
            "29     \t [ 1.02651036 -0.45605739]. \t  -0.746205530260903 \t -0.3114572247523004\n",
            "30     \t [-5.37682421e-01 -3.09277148e-04]. \t  -2.9426724109848945 \t -0.3114572247523004\n",
            "31     \t [ 1.08946317 -1.08458702]. \t  -3.199326099425357 \t -0.3114572247523004\n",
            "32     \t [-2.6048032  -0.22299686]. \t  -27.620633109456243 \t -0.3114572247523004\n",
            "33     \t [ 0.83978172 -0.30290454]. \t  -0.8870751760697538 \t -0.3114572247523004\n",
            "34     \t [ 0.01735151 -0.3165693 ]. \t  -1.0326351704235495 \t -0.3114572247523004\n",
            "35     \t [9.7801873  0.77394804]. \t  -224.39987157760925 \t -0.3114572247523004\n",
            "36     \t [-6.82928239  2.46567943]. \t  -782.4187970264329 \t -0.3114572247523004\n",
            "37     \t [ 4.55467229 -1.1813127 ]. \t  -18.85677924638068 \t -0.3114572247523004\n",
            "38     \t [ 6.70995594 -1.43881801]. \t  -45.80888830609692 \t -0.3114572247523004\n",
            "39     \t [3.22811239 9.7546806 ]. \t  -70002.42429157911 \t -0.3114572247523004\n",
            "40     \t [ 1.51388645 -1.29011615]. \t  -6.851896947195383 \t -0.3114572247523004\n",
            "41     \t [0.62832441 0.61913564]. \t  \u001b[92m-0.1764150419437035\u001b[0m \t -0.1764150419437035\n",
            "42     \t [-1.45873764  3.37094123]. \t  -1175.8958193529193 \t -0.1764150419437035\n",
            "43     \t [5.42951099 1.55966529]. \t  -20.25766097753674 \t -0.1764150419437035\n",
            "44     \t [ 2.16171406 -1.20108654]. \t  -2.3964947967160977 \t -0.1764150419437035\n",
            "45     \t [-5.06459918  9.93321927]. \t  -81970.15272696476 \t -0.1764150419437035\n",
            "46     \t [ 4.56340341 -3.89517175]. \t  -1342.0510182032006 \t -0.1764150419437035\n",
            "47     \t [-3.96348555 -0.80264161]. \t  -79.80220245638867 \t -0.1764150419437035\n",
            "48     \t [-1.62815751  8.31723316]. \t  -39196.207626338575 \t -0.1764150419437035\n",
            "49     \t [-9.99029967  9.72713166]. \t  -79501.57394069897 \t -0.1764150419437035\n",
            "50     \t [0.74600906 1.50252577]. \t  -28.477620255209693 \t -0.1764150419437035\n",
            "51     \t [ 2.65437388 -1.03915466]. \t  -3.226387490781242 \t -0.1764150419437035\n",
            "52     \t [ 0.58718077 -0.84973468]. \t  -1.6390342217722944 \t -0.1764150419437035\n",
            "53     \t [-9.47411852 -4.2405916 ]. \t  -4239.1767153471865 \t -0.1764150419437035\n",
            "54     \t [0.24495725 0.31633115]. \t  -0.5741083784610287 \t -0.1764150419437035\n",
            "55     \t [2.50544651 0.48761377]. \t  -10.507455765258028 \t -0.1764150419437035\n",
            "56     \t [0.13205602 0.08168474]. \t  -0.781511468791801 \t -0.1764150419437035\n",
            "57     \t [-1.68821332  0.45243415]. \t  -16.02639814860297 \t -0.1764150419437035\n",
            "58     \t [1.09198943 9.34927193]. \t  -60361.30298145525 \t -0.1764150419437035\n",
            "59     \t [6.01736486 1.94151523]. \t  -29.804470395611276 \t -0.1764150419437035\n",
            "60     \t [-6.35952935 -0.36455175]. \t  -141.95254316345068 \t -0.1764150419437035\n",
            "61     \t [1.02162957 0.52888938]. \t  -0.42769151666654354 \t -0.1764150419437035\n",
            "62     \t [-0.9925755  -0.25647382]. \t  -6.497707744326874 \t -0.1764150419437035\n",
            "63     \t [-6.16593836  8.44297235]. \t  -44294.66023866623 \t -0.1764150419437035\n",
            "64     \t [ 2.91741373 -6.11095378]. \t  -10305.570540281722 \t -0.1764150419437035\n",
            "65     \t [-6.47398976 -8.29725134]. \t  -41621.6570495819 \t -0.1764150419437035\n",
            "66     \t [ 5.80697974 -3.99152321]. \t  -1381.0973566289983 \t -0.1764150419437035\n",
            "67     \t [ 7.65122881 -2.76088774]. \t  -159.56963396417095 \t -0.1764150419437035\n",
            "68     \t [ 5.66563522 -1.74182199]. \t  -22.091766105963604 \t -0.1764150419437035\n",
            "69     \t [-6.18178689  6.26894534]. \t  -14427.260880981674 \t -0.1764150419437035\n",
            "70     \t [1.51331173 0.73938124]. \t  -0.6161923236388545 \t -0.1764150419437035\n",
            "71     \t [-8.1245724  -0.05300373]. \t  -215.4578390440497 \t -0.1764150419437035\n",
            "72     \t [-4.11978008  9.49501985]. \t  -68055.49271965375 \t -0.1764150419437035\n",
            "73     \t [ 2.10945507 -0.96475045]. \t  -1.3538670190258737 \t -0.1764150419437035\n",
            "74     \t [ 6.42254531 -2.98170781]. \t  -287.44038370868657 \t -0.1764150419437035\n",
            "75     \t [ 6.90773279 -0.74031751]. \t  -102.45052762211519 \t -0.1764150419437035\n",
            "76     \t [8.15566945 8.31233999]. \t  -33869.05380235113 \t -0.1764150419437035\n",
            "77     \t [ 4.87203415 -1.90860132]. \t  -26.642457297939035 \t -0.1764150419437035\n",
            "78     \t [-4.30895206  9.99717508]. \t  -83420.17417554371 \t -0.1764150419437035\n",
            "79     \t [-2.12642766 -9.85576909]. \t  -77154.74896888451 \t -0.1764150419437035\n",
            "80     \t [ 5.22753354 -1.14952532]. \t  -31.23356002927212 \t -0.1764150419437035\n",
            "81     \t [4.71774972 1.67006806]. \t  -15.302600462320575 \t -0.1764150419437035\n",
            "82     \t [ 6.20097963 -8.15946825]. \t  -32261.109856807794 \t -0.1764150419437035\n",
            "83     \t [0.18833227 9.67605813]. \t  -69986.44152893897 \t -0.1764150419437035\n",
            "84     \t [5.89672057 3.89720334]. \t  -1222.4860890315838 \t -0.1764150419437035\n",
            "85     \t [ 1.86863242 -6.05169292]. \t  -10190.204384489207 \t -0.1764150419437035\n",
            "86     \t [6.78639434 1.59289557]. \t  -39.34261612961099 \t -0.1764150419437035\n",
            "87     \t [3.41233891 1.22529488]. \t  -6.154995168904883 \t -0.1764150419437035\n",
            "88     \t [-4.5253429  -5.57373991]. \t  -8917.239622642455 \t -0.1764150419437035\n",
            "89     \t [-2.56543486 -7.77172208]. \t  -30450.44183915338 \t -0.1764150419437035\n",
            "90     \t [-9.23238755 -6.67352048]. \t  -19432.10910463239 \t -0.1764150419437035\n",
            "91     \t [-1.41444092 -3.81254297]. \t  -1864.549814117909 \t -0.1764150419437035\n",
            "92     \t [-5.58080905 -2.34532776]. \t  -593.2280959541445 \t -0.1764150419437035\n",
            "93     \t [-4.25079556 -3.48029923]. \t  -1649.3087842062514 \t -0.1764150419437035\n",
            "94     \t [0.79344463 0.46348984]. \t  -0.3073644911053172 \t -0.1764150419437035\n",
            "95     \t [-1.46501658 -2.1326753 ]. \t  -229.1721286101595 \t -0.1764150419437035\n",
            "96     \t [-0.86771731 -1.13962213]. \t  -27.503514008549367 \t -0.1764150419437035\n",
            "97     \t [-2.85345816 -9.83264323]. \t  -77015.66153958983 \t -0.1764150419437035\n",
            "98     \t [-4.75559923  8.75778337]. \t  -50057.95735549469 \t -0.1764150419437035\n",
            "99     \t [ 9.77428042 -6.17916373]. \t  -8945.403788429308 \t -0.1764150419437035\n",
            "100    \t [-0.98210894  5.46957316]. \t  -7400.7526049408825 \t -0.1764150419437035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "65644838-b787-48b8-9d81-b00e2b72ce52"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_19 = d2GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.18891153 5.22983981]. \t  -5727.433065247983 \t -79.36780179787098\n",
            "init   \t [-5.10936891  5.43378939]. \t  -8270.721382692287 \t -79.36780179787098\n",
            "init   \t [-2.91055837  5.93394799]. \t  -10771.054116716396 \t -79.36780179787098\n",
            "init   \t [-3.07999955 -1.12251129]. \t  -79.36780179787098 \t -79.36780179787098\n",
            "init   \t [-3.93392821 -1.1577216 ]. \t  -111.8486364354089 \t -79.36780179787098\n",
            "1      \t [ 1.46418724 -8.68671967]. \t  -44673.21769976243 \t -79.36780179787098\n",
            "2      \t [ 9.77192101 -1.83559751]. \t  -95.34580222090166 \t -79.36780179787098\n",
            "3      \t [-9.83688426 -8.54529444]. \t  -48715.20932349114 \t -79.36780179787098\n",
            "4      \t [9.60900479 6.31645299]. \t  -9926.306792942905 \t -79.36780179787098\n",
            "5      \t [-26.78643605 -24.57394529]. \t  -3048969.9142417214 \t -79.36780179787098\n",
            "6      \t [-11.03676259   1.68012824]. \t  -701.4902218345775 \t -79.36780179787098\n",
            "7      \t [-430.78094782 -429.39999149]. \t  -272616707836.9833 \t -79.36780179787098\n",
            "8      \t [ 3.53089292 -1.68300705]. \t  \u001b[92m-15.514462315752642\u001b[0m \t -15.514462315752642\n",
            "9      \t [-39.0534876  -43.44276615]. \t  -29088714.79455271 \t -15.514462315752642\n",
            "10     \t [ 9.08833239 -8.41726161]. \t  -35237.44071241956 \t -15.514462315752642\n",
            "11     \t [-11.21035597   8.07677001]. \t  -40294.84820976276 \t -15.514462315752642\n",
            "12     \t [-10.88961706 -15.70182206]. \t  -508141.2065672537 \t -15.514462315752642\n",
            "13     \t [-4.02590171 -6.35603452]. \t  -14415.566928930524 \t -15.514462315752642\n",
            "14     \t [5.25069229 9.97986436]. \t  -75247.1563887796 \t -15.514462315752642\n",
            "15     \t [-11.269518    -3.03351391]. \t  -1911.6254695554508 \t -15.514462315752642\n",
            "16     \t [5.65016403 2.79599354]. \t  -221.02430348636958 \t -15.514462315752642\n",
            "17     \t [ 0.08382828 -2.68463806]. \t  -411.57928462976116 \t -15.514462315752642\n",
            "18     \t [ 4.87516172 -4.7773679 ]. \t  -3339.6189896394885 \t -15.514462315752642\n",
            "19     \t [-6.07511758  9.87539755]. \t  -80950.23212547012 \t -15.514462315752642\n",
            "20     \t [-0.46624495  9.8200571 ]. \t  -74757.67019528904 \t -15.514462315752642\n",
            "21     \t [-7.20581712 -0.86956928]. \t  -219.34661781913928 \t -15.514462315752642\n",
            "22     \t [-1.31480587  1.34948744]. \t  -54.50278922642848 \t -15.514462315752642\n",
            "23     \t [ 4.23663312 -0.44396474]. \t  -40.004234289360056 \t -15.514462315752642\n",
            "24     \t [-7.39635047 -4.0450174 ]. \t  -3289.8369465823266 \t -15.514462315752642\n",
            "25     \t [ 6.75011944 -0.63882227]. \t  -103.4869635145468 \t -15.514462315752642\n",
            "26     \t [1.91496278 0.83682115]. \t  \u001b[92m-1.366420018191062\u001b[0m \t -1.366420018191062\n",
            "27     \t [8.75142583 1.13581261]. \t  -136.25412622675006 \t -1.366420018191062\n",
            "28     \t [2.0236718  1.28833819]. \t  -4.406922413789859 \t -1.366420018191062\n",
            "29     \t [ 2.42161461 -1.37914937]. \t  -5.84355270491093 \t -1.366420018191062\n",
            "30     \t [-8.15286615  3.28267131]. \t  -1848.5166993936455 \t -1.366420018191062\n",
            "31     \t [2.09908464 0.14240797]. \t  -9.683033909128245 \t -1.366420018191062\n",
            "32     \t [1.54039677 1.50735894]. \t  -18.338440491069356 \t -1.366420018191062\n",
            "33     \t [5.08459558 5.57489663]. \t  -6531.646595624146 \t -1.366420018191062\n",
            "34     \t [ 2.79740563 -1.63587076]. \t  -16.28406693367883 \t -1.366420018191062\n",
            "35     \t [ 8.73088607 -3.76972616]. \t  -835.2205876673437 \t -1.366420018191062\n",
            "36     \t [2.8662484  1.52275885]. \t  -9.75817822447908 \t -1.366420018191062\n",
            "37     \t [ 1.9051533  -1.09309878]. \t  \u001b[92m-1.2889314453213312\u001b[0m \t -1.2889314453213312\n",
            "38     \t [ 1.68596507 -0.44724688]. \t  -3.7776541833733646 \t -1.2889314453213312\n",
            "39     \t [-4.56400672  1.13858851]. \t  -133.39700812527747 \t -1.2889314453213312\n",
            "40     \t [ 5.21811015 -9.970625  ]. \t  -74986.38444583106 \t -1.2889314453213312\n",
            "41     \t [2.70865269 0.97224623]. \t  -4.258158338237875 \t -1.2889314453213312\n",
            "42     \t [-5.49923748 -9.87419648]. \t  -80441.73748078574 \t -1.2889314453213312\n",
            "43     \t [-0.55451056 -0.09011807]. \t  -3.0680212516129677 \t -1.2889314453213312\n",
            "44     \t [ 1.43761199 -0.50218138]. \t  -1.9333770041252007 \t -1.2889314453213312\n",
            "45     \t [1.3957547 0.2511028]. \t  -3.3806413455333684 \t -1.2889314453213312\n",
            "46     \t [ 0.19685394 -0.14812509]. \t  \u001b[92m-0.6918443730698043\u001b[0m \t -0.6918443730698043\n",
            "47     \t [ 4.18180193 -1.6826508 ]. \t  -14.509551806986478 \t -0.6918443730698043\n",
            "48     \t [ 0.64782613 -0.20896758]. \t  -0.7523272766214226 \t -0.6918443730698043\n",
            "49     \t [2.41747657 0.99842409]. \t  -2.368410757284391 \t -0.6918443730698043\n",
            "50     \t [0.04101544 0.51743679]. \t  -1.4086450926190945 \t -0.6918443730698043\n",
            "51     \t [0.23781394 0.46694599]. \t  \u001b[92m-0.6595441592949518\u001b[0m \t -0.6595441592949518\n",
            "52     \t [-4.84351285 -1.7428499 ]. \t  -272.57673817873376 \t -0.6595441592949518\n",
            "53     \t [6.97633831 2.13582527]. \t  -44.937219368441475 \t -0.6595441592949518\n",
            "54     \t [7.33480785 0.61386146]. \t  -126.75302120090174 \t -0.6595441592949518\n",
            "55     \t [ 2.49263756 -0.64160804]. \t  -7.80119752836785 \t -0.6595441592949518\n",
            "56     \t [8.00927795 2.9520905 ]. \t  -226.61779950571525 \t -0.6595441592949518\n",
            "57     \t [-0.13029439  7.62149116]. \t  -27054.77398453055 \t -0.6595441592949518\n",
            "58     \t [-1.53378448  4.44504863]. \t  -3376.7398542969686 \t -0.6595441592949518\n",
            "59     \t [9.53325184 9.96976773]. \t  -71710.9705251735 \t -0.6595441592949518\n",
            "60     \t [-0.71799791  0.25619226]. \t  -4.394025244878004 \t -0.6595441592949518\n",
            "61     \t [ 9.88964467 -0.13437439]. \t  -273.2099569483209 \t -0.6595441592949518\n",
            "62     \t [0.05560062 0.3124502 ]. \t  -0.9308942278449218 \t -0.6595441592949518\n",
            "63     \t [-1.68264217  0.26806917]. \t  -13.86778231692757 \t -0.6595441592949518\n",
            "64     \t [0.1437718  0.00606477]. \t  -0.7744250973727614 \t -0.6595441592949518\n",
            "65     \t [-8.51581064 -6.35642805]. \t  -16048.163840611456 \t -0.6595441592949518\n",
            "66     \t [-0.12631791 -8.28057783]. \t  -37683.12253195882 \t -0.6595441592949518\n",
            "67     \t [ 8.99129795 -2.75625089]. \t  -140.80384847932044 \t -0.6595441592949518\n",
            "68     \t [1.419442   4.07963932]. \t  -2031.2474747312667 \t -0.6595441592949518\n",
            "69     \t [ 7.7338933 -1.5888312]. \t  -59.7651023698881 \t -0.6595441592949518\n",
            "70     \t [-8.65575034 -5.5448383 ]. \t  -9934.216145768465 \t -0.6595441592949518\n",
            "71     \t [2.56047254 2.46920757]. \t  -188.04370054627768 \t -0.6595441592949518\n",
            "72     \t [0.42442235 0.76532345]. \t  -1.4473602534846945 \t -0.6595441592949518\n",
            "73     \t [5.5072197  0.97956616]. \t  -46.06423921695451 \t -0.6595441592949518\n",
            "74     \t [0.91107239 0.69175703]. \t  \u001b[92m-0.012137030499946716\u001b[0m \t -0.012137030499946716\n",
            "75     \t [ 7.55133276 -4.59334745]. \t  -2443.658936169019 \t -0.012137030499946716\n",
            "76     \t [5.95833852 5.32796835]. \t  -5189.148320446915 \t -0.012137030499946716\n",
            "77     \t [3.50999643 1.03409304]. \t  -10.061007236470932 \t -0.012137030499946716\n",
            "78     \t [-1.24901293 -9.33659734]. \t  -61670.94055074521 \t -0.012137030499946716\n",
            "79     \t [-4.9410944  6.2398932]. \t  -13751.490771490682 \t -0.012137030499946716\n",
            "80     \t [-0.23602593  2.10356679]. \t  -166.63900281264824 \t -0.012137030499946716\n",
            "81     \t [-5.4314867   3.39297621]. \t  -1660.85893349174 \t -0.012137030499946716\n",
            "82     \t [-2.85898254  0.50397435]. \t  -37.56462363144546 \t -0.012137030499946716\n",
            "83     \t [-4.80892289  3.33160919]. \t  -1492.625271016946 \t -0.012137030499946716\n",
            "84     \t [-8.88001853  5.7413491 ]. \t  -11289.549130960664 \t -0.012137030499946716\n",
            "85     \t [2.81089834 1.15366833]. \t  -3.323753070715753 \t -0.012137030499946716\n",
            "86     \t [3.34271584 1.30056264]. \t  -5.491551272994518 \t -0.012137030499946716\n",
            "87     \t [-4.4992704  -8.03524031]. \t  -35743.89855337572 \t -0.012137030499946716\n",
            "88     \t [-0.68370568 -6.59381483]. \t  -15364.547152679263 \t -0.012137030499946716\n",
            "89     \t [-2.79604495  4.4167307 ]. \t  -3510.74028683285 \t -0.012137030499946716\n",
            "90     \t [-2.5565048   1.26777645]. \t  -79.2580482941634 \t -0.012137030499946716\n",
            "91     \t [0.08908324 8.48866671]. \t  -41487.71606296508 \t -0.012137030499946716\n",
            "92     \t [3.29588999 0.12087505]. \t  -26.61335613126223 \t -0.012137030499946716\n",
            "93     \t [ 4.31696136 -1.52430922]. \t  -11.22013278578763 \t -0.012137030499946716\n",
            "94     \t [4.33586629 0.94515392]. \t  -24.125196097896414 \t -0.012137030499946716\n",
            "95     \t [-4.56624156 -3.7311091 ]. \t  -2131.6166846930196 \t -0.012137030499946716\n",
            "96     \t [-4.2956356  -9.65005052]. \t  -72640.99768931672 \t -0.012137030499946716\n",
            "97     \t [5.96686349 3.20976704]. \t  -453.2320428473832 \t -0.012137030499946716\n",
            "98     \t [-7.3883821   2.35828361]. \t  -755.707722767639 \t -0.012137030499946716\n",
            "99     \t [1.245858   0.67491496]. \t  -0.2846785747141742 \t -0.012137030499946716\n",
            "100    \t [0.73771969 0.56174033]. \t  -0.09152460632348647 \t -0.012137030499946716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "1d90b62f-a380-4c76-83bb-f3d564d7e26b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_20 = d2GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.41034571 1.2905751 ]. \t  -7.547501684041513 \t -7.547501684041513\n",
            "init   \t [-0.23116335 -3.27044493]. \t  -936.6052904493877 \t -7.547501684041513\n",
            "init   \t [-2.48263644  0.64071738]. \t  -33.95727983513621 \t -7.547501684041513\n",
            "init   \t [-8.63787423  1.6905812 ]. \t  -504.96347244129606 \t -7.547501684041513\n",
            "init   \t [-5.24204473 -6.78486837]. \t  -18977.794322697024 \t -7.547501684041513\n",
            "1      \t [ 9.84240059 -0.83299926]. \t  -221.1494177066687 \t -7.547501684041513\n",
            "2      \t [6.73867561 8.36273465]. \t  -35481.183803304404 \t -7.547501684041513\n",
            "3      \t [ 8.38641873 -9.74808405]. \t  -66058.08004040587 \t -7.547501684041513\n",
            "4      \t [-3.83701174  9.3377774 ]. \t  -63551.82998472117 \t -7.547501684041513\n",
            "5      \t [-12.19903967  -8.2977456 ]. \t  -45116.755243920124 \t -7.547501684041513\n",
            "6      \t [  1.34696602 -11.50654778]. \t  -138816.47511578322 \t -7.547501684041513\n",
            "7      \t [-10.97478399   7.91175596]. \t  -37226.05349794493 \t -7.547501684041513\n",
            "8      \t [ 5.47512615 -4.02382947]. \t  -1468.0303061239524 \t -7.547501684041513\n",
            "9      \t [5.63665464 2.23412986]. \t  -59.27430779286016 \t -7.547501684041513\n",
            "10     \t [1.19303258 6.15434338]. \t  -11118.08220757746 \t -7.547501684041513\n",
            "11     \t [-9.54463926 -3.22235832]. \t  -1948.802955890587 \t -7.547501684041513\n",
            "12     \t [-5.01120488 -1.68701998]. \t  -265.2548904530954 \t -7.547501684041513\n",
            "13     \t [-4.42202499  3.78242732]. \t  -2212.0918859274752 \t -7.547501684041513\n",
            "14     \t [9.25668229 3.88152593]. \t  -939.7712605560133 \t -7.547501684041513\n",
            "15     \t [ 9.57479367 -4.59830407]. \t  -2213.9396015619786 \t -7.547501684041513\n",
            "16     \t [ 3.59108311 -0.24803557]. \t  -30.768312057866478 \t -7.547501684041513\n",
            "17     \t [ 3.13451676 -7.03526148]. \t  -18381.02999002828 \t -7.547501684041513\n",
            "18     \t [ 6.80318444 -0.80413179]. \t  -94.39557526986036 \t -7.547501684041513\n",
            "19     \t [0.07353678 1.37104359]. \t  -28.031294430127247 \t -7.547501684041513\n",
            "20     \t [2.24537612 1.87345321]. \t  -47.13841828913171 \t -7.547501684041513\n",
            "21     \t [ 1.56907163 -0.75434789]. \t  \u001b[92m-0.6953475680428638\u001b[0m \t -0.6953475680428638\n",
            "22     \t [-7.91645344  4.98784765]. \t  -6732.014903639481 \t -0.6953475680428638\n",
            "23     \t [0.97356652 0.45807815]. \t  \u001b[92m-0.6142988143004436\u001b[0m \t -0.6142988143004436\n",
            "24     \t [2.04126339 0.2563074 ]. \t  -8.379485400996032 \t -0.6142988143004436\n",
            "25     \t [2.89502442 9.87660257]. \t  -73884.91156089552 \t -0.6142988143004436\n",
            "26     \t [1.10455781 0.190319  ]. \t  -2.141455779588587 \t -0.6142988143004436\n",
            "27     \t [ 2.35187261 -0.88310616]. \t  -3.08246657976607 \t -0.6142988143004436\n",
            "28     \t [-1.49961084 -8.30852231]. \t  -38961.740645671736 \t -0.6142988143004436\n",
            "29     \t [4.75441337e+00 2.94046931e-03]. \t  -59.304183931531455 \t -0.6142988143004436\n",
            "30     \t [ 3.08937579 -1.91176107]. \t  -39.987102000861455 \t -0.6142988143004436\n",
            "31     \t [0.88675372 0.81948702]. \t  \u001b[92m-0.429361323646086\u001b[0m \t -0.429361323646086\n",
            "32     \t [1.08758492 1.09581677]. \t  -3.4610936757242396 \t -0.429361323646086\n",
            "33     \t [ 2.133047   -0.35171012]. \t  -8.395124681097528 \t -0.429361323646086\n",
            "34     \t [-0.46424221  0.38160145]. \t  -3.2855099476439045 \t -0.429361323646086\n",
            "35     \t [1.29907932 0.92614417]. \t  -0.4362375627798162 \t -0.429361323646086\n",
            "36     \t [-0.46140817 -0.15979036]. \t  -2.6609732203354115 \t -0.429361323646086\n",
            "37     \t [-0.31083107  0.17096781]. \t  -1.9910298573398262 \t -0.429361323646086\n",
            "38     \t [1.2794607  0.49242837]. \t  -1.3405248153936646 \t -0.429361323646086\n",
            "39     \t [4.15468728 3.76019348]. \t  -1173.8311036297891 \t -0.429361323646086\n",
            "40     \t [6.6803105  1.70083796]. \t  -33.86658484865959 \t -0.429361323646086\n",
            "41     \t [5.38047634 1.39580166]. \t  -23.592798871500698 \t -0.429361323646086\n",
            "42     \t [-8.46193236 -9.39442377]. \t  -68519.05264027971 \t -0.429361323646086\n",
            "43     \t [ 1.23830095 -0.64916763]. \t  \u001b[92m-0.3695704673718105\u001b[0m \t -0.3695704673718105\n",
            "44     \t [-0.78664609  0.09319311]. \t  -4.4849877300655745 \t -0.3695704673718105\n",
            "45     \t [-5.31984363  0.58723395]. \t  -112.16935051451406 \t -0.3695704673718105\n",
            "46     \t [-1.79830301  1.68982602]. \t  -120.61048256335266 \t -0.3695704673718105\n",
            "47     \t [ 0.04686235 -0.1308756 ]. \t  -0.9087891819257023 \t -0.3695704673718105\n",
            "48     \t [7.41942349 4.26504765]. \t  -1718.785273908734 \t -0.3695704673718105\n",
            "49     \t [0.94526687 0.5560444 ]. \t  \u001b[92m-0.21671786321625783\u001b[0m \t -0.21671786321625783\n",
            "50     \t [-7.40554879 -0.92130892]. \t  -236.3886239345398 \t -0.21671786321625783\n",
            "51     \t [-1.50998149 -0.59902231]. \t  -16.224739730985476 \t -0.21671786321625783\n",
            "52     \t [9.55322663 6.75971883]. \t  -13466.88613990843 \t -0.21671786321625783\n",
            "53     \t [ 8.36030492 -9.70579298]. \t  -64886.2395979656 \t -0.21671786321625783\n",
            "54     \t [ 7.54899001 -6.5777006 ]. \t  -12519.612770632544 \t -0.21671786321625783\n",
            "55     \t [-2.88497728  0.00483808]. \t  -31.739776471545348 \t -0.21671786321625783\n",
            "56     \t [ 0.175016   -0.44304169]. \t  -0.7752597263033065 \t -0.21671786321625783\n",
            "57     \t [-3.38469479 -3.12090564]. \t  -1064.8236002785536 \t -0.21671786321625783\n",
            "58     \t [-8.13851557  3.27121202]. \t  -1828.7559536916694 \t -0.21671786321625783\n",
            "59     \t [0.99807614 0.7342779 ]. \t  \u001b[92m-0.012884440724339462\u001b[0m \t -0.012884440724339462\n",
            "60     \t [ 0.98001672 -0.49114353]. \t  -0.49555669428749133 \t -0.012884440724339462\n",
            "61     \t [0.18353621 0.59372552]. \t  -1.2105038064661753 \t -0.012884440724339462\n",
            "62     \t [-4.58992143  9.98676407]. \t  -83312.89560596988 \t -0.012884440724339462\n",
            "63     \t [1.58271516 0.93359517]. \t  -0.3910676451479479 \t -0.012884440724339462\n",
            "64     \t [ 1.70320171 -1.40155187]. \t  -10.400136237342533 \t -0.012884440724339462\n",
            "65     \t [8.29375657 1.08428206]. \t  -123.82362914870536 \t -0.012884440724339462\n",
            "66     \t [-4.44737865 -6.16367525]. \t  -12967.374706732573 \t -0.012884440724339462\n",
            "67     \t [0.40128712 2.84430967]. \t  -498.30632354137344 \t -0.012884440724339462\n",
            "68     \t [5.62072092 2.03270808]. \t  -35.32284101790895 \t -0.012884440724339462\n",
            "69     \t [2.26004594 0.99505905]. \t  -1.7442481176453555 \t -0.012884440724339462\n",
            "70     \t [0.02052282 1.1664541 ]. \t  -15.547017694471727 \t -0.012884440724339462\n",
            "71     \t [2.92186663 4.80081643]. \t  -3731.649350686981 \t -0.012884440724339462\n",
            "72     \t [9.87482425 3.93322583]. \t  -966.2904962785457 \t -0.012884440724339462\n",
            "73     \t [-3.07061282 -5.73791022]. \t  -9515.907317149677 \t -0.012884440724339462\n",
            "74     \t [7.42857409 3.56574496]. \t  -689.3625783898757 \t -0.012884440724339462\n",
            "75     \t [1.8838812  8.48582528]. \t  -40405.25904897171 \t -0.012884440724339462\n",
            "76     \t [7.92961083 2.36213652]. \t  -68.88229657360594 \t -0.012884440724339462\n",
            "77     \t [ 5.63854004 -4.65830064]. \t  -2873.3007456023993 \t -0.012884440724339462\n",
            "78     \t [-2.49276664  8.22484432]. \t  -37983.75841842815 \t -0.012884440724339462\n",
            "79     \t [ 5.04853492 -2.29886935]. \t  -77.35496716677031 \t -0.012884440724339462\n",
            "80     \t [ 3.8563348  -3.17181931]. \t  -537.2292181586561 \t -0.012884440724339462\n",
            "81     \t [ 5.42135206 -2.77774505]. \t  -219.96389294977982 \t -0.012884440724339462\n",
            "82     \t [ 2.03856719 -1.30084846]. \t  -4.701226034498959 \t -0.012884440724339462\n",
            "83     \t [7.10913167 2.32386383]. \t  -64.57663965379322 \t -0.012884440724339462\n",
            "84     \t [ 5.19754341 -1.9160305 ]. \t  -26.819724951313702 \t -0.012884440724339462\n",
            "85     \t [-5.14931039  8.85200777]. \t  -52438.625272453966 \t -0.012884440724339462\n",
            "86     \t [ 4.79820984 -0.96405619]. \t  -31.706556542628896 \t -0.012884440724339462\n",
            "87     \t [6.39721098 0.9530691 ]. \t  -71.09238850821002 \t -0.012884440724339462\n",
            "88     \t [-6.98147209  5.98337434]. \t  -12414.282514773455 \t -0.012884440724339462\n",
            "89     \t [6.09896331 1.92557693]. \t  -29.466981204619696 \t -0.012884440724339462\n",
            "90     \t [ 4.33423879 -1.17338163]. \t  -16.113677219719783 \t -0.012884440724339462\n",
            "91     \t [3.9288047  0.96962721]. \t  -16.97019880376162 \t -0.012884440724339462\n",
            "92     \t [7.8119471  4.05705686]. \t  -1307.1730922817023 \t -0.012884440724339462\n",
            "93     \t [-0.9094418   3.29480777]. \t  -1027.0614777163194 \t -0.012884440724339462\n",
            "94     \t [5.82498226 4.34207538]. \t  -2056.236826655591 \t -0.012884440724339462\n",
            "95     \t [ 2.69762662 -1.07434852]. \t  -3.184853875291262 \t -0.012884440724339462\n",
            "96     \t [7.6995205  9.17775011]. \t  -51734.15813973444 \t -0.012884440724339462\n",
            "97     \t [2.67310973 0.43658823]. \t  -13.304826326080933 \t -0.012884440724339462\n",
            "98     \t [-9.7307755  -8.74354945]. \t  -53012.24150789965 \t -0.012884440724339462\n",
            "99     \t [ 7.44174365 -0.70432951]. \t  -124.69031671498486 \t -0.012884440724339462\n",
            "100    \t [ 4.71125637 -1.47491966]. \t  -14.033316054559677 \t -0.012884440724339462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "f51d39b9-2f4b-442b-d442-0fb291a29d4b"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1612261121.2020652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "c620aeb2-ffb8-4982-b88f-04ec419acec9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.24340351 -6.61860491]. \t  -14577.093458365041 \t -540.0726783817402\n",
            "init   \t [-1.27881961  5.38524945]. \t  -7033.582264098692 \t -540.0726783817402\n",
            "init   \t [-4.09349391 -7.01674086]. \t  -21064.19892235138 \t -540.0726783817402\n",
            "init   \t [-9.55043351 -1.59551015]. \t  -540.0726783817402 \t -540.0726783817402\n",
            "init   \t [-5.22635718 -3.24687616]. \t  -1423.2804229542423 \t -540.0726783817402\n",
            "1      \t [8.37416725 6.27688756]. \t  -9973.589615047737 \t -540.0726783817402\n",
            "2      \t [-9.76713775  9.96269922]. \t  -86875.28945668103 \t -540.0726783817402\n",
            "3      \t [ 9.98926049 -9.3463478 ]. \t  -54345.61921377557 \t -540.0726783817402\n",
            "4      \t [ 9.44490266 -0.94685252]. \t  \u001b[92m-188.41779181630653\u001b[0m \t -188.41779181630653\n",
            "5      \t [3.33952823 0.29152683]. \t  \u001b[92m-25.56551788590987\u001b[0m \t -25.56551788590987\n",
            "6      \t [-10. -10.]. \t  -88321.0 \t -25.56551788590987\n",
            "7      \t [3.1213907  9.77966003]. \t  -70814.45944220599 \t -25.56551788590987\n",
            "8      \t [-8.40354975  3.78083758]. \t  -2825.3931228920173 \t -25.56551788590987\n",
            "9      \t [-1.04556474 -0.21769303]. \t  \u001b[92m-6.785109720003275\u001b[0m \t -6.785109720003275\n",
            "10     \t [3.50973915 3.6452702 ]. \t  -1070.402613384704 \t -6.785109720003275\n",
            "11     \t [ 6.55850953 -3.9576482 ]. \t  -1257.7500939746667 \t -6.785109720003275\n",
            "12     \t [-3.45829526  9.5441169 ]. \t  -68943.27824726085 \t -6.785109720003275\n",
            "13     \t [-4.9256546   1.42566147]. \t  -196.77788657540822 \t -6.785109720003275\n",
            "14     \t [-0.72368514 -2.88291368]. \t  -604.7432172930214 \t -6.785109720003275\n",
            "15     \t [-1.11440303  0.99952621]. \t  -23.84611624482858 \t -6.785109720003275\n",
            "16     \t [6.90027009 1.84012577]. \t  -34.846029112517186 \t -6.785109720003275\n",
            "17     \t [-8.93589578 -5.22199444]. \t  -8156.7076346064505 \t -6.785109720003275\n",
            "18     \t [-1.64646912 -0.12850724]. \t  -12.645221412969676 \t -6.785109720003275\n",
            "19     \t [-0.56192557  0.12097503]. \t  \u001b[92m-3.1386356909860638\u001b[0m \t -3.1386356909860638\n",
            "20     \t [ 3.25719618 -2.46719175]. \t  -164.1162203408745 \t -3.1386356909860638\n",
            "21     \t [ 5.19480986 -9.89695002]. \t  -72753.95543989551 \t -3.1386356909860638\n",
            "22     \t [-0.74112844 -9.36999661]. \t  -62190.955873483545 \t -3.1386356909860638\n",
            "23     \t [ 0.47070545 -0.19685938]. \t  \u001b[92m-0.5893623930947876\u001b[0m \t -0.5893623930947876\n",
            "24     \t [-5.40899309  5.20057141]. \t  -7121.784423160005 \t -0.5893623930947876\n",
            "25     \t [9.37343096 2.47722689]. \t  -86.93289763443296 \t -0.5893623930947876\n",
            "26     \t [ 0.68579447 -0.17479543]. \t  -0.8791942851651047 \t -0.5893623930947876\n",
            "27     \t [ 0.825319   -0.49701531]. \t  \u001b[92m-0.24999380850990044\u001b[0m \t -0.24999380850990044\n",
            "28     \t [ 9.78395948 -4.65410539]. \t  -2326.6769597281736 \t -0.24999380850990044\n",
            "29     \t [0.52916802 1.31240359]. \t  -17.223576910828335 \t -0.24999380850990044\n",
            "30     \t [ 6.22023325 -0.03880884]. \t  -104.55850909914912 \t -0.24999380850990044\n",
            "31     \t [0.53704974 2.33728369]. \t  -216.0661741304401 \t -0.24999380850990044\n",
            "32     \t [0.27626801 0.49835161]. \t  -0.6209761598045039 \t -0.24999380850990044\n",
            "33     \t [7.43336651 9.91035573]. \t  -71481.07096276061 \t -0.24999380850990044\n",
            "34     \t [-2.37414951 -0.03322398]. \t  -22.679031692215098 \t -0.24999380850990044\n",
            "35     \t [2.45926948 0.31759795]. \t  -12.322373998548077 \t -0.24999380850990044\n",
            "36     \t [1.48639591 0.50099665]. \t  -2.174670135041085 \t -0.24999380850990044\n",
            "37     \t [1.74675114 1.18253548]. \t  -2.7627598092038275 \t -0.24999380850990044\n",
            "38     \t [-7.17646816  0.17192206]. \t  -171.56194147368984 \t -0.24999380850990044\n",
            "39     \t [-8.2305757   0.91057587]. \t  -280.783127662128 \t -0.24999380850990044\n",
            "40     \t [1.26786824 0.77085391]. \t  \u001b[92m-0.08437377786978162\u001b[0m \t -0.08437377786978162\n",
            "41     \t [1.91296055 0.55388432]. \t  -4.210299085552981 \t -0.08437377786978162\n",
            "42     \t [1.05317334 1.00301103]. \t  -1.841763258179595 \t -0.08437377786978162\n",
            "43     \t [-5.67495195 -6.97506722]. \t  -21253.523814713266 \t -0.08437377786978162\n",
            "44     \t [0.12158495 0.83992557]. \t  -4.0965370880283025 \t -0.08437377786978162\n",
            "45     \t [ 2.06030962 -1.13957452]. \t  -1.7008883130607342 \t -0.08437377786978162\n",
            "46     \t [-9.81678463 -3.62781302]. \t  -2729.0342417360357 \t -0.08437377786978162\n",
            "47     \t [-1.73199357  1.81661207]. \t  -146.31331489558886 \t -0.08437377786978162\n",
            "48     \t [ 2.69394359 -0.76222108]. \t  -7.563380404367846 \t -0.08437377786978162\n",
            "49     \t [ 1.63173624 -0.8250225 ]. \t  -0.5453359561893254 \t -0.08437377786978162\n",
            "50     \t [1.75542957 0.92587339]. \t  -0.5740270696740744 \t -0.08437377786978162\n",
            "51     \t [ 1.67184643 -1.46621762]. \t  -14.261431389835423 \t -0.08437377786978162\n",
            "52     \t [-1.67320094  0.5138982 ]. \t  -16.838183282366327 \t -0.08437377786978162\n",
            "53     \t [1.04414074 0.77201658]. \t  \u001b[92m-0.04568447350163922\u001b[0m \t -0.04568447350163922\n",
            "54     \t [ 2.05926448 -6.1821805 ]. \t  -11065.72629953515 \t -0.04568447350163922\n",
            "55     \t [ 2.60426264 -3.59796671]. \t  -1087.0926778505834 \t -0.04568447350163922\n",
            "56     \t [ 4.15932236 -1.70025762]. \t  -15.24587353229347 \t -0.04568447350163922\n",
            "57     \t [ 3.27200088 -1.7265149 ]. \t  -19.631030064916846 \t -0.04568447350163922\n",
            "58     \t [-2.76607216 -1.31986411]. \t  -92.31216733697305 \t -0.04568447350163922\n",
            "59     \t [ 0.27243091 -4.74427947]. \t  -4004.5705020465575 \t -0.04568447350163922\n",
            "60     \t [0.66436043 0.79101033]. \t  -0.8018723891737705 \t -0.04568447350163922\n",
            "61     \t [ 0.86273588 -1.15879295]. \t  -6.664524740695237 \t -0.04568447350163922\n",
            "62     \t [1.26768323 6.17745561]. \t  -11266.3472363487 \t -0.04568447350163922\n",
            "63     \t [ 4.42176327 -2.4472851 ]. \t  -125.91424345705227 \t -0.04568447350163922\n",
            "64     \t [ 4.51285463 -0.93057104]. \t  -27.807287652889187 \t -0.04568447350163922\n",
            "65     \t [-1.05056018  9.72447146]. \t  -72342.02090492527 \t -0.04568447350163922\n",
            "66     \t [-0.28079526 -1.18962417]. \t  -20.999649416284967 \t -0.04568447350163922\n",
            "67     \t [ 7.14591226 -7.2748436 ]. \t  -19521.462791031376 \t -0.04568447350163922\n",
            "68     \t [-9.52462087 -4.31583999]. \t  -4487.0498646714095 \t -0.04568447350163922\n",
            "69     \t [0.79093706 0.52932439]. \t  -0.1500309296156641 \t -0.04568447350163922\n",
            "70     \t [ 1.3457882 -0.9215255]. \t  -0.36826573852283984 \t -0.04568447350163922\n",
            "71     \t [6.22407215 1.35304227]. \t  -40.42502717286698 \t -0.04568447350163922\n",
            "72     \t [-1.36766416  6.02301355]. \t  -10934.248704746018 \t -0.04568447350163922\n",
            "73     \t [-0.95100862  3.28711088]. \t  -1021.8223332615473 \t -0.04568447350163922\n",
            "74     \t [1.17646865 0.71149864]. \t  -0.08493843699814925 \t -0.04568447350163922\n",
            "75     \t [6.54695015 2.99858577]. \t  -292.336650454756 \t -0.04568447350163922\n",
            "76     \t [7.65052871 1.1167461 ]. \t  -97.40408255118578 \t -0.04568447350163922\n",
            "77     \t [-3.20877454  1.29014068]. \t  -103.19683880635628 \t -0.04568447350163922\n",
            "78     \t [5.02926629 9.79752204]. \t  -69919.53944352253 \t -0.04568447350163922\n",
            "79     \t [-2.11542918 -0.94077489]. \t  -39.90080283256739 \t -0.04568447350163922\n",
            "80     \t [-2.53249873 -8.90909798]. \t  -52032.73112440396 \t -0.04568447350163922\n",
            "81     \t [ 3.2481393  -1.14003583]. \t  -5.895950675392666 \t -0.04568447350163922\n",
            "82     \t [ 3.18489405 -0.40948539]. \t  -21.013489660021918 \t -0.04568447350163922\n",
            "83     \t [ 0.82431349 -0.75352205]. \t  -0.22465309077004153 \t -0.04568447350163922\n",
            "84     \t [0.86796068 0.72127877]. \t  -0.0769644466380927 \t -0.04568447350163922\n",
            "85     \t [ 3.63193197 -1.61788252]. \t  -12.067282621921752 \t -0.04568447350163922\n",
            "86     \t [ 3.62652117 -8.97584202]. \t  -49622.52268386854 \t -0.04568447350163922\n",
            "87     \t [-1.94956536  6.35408321]. \t  -13686.721634452053 \t -0.04568447350163922\n",
            "88     \t [-5.28205775  0.69650269]. \t  -117.64650369098928 \t -0.04568447350163922\n",
            "89     \t [4.32847395 3.52225331]. \t  -850.2723996448592 \t -0.04568447350163922\n",
            "90     \t [ 5.95507955 -0.58533502]. \t  -80.09535385780426 \t -0.04568447350163922\n",
            "91     \t [1.27144605 1.61000405]. \t  -30.693378009561993 \t -0.04568447350163922\n",
            "92     \t [ 0.65034181 -0.54608166]. \t  -0.12807805140304823 \t -0.04568447350163922\n",
            "93     \t [ 5.26403367 -6.36853727]. \t  -11525.385601354039 \t -0.04568447350163922\n",
            "94     \t [-5.71866905  3.75727751]. \t  -2350.7444695066465 \t -0.04568447350163922\n",
            "95     \t [ 8.07429825 -8.26933835]. \t  -33172.07978023832 \t -0.04568447350163922\n",
            "96     \t [-8.07480962 -7.10576937]. \t  -23869.963676107633 \t -0.04568447350163922\n",
            "97     \t [-1.0387751   5.84805472]. \t  -9647.50139297433 \t -0.04568447350163922\n",
            "98     \t [ 8.40402256 -4.17626348]. \t  -1457.0235509451786 \t -0.04568447350163922\n",
            "99     \t [1.67328896 1.86655206]. \t  -56.52195071756115 \t -0.04568447350163922\n",
            "100    \t [-5.34906297  1.65464129]. \t  -274.660530146399 \t -0.04568447350163922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "9607e090-3885-44a7-8311-987fdca8a824"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.03970985 -8.52192802]. \t  -38238.719775621794 \t -57.443300302345605\n",
            "init   \t [ 7.89863525 -1.27012905]. \t  -91.24969381804144 \t -57.443300302345605\n",
            "init   \t [-7.44644532  1.51715749]. \t  -361.74642651372136 \t -57.443300302345605\n",
            "init   \t [ 6.80941839 -1.29758898]. \t  -57.443300302345605 \t -57.443300302345605\n",
            "init   \t [3.91821121 3.69276194]. \t  -1099.406544633519 \t -57.443300302345605\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -57.443300302345605\n",
            "2      \t [-2.87227721  9.90072912]. \t  -79154.24352053565 \t -57.443300302345605\n",
            "3      \t [-1.3885243  -5.74216696]. \t  -9073.302042724144 \t -57.443300302345605\n",
            "4      \t [9.87440114 9.90717168]. \t  -69590.82131163792 \t -57.443300302345605\n",
            "5      \t [-9.80908004  8.2473246 ]. \t  -42658.83446871175 \t -57.443300302345605\n",
            "6      \t [-1.55405541  0.87137313]. \t  \u001b[92m-25.405403666361742\u001b[0m \t -25.405403666361742\n",
            "7      \t [-10.          -3.45979269]. \t  -2424.8921182511135 \t -25.405403666361742\n",
            "8      \t [9.55883762 3.47985245]. \t  -503.0795369746567 \t -25.405403666361742\n",
            "9      \t [ 2.64070403 -1.94135023]. \t  -50.652685602546725 \t -25.405403666361742\n",
            "10     \t [3.57379202 9.11909317]. \t  -52976.494874433454 \t -25.405403666361742\n",
            "11     \t [ -4.33898468 -10.        ]. \t  -83537.34608091283 \t -25.405403666361742\n",
            "12     \t [ 1.56277992 -9.96014101]. \t  -77497.04190833536 \t -25.405403666361742\n",
            "13     \t [-1.18465404  4.64334803]. \t  -3930.818794877854 \t -25.405403666361742\n",
            "14     \t [-5.56024124 -2.13486252]. \t  -473.77837322367265 \t -25.405403666361742\n",
            "15     \t [ 5.85332745 -4.1460442 ]. \t  -1651.0244287625735 \t -25.405403666361742\n",
            "16     \t [-5.70845475  4.88694372]. \t  -5763.703838600713 \t -25.405403666361742\n",
            "17     \t [ 5.32609094 -0.33439491]. \t  -70.78507602221705 \t -25.405403666361742\n",
            "18     \t [ 9.99041516 -4.36197466]. \t  -1655.9174046015619 \t -25.405403666361742\n",
            "19     \t [1.07125622 0.64241602]. \t  \u001b[92m-0.12597128540915487\u001b[0m \t -0.12597128540915487\n",
            "20     \t [1.16962958 0.82930856]. \t  \u001b[92m-0.11354388592341896\u001b[0m \t -0.11354388592341896\n",
            "21     \t [6.85598088 6.33790454]. \t  -10833.526750495294 \t -0.11354388592341896\n",
            "22     \t [-0.38650204 -1.35346738]. \t  -34.73143746689662 \t -0.11354388592341896\n",
            "23     \t [-5.86919609 -5.11240596]. \t  -6808.306331484121 \t -0.11354388592341896\n",
            "24     \t [9.72250327 0.50940577]. \t  -245.49143252935806 \t -0.11354388592341896\n",
            "25     \t [6.87793692 1.39642928]. \t  -52.28600779234054 \t -0.11354388592341896\n",
            "26     \t [-4.06893245  0.85512085]. \t  -86.88675444822144 \t -0.11354388592341896\n",
            "27     \t [2.07262921 0.01085187]. \t  -9.7401645670528 \t -0.11354388592341896\n",
            "28     \t [-2.51662921 -2.09177896]. \t  -266.2891512710654 \t -0.11354388592341896\n",
            "29     \t [0.82380382 0.75361347]. \t  -0.22581134127282743 \t -0.11354388592341896\n",
            "30     \t [-9.72123178  3.55352629]. \t  -2561.632136736642 \t -0.11354388592341896\n",
            "31     \t [1.7497237  0.56984027]. \t  -2.983352257081203 \t -0.11354388592341896\n",
            "32     \t [ 2.4849051  -5.23908941]. \t  -5496.074919485441 \t -0.11354388592341896\n",
            "33     \t [0.58975652 1.12552854]. \t  -7.7255800472495375 \t -0.11354388592341896\n",
            "34     \t [0.23272385 0.11518293]. \t  -0.6737410248369122 \t -0.11354388592341896\n",
            "35     \t [ 4.04330231 -0.61293341]. \t  -30.935263350021415 \t -0.11354388592341896\n",
            "36     \t [-0.60854477  0.27124232]. \t  -3.7295496879471584 \t -0.11354388592341896\n",
            "37     \t [-9.30549929 -0.21950445]. \t  -282.9933972414611 \t -0.11354388592341896\n",
            "38     \t [ 2.91326097 -0.04129658]. \t  -20.59502335940865 \t -0.11354388592341896\n",
            "39     \t [ 0.75508995 -1.46818765]. \t  -25.351106056591572 \t -0.11354388592341896\n",
            "40     \t [-3.95348592 -3.1273378 ]. \t  -1130.3505347278744 \t -0.11354388592341896\n",
            "41     \t [5.03739579 3.24304436]. \t  -528.1263482536671 \t -0.11354388592341896\n",
            "42     \t [1.10672891 0.71665842]. \t  \u001b[92m-0.024041201759163355\u001b[0m \t -0.024041201759163355\n",
            "43     \t [-3.69818796  1.71305488]. \t  -205.1395047257916 \t -0.024041201759163355\n",
            "44     \t [0.53653244 0.3512852 ]. \t  -0.38268895072223397 \t -0.024041201759163355\n",
            "45     \t [-2.70827301 -0.16005325]. \t  -28.981047564501424 \t -0.024041201759163355\n",
            "46     \t [-0.32847077  0.09562389]. \t  -2.005317669323495 \t -0.024041201759163355\n",
            "47     \t [-9.39681078  9.86068833]. \t  -83228.4691757223 \t -0.024041201759163355\n",
            "48     \t [-3.86201655 -7.50572326]. \t  -27183.88286113719 \t -0.024041201759163355\n",
            "49     \t [-1.65781221 -0.31130689]. \t  -13.921078809894443 \t -0.024041201759163355\n",
            "50     \t [-8.28999214 -2.10020643]. \t  -671.9263016290016 \t -0.024041201759163355\n",
            "51     \t [9.87931521 1.11187389]. \t  -188.56325899224535 \t -0.024041201759163355\n",
            "52     \t [-7.41448581  7.87189908]. \t  -34575.4376890135 \t -0.024041201759163355\n",
            "53     \t [9.76705851 8.21340072]. \t  -31403.330122896183 \t -0.024041201759163355\n",
            "54     \t [-2.80091004  1.81608432]. \t  -191.06295292016443 \t -0.024041201759163355\n",
            "55     \t [-4.1069211   2.83806047]. \t  -843.4616834917406 \t -0.024041201759163355\n",
            "56     \t [6.41619118 2.27686027]. \t  -60.57164316360735 \t -0.024041201759163355\n",
            "57     \t [9.60190153 1.30557436]. \t  -150.69555963189242 \t -0.024041201759163355\n",
            "58     \t [-9.42952584 -4.1515827 ]. \t  -3963.336127203735 \t -0.024041201759163355\n",
            "59     \t [-6.46618709  1.68563166]. \t  -350.9352624725696 \t -0.024041201759163355\n",
            "60     \t [1.390785   7.89766594]. \t  -30433.296516509945 \t -0.024041201759163355\n",
            "61     \t [7.67868447 0.02907516]. \t  -162.47729186586284 \t -0.024041201759163355\n",
            "62     \t [9.06782253 7.72878671]. \t  -24441.615352562665 \t -0.024041201759163355\n",
            "63     \t [-2.83471316 -2.43994193]. \t  -449.31961196714644 \t -0.024041201759163355\n",
            "64     \t [8.33220336 7.60650159]. \t  -23117.065066203402 \t -0.024041201759163355\n",
            "65     \t [-0.54379441  6.45592031]. \t  -14081.343119217587 \t -0.024041201759163355\n",
            "66     \t [ 0.41045765 -9.86409628]. \t  -75420.11863943869 \t -0.024041201759163355\n",
            "67     \t [ 2.97072111 -4.8998992 ]. \t  -4062.4027889644326 \t -0.024041201759163355\n",
            "68     \t [1.75681562 0.88938156]. \t  -0.6338915115745557 \t -0.024041201759163355\n",
            "69     \t [1.56267888 1.38428708]. \t  -10.620796638206087 \t -0.024041201759163355\n",
            "70     \t [0.72006032 0.58500891]. \t  -0.08089944663980621 \t -0.024041201759163355\n",
            "71     \t [-8.25568007  1.4720835 ]. \t  -402.67070773585726 \t -0.024041201759163355\n",
            "72     \t [ 7.70530973 -4.0913636 ]. \t  -1373.4771047270685 \t -0.024041201759163355\n",
            "73     \t [ 1.25910626 -0.77446087]. \t  -0.07422297869913888 \t -0.024041201759163355\n",
            "74     \t [-0.31603546 -0.55825071]. \t  -3.4966053616034607 \t -0.024041201759163355\n",
            "75     \t [ 0.67079457 -0.51664122]. \t  -0.14589134626262357 \t -0.024041201759163355\n",
            "76     \t [8.63905631 5.4974074 ]. \t  -5425.6474319473755 \t -0.024041201759163355\n",
            "77     \t [-5.7912885   4.92505611]. \t  -5943.893075288378 \t -0.024041201759163355\n",
            "78     \t [ 1.43691297 -0.75656224]. \t  -0.36158465696657627 \t -0.024041201759163355\n",
            "79     \t [-5.04643942  6.02138366]. \t  -12067.840893946795 \t -0.024041201759163355\n",
            "80     \t [-5.05007529  3.03266977]. \t  -1135.8688586114838 \t -0.024041201759163355\n",
            "81     \t [-0.18900844 -0.16207887]. \t  -1.5304315105988628 \t -0.024041201759163355\n",
            "82     \t [-5.20499438  9.18626805]. \t  -60576.60751066257 \t -0.024041201759163355\n",
            "83     \t [9.81821777 7.46230331]. \t  -20704.07848571376 \t -0.024041201759163355\n",
            "84     \t [ 7.96364668 -3.79670713]. \t  -919.2992796888569 \t -0.024041201759163355\n",
            "85     \t [-0.42287358 -3.42135285]. \t  -1138.1612419423348 \t -0.024041201759163355\n",
            "86     \t [ 7.89604122 -2.16448552]. \t  -51.90046479188232 \t -0.024041201759163355\n",
            "87     \t [-1.54914016  0.13320121]. \t  -11.520190187763603 \t -0.024041201759163355\n",
            "88     \t [-1.6844257  -7.01140427]. \t  -20008.808295517654 \t -0.024041201759163355\n",
            "89     \t [ 6.95948576 -4.63130315]. \t  -2618.660143559438 \t -0.024041201759163355\n",
            "90     \t [2.10769287 0.98405062]. \t  -1.2854529222587505 \t -0.024041201759163355\n",
            "91     \t [-4.78552334  8.83038623]. \t  -51706.21634292386 \t -0.024041201759163355\n",
            "92     \t [-0.07962577 -0.60678907]. \t  -2.497342076920563 \t -0.024041201759163355\n",
            "93     \t [-1.75019318  1.96533379]. \t  -187.12493048878747 \t -0.024041201759163355\n",
            "94     \t [ 1.25097145 -0.54566878]. \t  -0.9222491649755977 \t -0.024041201759163355\n",
            "95     \t [ 6.33238924 -5.55543991]. \t  -6165.2998444628665 \t -0.024041201759163355\n",
            "96     \t [5.71707915 4.2191094 ]. \t  -1808.44012269075 \t -0.024041201759163355\n",
            "97     \t [-2.65518918  7.78839684]. \t  -30752.187944470195 \t -0.024041201759163355\n",
            "98     \t [5.96123178 1.68952745]. \t  -24.74105647502209 \t -0.024041201759163355\n",
            "99     \t [5.08171765 1.85135828]. \t  -22.94986917959144 \t -0.024041201759163355\n",
            "100    \t [ 9.05556639 -2.87192585]. \t  -175.60975967601863 \t -0.024041201759163355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "98cf5b4f-5334-4ca3-f7bb-260512f76eb9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.03151214 -7.81497449]. \t  -27448.6338140421 \t -1.7863168283775226\n",
            "init   \t [-0.27757442 -0.00337645]. \t  -1.7863168283775226 \t -1.7863168283775226\n",
            "init   \t [ 3.14219998 -5.28230578]. \t  -5551.437171353593 \t -1.7863168283775226\n",
            "init   \t [ 2.25589781 -7.60695193]. \t  -25754.9725373955 \t -1.7863168283775226\n",
            "init   \t [ 4.24404593 -2.54878916]. \t  -163.60006264419064 \t -1.7863168283775226\n",
            "1      \t [-5.61872775  6.96371276]. \t  -21099.51033227109 \t -1.7863168283775226\n",
            "2      \t [-9.6736302  -6.79831075]. \t  -20965.88636571433 \t -1.7863168283775226\n",
            "3      \t [5.95256263 7.26277879]. \t  -19842.284445028028 \t -1.7863168283775226\n",
            "4      \t [-6.97625084 -0.17970429]. \t  -162.76737979398283 \t -1.7863168283775226\n",
            "5      \t [ 9.92174856 -1.24037905]. \t  -173.29656257358286 \t -1.7863168283775226\n",
            "6      \t [-3.49606039 -4.56474193]. \t  -4100.835906617219 \t -1.7863168283775226\n",
            "7      \t [-0.46164031  3.92306437]. \t  -1954.325050012419 \t -1.7863168283775226\n",
            "8      \t [ -4.75865208 -10.        ]. \t  -83885.37328007858 \t -1.7863168283775226\n",
            "9      \t [-10.           3.41006743]. \t  -2333.072034205172 \t -1.7863168283775226\n",
            "10     \t [0.18037714 9.89102847]. \t  -76429.06005447076 \t -1.7863168283775226\n",
            "11     \t [6.01767562 1.64742348]. \t  -25.87248377176882 \t -1.7863168283775226\n",
            "12     \t [2.14917723 0.24623328]. \t  -9.545491484296612 \t -1.7863168283775226\n",
            "13     \t [ 9.97923216 -7.68882125]. \t  -23519.65817380143 \t -1.7863168283775226\n",
            "14     \t [9.78265058 3.98023095]. \t  -1036.5149354350756 \t -1.7863168283775226\n",
            "15     \t [-1.85167735 -0.45901797]. \t  -18.46577950085674 \t -1.7863168283775226\n",
            "16     \t [-9.99669574  9.15887933]. \t  -63323.023962983476 \t -1.7863168283775226\n",
            "17     \t [9.91784296 9.46154543]. \t  -57285.27507142265 \t -1.7863168283775226\n",
            "18     \t [ 7.48052001 -3.57861607]. \t  -699.5697774887861 \t -1.7863168283775226\n",
            "19     \t [3.55851691 2.15501687]. \t  -72.20444110771221 \t -1.7863168283775226\n",
            "20     \t [ 0.52472216 -0.84470748]. \t  -1.854321352340192 \t -1.7863168283775226\n",
            "21     \t [-9.93660372 -2.13273449]. \t  -844.1740305694075 \t -1.7863168283775226\n",
            "22     \t [-4.52901475  2.21862741]. \t  -443.77248086434787 \t -1.7863168283775226\n",
            "23     \t [-0.02525859 -2.35272874]. \t  -247.29019715271906 \t -1.7863168283775226\n",
            "24     \t [ 1.30231232 -0.72965611]. \t  \u001b[92m-0.2042206657470472\u001b[0m \t -0.2042206657470472\n",
            "25     \t [-0.16112697 -0.43879   ]. \t  -1.944885381909589 \t -0.2042206657470472\n",
            "26     \t [ 7.55898617 -0.55011412]. \t  -139.72916230694278 \t -0.2042206657470472\n",
            "27     \t [ 0.5963662  -0.47838431]. \t  \u001b[92m-0.20137515636604097\u001b[0m \t -0.20137515636604097\n",
            "28     \t [-0.73836747  0.16316297]. \t  -4.275219850945589 \t -0.20137515636604097\n",
            "29     \t [-3.39741546 -0.38480881]. \t  -46.6221976692182 \t -0.20137515636604097\n",
            "30     \t [ 1.2839222 -0.9547989]. \t  -0.6624295437835912 \t -0.20137515636604097\n",
            "31     \t [4.25025825 0.67153897]. \t  -32.986793741507334 \t -0.20137515636604097\n",
            "32     \t [ 0.93302348 -0.81341923]. \t  -0.30912001302799197 \t -0.20137515636604097\n",
            "33     \t [ 5.23299244 -0.09596997]. \t  -72.30174667103876 \t -0.20137515636604097\n",
            "34     \t [6.21132536 2.77450206]. \t  -195.8642463605286 \t -0.20137515636604097\n",
            "35     \t [3.30945722 4.10580617]. \t  -1854.36276562946 \t -0.20137515636604097\n",
            "36     \t [3.4566032  1.24148671]. \t  -6.314688177724788 \t -0.20137515636604097\n",
            "37     \t [1.97889245 1.18719164]. \t  -2.3692810691417105 \t -0.20137515636604097\n",
            "38     \t [4.94199565 1.09120449]. \t  -28.652072008152853 \t -0.20137515636604097\n",
            "39     \t [6.68878002 1.07082459]. \t  -71.00216909939596 \t -0.20137515636604097\n",
            "40     \t [2.71996812 1.6840754 ]. \t  -20.38987169286893 \t -0.20137515636604097\n",
            "41     \t [ 2.02003379 -0.73488523]. \t  -2.8073725910165686 \t -0.20137515636604097\n",
            "42     \t [4.92751081 1.58929598]. \t  -15.456198727136483 \t -0.20137515636604097\n",
            "43     \t [3.41761367 1.48724267]. \t  -7.869603354782957 \t -0.20137515636604097\n",
            "44     \t [-6.93771709 -4.15233565]. \t  -3494.4886411944485 \t -0.20137515636604097\n",
            "45     \t [ 1.73783123 -1.02675251]. \t  -0.8190987474324356 \t -0.20137515636604097\n",
            "46     \t [ 1.60115456 -0.89152859]. \t  -0.3616516745598857 \t -0.20137515636604097\n",
            "47     \t [3.10384242 0.02415575]. \t  -23.679342430315295 \t -0.20137515636604097\n",
            "48     \t [1.44669198 0.51359445]. \t  -1.8891463949458125 \t -0.20137515636604097\n",
            "49     \t [1.40945551 1.34006941]. \t  -9.690919003033438 \t -0.20137515636604097\n",
            "50     \t [1.03450793 0.42992548]. \t  -0.8852048554706723 \t -0.20137515636604097\n",
            "51     \t [-7.55261153 -6.98049741]. \t  -22126.213147022947 \t -0.20137515636604097\n",
            "52     \t [-4.79538118 -0.90116801]. \t  -116.00868270749399 \t -0.20137515636604097\n",
            "53     \t [ 0.13867182 -0.29013308]. \t  -0.7436483448271808 \t -0.20137515636604097\n",
            "54     \t [0.69268672 0.57984457]. \t  \u001b[92m-0.09526135668747171\u001b[0m \t -0.09526135668747171\n",
            "55     \t [-7.74309729  4.8701817 ]. \t  -6166.202986000307 \t -0.09526135668747171\n",
            "56     \t [-4.54761256 -1.40127031]. \t  -174.4180854688124 \t -0.09526135668747171\n",
            "57     \t [ 0.42592841 -0.46920543]. \t  -0.32997170344938803 \t -0.09526135668747171\n",
            "58     \t [9.29554307 0.76994626]. \t  -200.3572698422652 \t -0.09526135668747171\n",
            "59     \t [0.86581375 0.06037644]. \t  -1.4921298462541541 \t -0.09526135668747171\n",
            "60     \t [-2.63153815  0.6342395 ]. \t  -36.80105328140602 \t -0.09526135668747171\n",
            "61     \t [0.23636983 0.44641073]. \t  -0.6357456449868493 \t -0.09526135668747171\n",
            "62     \t [-1.97006732 -0.32444149]. \t  -18.331261772302526 \t -0.09526135668747171\n",
            "63     \t [-8.44339496  0.08126659]. \t  -232.20599304231348 \t -0.09526135668747171\n",
            "64     \t [ 3.03252856 -4.64622883]. \t  -3226.9486716463393 \t -0.09526135668747171\n",
            "65     \t [-5.92098836 -9.43033498]. \t  -67600.59403596063 \t -0.09526135668747171\n",
            "66     \t [8.13498007 1.93468773]. \t  -51.750204667839355 \t -0.09526135668747171\n",
            "67     \t [ 2.57030794 -2.14737887]. \t  -90.968441299027 \t -0.09526135668747171\n",
            "68     \t [1.30568365 0.85184234]. \t  -0.13583369496260622 \t -0.09526135668747171\n",
            "69     \t [4.73898245 3.74417699]. \t  -1099.64257608463 \t -0.09526135668747171\n",
            "70     \t [8.62350834 1.41865155]. \t  -100.40778028227365 \t -0.09526135668747171\n",
            "71     \t [-1.62358223 -1.94224028]. \t  -174.9941159860788 \t -0.09526135668747171\n",
            "72     \t [3.87107329 1.69555418]. \t  -15.302349597613741 \t -0.09526135668747171\n",
            "73     \t [ 5.26578534 -6.19497163]. \t  -10239.711748289283 \t -0.09526135668747171\n",
            "74     \t [ 0.25318592 -3.02381372]. \t  -650.9873297211229 \t -0.09526135668747171\n",
            "75     \t [0.3848995  0.38581899]. \t  -0.39355174081710126 \t -0.09526135668747171\n",
            "76     \t [ 3.20228902 -1.32563536]. \t  -5.045175958816867 \t -0.09526135668747171\n",
            "77     \t [2.14142785 0.90934274]. \t  -1.7784028918494856 \t -0.09526135668747171\n",
            "78     \t [-0.7973397 -0.612444 ]. \t  -8.020035398703488 \t -0.09526135668747171\n",
            "79     \t [5.50025847 2.631525  ]. \t  -159.68360540471554 \t -0.09526135668747171\n",
            "80     \t [ 7.7878485  -0.83363241]. \t  -127.94273632673325 \t -0.09526135668747171\n",
            "81     \t [-1.38890174 -0.55778568]. \t  -13.796311972686956 \t -0.09526135668747171\n",
            "82     \t [ 2.71334766 -1.11340444]. \t  -3.0450803923828857 \t -0.09526135668747171\n",
            "83     \t [-6.70974456  6.86359402]. \t  -20432.19709372185 \t -0.09526135668747171\n",
            "84     \t [-3.50885816  4.46413975]. \t  -3781.5402603850184 \t -0.09526135668747171\n",
            "85     \t [-8.65102928  9.910253  ]. \t  -84206.5061584383 \t -0.09526135668747171\n",
            "86     \t [1.82850804 4.73345635]. \t  -3695.7131544669023 \t -0.09526135668747171\n",
            "87     \t [ 0.65459451 -1.51311626]. \t  -30.92187508731874 \t -0.09526135668747171\n",
            "88     \t [-9.65611648  3.01445109]. \t  -1662.5647470123702 \t -0.09526135668747171\n",
            "89     \t [-0.21351966 -3.97757932]. \t  -2031.0558726958338 \t -0.09526135668747171\n",
            "90     \t [-9.26743751  1.50557426]. \t  -486.3524509050705 \t -0.09526135668747171\n",
            "91     \t [2.49362315 1.53912676]. \t  -12.30377062266507 \t -0.09526135668747171\n",
            "92     \t [ 2.78182392 -1.27265521]. \t  -3.593469907187097 \t -0.09526135668747171\n",
            "93     \t [ 3.47587984 -4.77822747]. \t  -3565.6219455686582 \t -0.09526135668747171\n",
            "94     \t [-3.33938631 -3.56105429]. \t  -1666.394147176159 \t -0.09526135668747171\n",
            "95     \t [3.73924683 5.81216801]. \t  -8154.342748064939 \t -0.09526135668747171\n",
            "96     \t [ 8.41909521 -0.51026664]. \t  -179.81087431797224 \t -0.09526135668747171\n",
            "97     \t [-6.11691153  5.8954905 ]. \t  -11490.603962595726 \t -0.09526135668747171\n",
            "98     \t [-8.88127832 -9.83888317]. \t  -82100.84304076748 \t -0.09526135668747171\n",
            "99     \t [1.39639806 5.72636826]. \t  -8239.890787569202 \t -0.09526135668747171\n",
            "100    \t [ 9.497473   -0.19024079]. \t  -249.87168618536793 \t -0.09526135668747171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "d0b4180d-4119-4a4e-f110-b07eef646dae"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.78722095 6.4258993 ]. \t  -11523.905614569041 \t -765.9755148718126\n",
            "init   \t [2.8441433  3.34445237]. \t  -765.9755148718126 \t -765.9755148718126\n",
            "init   \t [-9.21720189 -8.68655223]. \t  -51387.35679532384 \t -765.9755148718126\n",
            "init   \t [-4.4699872  -3.67278993]. \t  -2007.9696367442386 \t -765.9755148718126\n",
            "init   \t [-6.12815176 -6.05814236]. \t  -12700.957148627067 \t -765.9755148718126\n",
            "1      \t [ 5.15020416 -5.71066603]. \t  -7234.8050437839365 \t -765.9755148718126\n",
            "2      \t [-3.97305893  9.21389704]. \t  -60413.181901672484 \t -765.9755148718126\n",
            "3      \t [-8.51899843  2.33131307]. \t  -842.4810529068852 \t -765.9755148718126\n",
            "4      \t [ -0.06016037 -10.        ]. \t  -80049.25947336094 \t -765.9755148718126\n",
            "5      \t [8.27826429 0.32767838]. \t  \u001b[92m-183.01377775252104\u001b[0m \t -183.01377775252104\n",
            "6      \t [ 9.67802381 -9.83797028]. \t  -67708.7958025097 \t -183.01377775252104\n",
            "7      \t [-2.83803863  2.67646344]. \t  -604.0016776146306 \t -183.01377775252104\n",
            "8      \t [-10.           7.83202438]. \t  -35329.60784211856 \t -183.01377775252104\n",
            "9      \t [ 0.75311711 -1.78511488]. \t  \u001b[92m-63.23319403070382\u001b[0m \t -63.23319403070382\n",
            "10     \t [1.85893393 9.46393719]. \t  -62852.36972535914 \t -63.23319403070382\n",
            "11     \t [-10.          -2.98618351]. \t  -1670.528125054205 \t -63.23319403070382\n",
            "12     \t [-0.435952   -4.81818512]. \t  -4394.862324927466 \t -63.23319403070382\n",
            "13     \t [ 4.12526509 -0.68790625]. \t  \u001b[92m-29.977266837668417\u001b[0m \t -29.977266837668417\n",
            "14     \t [ 7.3902696  -2.37623755]. \t  -71.29830686995523 \t -29.977266837668417\n",
            "15     \t [9.86518382 3.28980966]. \t  -356.1523837990128 \t -29.977266837668417\n",
            "16     \t [-6.09531934  4.98941368]. \t  -6296.345576567838 \t -29.977266837668417\n",
            "17     \t [9.19083537 9.88939013]. \t  -69563.89849609062 \t -29.977266837668417\n",
            "18     \t [0.01754033 0.89207855]. \t  \u001b[92m-5.920606817735636\u001b[0m \t -5.920606817735636\n",
            "19     \t [-5.46104353  0.06663806]. \t  -101.58523788980528 \t -5.920606817735636\n",
            "20     \t [-2.14448106 -0.60138612]. \t  -26.336447914722754 \t -5.920606817735636\n",
            "21     \t [ 8.8265108  -4.11307361]. \t  -1312.078224120276 \t -5.920606817735636\n",
            "22     \t [ 6.28528187 -1.67829214]. \t  -28.78428950351332 \t -5.920606817735636\n",
            "23     \t [1.20658613 0.17875079]. \t  \u001b[92m-2.6541241552699657\u001b[0m \t -2.6541241552699657\n",
            "24     \t [0.7701667  0.54481071]. \t  \u001b[92m-0.11514851975394441\u001b[0m \t -0.11514851975394441\n",
            "25     \t [5.2730853 1.4697123]. \t  -20.075587573179618 \t -0.11514851975394441\n",
            "26     \t [ 2.25344089 -0.05056995]. \t  -11.681055965675736 \t -0.11514851975394441\n",
            "27     \t [-0.33857912  4.46069147]. \t  -3223.286325311457 \t -0.11514851975394441\n",
            "28     \t [0.10783759 0.16183711]. \t  -0.8021042933517469 \t -0.11514851975394441\n",
            "29     \t [0.28729995 0.16287836]. \t  -0.617679268408989 \t -0.11514851975394441\n",
            "30     \t [5.55776446 2.03109479]. \t  -35.27693498718469 \t -0.11514851975394441\n",
            "31     \t [ 2.45757268 -2.05718961]. \t  -74.2802539353305 \t -0.11514851975394441\n",
            "32     \t [5.45050377 0.18394947]. \t  -77.75667876664265 \t -0.11514851975394441\n",
            "33     \t [-1.72077451 -1.31499254]. \t  -61.05053440181304 \t -0.11514851975394441\n",
            "34     \t [0.59808999 0.3921034 ]. \t  -0.33042819562297066 \t -0.11514851975394441\n",
            "35     \t [1.25592922 0.62408964]. \t  -0.5204689821455685 \t -0.11514851975394441\n",
            "36     \t [ 4.93353927 -9.42015033]. \t  -59558.9750999321 \t -0.11514851975394441\n",
            "37     \t [-4.66553059 -9.24547315]. \t  -61719.02942258552 \t -0.11514851975394441\n",
            "38     \t [1.01942292 0.83118324]. \t  -0.2629117530741221 \t -0.11514851975394441\n",
            "39     \t [ 9.83571356 -1.37375908]. \t  -151.54819953325176 \t -0.11514851975394441\n",
            "40     \t [-7.52040435 -0.40475527]. \t  -195.78131945034403 \t -0.11514851975394441\n",
            "41     \t [0.44352387 0.32265677]. \t  -0.4204064130432797 \t -0.11514851975394441\n",
            "42     \t [0.85880289 0.72588764]. \t  \u001b[92m-0.09600444789997101\u001b[0m \t -0.09600444789997101\n",
            "43     \t [ 4.38070921 -2.32010754]. \t  -92.9679114032637 \t -0.09600444789997101\n",
            "44     \t [-0.00252962 -2.44120698]. \t  -285.2499930557461 \t -0.09600444789997101\n",
            "45     \t [4.10389447 0.18029847]. \t  -42.25925442144667 \t -0.09600444789997101\n",
            "46     \t [-2.61883778 -0.06455385]. \t  -26.90005418277236 \t -0.09600444789997101\n",
            "47     \t [0.66484473 4.49089149]. \t  -3147.7471973442102 \t -0.09600444789997101\n",
            "48     \t [ 6.92537434 -1.52019982]. \t  -45.72098982266637 \t -0.09600444789997101\n",
            "49     \t [-0.8967665  0.6395546]. \t  -9.478984065608191 \t -0.09600444789997101\n",
            "50     \t [-9.35818564 -3.93199631]. \t  -3352.1482120830033 \t -0.09600444789997101\n",
            "51     \t [8.81606978 0.28697194]. \t  -210.7831447781824 \t -0.09600444789997101\n",
            "52     \t [-1.06888511 -0.05893148]. \t  -6.595110055922827 \t -0.09600444789997101\n",
            "53     \t [9.18772397 4.36842699]. \t  -1746.5555808189845 \t -0.09600444789997101\n",
            "54     \t [ 6.49196718 -7.21609816]. \t  -19102.05751112033 \t -0.09600444789997101\n",
            "55     \t [ 3.85389757 -5.01233296]. \t  -4312.777861907882 \t -0.09600444789997101\n",
            "56     \t [-5.33888765 -4.42680939]. \t  -4006.4098859951646 \t -0.09600444789997101\n",
            "57     \t [-9.91590624 -0.1879394 ]. \t  -318.61931792450275 \t -0.09600444789997101\n",
            "58     \t [-5.54828522 -2.83642507]. \t  -979.3639882971961 \t -0.09600444789997101\n",
            "59     \t [-9.31011341  2.2251477 ]. \t  -844.5524321080262 \t -0.09600444789997101\n",
            "60     \t [ 0.78374146 -0.25038619]. \t  -0.9136302895674375 \t -0.09600444789997101\n",
            "61     \t [ 7.7589623  -0.11838766]. \t  -165.21816050757536 \t -0.09600444789997101\n",
            "62     \t [ 5.10272363 -8.37402313]. \t  -36545.6085694846 \t -0.09600444789997101\n",
            "63     \t [-3.36428627 -4.54178341]. \t  -4000.9142702923155 \t -0.09600444789997101\n",
            "64     \t [ 6.06450706 -2.22794076]. \t  -55.493734112387116 \t -0.09600444789997101\n",
            "65     \t [0.70007031 2.81598637]. \t  -459.71012085825157 \t -0.09600444789997101\n",
            "66     \t [-3.66699095 -1.76817563]. \t  -218.58888550221934 \t -0.09600444789997101\n",
            "67     \t [ 3.74941644 -7.90499041]. \t  -29400.175506072163 \t -0.09600444789997101\n",
            "68     \t [9.79458739 1.91482976]. \t  -89.46215477425872 \t -0.09600444789997101\n",
            "69     \t [0.58459966 1.73037199]. \t  -58.57412175596887 \t -0.09600444789997101\n",
            "70     \t [0.4172145  0.51100155]. \t  -0.36170182256797667 \t -0.09600444789997101\n",
            "71     \t [-0.35825835  0.57811025]. \t  -3.9530146075919443 \t -0.09600444789997101\n",
            "72     \t [0.66141394 4.89445901]. \t  -4465.2479659832015 \t -0.09600444789997101\n",
            "73     \t [2.66983246 0.44301131]. \t  -13.16066235821813 \t -0.09600444789997101\n",
            "74     \t [-4.11355789 -8.05623696]. \t  -35894.995196675634 \t -0.09600444789997101\n",
            "75     \t [ 0.31874756 -0.1894839 ]. \t  -0.5860628912546688 \t -0.09600444789997101\n",
            "76     \t [0.65820342 1.05510766]. \t  -5.035960367683335 \t -0.09600444789997101\n",
            "77     \t [1.03026855 0.47919599]. \t  -0.6530232121773794 \t -0.09600444789997101\n",
            "78     \t [ 0.85826919 -1.25332309]. \t  -10.447629660666198 \t -0.09600444789997101\n",
            "79     \t [ 2.12312966 -8.41193406]. \t  -38864.9440071836 \t -0.09600444789997101\n",
            "80     \t [ 0.70851533 -0.76060814]. \t  -0.4873291184625486 \t -0.09600444789997101\n",
            "81     \t [9.96956206 2.59438812]. \t  -104.84309116067112 \t -0.09600444789997101\n",
            "82     \t [-0.70952199 -1.11219777]. \t  -23.1916795695526 \t -0.09600444789997101\n",
            "83     \t [ 2.40932222 -0.51328481]. \t  -9.0730459066795 \t -0.09600444789997101\n",
            "84     \t [ 1.02483163 -1.29486753]. \t  -10.844741345341658 \t -0.09600444789997101\n",
            "85     \t [ 0.08901897 -0.6226837 ]. \t  -1.7723164481564284 \t -0.09600444789997101\n",
            "86     \t [ 1.36357184 -0.56019412]. \t  -1.2153908256735835 \t -0.09600444789997101\n",
            "87     \t [7.06216457 1.56158733]. \t  -46.2987666190769 \t -0.09600444789997101\n",
            "88     \t [ 9.77528004 -4.79792497]. \t  -2707.289737859233 \t -0.09600444789997101\n",
            "89     \t [-3.70262294 -8.69285128]. \t  -47969.21868999177 \t -0.09600444789997101\n",
            "90     \t [ 6.15494206 -2.46099085]. \t  -97.56919143620553 \t -0.09600444789997101\n",
            "91     \t [ 3.06804531 -1.30239108]. \t  -4.487281786459186 \t -0.09600444789997101\n",
            "92     \t [ 5.37857082 -2.58028248]. \t  -145.16840990953483 \t -0.09600444789997101\n",
            "93     \t [5.45426873 5.76928074]. \t  -7489.910291650112 \t -0.09600444789997101\n",
            "94     \t [ 3.98629541 -1.56943514]. \t  -10.685001924511035 \t -0.09600444789997101\n",
            "95     \t [ 3.07399665 -0.75216141]. \t  -11.848098606703829 \t -0.09600444789997101\n",
            "96     \t [ 6.62625188 -0.43495995]. \t  -109.72650818812865 \t -0.09600444789997101\n",
            "97     \t [3.86956944 0.02692883]. \t  -38.15911986727106 \t -0.09600444789997101\n",
            "98     \t [-5.98245003  7.30174925]. \t  -25412.370855398018 \t -0.09600444789997101\n",
            "99     \t [-1.28302942  5.55932414]. \t  -7967.22056861976 \t -0.09600444789997101\n",
            "100    \t [ 6.29792024 -4.77555238]. \t  -3119.230411100237 \t -0.09600444789997101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "329f14f6-378c-40ba-aa18-6b270a8b4f98"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.14706549 0.83521255]. \t  -0.14473002518929054 \t -0.14473002518929054\n",
            "init   \t [ 1.20847567 -2.35903813]. \t  -196.92158721117866 \t -0.14473002518929054\n",
            "init   \t [-7.61811642  6.91998231]. \t  -21453.432632023236 \t -0.14473002518929054\n",
            "init   \t [4.79483225 8.64715129]. \t  -41920.46290057033 \t -0.14473002518929054\n",
            "init   \t [7.18656013 4.16209252]. \t  -1546.3183076435794 \t -0.14473002518929054\n",
            "1      \t [-0.83929064  3.0099781 ]. \t  -722.2875605329513 \t -0.14473002518929054\n",
            "2      \t [2.20317475 0.72550179]. \t  -4.094787565175626 \t -0.14473002518929054\n",
            "3      \t [ 9.14614046 -9.26496158]. \t  -52900.23738128742 \t -0.14473002518929054\n",
            "4      \t [-8.79367401 -8.68508685]. \t  -51075.444313405846 \t -0.14473002518929054\n",
            "5      \t [-6.79044491 -1.098666  ]. \t  -230.13957570574365 \t -0.14473002518929054\n",
            "6      \t [-1.09714177 -9.06979247]. \t  -54863.977603543106 \t -0.14473002518929054\n",
            "7      \t [ 7.29953272 -2.28790439]. \t  -59.77532331256224 \t -0.14473002518929054\n",
            "8      \t [-2.37867812  7.92793379]. \t  -32821.89775160643 \t -0.14473002518929054\n",
            "9      \t [1.09080072 0.52793826]. \t  -0.5771971729223118 \t -0.14473002518929054\n",
            "10     \t [ 4.08700035 -6.30739689]. \t  -11403.842643711245 \t -0.14473002518929054\n",
            "11     \t [-10.           1.99867488]. \t  -768.237209216572 \t -0.14473002518929054\n",
            "12     \t [1.50417492 0.90673184]. \t  -0.29347657621091816 \t -0.14473002518929054\n",
            "13     \t [9.7646637  7.23264849]. \t  -18072.802721559758 \t -0.14473002518929054\n",
            "14     \t [-2.98691147 -3.90430129]. \t  -2256.9193119425836 \t -0.14473002518929054\n",
            "15     \t [4.20044423 0.41665135]. \t  -39.93787654770618 \t -0.14473002518929054\n",
            "16     \t [ 9.77356591 -4.70226344]. \t  -2450.442238167864 \t -0.14473002518929054\n",
            "17     \t [9.98706881 0.45188222]. \t  -264.2693865262832 \t -0.14473002518929054\n",
            "18     \t [-9.85051585 -3.62895878]. \t  -2737.050011312611 \t -0.14473002518929054\n",
            "19     \t [-4.46043532  1.60122281]. \t  -213.68597821219254 \t -0.14473002518929054\n",
            "20     \t [-1.13966357 -0.0320076 ]. \t  -7.1851752484479725 \t -0.14473002518929054\n",
            "21     \t [3.27425219 4.66674891]. \t  -3250.5863923262946 \t -0.14473002518929054\n",
            "22     \t [ 4.48310826 -1.68217338]. \t  -14.899436149451605 \t -0.14473002518929054\n",
            "23     \t [5.90469344 0.64106994]. \t  -75.72475554508046 \t -0.14473002518929054\n",
            "24     \t [-2.15218401 -0.15804429]. \t  -19.635105210851187 \t -0.14473002518929054\n",
            "25     \t [0.28203088 0.0848536 ]. \t  -0.6587319495480773 \t -0.14473002518929054\n",
            "26     \t [ 3.83478808 -9.9376873 ]. \t  -75032.28455584684 \t -0.14473002518929054\n",
            "27     \t [-4.51813919  4.23224751]. \t  -3285.4001134864834 \t -0.14473002518929054\n",
            "28     \t [-0.51342463 -1.06629413]. \t  -17.829551174013496 \t -0.14473002518929054\n",
            "29     \t [-6.1206579  -5.28545139]. \t  -7736.881267671692 \t -0.14473002518929054\n",
            "30     \t [0.6911461  0.41591498]. \t  -0.333683065651094 \t -0.14473002518929054\n",
            "31     \t [ 5.07614673 -2.35310867]. \t  -88.56923741016979 \t -0.14473002518929054\n",
            "32     \t [ 3.18265453 -0.82755009]. \t  -11.337746322818317 \t -0.14473002518929054\n",
            "33     \t [-0.27780683 -0.24833815]. \t  -1.9546337286343718 \t -0.14473002518929054\n",
            "34     \t [ 0.45730514 -5.35960526]. \t  -6496.797064097895 \t -0.14473002518929054\n",
            "35     \t [0.44948625 0.34367089]. \t  -0.39403092200703094 \t -0.14473002518929054\n",
            "36     \t [1.50050498 1.10531234]. \t  -2.0287232367435513 \t -0.14473002518929054\n",
            "37     \t [ 3.81291412 -1.70491774]. \t  -15.917085658895143 \t -0.14473002518929054\n",
            "38     \t [ 4.3437781  -1.04148938]. \t  -20.636690017057383 \t -0.14473002518929054\n",
            "39     \t [ 7.39241369 -0.47580873]. \t  -137.17976476907685 \t -0.14473002518929054\n",
            "40     \t [-0.28103395  0.03961992]. \t  -1.802557068230817 \t -0.14473002518929054\n",
            "41     \t [ 3.01661631 -0.0180815 ]. \t  -22.258800098061155 \t -0.14473002518929054\n",
            "42     \t [1.21505183 0.67809029]. \t  -0.22081563759203085 \t -0.14473002518929054\n",
            "43     \t [-7.25734413  1.60119814]. \t  -374.96092983846995 \t -0.14473002518929054\n",
            "44     \t [-3.46511161 -0.70615158]. \t  -59.76345833355229 \t -0.14473002518929054\n",
            "45     \t [ 1.6725133  -0.11314975]. \t  -5.87688325640008 \t -0.14473002518929054\n",
            "46     \t [-1.41683764 -0.5840477 ]. \t  -14.653218942559906 \t -0.14473002518929054\n",
            "47     \t [-7.61923778 -6.19882113]. \t  -14344.654571017867 \t -0.14473002518929054\n",
            "48     \t [2.33876796 1.16434649]. \t  -2.0700170911971227 \t -0.14473002518929054\n",
            "49     \t [3.62420017 1.42991181]. \t  -7.319053993876928 \t -0.14473002518929054\n",
            "50     \t [5.50806031 6.92633429]. \t  -16379.165169186766 \t -0.14473002518929054\n",
            "51     \t [0.86233555 9.80538294]. \t  -73289.93905120001 \t -0.14473002518929054\n",
            "52     \t [1.67859971 0.84994363]. \t  -0.5698143543861635 \t -0.14473002518929054\n",
            "53     \t [-2.82434489 -7.33792024]. \t  -24441.53138338678 \t -0.14473002518929054\n",
            "54     \t [2.43960564 1.12725848]. \t  -2.0931980872224347 \t -0.14473002518929054\n",
            "55     \t [5.09988088 1.63586808]. \t  -16.93628122109199 \t -0.14473002518929054\n",
            "56     \t [-3.15868785  7.96922798]. \t  -33908.812544066655 \t -0.14473002518929054\n",
            "57     \t [-9.12550271 -8.71878829]. \t  -52047.64348572759 \t -0.14473002518929054\n",
            "58     \t [-7.09555014 -1.87065862]. \t  -462.83524980013703 \t -0.14473002518929054\n",
            "59     \t [7.72315009 2.04620135]. \t  -46.04764586113081 \t -0.14473002518929054\n",
            "60     \t [7.80444812 1.54109997]. \t  -64.96008670293061 \t -0.14473002518929054\n",
            "61     \t [-5.53144806 -0.19987121]. \t  -105.63420061980217 \t -0.14473002518929054\n",
            "62     \t [-9.67407428  9.93999956]. \t  -86044.97455180518 \t -0.14473002518929054\n",
            "63     \t [-2.98420544 -5.26783978]. \t  -6856.742089050018 \t -0.14473002518929054\n",
            "64     \t [4.28765407 1.36190446]. \t  -11.477037355492802 \t -0.14473002518929054\n",
            "65     \t [6.80905509 0.11733726]. \t  -125.72312142648643 \t -0.14473002518929054\n",
            "66     \t [4.83577875 1.65862836]. \t  -15.601156239351546 \t -0.14473002518929054\n",
            "67     \t [-2.01667832  4.6241505 ]. \t  -4019.993725988372 \t -0.14473002518929054\n",
            "68     \t [ 2.51281922 -0.53561285]. \t  -9.808505974204905 \t -0.14473002518929054\n",
            "69     \t [-6.24072439  0.690922  ]. \t  -155.97769002463605 \t -0.14473002518929054\n",
            "70     \t [-7.98074325 -9.01144947]. \t  -58148.32966529846 \t -0.14473002518929054\n",
            "71     \t [9.30506253 2.56877681]. \t  -99.27197689341595 \t -0.14473002518929054\n",
            "72     \t [ 2.49053743 -3.98211312]. \t  -1710.2958083542276 \t -0.14473002518929054\n",
            "73     \t [0.378457   0.56875501]. \t  -0.5305082921306717 \t -0.14473002518929054\n",
            "74     \t [ 1.17169108 -1.2430369 ]. \t  -7.391455936184261 \t -0.14473002518929054\n",
            "75     \t [-7.43781506 -0.14536251]. \t  -183.0997841788702 \t -0.14473002518929054\n",
            "76     \t [ 1.25417438 -8.4556558 ]. \t  -40181.68519036071 \t -0.14473002518929054\n",
            "77     \t [9.54647128 9.218395  ]. \t  -51536.510783866055 \t -0.14473002518929054\n",
            "78     \t [ 1.64567548 -0.79931802]. \t  -0.6875341970480675 \t -0.14473002518929054\n",
            "79     \t [-2.30321477  0.73753843]. \t  -33.910897669307026 \t -0.14473002518929054\n",
            "80     \t [ 0.99525104 -4.28503412]. \t  -2552.949101192402 \t -0.14473002518929054\n",
            "81     \t [ 1.22398182 -0.26939809]. \t  -2.3779212132564647 \t -0.14473002518929054\n",
            "82     \t [ 1.34744028 -0.62260604]. \t  -0.7754574164000526 \t -0.14473002518929054\n",
            "83     \t [1.9516029  9.10809664]. \t  -53768.79758512499 \t -0.14473002518929054\n",
            "84     \t [1.20548704 0.764881  ]. \t  \u001b[92m-0.0447314057827467\u001b[0m \t -0.0447314057827467\n",
            "85     \t [ 0.21149934 -0.6606706 ]. \t  -1.4968235897839395 \t -0.0447314057827467\n",
            "86     \t [2.56789062 4.34321879]. \t  -2474.798140262368 \t -0.0447314057827467\n",
            "87     \t [-7.69365142  1.92166151]. \t  -530.3451666391649 \t -0.0447314057827467\n",
            "88     \t [ 1.57698761 -7.3710349 ]. \t  -22935.722571382918 \t -0.0447314057827467\n",
            "89     \t [ 7.25300217 -4.22881872]. \t  -1665.061079179567 \t -0.0447314057827467\n",
            "90     \t [1.2900814  0.79116915]. \t  -0.08706327757258457 \t -0.0447314057827467\n",
            "91     \t [ 7.49973672 -2.08166917]. \t  -44.97015159263558 \t -0.0447314057827467\n",
            "92     \t [ 8.64980484 -6.85356859]. \t  -14608.311187269232 \t -0.0447314057827467\n",
            "93     \t [ 0.42893247 -0.62195036]. \t  -0.5637709012148091 \t -0.0447314057827467\n",
            "94     \t [ 6.30094338 -1.56022073]. \t  -32.20334495191881 \t -0.0447314057827467\n",
            "95     \t [ 6.61192955 -1.75416317]. \t  -31.9128283400123 \t -0.0447314057827467\n",
            "96     \t [7.94956547 1.96494096]. \t  -48.40004505770033 \t -0.0447314057827467\n",
            "97     \t [ 9.17998145 -4.28924253]. \t  -1592.1130148597142 \t -0.0447314057827467\n",
            "98     \t [-8.86077216  7.52949317]. \t  -29986.042979117836 \t -0.0447314057827467\n",
            "99     \t [-3.64129716  4.08824078]. \t  -2769.7220190796297 \t -0.0447314057827467\n",
            "100    \t [1.1011717 0.6757056]. \t  -0.08093542227958248 \t -0.0447314057827467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "3fe644f6-5972-400b-e7aa-ece52ad9967c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.85720303 -3.36040389]. \t  -480.81536490368427 \t -43.91981950896418\n",
            "init   \t [ 6.42458246 -9.16606749]. \t  -52264.36633669738 \t -43.91981950896418\n",
            "init   \t [-7.8468664   1.90104128]. \t  -532.7651696660866 \t -43.91981950896418\n",
            "init   \t [ 0.59634724 -1.62385143]. \t  -43.91981950896418 \t -43.91981950896418\n",
            "init   \t [-3.29184301  2.45038864]. \t  -486.6398213674611 \t -43.91981950896418\n",
            "1      \t [-10.          -8.94716372]. \t  -57991.385884906354 \t -43.91981950896418\n",
            "2      \t [7.02990054 8.4753693 ]. \t  -37373.988071206615 \t -43.91981950896418\n",
            "3      \t [-7.69566798  9.86414506]. \t  -81924.88656432132 \t -43.91981950896418\n",
            "4      \t [-1.71570199 -9.52643263]. \t  -67147.64008994015 \t -43.91981950896418\n",
            "5      \t [0.13020197 7.74415123]. \t  -28711.33223119685 \t -43.91981950896418\n",
            "6      \t [-5.78646368 -3.6669882 ]. \t  -2182.029857633099 \t -43.91981950896418\n",
            "7      \t [5.18997236 1.88661295]. \t  \u001b[92m-24.99520734453692\u001b[0m \t -24.99520734453692\n",
            "8      \t [ 2.16839488 -5.29199189]. \t  -5799.278789405504 \t -24.99520734453692\n",
            "9      \t [1.40352754 2.27618595]. \t  -160.67290344848865 \t -24.99520734453692\n",
            "10     \t [7.84511055 0.73655756]. \t  -138.252805876501 \t -24.99520734453692\n",
            "11     \t [-9.73967012 -3.23531348]. \t  -1997.15075683132 \t -24.99520734453692\n",
            "12     \t [ 4.14287655 -1.05496914]. \t  \u001b[92m-17.227119414253558\u001b[0m \t -17.227119414253558\n",
            "13     \t [-2.49495623 -1.44940582]. \t  -101.90123029139617 \t -17.227119414253558\n",
            "14     \t [-9.60324089  5.1240469 ]. \t  -7828.963772259798 \t -17.227119414253558\n",
            "15     \t [3.56051751 4.57228876]. \t  -2932.8543802022127 \t -17.227119414253558\n",
            "16     \t [-4.26590437  5.84803192]. \t  -10588.093214096301 \t -17.227119414253558\n",
            "17     \t [9.24917075 4.02536331]. \t  -1140.6281518901199 \t -17.227119414253558\n",
            "18     \t [ 2.47935555 -0.49736062]. \t  \u001b[92m-10.065928957413744\u001b[0m \t -10.065928957413744\n",
            "19     \t [-1.92347642 -4.84081761]. \t  -4769.5737884404 \t -10.065928957413744\n",
            "20     \t [ 9.94339114 -5.83893736]. \t  -6864.475500501584 \t -10.065928957413744\n",
            "21     \t [4.59832225 0.71684608]. \t  -38.446086554663424 \t -10.065928957413744\n",
            "22     \t [-5.84654667 -7.22626925]. \t  -24372.208817635834 \t -10.065928957413744\n",
            "23     \t [ 9.96499222 -0.71356254]. \t  -240.45615016727385 \t -10.065928957413744\n",
            "24     \t [-0.92096538  0.71057555]. \t  -11.146089933675487 \t -10.065928957413744\n",
            "25     \t [ 4.55144515 -2.53502668]. \t  -150.43510934882542 \t -10.065928957413744\n",
            "26     \t [-5.61427526  0.08836312]. \t  -107.13999011846055 \t -10.065928957413744\n",
            "27     \t [ 3.51834815 -0.68742235]. \t  -19.585300125410846 \t -10.065928957413744\n",
            "28     \t [ 1.50752599 -0.0854242 ]. \t  \u001b[92m-4.71527098040802\u001b[0m \t -4.71527098040802\n",
            "29     \t [ 4.75196923 -0.85322805]. \t  -35.80414943042064 \t -4.71527098040802\n",
            "30     \t [ 1.95165392 -0.92110338]. \t  \u001b[92m-1.0354821205920615\u001b[0m \t -1.0354821205920615\n",
            "31     \t [-0.09522157  0.76541955]. \t  -4.409863937399255 \t -1.0354821205920615\n",
            "32     \t [-0.42019684  3.3457463 ]. \t  -1042.447981117573 \t -1.0354821205920615\n",
            "33     \t [ 0.17159355 -0.09583341]. \t  \u001b[92m-0.7332133365035686\u001b[0m \t -0.7332133365035686\n",
            "34     \t [ 2.38209451 -9.36475898]. \t  -59870.51695466323 \t -0.7332133365035686\n",
            "35     \t [2.50656106 0.80738462]. \t  -5.1632839508269495 \t -0.7332133365035686\n",
            "36     \t [-0.74399538  0.24568234]. \t  -4.5369839410219015 \t -0.7332133365035686\n",
            "37     \t [-9.54461185 -6.92456773]. \t  -22348.016015363617 \t -0.7332133365035686\n",
            "38     \t [5.76215049 1.96773236]. \t  -30.533066502561148 \t -0.7332133365035686\n",
            "39     \t [ 5.94554724 -8.19734584]. \t  -33021.93297494271 \t -0.7332133365035686\n",
            "40     \t [1.65707635 0.67832272]. \t  -1.517594837777481 \t -0.7332133365035686\n",
            "41     \t [1.18761478 3.40904926]. \t  -972.9358498398087 \t -0.7332133365035686\n",
            "42     \t [-0.15983892  0.34750606]. \t  -1.6674057832362799 \t -0.7332133365035686\n",
            "43     \t [ 0.90426058 -0.53257564]. \t  \u001b[92m-0.23628645233847545\u001b[0m \t -0.23628645233847545\n",
            "44     \t [1.91061021 1.51895833]. \t  -15.450913537896874 \t -0.23628645233847545\n",
            "45     \t [-0.3398488   1.92037519]. \t  -120.85401780007317 \t -0.23628645233847545\n",
            "46     \t [0.67472153 1.32967604]. \t  -16.480513055297433 \t -0.23628645233847545\n",
            "47     \t [2.91044103 0.17532819]. \t  -19.88294340642394 \t -0.23628645233847545\n",
            "48     \t [ 2.36985646 -1.58086032]. \t  -15.693293004716827 \t -0.23628645233847545\n",
            "49     \t [-0.38652882 -0.36128578]. \t  -2.7611913571391096 \t -0.23628645233847545\n",
            "50     \t [-9.52347164  0.21105808]. \t  -295.5461770910723 \t -0.23628645233847545\n",
            "51     \t [ 0.12719113 -0.38223844]. \t  -0.8162593971404267 \t -0.23628645233847545\n",
            "52     \t [-0.4451848  1.4532218]. \t  -45.685663811473155 \t -0.23628645233847545\n",
            "53     \t [ 0.69103036 -0.64393987]. \t  \u001b[92m-0.13370868303470113\u001b[0m \t -0.13370868303470113\n",
            "54     \t [-1.93619994 -0.062346  ]. \t  -16.179339840733512 \t -0.13370868303470113\n",
            "55     \t [ 6.17464611 -5.67732354]. \t  -6822.075727476668 \t -0.13370868303470113\n",
            "56     \t [-0.88995746 -0.74343058]. \t  -11.53466674125904 \t -0.13370868303470113\n",
            "57     \t [-3.26132127  9.54293313]. \t  -68761.85871713061 \t -0.13370868303470113\n",
            "58     \t [ 0.03027053 -0.53091586]. \t  -1.5095616612963 \t -0.13370868303470113\n",
            "59     \t [ 1.36979063 -0.95337405]. \t  -0.5382490310807839 \t -0.13370868303470113\n",
            "60     \t [-1.95194259  0.32229501]. \t  -18.042493218563727 \t -0.13370868303470113\n",
            "61     \t [6.93219279 7.74582913]. \t  -25601.921290181013 \t -0.13370868303470113\n",
            "62     \t [1.78892318 0.47841018]. \t  -4.166430019061239 \t -0.13370868303470113\n",
            "63     \t [0.48089106 0.28824158]. \t  -0.4675772856983608 \t -0.13370868303470113\n",
            "64     \t [3.29021903 0.93220431]. \t  -10.063810502195874 \t -0.13370868303470113\n",
            "65     \t [-9.37022401 -5.04390326]. \t  -7368.183318605593 \t -0.13370868303470113\n",
            "66     \t [6.65320549 4.86409606]. \t  -3339.3497955448233 \t -0.13370868303470113\n",
            "67     \t [ 2.94638521 -1.49718838]. \t  -8.511683431768233 \t -0.13370868303470113\n",
            "68     \t [ 1.71770918 -0.76035962]. \t  -1.1454815744750615 \t -0.13370868303470113\n",
            "69     \t [-6.25673327 -5.84750285]. \t  -11195.908384770653 \t -0.13370868303470113\n",
            "70     \t [5.73857878 1.29789408]. \t  -33.68338548335779 \t -0.13370868303470113\n",
            "71     \t [-7.45020871 -5.3634945 ]. \t  -8517.338851062503 \t -0.13370868303470113\n",
            "72     \t [8.54124136 6.9162167 ]. \t  -15239.051709360609 \t -0.13370868303470113\n",
            "73     \t [ 1.31820967 -0.04303052]. \t  -3.5571116527695414 \t -0.13370868303470113\n",
            "74     \t [ 1.05122142 -0.74109144]. \t  \u001b[92m-0.00708151029528118\u001b[0m \t -0.00708151029528118\n",
            "75     \t [-5.54358842  5.51304305]. \t  -8842.3887107496 \t -0.00708151029528118\n",
            "76     \t [7.78650154 3.89916615]. \t  -1069.4298990570135 \t -0.00708151029528118\n",
            "77     \t [4.66219434 9.80236653]. \t  -70333.85057301067 \t -0.00708151029528118\n",
            "78     \t [ 2.24003725 -8.95614477]. \t  -50046.53950081282 \t -0.00708151029528118\n",
            "79     \t [-2.66403716 -9.86890604]. \t  -77990.1026981319 \t -0.00708151029528118\n",
            "80     \t [-1.42472207  0.36586805]. \t  -11.607989665038925 \t -0.00708151029528118\n",
            "81     \t [5.70377261 4.76196462]. \t  -3166.186738470946 \t -0.00708151029528118\n",
            "82     \t [-8.11969226  2.93973397]. \t  -1373.8712307481312 \t -0.00708151029528118\n",
            "83     \t [-8.42229385  1.19055814]. \t  -342.2265293740461 \t -0.00708151029528118\n",
            "84     \t [-8.6698848  -1.14240882]. \t  -347.9871088865486 \t -0.00708151029528118\n",
            "85     \t [-1.97214442 -6.10811556]. \t  -11740.98045602341 \t -0.00708151029528118\n",
            "86     \t [-3.09059528  0.150984  ]. \t  -36.404315259603344 \t -0.00708151029528118\n",
            "87     \t [8.90445694 8.69235288]. \t  -40509.58705670207 \t -0.00708151029528118\n",
            "88     \t [ 1.11231044 -0.80776528]. \t  -0.08684866822332754 \t -0.00708151029528118\n",
            "89     \t [ 4.69823995 -4.30866854]. \t  -2117.2176944510416 \t -0.00708151029528118\n",
            "90     \t [-3.93194223 -1.05429257]. \t  -100.09229765168764 \t -0.00708151029528118\n",
            "91     \t [-8.21078768 -9.43408712]. \t  -69436.73252782355 \t -0.00708151029528118\n",
            "92     \t [ 7.61381756 -5.9672286 ]. \t  -8134.124752290816 \t -0.00708151029528118\n",
            "93     \t [-3.60856824  0.66276031]. \t  -61.50650783442721 \t -0.00708151029528118\n",
            "94     \t [ 7.07598892 -1.31616289]. \t  -63.002341568953 \t -0.00708151029528118\n",
            "95     \t [6.57010489 8.77670648]. \t  -43538.24471272127 \t -0.00708151029528118\n",
            "96     \t [ 5.19003179 -1.27635011]. \t  -25.02078425435436 \t -0.00708151029528118\n",
            "97     \t [-7.38079581 -5.17206868]. \t  -7483.323761270523 \t -0.00708151029528118\n",
            "98     \t [-5.93568273 -5.75394603]. \t  -10459.773688473073 \t -0.00708151029528118\n",
            "99     \t [-3.06810703  6.60137068]. \t  -16297.396964435744 \t -0.00708151029528118\n",
            "100    \t [-7.12382562 -8.06620731]. \t  -37741.79479062148 \t -0.00708151029528118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "a71ff132-97df-4a4b-b9ca-3698f09d1a9b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.9467253  -3.95286781]. \t  -2981.1801714787616 \t -297.17705346549894\n",
            "init   \t [-8.75927171 -0.80279317]. \t  -297.17705346549894 \t -297.17705346549894\n",
            "init   \t [6.70506769 8.53994095]. \t  -38761.39667089972 \t -297.17705346549894\n",
            "init   \t [4.53977969 5.36992444]. \t  -5658.63331787327 \t -297.17705346549894\n",
            "init   \t [-4.61589867  2.88058584]. \t  -931.3893916140455 \t -297.17705346549894\n",
            "1      \t [ 2.44677649 -4.93762112]. \t  -4291.960403637314 \t -297.17705346549894\n",
            "2      \t [-8.63046085  9.55793493]. \t  -73313.74290460398 \t -297.17705346549894\n",
            "3      \t [ 9.83790456 -9.57130198]. \t  -60200.563023412855 \t -297.17705346549894\n",
            "4      \t [-2.45632157 -9.94001756]. \t  -80063.330232903 \t -297.17705346549894\n",
            "5      \t [ 9.0936247  -0.78669084]. \t  \u001b[92m-188.93582542124247\u001b[0m \t -188.93582542124247\n",
            "6      \t [-10. -10.]. \t  -88321.0 \t -188.93582542124247\n",
            "7      \t [0.48374205 0.70472029]. \t  \u001b[92m-0.7857421466979371\u001b[0m \t -0.7857421466979371\n",
            "8      \t [-1.02353759  7.00052582]. \t  -19621.24907500278 \t -0.7857421466979371\n",
            "9      \t [-10.           3.77967849]. \t  -3096.588961860001 \t -0.7857421466979371\n",
            "10     \t [-2.16064018 -2.62912577]. \t  -521.0457469390218 \t -0.7857421466979371\n",
            "11     \t [3.96440766 0.10675119]. \t  -39.86038649757861 \t -0.7857421466979371\n",
            "12     \t [ 7.07786567 -4.73559299]. \t  -2890.662850586515 \t -0.7857421466979371\n",
            "13     \t [  3.20177511 -10.        ]. \t  -77463.93045395311 \t -0.7857421466979371\n",
            "14     \t [9.28771153 3.99441342]. \t  -1092.2833095201354 \t -0.7857421466979371\n",
            "15     \t [0.74527149 0.00440792]. \t  -1.1756299586084873 \t -0.7857421466979371\n",
            "16     \t [0.84508712 2.04693174]. \t  -113.569576148769 \t -0.7857421466979371\n",
            "17     \t [0.13650386 0.11302188]. \t  \u001b[92m-0.7702480382376174\u001b[0m \t -0.7702480382376174\n",
            "18     \t [2.30744591 9.3914346 ]. \t  -60616.81261757289 \t -0.7702480382376174\n",
            "19     \t [5.84731745 1.55293088]. \t  -25.594166096179293 \t -0.7702480382376174\n",
            "20     \t [-0.54601525  0.68471489]. \t  -6.792800857825023 \t -0.7702480382376174\n",
            "21     \t [-6.22633622  5.75282287]. \t  -10540.454617234245 \t -0.7702480382376174\n",
            "22     \t [-5.60280791 -0.25058201]. \t  -109.22598995052391 \t -0.7702480382376174\n",
            "23     \t [0.50977244 0.19575997]. \t  \u001b[92m-0.6155236750725124\u001b[0m \t -0.6155236750725124\n",
            "24     \t [ 1.27227301 -0.52931249]. \t  -1.0878200712730328 \t -0.6155236750725124\n",
            "25     \t [2.53439854 0.13390985]. \t  -14.83973165469021 \t -0.6155236750725124\n",
            "26     \t [ 5.47169558 -0.97076746]. \t  -45.72800371793056 \t -0.6155236750725124\n",
            "27     \t [-3.07015158 -5.64971894]. \t  -8970.141083313809 \t -0.6155236750725124\n",
            "28     \t [ 0.37922296 -0.50519886]. \t  \u001b[92m-0.4198061371296233\u001b[0m \t -0.4198061371296233\n",
            "29     \t [ 0.57594447 -1.14719725]. \t  -8.635563854328094 \t -0.4198061371296233\n",
            "30     \t [ 3.35635613 -0.94804479]. \t  -10.411993704469197 \t -0.4198061371296233\n",
            "31     \t [ 1.81277462 -1.25244666]. \t  -4.1690475702031025 \t -0.4198061371296233\n",
            "32     \t [ 0.0161191  -0.11549953]. \t  -0.968244709594219 \t -0.4198061371296233\n",
            "33     \t [ 2.18378448 -1.06049389]. \t  -1.4099288405088621 \t -0.4198061371296233\n",
            "34     \t [-0.02663715 -0.6129153 ]. \t  -2.2644506952403427 \t -0.4198061371296233\n",
            "35     \t [ 1.33632763 -0.07436351]. \t  -3.625785579881615 \t -0.4198061371296233\n",
            "36     \t [ 1.06265215 -0.75733626]. \t  \u001b[92m-0.018193713930163783\u001b[0m \t -0.018193713930163783\n",
            "37     \t [-4.43300249  9.32219148]. \t  -63568.16363615236 \t -0.018193713930163783\n",
            "38     \t [6.81335206 1.08989066]. \t  -73.1801599349233 \t -0.018193713930163783\n",
            "39     \t [3.70888647 1.99758835]. \t  -43.83516238631414 \t -0.018193713930163783\n",
            "40     \t [-2.42153327  0.30961151]. \t  -25.365060205979525 \t -0.018193713930163783\n",
            "41     \t [-1.46902068  2.63930342]. \t  -480.4693947990232 \t -0.018193713930163783\n",
            "42     \t [-1.21939027 -0.22142934]. \t  -8.397053644279984 \t -0.018193713930163783\n",
            "43     \t [-0.29581937  0.15495409]. \t  -1.9156009844946225 \t -0.018193713930163783\n",
            "44     \t [ 2.55097521 -0.7044754 ]. \t  -7.262770347372372 \t -0.018193713930163783\n",
            "45     \t [1.9295137  0.81538814]. \t  -1.5835111228497787 \t -0.018193713930163783\n",
            "46     \t [ 2.90744647 -1.50894482]. \t  -9.059502400229494 \t -0.018193713930163783\n",
            "47     \t [-9.99027234 -4.90305924]. \t  -6865.094476833952 \t -0.018193713930163783\n",
            "48     \t [ 6.26972533 -8.79393908]. \t  -44071.11153816772 \t -0.018193713930163783\n",
            "49     \t [ 9.94343064 -3.91729509]. \t  -940.8585646067031 \t -0.018193713930163783\n",
            "50     \t [-1.68077183  6.07669278]. \t  -11417.703688746778 \t -0.018193713930163783\n",
            "51     \t [0.09078931 0.49741316]. \t  -1.1531775245016371 \t -0.018193713930163783\n",
            "52     \t [ 0.70468157 -0.497099  ]. \t  -0.1758054681011272 \t -0.018193713930163783\n",
            "53     \t [ 3.94567893 -1.31045478]. \t  -9.199461528723564 \t -0.018193713930163783\n",
            "54     \t [ 1.21790954 -0.79675347]. \t  -0.052835028882927494 \t -0.018193713930163783\n",
            "55     \t [-6.06641255 -7.72376087]. \t  -31489.91030820426 \t -0.018193713930163783\n",
            "56     \t [1.38284893 0.44448254]. \t  -2.0977528336743543 \t -0.018193713930163783\n",
            "57     \t [-1.09754543  0.24237024]. \t  -7.352302813837978 \t -0.018193713930163783\n",
            "58     \t [2.58561857 1.90258421]. \t  -45.83426613540559 \t -0.018193713930163783\n",
            "59     \t [ 4.91041875 -1.98267389]. \t  -32.71493808932828 \t -0.018193713930163783\n",
            "60     \t [-2.37426724  7.82643393]. \t  -31201.6159004122 \t -0.018193713930163783\n",
            "61     \t [ 8.3389707  -1.14735903]. \t  -118.9797646176284 \t -0.018193713930163783\n",
            "62     \t [9.45672564 4.78440042]. \t  -2710.4183504109697 \t -0.018193713930163783\n",
            "63     \t [5.05992032 1.65626816]. \t  -16.84680550626666 \t -0.018193713930163783\n",
            "64     \t [8.92143672 2.8015432 ]. \t  -154.5734982537207 \t -0.018193713930163783\n",
            "65     \t [ 1.0985074  -0.80371193]. \t  -0.0845095280332432 \t -0.018193713930163783\n",
            "66     \t [-7.35504328 -2.27957323]. \t  -699.7863099239688 \t -0.018193713930163783\n",
            "67     \t [ 6.34112375 -3.83553244]. \t  -1094.038378129007 \t -0.018193713930163783\n",
            "68     \t [-1.73689293 -2.46917106]. \t  -395.6084880472298 \t -0.018193713930163783\n",
            "69     \t [ 7.22569951 -2.03312765]. \t  -40.92884803282169 \t -0.018193713930163783\n",
            "70     \t [-7.09636232  3.45590826]. \t  -1985.4394576424781 \t -0.018193713930163783\n",
            "71     \t [ 5.43924997 -2.69990559]. \t  -186.77628424485692 \t -0.018193713930163783\n",
            "72     \t [-0.60794976 -6.81148879]. \t  -17449.970281161743 \t -0.018193713930163783\n",
            "73     \t [ 4.29054986 -1.7287162 ]. \t  -16.51540296948313 \t -0.018193713930163783\n",
            "74     \t [ 4.46767387 -0.86910936]. \t  -29.512125267131616 \t -0.018193713930163783\n",
            "75     \t [3.92566207 1.19720281]. \t  -10.80276944566813 \t -0.018193713930163783\n",
            "76     \t [0.5117976 0.4381675]. \t  -0.2710154883400123 \t -0.018193713930163783\n",
            "77     \t [-5.43939435 -1.20356317]. \t  -180.46102889329177 \t -0.018193713930163783\n",
            "78     \t [1.07329673 0.74406266]. \t  \u001b[92m-0.007679210655349976\u001b[0m \t -0.007679210655349976\n",
            "79     \t [5.61940086 1.89146486]. \t  -26.05670548141057 \t -0.007679210655349976\n",
            "80     \t [ 5.13145893 -6.98450545]. \t  -17105.591663517112 \t -0.007679210655349976\n",
            "81     \t [0.99500481 3.75849543]. \t  -1485.9503694431025 \t -0.007679210655349976\n",
            "82     \t [-6.86179769 -6.00034947]. \t  -12502.820114758297 \t -0.007679210655349976\n",
            "83     \t [9.01949008 0.36589373]. \t  -217.49790444167454 \t -0.007679210655349976\n",
            "84     \t [ 5.39378092 -7.0924974 ]. \t  -18150.434280823014 \t -0.007679210655349976\n",
            "85     \t [4.32363851 5.40727383]. \t  -5876.267471595092 \t -0.007679210655349976\n",
            "86     \t [1.0762229  1.21687051]. \t  -7.1147089069523295 \t -0.007679210655349976\n",
            "87     \t [-2.10379631 -5.07196877]. \t  -5745.594840224066 \t -0.007679210655349976\n",
            "88     \t [-1.65719838  1.17430137]. \t  -46.04808128627561 \t -0.007679210655349976\n",
            "89     \t [6.63753775 3.99835237]. \t  -1315.6183299249508 \t -0.007679210655349976\n",
            "90     \t [-9.46301857  0.66459733]. \t  -323.5706524987455 \t -0.007679210655349976\n",
            "91     \t [ 4.69313292 -1.60502908]. \t  -14.060783298348543 \t -0.007679210655349976\n",
            "92     \t [ 0.71210475 -0.51965567]. \t  -0.14206593561364175 \t -0.007679210655349976\n",
            "93     \t [3.60505136 0.91041019]. \t  -14.370698529723422 \t -0.007679210655349976\n",
            "94     \t [ 0.75676517 -4.31883246]. \t  -2671.551655887892 \t -0.007679210655349976\n",
            "95     \t [-2.55040769 -1.69504259]. \t  -150.27739847390453 \t -0.007679210655349976\n",
            "96     \t [2.32485898 1.07314851]. \t  -1.7561812851922909 \t -0.007679210655349976\n",
            "97     \t [ 1.37175544 -1.3771187 ]. \t  -11.862198666920797 \t -0.007679210655349976\n",
            "98     \t [-2.43027751 -0.81777835]. \t  -40.15944320558931 \t -0.007679210655349976\n",
            "99     \t [-6.33179118  5.3289402 ]. \t  -8023.791023469346 \t -0.007679210655349976\n",
            "100    \t [-2.64019465  8.15832871]. \t  -36873.09521062235 \t -0.007679210655349976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "561efc44-ed46-4d98-afe2-a9a76d14ee6c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-7.8100025  -9.15569971]. \t  -61652.61989589963 \t -123.0699213709516\n",
            "init   \t [ 1.9938517 -4.0025859]. \t  -1806.696586806874 \t -123.0699213709516\n",
            "init   \t [ 5.93223564 -2.6760775 ]. \t  -165.12946982275756 \t -123.0699213709516\n",
            "init   \t [ 7.61216688 -4.04316733]. \t  -1301.9580077172761 \t -123.0699213709516\n",
            "init   \t [-3.41793472  1.37433832]. \t  -123.0699213709516 \t -123.0699213709516\n",
            "1      \t [6.56161104 9.13365295]. \t  -51413.89230031877 \t -123.0699213709516\n",
            "2      \t [-9.10684612  8.65627415]. \t  -50644.440793557515 \t -123.0699213709516\n",
            "3      \t [-10.          -1.01612212]. \t  -412.128851724219 \t -123.0699213709516\n",
            "4      \t [-0.33238026  7.74858887]. \t  -29000.664238133446 \t -123.0699213709516\n",
            "5      \t [  4.82192059 -10.        ]. \t  -76203.57244005633 \t -123.0699213709516\n",
            "6      \t [4.00756187 2.63942119]. \t  -206.07758253038918 \t -123.0699213709516\n",
            "7      \t [9.9050887  3.11370436]. \t  -259.2394377182353 \t -123.0699213709516\n",
            "8      \t [-3.84056122 -4.11254245]. \t  -2860.972337296006 \t -123.0699213709516\n",
            "9      \t [-1.55042101 -9.50150381]. \t  -66332.8389820278 \t -123.0699213709516\n",
            "10     \t [-7.64908478  3.1532412 ]. \t  -1591.1530491456895 \t -123.0699213709516\n",
            "11     \t [-0.08954115  0.42372953]. \t  \u001b[92m-1.5896458762830885\u001b[0m \t -1.5896458762830885\n",
            "12     \t [-0.2702215   2.92725187]. \t  -607.6784508897609 \t -1.5896458762830885\n",
            "13     \t [-0.16616717 -1.14807649]. \t  -17.066012420053486 \t -1.5896458762830885\n",
            "14     \t [ 2.33873718 -0.2520255 ]. \t  -11.575481765529673 \t -1.5896458762830885\n",
            "15     \t [ 9.84333107 -8.2419934 ]. \t  -31839.06504341117 \t -1.5896458762830885\n",
            "16     \t [ 0.47114081 -0.40063569]. \t  \u001b[92m-0.32476581065786236\u001b[0m \t -0.32476581065786236\n",
            "17     \t [7.72475513 0.14427362]. \t  -163.28316254284846 \t -0.32476581065786236\n",
            "18     \t [-0.25793126 -0.22061096]. \t  -1.8348241205470743 \t -0.32476581065786236\n",
            "19     \t [-3.96251778  4.77561912]. \t  -4940.10555501439 \t -0.32476581065786236\n",
            "20     \t [-10.          -5.09747177]. \t  -7801.174337940854 \t -0.32476581065786236\n",
            "21     \t [9.96547899 6.16124452]. \t  -8780.870351741778 \t -0.32476581065786236\n",
            "22     \t [-5.94649247 -0.85121349]. \t  -157.64418563886866 \t -0.32476581065786236\n",
            "23     \t [6.42256851 3.78006133]. \t  -1011.1063774843094 \t -0.32476581065786236\n",
            "24     \t [4.60808273 0.40159656]. \t  -49.749678074431955 \t -0.32476581065786236\n",
            "25     \t [-4.2013882   9.31820224]. \t  -63294.82624002339 \t -0.32476581065786236\n",
            "26     \t [-0.4692321   0.01873558]. \t  -2.600319184299668 \t -0.32476581065786236\n",
            "27     \t [ 0.24332737 -0.26341393]. \t  -0.5944163631245485 \t -0.32476581065786236\n",
            "28     \t [0.28661224 0.0154986 ]. \t  -0.6726649420432028 \t -0.32476581065786236\n",
            "29     \t [ 1.21617242 -0.28224862]. \t  -2.280568347954156 \t -0.32476581065786236\n",
            "30     \t [-0.3596398   0.21727254]. \t  -2.260951379250973 \t -0.32476581065786236\n",
            "31     \t [ 1.72393385 -0.72025669]. \t  -1.4663548696404263 \t -0.32476581065786236\n",
            "32     \t [2.79919775 4.98374164]. \t  -4397.9865654715295 \t -0.32476581065786236\n",
            "33     \t [ 4.99079925 -5.77435322]. \t  -7628.598605526773 \t -0.32476581065786236\n",
            "34     \t [-2.86127997 -1.0613488 ]. \t  -67.219617532481 \t -0.32476581065786236\n",
            "35     \t [ 9.51741271 -0.14869969]. \t  -252.02895826208731 \t -0.32476581065786236\n",
            "36     \t [-0.11224753  0.1229503 ]. \t  -1.277696284017129 \t -0.32476581065786236\n",
            "37     \t [ 5.3535471  -1.84634284]. \t  -23.242404880016746 \t -0.32476581065786236\n",
            "38     \t [1.23650641 0.8177936 ]. \t  \u001b[92m-0.07636408731872549\u001b[0m \t -0.07636408731872549\n",
            "39     \t [ 2.85241623 -0.72254642]. \t  -9.971123583698848 \t -0.07636408731872549\n",
            "40     \t [-1.1702145   0.38344922]. \t  -8.998069265075667 \t -0.07636408731872549\n",
            "41     \t [-1.03162895 -0.65770837]. \t  -11.323137324881966 \t -0.07636408731872549\n",
            "42     \t [3.01237488 1.08526533]. \t  -4.912354766450856 \t -0.07636408731872549\n",
            "43     \t [9.13656295 3.73589931]. \t  -771.3794772485862 \t -0.07636408731872549\n",
            "44     \t [0.26388089 0.44404726]. \t  -0.5759188141122675 \t -0.07636408731872549\n",
            "45     \t [2.02891662 1.14422771]. \t  -1.753919802742928 \t -0.07636408731872549\n",
            "46     \t [3.01524324 0.56180564]. \t  -15.428041943388571 \t -0.07636408731872549\n",
            "47     \t [-7.18336567 -3.11886089]. \t  -1486.128460384242 \t -0.07636408731872549\n",
            "48     \t [ 1.29634866 -0.745742  ]. \t  -0.15559812718254662 \t -0.07636408731872549\n",
            "49     \t [ 3.90165304 -0.78526379]. \t  -22.66003626761829 \t -0.07636408731872549\n",
            "50     \t [-1.92521674  5.3810708 ]. \t  -7169.505023647784 \t -0.07636408731872549\n",
            "51     \t [5.06834609 5.96825386]. \t  -8773.955635900966 \t -0.07636408731872549\n",
            "52     \t [2.45394383 1.13319004]. \t  -2.140079578398679 \t -0.07636408731872549\n",
            "53     \t [1.00927738 0.78270075]. \t  -0.09336657669638086 \t -0.07636408731872549\n",
            "54     \t [0.63618974 0.33386174]. \t  -0.47392955243052554 \t -0.07636408731872549\n",
            "55     \t [ 6.13223941 -8.32988771]. \t  -35214.04422492559 \t -0.07636408731872549\n",
            "56     \t [-6.13893734 -1.59182807]. \t  -302.14783823643666 \t -0.07636408731872549\n",
            "57     \t [ 4.31783886 -1.62035979]. \t  -12.750125828786896 \t -0.07636408731872549\n",
            "58     \t [4.30514143 8.85815479]. \t  -46601.94656579291 \t -0.07636408731872549\n",
            "59     \t [-6.16955622  0.44482737]. \t  -137.60883826786048 \t -0.07636408731872549\n",
            "60     \t [ 5.61975548 -0.90315148]. \t  -53.15665473821805 \t -0.07636408731872549\n",
            "61     \t [-1.31521669 -1.20963095]. \t  -41.343091088664025 \t -0.07636408731872549\n",
            "62     \t [-7.01656824  4.39875358]. \t  -4243.920950143667 \t -0.07636408731872549\n",
            "63     \t [-9.667712    2.35998694]. \t  -979.6449948408659 \t -0.07636408731872549\n",
            "64     \t [ 7.72355912 -9.4819064 ]. \t  -59274.82494528694 \t -0.07636408731872549\n",
            "65     \t [9.06425658 7.71589085]. \t  -24267.53443867601 \t -0.07636408731872549\n",
            "66     \t [-9.87418281 -2.53173674]. \t  -1148.243606507633 \t -0.07636408731872549\n",
            "67     \t [ 9.98143567 -2.49252006]. \t  -92.61125422971762 \t -0.07636408731872549\n",
            "68     \t [-5.39998576 -6.84540931]. \t  -19690.208465023108 \t -0.07636408731872549\n",
            "69     \t [4.31523028 4.55266855]. \t  -2769.505070887002 \t -0.07636408731872549\n",
            "70     \t [ 1.0094345  -0.76651103]. \t  \u001b[92m-0.05496475915320519\u001b[0m \t -0.05496475915320519\n",
            "71     \t [ 9.42253238 -0.3995322 ]. \t  -236.67848034395837 \t -0.05496475915320519\n",
            "72     \t [ 1.08284567 -0.76123167]. \t  \u001b[92m-0.018446319864058494\u001b[0m \t -0.018446319864058494\n",
            "73     \t [ 2.78685118 -0.15378793]. \t  -18.203102605247672 \t -0.018446319864058494\n",
            "74     \t [-8.84729879 -5.24014396]. \t  -8229.051424410012 \t -0.018446319864058494\n",
            "75     \t [-4.09492747 -2.35923518]. \t  -489.6757853579257 \t -0.018446319864058494\n",
            "76     \t [ 8.86426913 -4.30594698]. \t  -1654.3678980790066 \t -0.018446319864058494\n",
            "77     \t [-5.91462627e+00 -2.10471570e-03]. \t  -117.77787374168585 \t -0.018446319864058494\n",
            "78     \t [ 9.87354977 -8.45660868]. \t  -35539.20050149259 \t -0.018446319864058494\n",
            "79     \t [-0.61430943 -9.6560787 ]. \t  -70010.95138703822 \t -0.018446319864058494\n",
            "80     \t [1.67849088 7.00937803]. \t  -18657.502736448616 \t -0.018446319864058494\n",
            "81     \t [1.13377697 0.69766944]. \t  -0.06928311330863457 \t -0.018446319864058494\n",
            "82     \t [-5.28630706  7.33387952]. \t  -25513.321947840155 \t -0.018446319864058494\n",
            "83     \t [ 7.09723595 -3.04258965]. \t  -297.89342045477264 \t -0.018446319864058494\n",
            "84     \t [ 7.18282693 -1.81949646]. \t  -38.85834506854252 \t -0.018446319864058494\n",
            "85     \t [-1.53502454 -5.06789235]. \t  -5603.688678125964 \t -0.018446319864058494\n",
            "86     \t [ 0.93232909 -0.2382274 ]. \t  -1.3455264914714893 \t -0.018446319864058494\n",
            "87     \t [1.27632162 2.44985546]. \t  -230.22465187294836 \t -0.018446319864058494\n",
            "88     \t [-4.54396181  0.84839228]. \t  -102.34007448953388 \t -0.018446319864058494\n",
            "89     \t [ 1.52017907 -1.78452586]. \t  -47.29397676328524 \t -0.018446319864058494\n",
            "90     \t [ 0.66501211 -3.19880746]. \t  -784.1705345699172 \t -0.018446319864058494\n",
            "91     \t [-6.54058379  8.44174   ]. \t  -44498.522389758466 \t -0.018446319864058494\n",
            "92     \t [-0.39591866  9.01209183]. \t  -53030.154493987946 \t -0.018446319864058494\n",
            "93     \t [ 5.00549469 -1.59682851]. \t  -16.061745474126276 \t -0.018446319864058494\n",
            "94     \t [3.22035406 7.00700377]. \t  -18045.75399055018 \t -0.018446319864058494\n",
            "95     \t [-4.18805224  5.0527217 ]. \t  -6131.608945360351 \t -0.018446319864058494\n",
            "96     \t [-9.842156   -3.27093426]. \t  -2069.4497773451208 \t -0.018446319864058494\n",
            "97     \t [ 9.86423471 -7.70892022]. \t  -23836.50570445349 \t -0.018446319864058494\n",
            "98     \t [ 1.97711577 -1.28745106]. \t  -4.534947161869551 \t -0.018446319864058494\n",
            "99     \t [-3.15848066  2.69480699]. \t  -642.6310423143477 \t -0.018446319864058494\n",
            "100    \t [-3.12727461 -0.16172416]. \t  -37.25390468137874 \t -0.018446319864058494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "8a4d1834-47a5-4b85-ad34-4cc2907545dc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.44557117 -0.23843202]. \t  -28.18355402366269 \t -28.18355402366269\n",
            "init   \t [ 6.50990348 -9.37107225]. \t  -57236.27240949214 \t -28.18355402366269\n",
            "init   \t [6.16099927 1.31234839]. \t  -41.39446954533859 \t -28.18355402366269\n",
            "init   \t [-4.04755003 -9.06608559]. \t  -56766.42271365435 \t -28.18355402366269\n",
            "init   \t [ 9.81254799 -9.86348534]. \t  -68353.23444364112 \t -28.18355402366269\n",
            "1      \t [-8.12773813  6.26616826]. \t  -15102.348898507695 \t -28.18355402366269\n",
            "2      \t [3.58702664 9.74831565]. \t  -69550.52203005194 \t -28.18355402366269\n",
            "3      \t [-9.43477705 -1.75785015]. \t  -596.5317353689182 \t -28.18355402366269\n",
            "4      \t [-2.23830225 -1.73570789]. \t  -147.0629539988178 \t -28.18355402366269\n",
            "5      \t [-0.99416439  4.28848031]. \t  -2858.073046912701 \t -28.18355402366269\n",
            "6      \t [ 9.27370946 -3.20211659]. \t  -320.83245363494626 \t -28.18355402366269\n",
            "7      \t [8.89638698 5.527643  ]. \t  -5514.807675936192 \t -28.18355402366269\n",
            "8      \t [ 1.5148952  -5.57294389]. \t  -7345.109236179049 \t -28.18355402366269\n",
            "9      \t [-10.          -7.37120879]. \t  -28285.871016413024 \t -28.18355402366269\n",
            "10     \t [-2.75579435  9.61812412]. \t  -70531.06769211043 \t -28.18355402366269\n",
            "11     \t [-5.8122193   1.45819911]. \t  -249.01110059002974 \t -28.18355402366269\n",
            "12     \t [9.87513368 9.90474429]. \t  -69518.58177407873 \t -28.18355402366269\n",
            "13     \t [-6.14333713 -3.84498231]. \t  -2601.5947386103476 \t -28.18355402366269\n",
            "14     \t [ 5.31366882 -3.04055148]. \t  -365.8342206526909 \t -28.18355402366269\n",
            "15     \t [3.83695092 3.30455624]. \t  -656.281086102878 \t -28.18355402366269\n",
            "16     \t [ 0.52414869 -0.1072242 ]. \t  \u001b[92m-0.7287464016565732\u001b[0m \t -0.7287464016565732\n",
            "17     \t [ 0.83764716 -0.42010128]. \t  \u001b[92m-0.4961820089571961\u001b[0m \t -0.4961820089571961\n",
            "18     \t [9.31253201 0.86788036]. \t  -190.96856354894862 \t -0.4961820089571961\n",
            "19     \t [ 0.57664004 -0.89594019]. \t  -2.2960003794156894 \t -0.4961820089571961\n",
            "20     \t [-0.40773134  0.39363215]. \t  -3.0116755985543158 \t -0.4961820089571961\n",
            "21     \t [ 0.88766881 -0.055877  ]. \t  -1.5664360062152622 \t -0.4961820089571961\n",
            "22     \t [ 0.44528415 -0.28429176]. \t  \u001b[92m-0.46861358509810086\u001b[0m \t -0.46861358509810086\n",
            "23     \t [  1.13599088 -10.        ]. \t  -79093.8067387213 \t -0.46861358509810086\n",
            "24     \t [-10.           1.88319662]. \t  -705.3316599066405 \t -0.46861358509810086\n",
            "25     \t [ 1.1465174  -1.52825582]. \t  -24.867278945912584 \t -0.46861358509810086\n",
            "26     \t [ 5.86890583 -0.40519695]. \t  -85.10133798247173 \t -0.46861358509810086\n",
            "27     \t [-2.64357647  0.85147194]. \t  -46.79053217013646 \t -0.46861358509810086\n",
            "28     \t [0.78674205 0.45759508]. \t  \u001b[92m-0.31626149785962837\u001b[0m \t -0.31626149785962837\n",
            "29     \t [-0.14794848  0.42494163]. \t  -1.8361498491509356 \t -0.31626149785962837\n",
            "30     \t [-2.40402673 -5.44219706]. \t  -7610.331000610567 \t -0.31626149785962837\n",
            "31     \t [1.91406395 0.53132709]. \t  -4.4775273112542715 \t -0.31626149785962837\n",
            "32     \t [0.38439037 0.83173806]. \t  -2.3757206652801393 \t -0.31626149785962837\n",
            "33     \t [-4.50809061  4.88518664]. \t  -5487.995544544278 \t -0.31626149785962837\n",
            "34     \t [ 7.57892659 -5.18912668]. \t  -4326.060330460954 \t -0.31626149785962837\n",
            "35     \t [1.97194873 5.95761444]. \t  -9526.916957846863 \t -0.31626149785962837\n",
            "36     \t [1.02523531 1.14361819]. \t  -5.0599526580928 \t -0.31626149785962837\n",
            "37     \t [7.28286031 1.91169004]. \t  -39.475712508551204 \t -0.31626149785962837\n",
            "38     \t [ 2.77461248 -1.02101885]. \t  -4.10049332447318 \t -0.31626149785962837\n",
            "39     \t [ 2.1479141  -0.49235396]. \t  -6.849438590481957 \t -0.31626149785962837\n",
            "40     \t [-4.3517255  -0.64133564]. \t  -82.18872652515466 \t -0.31626149785962837\n",
            "41     \t [-0.5379206  -0.46630239]. \t  -4.257865651761023 \t -0.31626149785962837\n",
            "42     \t [ 2.62157826 -1.27249514]. \t  -3.3906707001495215 \t -0.31626149785962837\n",
            "43     \t [-0.11073318 -0.01054058]. \t  -1.258350394263938 \t -0.31626149785962837\n",
            "44     \t [4.84022995 4.91379184]. \t  -3790.6341503370104 \t -0.31626149785962837\n",
            "45     \t [ 7.39616664 -0.73667976]. \t  -120.5626468185589 \t -0.31626149785962837\n",
            "46     \t [2.30291155 2.193541  ]. \t  -108.87212053293173 \t -0.31626149785962837\n",
            "47     \t [4.28737905 1.12026767]. \t  -17.12501863145796 \t -0.31626149785962837\n",
            "48     \t [3.12787377 9.51196001]. \t  -63249.335349295434 \t -0.31626149785962837\n",
            "49     \t [0.87871973 0.68984091]. \t  \u001b[92m-0.025378949535781056\u001b[0m \t -0.025378949535781056\n",
            "50     \t [-9.68754667  9.79829972]. \t  -81480.73107563582 \t -0.025378949535781056\n",
            "51     \t [ 3.15553514 -1.82589276]. \t  -29.317901276308568 \t -0.025378949535781056\n",
            "52     \t [-1.2689238  -1.77401796]. \t  -119.55210110591288 \t -0.025378949535781056\n",
            "53     \t [-7.54571784  4.77818483]. \t  -5735.1731847284855 \t -0.025378949535781056\n",
            "54     \t [-5.06554453  3.76103289]. \t  -2262.073843733766 \t -0.025378949535781056\n",
            "55     \t [-8.55978671 -9.9309001 ]. \t  -84803.06583085285 \t -0.025378949535781056\n",
            "56     \t [ 7.98605608 -0.48736107]. \t  -161.63565597696189 \t -0.025378949535781056\n",
            "57     \t [-2.64801284  0.01406951]. \t  -27.33613532821893 \t -0.025378949535781056\n",
            "58     \t [-7.30446077 -4.45298308]. \t  -4479.931410554043 \t -0.025378949535781056\n",
            "59     \t [3.19764279 4.65963946]. \t  -3241.2264667529867 \t -0.025378949535781056\n",
            "60     \t [1.49521071 3.85263359]. \t  -1589.6380746331806 \t -0.025378949535781056\n",
            "61     \t [-1.1070838  -0.01178946]. \t  -6.892302351476257 \t -0.025378949535781056\n",
            "62     \t [3.21187392 1.58700168]. \t  -11.555642102741125 \t -0.025378949535781056\n",
            "63     \t [2.28876159 9.60659356]. \t  -66456.95186784181 \t -0.025378949535781056\n",
            "64     \t [ 1.35127257 -6.48666679]. \t  -13712.604773039777 \t -0.025378949535781056\n",
            "65     \t [7.82567942 8.46717967]. \t  -36799.93362102439 \t -0.025378949535781056\n",
            "66     \t [ 0.89919885 -2.90745419]. \t  -512.4826454303164 \t -0.025378949535781056\n",
            "67     \t [ 1.32890142 -1.14151099]. \t  -3.3706212636673056 \t -0.025378949535781056\n",
            "68     \t [-8.75020178 -5.68207233]. \t  -10847.325259422847 \t -0.025378949535781056\n",
            "69     \t [1.30576307 0.55462881]. \t  -1.0471732846928483 \t -0.025378949535781056\n",
            "70     \t [ 2.75394985 -3.95416581]. \t  -1629.5040380870096 \t -0.025378949535781056\n",
            "71     \t [ 3.75929949 -1.62624911]. \t  -12.29597963833867 \t -0.025378949535781056\n",
            "72     \t [3.42076995 2.00378041]. \t  -48.35514228378264 \t -0.025378949535781056\n",
            "73     \t [ 1.15990097 -2.97987577]. \t  -551.1069438955279 \t -0.025378949535781056\n",
            "74     \t [2.80736656 0.9606339 ]. \t  -5.116429121354344 \t -0.025378949535781056\n",
            "75     \t [ 0.67615036 -0.89514612]. \t  -1.8213969141604853 \t -0.025378949535781056\n",
            "76     \t [ 3.46310542 -1.53504116]. \t  -9.189875202473992 \t -0.025378949535781056\n",
            "77     \t [0.5090909  9.01203222]. \t  -52439.236761347966 \t -0.025378949535781056\n",
            "78     \t [-3.40110708 -5.96485396]. \t  -11137.778595501333 \t -0.025378949535781056\n",
            "79     \t [-2.28071149 -6.41422133]. \t  -14313.303102479305 \t -0.025378949535781056\n",
            "80     \t [-3.22188164 -3.48334611]. \t  -1529.1455250863169 \t -0.025378949535781056\n",
            "81     \t [3.90257918 1.03769522]. \t  -14.54266304784133 \t -0.025378949535781056\n",
            "82     \t [-9.04863361  1.2583201 ]. \t  -399.40569125181196 \t -0.025378949535781056\n",
            "83     \t [-2.83232488 -0.98141774]. \t  -59.97690712122156 \t -0.025378949535781056\n",
            "84     \t [-3.18441793 -4.2986316 ]. \t  -3240.090681636895 \t -0.025378949535781056\n",
            "85     \t [-2.56593163  1.55682023]. \t  -122.63020144000545 \t -0.025378949535781056\n",
            "86     \t [8.73188655 3.77820009]. \t  -845.2649612317308 \t -0.025378949535781056\n",
            "87     \t [8.49147411 2.59623271]. \t  -105.90989941353337 \t -0.025378949535781056\n",
            "88     \t [-6.99691523  3.70409887]. \t  -2435.848917227575 \t -0.025378949535781056\n",
            "89     \t [0.39383812 0.63744797]. \t  -0.7182889889686307 \t -0.025378949535781056\n",
            "90     \t [1.53682398 0.84101353]. \t  -0.3180537056438522 \t -0.025378949535781056\n",
            "91     \t [-5.39542752 -4.86496958]. \t  -5602.078430337968 \t -0.025378949535781056\n",
            "92     \t [-5.34625143 -7.38849223]. \t  -26272.63213176849 \t -0.025378949535781056\n",
            "93     \t [ 4.34379926 -2.21067563]. \t  -70.158921569656 \t -0.025378949535781056\n",
            "94     \t [-6.10772408 -4.44348195]. \t  -4208.653841123318 \t -0.025378949535781056\n",
            "95     \t [-5.31785502 -2.00414361]. \t  -396.4157381078131 \t -0.025378949535781056\n",
            "96     \t [ 9.69587589 -7.31644498]. \t  -19035.44955913957 \t -0.025378949535781056\n",
            "97     \t [-5.07031387 -4.65553508]. \t  -4725.5175973484165 \t -0.025378949535781056\n",
            "98     \t [-3.20025038  3.94954699]. \t  -2384.099734581828 \t -0.025378949535781056\n",
            "99     \t [-2.21002255  4.40918558]. \t  -3387.3857635547797 \t -0.025378949535781056\n",
            "100    \t [ 5.85237815 -0.64502844]. \t  -73.95148943738262 \t -0.025378949535781056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "a997494e-e659-4778-c496-135448cfb6f4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.07179171 -7.69986114]. \t  -26686.60864226137 \t -217.28346804397978\n",
            "init   \t [ 9.00565729 -0.35617197]. \t  -217.28346804397978 \t -217.28346804397978\n",
            "init   \t [ 7.4494907  -5.75334638]. \t  -6945.3080829060855 \t -217.28346804397978\n",
            "init   \t [-9.1858075  -2.05611077]. \t  -726.1597680698058 \t -217.28346804397978\n",
            "init   \t [-5.33735605  6.83481449]. \t  -19549.897628464034 \t -217.28346804397978\n",
            "1      \t [7.70674409 9.05288798]. \t  -48843.620887164754 \t -217.28346804397978\n",
            "2      \t [-6.08246046 -9.42114812]. \t  -67466.99755823516 \t -217.28346804397978\n",
            "3      \t [1.36988239 1.21701407]. \t  \u001b[92m-5.2080597150791546\u001b[0m \t -5.2080597150791546\n",
            "4      \t [-2.2657594  -2.94987257]. \t  -784.4244249986725 \t -5.2080597150791546\n",
            "5      \t [1.36029598 6.5640609 ]. \t  -14386.78766565215 \t -5.2080597150791546\n",
            "6      \t [-10.           3.22899282]. \t  -2024.7892163098 \t -5.2080597150791546\n",
            "7      \t [ 3.89763307 -1.60847162]. \t  -11.65635053336267 \t -5.2080597150791546\n",
            "8      \t [-5.21810853  1.00328333]. \t  -143.24721520946702 \t -5.2080597150791546\n",
            "9      \t [7.29815912 3.47480791]. \t  -607.5401820018474 \t -5.2080597150791546\n",
            "10     \t [ -1.14380564 -10.        ]. \t  -80922.2569933416 \t -5.2080597150791546\n",
            "11     \t [-9.94874004 -6.37397454]. \t  -16756.15687045089 \t -5.2080597150791546\n",
            "12     \t [-2.0081584   2.18781973]. \t  -277.3005891531996 \t -5.2080597150791546\n",
            "13     \t [3.76960642 1.35168764]. \t  -7.69739446412836 \t -5.2080597150791546\n",
            "14     \t [  7.66990468 -10.        ]. \t  -74026.21876210123 \t -5.2080597150791546\n",
            "15     \t [ 1.62675198 -3.01589835]. \t  -549.1603743425381 \t -5.2080597150791546\n",
            "16     \t [ 5.2868635  -1.42033046]. \t  -21.513139630185915 \t -5.2080597150791546\n",
            "17     \t [-10.           9.75307564]. \t  -80317.09512636824 \t -5.2080597150791546\n",
            "18     \t [2.51787116 0.33260942]. \t  -12.852796426810821 \t -5.2080597150791546\n",
            "19     \t [-5.51866976 -4.89760087]. \t  -5765.208079019232 \t -5.2080597150791546\n",
            "20     \t [4.65853419 1.07853318]. \t  -24.261941311519138 \t -5.2080597150791546\n",
            "21     \t [2.37248636 2.25532211]. \t  -123.57836054976343 \t -5.2080597150791546\n",
            "22     \t [-0.23176857  0.30538848]. \t  \u001b[92m-1.8671915633519056\u001b[0m \t -1.8671915633519056\n",
            "23     \t [1.47057632 0.09565573]. \t  -4.43965489103856 \t -1.8671915633519056\n",
            "24     \t [0.61580138 0.83697105]. \t  \u001b[92m-1.3808113229146892\u001b[0m \t -1.3808113229146892\n",
            "25     \t [-0.08589227 -0.74983567]. \t  -4.10929510607212 \t -1.3808113229146892\n",
            "26     \t [ 3.73260862 -0.82127331]. \t  -18.830523520483272 \t -1.3808113229146892\n",
            "27     \t [ 4.53873862 -2.41529192]. \t  -114.15459448015595 \t -1.3808113229146892\n",
            "28     \t [1.00089094 0.70059932]. \t  \u001b[92m-0.000739005083901204\u001b[0m \t -0.000739005083901204\n",
            "29     \t [-1.71968977  9.80761426]. \t  -75355.68505306277 \t -0.000739005083901204\n",
            "30     \t [4.22750304 2.95391764]. \t  -360.15220898852897 \t -0.000739005083901204\n",
            "31     \t [0.94313508 0.9554011 ]. \t  -1.5606606726115926 \t -0.000739005083901204\n",
            "32     \t [1.13406832 0.3469823 ]. \t  -1.613854347387967 \t -0.000739005083901204\n",
            "33     \t [3.50401141 0.45794937]. \t  -25.29929312783836 \t -0.000739005083901204\n",
            "34     \t [0.53370241 0.59582478]. \t  -0.2796052337667702 \t -0.000739005083901204\n",
            "35     \t [ 5.85761216 -0.86729645]. \t  -61.49719909410684 \t -0.000739005083901204\n",
            "36     \t [-0.40620047 -0.41400662]. \t  -3.099412039461993 \t -0.000739005083901204\n",
            "37     \t [1.66145607 0.57800211]. \t  -2.410747094671492 \t -0.000739005083901204\n",
            "38     \t [2.65620868 0.91382411]. \t  -4.687654549085248 \t -0.000739005083901204\n",
            "39     \t [ 3.12613065 -1.20084956]. \t  -4.637609237986539 \t -0.000739005083901204\n",
            "40     \t [9.8171778  4.96580811]. \t  -3198.4516038618103 \t -0.000739005083901204\n",
            "41     \t [ 9.8356841  -3.07346653]. \t  -242.11726431745427 \t -0.000739005083901204\n",
            "42     \t [-0.99908876 -5.6395217 ]. \t  -8352.2541439851 \t -0.000739005083901204\n",
            "43     \t [3.56556248 9.60408146]. \t  -65464.30595808899 \t -0.000739005083901204\n",
            "44     \t [3.53374287 0.87812495]. \t  -14.352284242742854 \t -0.000739005083901204\n",
            "45     \t [ 0.59566259 -0.23651319]. \t  -0.6315857806922273 \t -0.000739005083901204\n",
            "46     \t [-6.52327647  2.87128272]. \t  -1115.686065094399 \t -0.000739005083901204\n",
            "47     \t [-5.62294251 -0.93869699]. \t  -152.94710138100294 \t -0.000739005083901204\n",
            "48     \t [ 1.01870563 -0.71130785]. \t  \u001b[92m-0.000442052499338034\u001b[0m \t -0.000442052499338034\n",
            "49     \t [ 1.65413147 -0.56809754]. \t  -2.4626854038665797 \t -0.000442052499338034\n",
            "50     \t [5.48219774 3.88996803]. \t  -1248.3360607294771 \t -0.000442052499338034\n",
            "51     \t [ 1.27041772 -0.58512596]. \t  -0.7591513505381631 \t -0.000442052499338034\n",
            "52     \t [-4.12610465 -8.98313584]. \t  -54819.728817632174 \t -0.000442052499338034\n",
            "53     \t [4.12422151 9.30667802]. \t  -57202.28252018437 \t -0.000442052499338034\n",
            "54     \t [ 0.72766745 -0.28826936]. \t  -0.7046598830045853 \t -0.000442052499338034\n",
            "55     \t [-7.30615915  7.14135777]. \t  -23963.782673163754 \t -0.000442052499338034\n",
            "56     \t [-3.76757267  0.39468033]. \t  -56.00815295654134 \t -0.000442052499338034\n",
            "57     \t [ 6.76050866 -2.05206792]. \t  -38.70433769107767 \t -0.000442052499338034\n",
            "58     \t [-4.24352113  0.81128115]. \t  -89.31894166189932 \t -0.000442052499338034\n",
            "59     \t [-0.55197025  0.03117998]. \t  -3.0222544887075697 \t -0.000442052499338034\n",
            "60     \t [-2.15787393  4.9135166 ]. \t  -5098.997888001736 \t -0.000442052499338034\n",
            "61     \t [-8.11865691 -7.58792165]. \t  -30475.006856139753 \t -0.000442052499338034\n",
            "62     \t [1.34370983 0.71331367]. \t  -0.33078893105549717 \t -0.000442052499338034\n",
            "63     \t [9.86521522 1.45677221]. \t  -141.77982989370372 \t -0.000442052499338034\n",
            "64     \t [2.30669218 5.53075925]. \t  -6933.509486673679 \t -0.000442052499338034\n",
            "65     \t [ 6.22064745 -9.68865117]. \t  -65925.74995138083 \t -0.000442052499338034\n",
            "66     \t [-6.8674043  -1.07907733]. \t  -231.0369781008103 \t -0.000442052499338034\n",
            "67     \t [ 0.70598758 -0.89856625]. \t  -1.738478235398563 \t -0.000442052499338034\n",
            "68     \t [-1.70729211 -9.56820662]. \t  -68315.6542930769 \t -0.000442052499338034\n",
            "69     \t [-2.33620395  0.0089144 ]. \t  -22.047439886077974 \t -0.000442052499338034\n",
            "70     \t [ 0.23797604 -0.43970884]. \t  -0.6249108354130698 \t -0.000442052499338034\n",
            "71     \t [ 6.86119635 -4.18249647]. \t  -1616.4250433924637 \t -0.000442052499338034\n",
            "72     \t [ 1.50812225 -1.30782685]. \t  -7.575030105411535 \t -0.000442052499338034\n",
            "73     \t [ 4.79010638 -1.18871613]. \t  -22.079610760707066 \t -0.000442052499338034\n",
            "74     \t [9.94231219 3.27114541]. \t  -342.55812679072415 \t -0.000442052499338034\n",
            "75     \t [-8.46680573  8.44266127]. \t  -45706.03566664887 \t -0.000442052499338034\n",
            "76     \t [ 3.25293074 -5.22260964]. \t  -5268.121662610138 \t -0.000442052499338034\n",
            "77     \t [-9.60702549  8.65089734]. \t  -50854.704943960656 \t -0.000442052499338034\n",
            "78     \t [ 1.43472543 -8.73045771]. \t  -45606.45220107105 \t -0.000442052499338034\n",
            "79     \t [1.71818045 9.78017345]. \t  -71885.7673970747 \t -0.000442052499338034\n",
            "80     \t [-9.36547476  2.02581118]. \t  -725.0845732068685 \t -0.000442052499338034\n",
            "81     \t [ 9.86970464 -8.11441696]. \t  -29757.834494110662 \t -0.000442052499338034\n",
            "82     \t [ 2.18196112 -1.1497247 ]. \t  -1.8235000755032797 \t -0.000442052499338034\n",
            "83     \t [ 1.86065053 -1.11263974]. \t  -1.4978677470732775 \t -0.000442052499338034\n",
            "84     \t [9.86765527 2.27619797]. \t  -79.12436879614377 \t -0.000442052499338034\n",
            "85     \t [ 1.22662057 -0.69554102]. \t  -0.18558720608477844 \t -0.000442052499338034\n",
            "86     \t [ 1.09998098 -8.34432714]. \t  -38173.95512799832 \t -0.000442052499338034\n",
            "87     \t [-5.05129396  0.33177034]. \t  -92.1942562736133 \t -0.000442052499338034\n",
            "88     \t [ 7.11878012 -1.20910938]. \t  -72.63365938492225 \t -0.000442052499338034\n",
            "89     \t [-8.17943794 -0.93216136]. \t  -280.967265407088 \t -0.000442052499338034\n",
            "90     \t [-0.06526875  2.98658386]. \t  -642.2867100258273 \t -0.000442052499338034\n",
            "91     \t [ 2.25504466 -3.58192906]. \t  -1097.1994125105232 \t -0.000442052499338034\n",
            "92     \t [3.84302816 0.29679479]. \t  -34.97444363718047 \t -0.000442052499338034\n",
            "93     \t [5.81368138 7.99447649]. \t  -29795.869878511236 \t -0.000442052499338034\n",
            "94     \t [-0.18516248 -0.2534437 ]. \t  -1.6013375158620602 \t -0.000442052499338034\n",
            "95     \t [-0.93490249  0.31466597]. \t  -6.310916704571323 \t -0.000442052499338034\n",
            "96     \t [-3.95788396  5.24743984]. \t  -6993.457889775188 \t -0.000442052499338034\n",
            "97     \t [-3.87570309  8.8357695 ]. \t  -51234.876069122845 \t -0.000442052499338034\n",
            "98     \t [-3.18917536 -1.71868298]. \t  -183.05701438345855 \t -0.000442052499338034\n",
            "99     \t [ 9.63310453 -6.07846482]. \t  -8333.833502275556 \t -0.000442052499338034\n",
            "100    \t [9.84274302 3.50032441]. \t  -508.1308030874569 \t -0.000442052499338034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "50e592c9-b3bc-43bf-d385-0f0554909fe0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.62071325 6.42495699]. \t  -10714.638013792866 \t -9.740000060755142\n",
            "init   \t [ 2.91787228 -1.52623973]. \t  -9.740000060755142 \t -9.740000060755142\n",
            "init   \t [-5.95378042 -0.16186459]. \t  -120.5034756901465 \t -9.740000060755142\n",
            "init   \t [-7.22883337 -0.94957929]. \t  -230.87623811313406 \t -9.740000060755142\n",
            "init   \t [-7.72530745 -9.94151023]. \t  -84448.33746856163 \t -9.740000060755142\n",
            "1      \t [ 9.71879818 -6.86089495]. \t  -14331.165445255861 \t -9.740000060755142\n",
            "2      \t [-0.41525252  9.69422331]. \t  -70969.40865155176 \t -9.740000060755142\n",
            "3      \t [-9.5194484  9.1420568]. \t  -62537.95985021731 \t -9.740000060755142\n",
            "4      \t [ 0.94955298 -9.31626642]. \t  -59606.43954696483 \t -9.740000060755142\n",
            "5      \t [-0.74917245  2.65474893]. \t  -443.7812994605227 \t -9.740000060755142\n",
            "6      \t [ 8.16826525 -0.05442229]. \t  -184.6316700015674 \t -9.740000060755142\n",
            "7      \t [-1.85090344 -3.03729904]. \t  -832.411130970655 \t -9.740000060755142\n",
            "8      \t [4.09380652 4.61516743]. \t  -2974.9547866200423 \t -9.740000060755142\n",
            "9      \t [-10.         -4.5993489]. \t  -5593.258045291162 \t -9.740000060755142\n",
            "10     \t [-10.          3.0375864]. \t  -1740.244559462631 \t -9.740000060755142\n",
            "11     \t [ 4.6459728  -4.90366942]. \t  -3788.398549867353 \t -9.740000060755142\n",
            "12     \t [-5.16186325  3.49261323]. \t  -1785.3852062800825 \t -9.740000060755142\n",
            "13     \t [6.87610821 9.95490249]. \t  -73244.31967606739 \t -9.740000060755142\n",
            "14     \t [-5.51957479 -4.84275595]. \t  -5539.086848821168 \t -9.740000060755142\n",
            "15     \t [3.80803425 0.42151296]. \t  -31.727163938244665 \t -9.740000060755142\n",
            "16     \t [ 1.21924563 -1.05885901]. \t  \u001b[92m-2.1416143497601556\u001b[0m \t -2.1416143497601556\n",
            "17     \t [ 5.55181601 -9.5015015 ]. \t  -61274.39091607238 \t -2.1416143497601556\n",
            "18     \t [ 0.91806768 -0.47120573]. \t  \u001b[92m-0.45606112642801944\u001b[0m \t -0.45606112642801944\n",
            "19     \t [ 0.50727398 -0.8185992 ]. \t  -1.6303413971309233 \t -0.45606112642801944\n",
            "20     \t [-4.64830786  7.22192058]. \t  -23776.720078012306 \t -0.45606112642801944\n",
            "21     \t [ 1.26074329 -0.25209238]. \t  -2.6382761585751155 \t -0.45606112642801944\n",
            "22     \t [-3.5787386   0.06074801]. \t  -46.685349674750675 \t -0.45606112642801944\n",
            "23     \t [ 9.22226494 -2.27702436]. \t  -70.23876294356214 \t -0.45606112642801944\n",
            "24     \t [7.5178201 2.2895434]. \t  -60.078638751211855 \t -0.45606112642801944\n",
            "25     \t [ 1.64189516 -1.89916961]. \t  -62.501834483538666 \t -0.45606112642801944\n",
            "26     \t [-3.22659653 -8.00334136]. \t  -34514.86282320982 \t -0.45606112642801944\n",
            "27     \t [ 4.42012724 -1.45599882]. \t  -11.762259154948016 \t -0.45606112642801944\n",
            "28     \t [ 0.51753171 -4.65136484]. \t  -3655.846522957253 \t -0.45606112642801944\n",
            "29     \t [ 5.80174066 -0.41318329]. \t  -82.68646126177453 \t -0.45606112642801944\n",
            "30     \t [ 3.57771275 -0.87993874]. \t  -14.87932702029648 \t -0.45606112642801944\n",
            "31     \t [-9.94522097 -0.65158699]. \t  -352.83394027924084 \t -0.45606112642801944\n",
            "32     \t [0.69236161 5.26030958]. \t  -5973.1980296971515 \t -0.45606112642801944\n",
            "33     \t [2.33199181 1.48496739]. \t  -10.412568833389955 \t -0.45606112642801944\n",
            "34     \t [9.88847254 2.36546456]. \t  -82.3972933037139 \t -0.45606112642801944\n",
            "35     \t [ 2.47617007 -0.74497698]. \t  -5.91202108079125 \t -0.45606112642801944\n",
            "36     \t [ 2.15944885 -0.0955775 ]. \t  -10.513614384293689 \t -0.45606112642801944\n",
            "37     \t [-0.34840984 -0.39820313]. \t  -2.7040995724777024 \t -0.45606112642801944\n",
            "38     \t [4.56399213 1.07898263]. \t  -22.697721440803242 \t -0.45606112642801944\n",
            "39     \t [-2.51726044  0.43586609]. \t  -29.15887607981675 \t -0.45606112642801944\n",
            "40     \t [6.02823581 1.43927816]. \t  -32.39105730692089 \t -0.45606112642801944\n",
            "41     \t [ 5.99506769 -7.48021104]. \t  -22459.674007112142 \t -0.45606112642801944\n",
            "42     \t [ 7.43668814 -2.52668697]. \t  -98.282997945517 \t -0.45606112642801944\n",
            "43     \t [1.54713351 0.75732063]. \t  -0.6194582100546269 \t -0.45606112642801944\n",
            "44     \t [ 0.57654225 -0.25928318]. \t  -0.5701977961669484 \t -0.45606112642801944\n",
            "45     \t [ 2.03874845 -1.00216889]. \t  -1.0808059614557575 \t -0.45606112642801944\n",
            "46     \t [1.77304233 0.86553198]. \t  -0.7485707889801749 \t -0.45606112642801944\n",
            "47     \t [ 0.67758561 -0.50797742]. \t  \u001b[92m-0.15611780076293735\u001b[0m \t -0.15611780076293735\n",
            "48     \t [-2.82695935 -5.75623563]. \t  -9563.009858686777 \t -0.15611780076293735\n",
            "49     \t [-2.77407276  2.99997184]. \t  -877.3397484821335 \t -0.15611780076293735\n",
            "50     \t [-0.38202811 -0.08194416]. \t  -2.222775432715206 \t -0.15611780076293735\n",
            "51     \t [3.01731532 8.09774599]. \t  -32838.49164715321 \t -0.15611780076293735\n",
            "52     \t [ 1.53432015 -1.01035666]. \t  -0.8002472121981175 \t -0.15611780076293735\n",
            "53     \t [ 0.21023847 -0.19336819]. \t  -0.6604199061614454 \t -0.15611780076293735\n",
            "54     \t [ 4.7535342  -6.15824446]. \t  -10122.920851352947 \t -0.15611780076293735\n",
            "55     \t [ 5.53284339 -8.2539833 ]. \t  -34197.885286768586 \t -0.15611780076293735\n",
            "56     \t [-0.23000497 -8.42508835]. \t  -40439.91226306904 \t -0.15611780076293735\n",
            "57     \t [-1.17872656 -0.17323914]. \t  -7.815853382713165 \t -0.15611780076293735\n",
            "58     \t [ 2.34349292 -6.85063238]. \t  -16753.199375838183 \t -0.15611780076293735\n",
            "59     \t [8.37035745 7.22947534]. \t  -18547.91722967999 \t -0.15611780076293735\n",
            "60     \t [ 9.12008991 -8.90683399]. \t  -44792.329225285444 \t -0.15611780076293735\n",
            "61     \t [-7.87187674 -4.59603992]. \t  -5102.547071025583 \t -0.15611780076293735\n",
            "62     \t [ 3.59099115 -1.57298081]. \t  -10.399097917172607 \t -0.15611780076293735\n",
            "63     \t [0.82529107 0.61207243]. \t  \u001b[92m-0.04208304017686198\u001b[0m \t -0.04208304017686198\n",
            "64     \t [4.27940725 6.92234959]. \t  -16776.65056137282 \t -0.04208304017686198\n",
            "65     \t [-0.17762626  6.75297554]. \t  -16703.08617415138 \t -0.04208304017686198\n",
            "66     \t [1.97432265 2.53632665]. \t  -238.20247058643878 \t -0.04208304017686198\n",
            "67     \t [3.19488932 0.7691128 ]. \t  -12.912381049471273 \t -0.04208304017686198\n",
            "68     \t [-5.97887901 -6.60691642]. \t  -17451.60666157324 \t -0.04208304017686198\n",
            "69     \t [2.56005949 0.76653637]. \t  -6.269700860879112 \t -0.04208304017686198\n",
            "70     \t [ 9.60337117 -6.017764  ]. \t  -7967.626440205948 \t -0.04208304017686198\n",
            "71     \t [-4.48846541 -1.73401253]. \t  -250.70996212841862 \t -0.04208304017686198\n",
            "72     \t [3.17094823 1.08497896]. \t  -6.046653199799676 \t -0.04208304017686198\n",
            "73     \t [ 2.32782963 -5.9831748 ]. \t  -9598.132944240877 \t -0.04208304017686198\n",
            "74     \t [4.63518173 0.35127548]. \t  -51.730529059530966 \t -0.04208304017686198\n",
            "75     \t [-3.94498833 -7.05855256]. \t  -21486.774807150385 \t -0.04208304017686198\n",
            "76     \t [-6.37212638  0.81928152]. \t  -173.37745216130904 \t -0.04208304017686198\n",
            "77     \t [2.45479211 1.15933157]. \t  -2.225284641940169 \t -0.04208304017686198\n",
            "78     \t [ 1.04691481 -0.74377153]. \t  \u001b[92m-0.009276111995164187\u001b[0m \t -0.009276111995164187\n",
            "79     \t [-0.69810267 -7.36598808]. \t  -23858.13215841738 \t -0.009276111995164187\n",
            "80     \t [7.64324379 4.56371502]. \t  -2357.7304008387296 \t -0.009276111995164187\n",
            "81     \t [7.43126588 1.47728842]. \t  -60.168070942265146 \t -0.009276111995164187\n",
            "82     \t [2.74498008 6.90529164]. \t  -17160.390579779105 \t -0.009276111995164187\n",
            "83     \t [0.88873648 0.29527899]. \t  -1.0329917558164663 \t -0.009276111995164187\n",
            "84     \t [ 8.32326456 -5.33167961]. \t  -4764.020014189273 \t -0.009276111995164187\n",
            "85     \t [6.46349691 1.98866019]. \t  -34.03187207897343 \t -0.009276111995164187\n",
            "86     \t [-7.98137333  0.2164104 ]. \t  -211.07761670808316 \t -0.009276111995164187\n",
            "87     \t [ 4.92837262 -1.77810293]. \t  -19.323756755255232 \t -0.009276111995164187\n",
            "88     \t [1.59236992 0.90722755]. \t  -0.3566810486906436 \t -0.009276111995164187\n",
            "89     \t [-8.72172539  6.95274767]. \t  -22314.149777184477 \t -0.009276111995164187\n",
            "90     \t [-2.07678822  8.62162833]. \t  -45455.60988660681 \t -0.009276111995164187\n",
            "91     \t [-1.13533016 -4.2962465 ]. \t  -2900.286107739129 \t -0.009276111995164187\n",
            "92     \t [-1.36834628  4.22089697]. \t  -2743.651788682231 \t -0.009276111995164187\n",
            "93     \t [ 3.08294269 -8.08402201]. \t  -32578.006703472063 \t -0.009276111995164187\n",
            "94     \t [7.61503009 3.48124151]. \t  -596.4105271155338 \t -0.009276111995164187\n",
            "95     \t [-0.67739714  1.25177006]. \t  -31.864970540403473 \t -0.009276111995164187\n",
            "96     \t [0.54794103 0.86379893]. \t  -1.9879743804767294 \t -0.009276111995164187\n",
            "97     \t [1.1249608  0.96009707]. \t  -1.04842152979462 \t -0.009276111995164187\n",
            "98     \t [-0.21372332  5.24303817]. \t  -6093.924112011358 \t -0.009276111995164187\n",
            "99     \t [-5.23804805  6.39593121]. \t  -15195.683301944866 \t -0.009276111995164187\n",
            "100    \t [ 9.75240598 -3.36757363]. \t  -410.9070903804599 \t -0.009276111995164187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98Nt7Tguvna",
        "outputId": "507d1f95-d05c-438a-c687-d929af50347d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.16961099  2.44217542]. \t  -706.482335023616 \t -706.482335023616\n",
            "init   \t [-1.24544522  5.70717167]. \t  -8820.05719953671 \t -706.482335023616\n",
            "init   \t [ 5.59951616 -4.54814789]. \t  -2580.3963775443385 \t -706.482335023616\n",
            "init   \t [-4.4707149   6.03744355]. \t  -12002.832292900235 \t -706.482335023616\n",
            "init   \t [9.16278707 7.51865269]. \t  -21656.00779050222 \t -706.482335023616\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -706.482335023616\n",
            "2      \t [-2.84346081 -5.74360125]. \t  -9487.534509082592 \t -706.482335023616\n",
            "3      \t [-10.          -2.68160737]. \t  -1309.967380003619 \t -706.482335023616\n",
            "4      \t [4.43482852 2.34334729]. \t  \u001b[92m-97.5434403285221\u001b[0m \t -97.5434403285221\n",
            "5      \t [ 2.10496748 -9.86915898]. \t  -74264.43287230382 \t -97.5434403285221\n",
            "6      \t [9.13459748 0.31787954]. \t  -225.7508839700879 \t -97.5434403285221\n",
            "7      \t [-10.           8.29868465]. \t  -43773.04861547808 \t -97.5434403285221\n",
            "8      \t [ 9.78203716 -8.58451552]. \t  -37947.82343894382 \t -97.5434403285221\n",
            "9      \t [-0.27590047 -0.32244235]. \t  \u001b[92m-2.0961216201119477\u001b[0m \t -2.0961216201119477\n",
            "10     \t [3.55784975 8.72798734]. \t  -44288.03948550045 \t -2.0961216201119477\n",
            "11     \t [-3.35152537 -0.94068265]. \t  -71.39105835866852 \t -2.0961216201119477\n",
            "12     \t [ 1.24254235 -3.47474034]. \t  -1049.345905084743 \t -2.0961216201119477\n",
            "13     \t [-0.44679667  0.87103709]. \t  -9.809446041675676 \t -2.0961216201119477\n",
            "14     \t [-7.24212415 -5.8661695 ]. \t  -11640.009363026247 \t -2.0961216201119477\n",
            "15     \t [-9.79655212  1.53092083]. \t  -536.1374293018484 \t -2.0961216201119477\n",
            "16     \t [ 9.60496963 -3.43585864]. \t  -466.3412103610556 \t -2.0961216201119477\n",
            "17     \t [1.66848954 0.18600199]. \t  -5.562374089505072 \t -2.0961216201119477\n",
            "18     \t [ 5.34019728 -0.53579255]. \t  -64.26777701745814 \t -2.0961216201119477\n",
            "19     \t [-6.87823681 -0.86352514]. \t  -202.16662728979242 \t -2.0961216201119477\n",
            "20     \t [-1.79784127  9.57157263]. \t  -68478.43588762768 \t -2.0961216201119477\n",
            "21     \t [ -4.65820751 -10.        ]. \t  -83801.97911248906 \t -2.0961216201119477\n",
            "22     \t [1.3385482  2.33455499]. \t  -182.9685821068807 \t -2.0961216201119477\n",
            "23     \t [7.729956   2.90476783]. \t  -212.56885482821832 \t -2.0961216201119477\n",
            "24     \t [-2.7254183   1.77422298]. \t  -176.64113226292991 \t -2.0961216201119477\n",
            "25     \t [ 3.23046965 -1.13503509]. \t  -5.830061609159467 \t -2.0961216201119477\n",
            "26     \t [ 1.59073033 -0.48953534]. \t  -2.8195628667886927 \t -2.0961216201119477\n",
            "27     \t [-0.30255068  0.1095489 ]. \t  \u001b[92m-1.909911483792163\u001b[0m \t -1.909911483792163\n",
            "28     \t [-0.62399236 -0.8233358 ]. \t  -10.476219127828683 \t -1.909911483792163\n",
            "29     \t [5.36712493 4.64100569]. \t  -2863.2720202879955 \t -1.909911483792163\n",
            "30     \t [ 2.39664585 -0.19390771]. \t  -12.72883755575248 \t -1.909911483792163\n",
            "31     \t [ 0.85062771 -0.37427065]. \t  \u001b[92m-0.6731856539483387\u001b[0m \t -0.6731856539483387\n",
            "32     \t [ 3.74748652 -1.6528553 ]. \t  -13.440566886508176 \t -0.6731856539483387\n",
            "33     \t [4.22547576 1.86197023]. \t  -25.074452447533126 \t -0.6731856539483387\n",
            "34     \t [ 0.16503917 -0.48360421]. \t  -0.8804225140118509 \t -0.6731856539483387\n",
            "35     \t [-1.26468288 -1.66789007]. \t  -98.38281184101038 \t -0.6731856539483387\n",
            "36     \t [ 7.33277826 -0.65107676]. \t  -124.21391714698167 \t -0.6731856539483387\n",
            "37     \t [9.9794062 3.1204156]. \t  -260.9238683374435 \t -0.6731856539483387\n",
            "38     \t [-0.88814345  0.22858237]. \t  -5.535766883494606 \t -0.6731856539483387\n",
            "39     \t [-9.49501442  4.07450863]. \t  -3756.4273926987407 \t -0.6731856539483387\n",
            "40     \t [-8.18910393 -0.74667754]. \t  -257.5743670756286 \t -0.6731856539483387\n",
            "41     \t [-4.44845563 -2.3948954 ]. \t  -536.5468574973396 \t -0.6731856539483387\n",
            "42     \t [ 1.12063844 -0.13623663]. \t  -2.3625744166889877 \t -0.6731856539483387\n",
            "43     \t [5.12432782 1.25433085]. \t  -24.832168832264657 \t -0.6731856539483387\n",
            "44     \t [ 3.60634149 -0.40615477]. \t  -28.262847018406568 \t -0.6731856539483387\n",
            "45     \t [-5.72363232  8.69383232]. \t  -49273.56976342589 \t -0.6731856539483387\n",
            "46     \t [ 0.16106039 -0.32524712]. \t  -0.7089223925176974 \t -0.6731856539483387\n",
            "47     \t [ 0.39359025 -0.50753292]. \t  \u001b[92m-0.3973005863171114\u001b[0m \t -0.3973005863171114\n",
            "48     \t [-7.76422898 -5.7229293 ]. \t  -10813.231452500617 \t -0.3973005863171114\n",
            "49     \t [-1.29103693 -0.49484572]. \t  -11.591215740150982 \t -0.3973005863171114\n",
            "50     \t [ 0.42410574 -0.16121444]. \t  -0.608609047374506 \t -0.3973005863171114\n",
            "51     \t [ 1.00972224 -9.34210833]. \t  -60232.43939583875 \t -0.3973005863171114\n",
            "52     \t [-3.89026592  2.63755423]. \t  -657.8546450613019 \t -0.3973005863171114\n",
            "53     \t [-1.39979407  0.56064018]. \t  -13.988059130674003 \t -0.3973005863171114\n",
            "54     \t [-9.62057036 -2.87362569]. \t  -1478.9795583210216 \t -0.3973005863171114\n",
            "55     \t [-2.28398952  0.15378405]. \t  -21.654400068723742 \t -0.3973005863171114\n",
            "56     \t [-3.52924567 -0.09729133]. \t  -45.69318460890511 \t -0.3973005863171114\n",
            "57     \t [-1.86474331 -0.52030063]. \t  -19.786050829585278 \t -0.3973005863171114\n",
            "58     \t [ 5.8727192  -7.84782855]. \t  -27544.250255842588 \t -0.3973005863171114\n",
            "59     \t [-5.13610495  4.50565333]. \t  -4221.568063430106 \t -0.3973005863171114\n",
            "60     \t [ 1.70740218 -2.41744087]. \t  -199.72671044738223 \t -0.3973005863171114\n",
            "61     \t [ 1.26902772 -1.36465713]. \t  -12.131832025769283 \t -0.3973005863171114\n",
            "62     \t [3.3416869  8.35157673]. \t  -37082.38566533463 \t -0.3973005863171114\n",
            "63     \t [ 2.25743716 -1.91458826]. \t  -53.06924291995145 \t -0.3973005863171114\n",
            "64     \t [ 3.75084621 -1.16045257]. \t  -9.80396137194499 \t -0.3973005863171114\n",
            "65     \t [ 6.8431608  -0.20728075]. \t  -125.46284750646188 \t -0.3973005863171114\n",
            "66     \t [ 0.65681684 -0.64620393]. \t  \u001b[92m-0.1813865615295248\u001b[0m \t -0.1813865615295248\n",
            "67     \t [-4.86501085  6.94757822]. \t  -20599.41225591806 \t -0.1813865615295248\n",
            "68     \t [2.3642836  5.64243907]. \t  -7519.682115469932 \t -0.1813865615295248\n",
            "69     \t [-5.86891714 -9.7963886 ]. \t  -78302.69821855053 \t -0.1813865615295248\n",
            "70     \t [3.28012838 3.6359738 ]. \t  -1078.0148941175532 \t -0.1813865615295248\n",
            "71     \t [-7.17862161  1.96772821]. \t  -512.2536691185422 \t -0.1813865615295248\n",
            "72     \t [6.21096425 1.06901135]. \t  -57.97158022380407 \t -0.1813865615295248\n",
            "73     \t [-3.60580754  8.70584072]. \t  -48188.54724637447 \t -0.1813865615295248\n",
            "74     \t [ 0.93753027 -8.85997468]. \t  -48709.933362130694 \t -0.1813865615295248\n",
            "75     \t [3.43072363 6.91320283]. \t  -16990.636376821025 \t -0.1813865615295248\n",
            "76     \t [6.63283614 3.72989125]. \t  -929.8747398348743 \t -0.1813865615295248\n",
            "77     \t [ 2.01271661 -6.01062813]. \t  -9869.066925274641 \t -0.1813865615295248\n",
            "78     \t [ 4.62301327 -5.15861912]. \t  -4736.985788552286 \t -0.1813865615295248\n",
            "79     \t [1.59412987 9.59705624]. \t  -66695.25680755447 \t -0.1813865615295248\n",
            "80     \t [6.26398562 3.76637446]. \t  -1005.1632549001689 \t -0.1813865615295248\n",
            "81     \t [ 0.65551178 -0.72041245]. \t  -0.411248545949778 \t -0.1813865615295248\n",
            "82     \t [8.0865315  1.66853601]. \t  -62.904680315937874 \t -0.1813865615295248\n",
            "83     \t [7.43594702 1.60305864]. \t  -51.967888692075356 \t -0.1813865615295248\n",
            "84     \t [5.28926316 2.49920395]. \t  -122.15779008534562 \t -0.1813865615295248\n",
            "85     \t [-1.8420648   5.55248144]. \t  -8073.127249019639 \t -0.1813865615295248\n",
            "86     \t [8.26484114 2.30601622]. \t  -64.01722003390985 \t -0.1813865615295248\n",
            "87     \t [3.9638764  6.95957822]. \t  -17272.422004480584 \t -0.1813865615295248\n",
            "88     \t [-9.54284769 -3.55890536]. \t  -2543.6078971587644 \t -0.1813865615295248\n",
            "89     \t [ 6.00106142 -6.84294708]. \t  -15390.331879948177 \t -0.1813865615295248\n",
            "90     \t [-7.64447184  4.58158647]. \t  -5000.274323967172 \t -0.1813865615295248\n",
            "91     \t [-3.17673625 -2.51350093]. \t  -517.4907760011075 \t -0.1813865615295248\n",
            "92     \t [ 6.01484388 -3.34460012]. \t  -560.3075707518249 \t -0.1813865615295248\n",
            "93     \t [-2.84232082 -7.60664852]. \t  -28129.85622966966 \t -0.1813865615295248\n",
            "94     \t [-9.46905503  2.44046659]. \t  -1023.8791326775374 \t -0.1813865615295248\n",
            "95     \t [-6.84857657  9.85863994]. \t  -81051.95808320583 \t -0.1813865615295248\n",
            "96     \t [-8.20828524 -1.9115838 ]. \t  -566.3216761690687 \t -0.1813865615295248\n",
            "97     \t [4.90338108 9.29475615]. \t  -56383.70145988153 \t -0.1813865615295248\n",
            "98     \t [-6.1461776   5.99112053]. \t  -12198.24388708779 \t -0.1813865615295248\n",
            "99     \t [-4.9205641  -9.43264269]. \t  -66917.97104986853 \t -0.1813865615295248\n",
            "100    \t [-2.10756868 -2.72354809]. \t  -583.7872836272707 \t -0.1813865615295248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bpn-kmNuvqC",
        "outputId": "8088719e-6f80-4841-abff-c7b44e459689"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.21288618 -1.72798696]. \t  -20.120935450808645 \t -20.120935450808645\n",
            "init   \t [ 3.36205116 -5.40113156]. \t  -6051.706374022101 \t -20.120935450808645\n",
            "init   \t [6.15356672 2.73636929]. \t  -182.2099268583363 \t -20.120935450808645\n",
            "init   \t [-6.55612296 -4.79228264]. \t  -5567.089701718418 \t -20.120935450808645\n",
            "init   \t [ 8.30639981 -0.74368981]. \t  -157.07069920616067 \t -20.120935450808645\n",
            "1      \t [-7.5961809  7.6177101]. \t  -30655.10899619148 \t -20.120935450808645\n",
            "2      \t [9.21200838 9.90573549]. \t  -70031.75643065354 \t -20.120935450808645\n",
            "3      \t [-1.19367463  2.76305479]. \t  -546.8477998622536 \t -20.120935450808645\n",
            "4      \t [0.09341338 9.35596567]. \t  -61233.164987376076 \t -20.120935450808645\n",
            "5      \t [ 8.38757304 -9.87720684]. \t  -69791.40203706689 \t -20.120935450808645\n",
            "6      \t [ -1.61708953 -10.        ]. \t  -81305.7507420366 \t -20.120935450808645\n",
            "7      \t [-10.           1.07065924]. \t  -423.21713175809816 \t -20.120935450808645\n",
            "8      \t [-10. -10.]. \t  -88321.0 \t -20.120935450808645\n",
            "9      \t [-1.48655746 -2.40772582]. \t  -348.3999650366886 \t -20.120935450808645\n",
            "10     \t [-5.14487519  0.16628671]. \t  -91.8431878150951 \t -20.120935450808645\n",
            "11     \t [9.81266998 4.60117173]. \t  -2193.9208508290335 \t -20.120935450808645\n",
            "12     \t [2.47398709 0.99755353]. \t  \u001b[92m-2.6406873562643765\u001b[0m \t -2.6406873562643765\n",
            "13     \t [5.28479258 6.32632235]. \t  -11196.44960523752 \t -2.6406873562643765\n",
            "14     \t [ 3.37581638 -9.60423542]. \t  -65605.03358473153 \t -2.6406873562643765\n",
            "15     \t [ 7.89858945 -4.59855715]. \t  -2413.604182841531 \t -2.6406873562643765\n",
            "16     \t [-10.         -2.5103018]. \t  -1142.8120389698788 \t -2.6406873562643765\n",
            "17     \t [ 4.1820351  -0.02427814]. \t  -45.08446521226741 \t -2.6406873562643765\n",
            "18     \t [2.38516182 2.6455727 ]. \t  -271.63979613146967 \t -2.6406873562643765\n",
            "19     \t [2.45564049 0.03206439]. \t  -14.15904041302095 \t -2.6406873562643765\n",
            "20     \t [-5.59070649  2.98907323]. \t  -1144.1640947746303 \t -2.6406873562643765\n",
            "21     \t [-3.08508123  5.82610149]. \t  -10090.73701421386 \t -2.6406873562643765\n",
            "22     \t [9.92099533 1.04107712]. \t  -199.8118569220052 \t -2.6406873562643765\n",
            "23     \t [ 1.31057711 -1.73330398]. \t  -44.240900478619544 \t -2.6406873562643765\n",
            "24     \t [-5.7367647  -9.23054515]. \t  -62097.88001443883 \t -2.6406873562643765\n",
            "25     \t [-2.55170817 -4.97105976]. \t  -5415.328227717557 \t -2.6406873562643765\n",
            "26     \t [-9.95071215  3.78058757]. \t  -3090.025872791581 \t -2.6406873562643765\n",
            "27     \t [0.9239044  0.78416632]. \t  \u001b[92m-0.19297593867661678\u001b[0m \t -0.19297593867661678\n",
            "28     \t [1.84359875 0.39157688]. \t  -5.435990115823973 \t -0.19297593867661678\n",
            "29     \t [ 4.49562466 -2.01725613]. \t  -38.7625803111762 \t -0.19297593867661678\n",
            "30     \t [-2.59111041 -0.07531467]. \t  -26.441618209494763 \t -0.19297593867661678\n",
            "31     \t [1.43396648 5.21270358]. \t  -5599.249571073457 \t -0.19297593867661678\n",
            "32     \t [1.96575651 0.7240668 ]. \t  -2.6152379244249984 \t -0.19297593867661678\n",
            "33     \t [2.32725991 0.67252188]. \t  -5.809704368857004 \t -0.19297593867661678\n",
            "34     \t [-7.18878946 -0.41995586]. \t  -180.80517921791682 \t -0.19297593867661678\n",
            "35     \t [-4.73985122 -1.55967865]. \t  -217.45971704430437 \t -0.19297593867661678\n",
            "36     \t [ 6.15587189 -0.9305172 ]. \t  -65.72917456350201 \t -0.19297593867661678\n",
            "37     \t [4.36893262 9.97743764]. \t  -75850.57694061658 \t -0.19297593867661678\n",
            "38     \t [ 2.68264592 -0.98901779]. \t  -3.886418079514091 \t -0.19297593867661678\n",
            "39     \t [-4.22247033  9.44091384]. \t  -66628.22498234517 \t -0.19297593867661678\n",
            "40     \t [3.97297373 1.12374458]. \t  -13.028332473217741 \t -0.19297593867661678\n",
            "41     \t [ 0.65204757 -0.0127086 ]. \t  -0.97056067804169 \t -0.19297593867661678\n",
            "42     \t [ 0.45253534 -0.0195731 ]. \t  -0.7079082445025824 \t -0.19297593867661678\n",
            "43     \t [-0.3173141  -0.06038501]. \t  -1.9460555918897724 \t -0.19297593867661678\n",
            "44     \t [1.1022715  0.33343506]. \t  -1.558955414124259 \t -0.19297593867661678\n",
            "45     \t [ 2.3717882  -1.06308583]. \t  -1.9066607910579674 \t -0.19297593867661678\n",
            "46     \t [3.39273305 1.41357007]. \t  -6.453904135368722 \t -0.19297593867661678\n",
            "47     \t [ 1.85138423 -0.60852145]. \t  -3.1925528987438794 \t -0.19297593867661678\n",
            "48     \t [-0.82947083  0.30856832]. \t  -5.427354142010782 \t -0.19297593867661678\n",
            "49     \t [3.39220378 1.09809473]. \t  -7.645712013919771 \t -0.19297593867661678\n",
            "50     \t [1.36237147 0.68901181]. \t  -0.47228081410991873 \t -0.19297593867661678\n",
            "51     \t [-3.03008776 -8.62582546]. \t  -46126.901882453436 \t -0.19297593867661678\n",
            "52     \t [-1.06968479 -9.60467806]. \t  -68876.2628835377 \t -0.19297593867661678\n",
            "53     \t [-8.31012613  7.17567217]. \t  -24857.904138309877 \t -0.19297593867661678\n",
            "54     \t [-5.64174875 -3.92160871]. \t  -2694.0005780827005 \t -0.19297593867661678\n",
            "55     \t [1.42639468 0.82993905]. \t  \u001b[92m-0.1865747189108803\u001b[0m \t -0.1865747189108803\n",
            "56     \t [-1.09163198 -0.29301909]. \t  -7.567042658848186 \t -0.1865747189108803\n",
            "57     \t [-2.41865885 -4.25492326]. \t  -2995.8397045771317 \t -0.1865747189108803\n",
            "58     \t [-2.77294806 -2.8325935 ]. \t  -722.6289628667599 \t -0.1865747189108803\n",
            "59     \t [-1.95170921  0.34967549]. \t  -18.35966023644317 \t -0.1865747189108803\n",
            "60     \t [7.40763866 5.51959415]. \t  -5770.737914270966 \t -0.1865747189108803\n",
            "61     \t [-9.9903605  -0.01517048]. \t  -320.42102403555316 \t -0.1865747189108803\n",
            "62     \t [0.63733264 0.65791808]. \t  -0.23584225442787377 \t -0.1865747189108803\n",
            "63     \t [0.22509263 0.30830902]. \t  -0.6029291595156401 \t -0.1865747189108803\n",
            "64     \t [0.80305287 0.55551303]. \t  \u001b[92m-0.10787859139408718\u001b[0m \t -0.10787859139408718\n",
            "65     \t [-9.90888229 -5.86212586]. \t  -12486.849087844843 \t -0.10787859139408718\n",
            "66     \t [ 5.96303625 -8.00820816]. \t  -29939.094089609996 \t -0.10787859139408718\n",
            "67     \t [4.08981309 0.7279362 ]. \t  -27.909119138463204 \t -0.10787859139408718\n",
            "68     \t [4.80742649 1.22440502]. \t  -21.042118328385396 \t -0.10787859139408718\n",
            "69     \t [ 3.57649453 -8.66059476]. \t  -42893.23753008888 \t -0.10787859139408718\n",
            "70     \t [-1.2227132  -0.02293176]. \t  -7.935655230172622 \t -0.10787859139408718\n",
            "71     \t [-3.04198407  0.31135951]. \t  -37.279390689834564 \t -0.10787859139408718\n",
            "72     \t [-3.98859714  0.66553368]. \t  -72.40696445989835 \t -0.10787859139408718\n",
            "73     \t [ 0.44784084 -0.14646621]. \t  -0.6328262900879182 \t -0.10787859139408718\n",
            "74     \t [0.90142078 0.69006679]. \t  \u001b[92m-0.014912437271652645\u001b[0m \t -0.014912437271652645\n",
            "75     \t [ 9.91286349 -2.22542941]. \t  -79.43925695773675 \t -0.014912437271652645\n",
            "76     \t [ 5.40287929 -3.75820845]. \t  -1063.208321874586 \t -0.014912437271652645\n",
            "77     \t [-0.01819507 -0.62203872]. \t  -2.2914374486404547 \t -0.014912437271652645\n",
            "78     \t [ 4.03493698 -1.35764318]. \t  -9.453812428570979 \t -0.014912437271652645\n",
            "79     \t [ 1.46228444 -6.62887934]. \t  -14937.668100017143 \t -0.014912437271652645\n",
            "80     \t [2.01928411 9.20045794]. \t  -55964.60885655887 \t -0.014912437271652645\n",
            "81     \t [ 5.67102364 -3.82147844]. \t  -1129.740002070439 \t -0.014912437271652645\n",
            "82     \t [2.139813  4.7225701]. \t  -3607.940565359596 \t -0.014912437271652645\n",
            "83     \t [-7.57356718  6.86182203]. \t  -20776.68867790153 \t -0.014912437271652645\n",
            "84     \t [-3.2540645  -8.91447672]. \t  -52629.19515382502 \t -0.014912437271652645\n",
            "85     \t [-6.61961806  4.9441077 ]. \t  -6220.339143468193 \t -0.014912437271652645\n",
            "86     \t [-8.69033094  3.93211189]. \t  -3232.3357421883297 \t -0.014912437271652645\n",
            "87     \t [-5.72642209 -1.03491284]. \t  -169.07169236201335 \t -0.014912437271652645\n",
            "88     \t [-1.84270439  9.92074067]. \t  -78959.45602882205 \t -0.014912437271652645\n",
            "89     \t [ 5.35086292 -4.73006518]. \t  -3123.046485571771 \t -0.014912437271652645\n",
            "90     \t [ 5.51750874 -2.52415985]. \t  -124.81656827996554 \t -0.014912437271652645\n",
            "91     \t [0.82574087 0.64960229]. \t  -0.03103057252125801 \t -0.014912437271652645\n",
            "92     \t [ 8.76963296 -9.60728067]. \t  -61892.78078871723 \t -0.014912437271652645\n",
            "93     \t [-2.97216071  7.09633929]. \t  -21518.27341684187 \t -0.014912437271652645\n",
            "94     \t [ 2.30768274 -0.74198071]. \t  -4.62185914155255 \t -0.014912437271652645\n",
            "95     \t [-0.17249246  1.00137998]. \t  -10.862247659516049 \t -0.014912437271652645\n",
            "96     \t [-7.06018259  3.18491234]. \t  -1560.7397815242407 \t -0.014912437271652645\n",
            "97     \t [6.53471198 0.41740267]. \t  -107.17271181394167 \t -0.014912437271652645\n",
            "98     \t [-5.39054826  7.86566842]. \t  -33388.93420504962 \t -0.014912437271652645\n",
            "99     \t [ 5.99666688 -0.69597377]. \t  -75.526395702937 \t -0.014912437271652645\n",
            "100    \t [9.84428029 6.27909718]. \t  -9602.938005260863 \t -0.014912437271652645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NdFRXtPuvsP",
        "outputId": "8ac7c16c-afe3-4a7c-e18f-13aac862e0ec"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.95102099 0.14299376]. \t  -20.744156010505872 \t -20.744156010505872\n",
            "init   \t [0.5668276  7.92570408]. \t  -31283.56791285067 \t -20.744156010505872\n",
            "init   \t [3.99982381 4.28594204]. \t  -2152.65365220131 \t -20.744156010505872\n",
            "init   \t [ 4.34676764 -5.54361076]. \t  -6535.783782469756 \t -20.744156010505872\n",
            "init   \t [-6.49690953 -0.86317023]. \t  -183.78911629570374 \t -20.744156010505872\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -20.744156010505872\n",
            "2      \t [-9.41010567  7.02228954]. \t  -23451.5819466236 \t -20.744156010505872\n",
            "3      \t [8.38403624 9.89223058]. \t  -70238.39460245872 \t -20.744156010505872\n",
            "4      \t [-2.12485184 -9.78521923]. \t  -74991.72959757554 \t -20.744156010505872\n",
            "5      \t [ 8.58953583 -1.38728645]. \t  -102.54399844056245 \t -20.744156010505872\n",
            "6      \t [ 9.69005186 -9.66164402]. \t  -62736.82929961719 \t -20.744156010505872\n",
            "7      \t [-1.38734199 -2.85429133]. \t  -630.9561435145587 \t -20.744156010505872\n",
            "8      \t [-2.75643648  2.76308594]. \t  -663.9642844620306 \t -20.744156010505872\n",
            "9      \t [9.62650923 3.68331222]. \t  -687.4115670990366 \t -20.744156010505872\n",
            "10     \t [-9.60444699 -4.2765978 ]. \t  -4378.197791966378 \t -20.744156010505872\n",
            "11     \t [-4.56374139  9.55345059]. \t  -70044.20753062582 \t -20.744156010505872\n",
            "12     \t [-10.           0.94331866]. \t  -398.5226727436221 \t -20.744156010505872\n",
            "13     \t [-6.58668241  2.83295162]. \t  -1082.5078832427519 \t -20.744156010505872\n",
            "14     \t [-5.36874577 -5.77036118]. \t  -10397.873689312017 \t -20.744156010505872\n",
            "15     \t [  3.25203895 -10.        ]. \t  -77424.59203182554 \t -20.744156010505872\n",
            "16     \t [ 0.85625768 -0.3419722 ]. \t  \u001b[92m-0.7953449884520477\u001b[0m \t -0.7953449884520477\n",
            "17     \t [0.01558116 0.34011125]. \t  -1.0621939903267643 \t -0.7953449884520477\n",
            "18     \t [ 8.83895874 -5.1022578 ]. \t  -3798.615463378062 \t -0.7953449884520477\n",
            "19     \t [ 4.93462122 -0.58925448]. \t  -51.43948921692635 \t -0.7953449884520477\n",
            "20     \t [0.71037304 0.17401274]. \t  -0.928395502471903 \t -0.7953449884520477\n",
            "21     \t [ 2.10769036 -1.01173501]. \t  -1.2342923708095817 \t -0.7953449884520477\n",
            "22     \t [6.71931316 2.01849781]. \t  -36.79664669915398 \t -0.7953449884520477\n",
            "23     \t [ 2.17913939 -1.8698723 ]. \t  -47.73389061150911 \t -0.7953449884520477\n",
            "24     \t [0.27196683 1.2178946 ]. \t  -15.051422273740284 \t -0.7953449884520477\n",
            "25     \t [ 0.11274661 -5.51946044]. \t  -7397.993126440041 \t -0.7953449884520477\n",
            "26     \t [ 1.24469202 -0.6080799 ]. \t  \u001b[92m-0.5702670145254232\u001b[0m \t -0.5702670145254232\n",
            "27     \t [1.27792077 0.21783491]. \t  -2.8762968466388927 \t -0.5702670145254232\n",
            "28     \t [ 0.66486726 -0.09235182]. \t  -0.9516283699422092 \t -0.5702670145254232\n",
            "29     \t [-3.96253065  0.15069778]. \t  -56.75404157242357 \t -0.5702670145254232\n",
            "30     \t [-0.99402435 -0.18049733]. \t  -6.219870043688403 \t -0.5702670145254232\n",
            "31     \t [8.18981499 0.86347931]. \t  -141.43651156154237 \t -0.5702670145254232\n",
            "32     \t [ 3.25512404 -0.73123433]. \t  -14.640299963132275 \t -0.5702670145254232\n",
            "33     \t [ 5.71998605 -2.02471248]. \t  -34.56850782683551 \t -0.5702670145254232\n",
            "34     \t [0.76601658 3.35811344]. \t  -949.4742757050986 \t -0.5702670145254232\n",
            "35     \t [-0.13033446  0.90990436]. \t  -7.6585793449859425 \t -0.5702670145254232\n",
            "36     \t [ 0.27522006 -0.68649423]. \t  -1.41596083198939 \t -0.5702670145254232\n",
            "37     \t [0.36449118 0.84063331]. \t  -2.603991879473371 \t -0.5702670145254232\n",
            "38     \t [-0.05862868 -0.43856892]. \t  -1.5137494086007584 \t -0.5702670145254232\n",
            "39     \t [-4.54876149 -1.83362547]. \t  -284.95550334261225 \t -0.5702670145254232\n",
            "40     \t [ 0.48895073 -0.53966146]. \t  \u001b[92m-0.27866268397816557\u001b[0m \t -0.27866268397816557\n",
            "41     \t [ 2.32179349 -0.1977404 ]. \t  -11.81453886503619 \t -0.27866268397816557\n",
            "42     \t [7.3332658  5.78681798]. \t  -7154.269766736508 \t -0.27866268397816557\n",
            "43     \t [ 1.98427218 -0.50928043]. \t  -5.26440119480513 \t -0.27866268397816557\n",
            "44     \t [-2.95252709  5.73694177]. \t  -9476.319402093864 \t -0.27866268397816557\n",
            "45     \t [ 1.58759931 -6.28185093]. \t  -11961.96712752897 \t -0.27866268397816557\n",
            "46     \t [-9.62072898  7.76694621]. \t  -34054.19813525778 \t -0.27866268397816557\n",
            "47     \t [0.25299138 0.04802039]. \t  -0.6814065959290689 \t -0.27866268397816557\n",
            "48     \t [-8.46579273 -3.57014299]. \t  -2395.843140625664 \t -0.27866268397816557\n",
            "49     \t [-0.96862353  0.06849528]. \t  -5.788472934535372 \t -0.27866268397816557\n",
            "50     \t [-0.47748819 -0.03447737]. \t  -2.6435132653010185 \t -0.27866268397816557\n",
            "51     \t [6.0555014  0.32590617]. \t  -93.84106891687061 \t -0.27866268397816557\n",
            "52     \t [ 0.89472625 -0.61788589]. \t  \u001b[92m-0.045488607899963125\u001b[0m \t -0.045488607899963125\n",
            "53     \t [-2.67661075 -0.03287002]. \t  -27.86910150412362 \t -0.045488607899963125\n",
            "54     \t [-9.96650856  3.81291883]. \t  -3169.0085721236137 \t -0.045488607899963125\n",
            "55     \t [-7.84198973 -5.30413911]. \t  -8298.306829346673 \t -0.045488607899963125\n",
            "56     \t [ 1.73803168 -0.87260757]. \t  -0.6372644408586302 \t -0.045488607899963125\n",
            "57     \t [ 1.14127837 -0.86980207]. \t  -0.29647898937252903 \t -0.045488607899963125\n",
            "58     \t [-8.47356511 -0.9459017 ]. \t  -300.4078070888851 \t -0.045488607899963125\n",
            "59     \t [ 1.34604961 -0.82054928]. \t  -0.11975094439969902 \t -0.045488607899963125\n",
            "60     \t [-6.44961741  0.89599703]. \t  -185.27054250434546 \t -0.045488607899963125\n",
            "61     \t [-4.80405407 -9.43966596]. \t  -67025.33281313878 \t -0.045488607899963125\n",
            "62     \t [3.81889512 9.9070637 ]. \t  -74105.7606993465 \t -0.045488607899963125\n",
            "63     \t [-8.14075251  2.79073766]. \t  -1208.563170401845 \t -0.045488607899963125\n",
            "64     \t [ 0.74047536 -0.6718284 ]. \t  -0.11999111469854173 \t -0.045488607899963125\n",
            "65     \t [8.5032172  2.33748767]. \t  -68.05447515116728 \t -0.045488607899963125\n",
            "66     \t [ 2.51530742 -0.89063939]. \t  -4.021608227469106 \t -0.045488607899963125\n",
            "67     \t [ 6.52116839 -9.49089424]. \t  -60327.305906501824 \t -0.045488607899963125\n",
            "68     \t [ 9.95090493 -5.4944436 ]. \t  -5165.866424489724 \t -0.045488607899963125\n",
            "69     \t [-8.34277372 -3.1991881 ]. \t  -1747.5942162311192 \t -0.045488607899963125\n",
            "70     \t [7.55974379 2.20276318]. \t  -52.228749290404686 \t -0.045488607899963125\n",
            "71     \t [-2.73199795  4.57772709]. \t  -3999.952819559216 \t -0.045488607899963125\n",
            "72     \t [ 4.00853311 -1.71810049]. \t  -16.23487912780488 \t -0.045488607899963125\n",
            "73     \t [9.95757063 0.84106842]. \t  -226.19619857552323 \t -0.045488607899963125\n",
            "74     \t [4.12031716 0.18517141]. \t  -42.56957734983289 \t -0.045488607899963125\n",
            "75     \t [-3.28487492 -7.62804324]. \t  -28654.8945934056 \t -0.045488607899963125\n",
            "76     \t [ 8.24492868 -8.27259918]. \t  -33142.230176738776 \t -0.045488607899963125\n",
            "77     \t [ 3.1867796 -9.2083516]. \t  -55383.167089094495 \t -0.045488607899963125\n",
            "78     \t [-8.28717958 -9.40295244]. \t  -68623.7541473987 \t -0.045488607899963125\n",
            "79     \t [ 6.74504647 -8.46425389]. \t  -37320.518206723325 \t -0.045488607899963125\n",
            "80     \t [-5.78481188  3.27956315]. \t  -1536.1638856835025 \t -0.045488607899963125\n",
            "81     \t [-7.29670082  2.55242434]. \t  -895.163642157233 \t -0.045488607899963125\n",
            "82     \t [-5.18300448 -5.94565128]. \t  -11555.15717335925 \t -0.045488607899963125\n",
            "83     \t [-2.35585868 -5.29245568]. \t  -6826.784218325494 \t -0.045488607899963125\n",
            "84     \t [ 6.79908003 -6.61180915]. \t  -13036.977935772078 \t -0.045488607899963125\n",
            "85     \t [ 3.3611301  -7.53643968]. \t  -24308.960123070992 \t -0.045488607899963125\n",
            "86     \t [4.96304012 2.96040326]. \t  -331.4608594387051 \t -0.045488607899963125\n",
            "87     \t [5.32045751 2.20607571]. \t  -57.61694887657228 \t -0.045488607899963125\n",
            "88     \t [8.16694511 2.96724987]. \t  -229.67532676975765 \t -0.045488607899963125\n",
            "89     \t [-0.52126876  7.08258155]. \t  -20342.626382327842 \t -0.045488607899963125\n",
            "90     \t [ 8.15236323 -4.41999009]. \t  -1963.2814553179464 \t -0.045488607899963125\n",
            "91     \t [-6.10918752  7.5731273 ]. \t  -29242.441528680607 \t -0.045488607899963125\n",
            "92     \t [-1.61079141  2.53916057]. \t  -427.6332137066193 \t -0.045488607899963125\n",
            "93     \t [ 4.88716679 -3.65327652]. \t  -966.0864564035858 \t -0.045488607899963125\n",
            "94     \t [-1.54270808  1.49471755]. \t  -78.73127016339454 \t -0.045488607899963125\n",
            "95     \t [2.50158433 4.22111784]. \t  -2197.9902345163655 \t -0.045488607899963125\n",
            "96     \t [ 5.84761504 -3.69315877]. \t  -942.0944713662909 \t -0.045488607899963125\n",
            "97     \t [-1.61694673  0.74261427]. \t  -21.644107299784466 \t -0.045488607899963125\n",
            "98     \t [ 9.7014008  -1.18928938]. \t  -170.17915159284428 \t -0.045488607899963125\n",
            "99     \t [ 5.97551382 -1.48390963]. \t  -29.695202438385856 \t -0.045488607899963125\n",
            "100    \t [-3.29970925 -9.55423711]. \t  -69111.27565440805 \t -0.045488607899963125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86panpOuvum",
        "outputId": "8f158a98-f071-46d3-df87-b5b83476ed45"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.55763717 -4.88997339]. \t  -4281.404311682752 \t -3267.0472724202245\n",
            "init   \t [-4.94946261  4.19808709]. \t  -3267.0472724202245 \t -3267.0472724202245\n",
            "init   \t [-1.04895274 -5.46114084]. \t  -7372.467127638806 \t -3267.0472724202245\n",
            "init   \t [-1.96580869  7.6451547 ]. \t  -28265.42637457811 \t -3267.0472724202245\n",
            "init   \t [-1.25573066  7.56850362]. \t  -26833.737231776184 \t -3267.0472724202245\n",
            "1      \t [-9.76533982 -4.68487047]. \t  -5874.976174155925 \t -3267.0472724202245\n",
            "2      \t [8.28335803 6.88744078]. \t  -15048.8154067911 \t -3267.0472724202245\n",
            "3      \t [ 9.8362843  -6.65190316]. \t  -12452.665647642647 \t -3267.0472724202245\n",
            "4      \t [-8.7683881   9.94621396]. \t  -85481.32833161899 \t -3267.0472724202245\n",
            "5      \t [3.6439568  1.63346221]. \t  \u001b[92m-12.719219233371792\u001b[0m \t -12.719219233371792\n",
            "6      \t [ -5.90897685 -10.        ]. \t  -84844.74745345925 \t -12.719219233371792\n",
            "7      \t [8.93553793 0.20050262]. \t  -219.79960571625585 \t -12.719219233371792\n",
            "8      \t [  2.82667191 -10.        ]. \t  -77757.9793521844 \t -12.719219233371792\n",
            "9      \t [-9.94996816  1.16607687]. \t  -440.93115686325655 \t -12.719219233371792\n",
            "10     \t [-5.40317049 -1.2329257 ]. \t  -183.5819922855759 \t -12.719219233371792\n",
            "11     \t [-0.70303921  0.5519691 ]. \t  \u001b[92m-6.345019780580339\u001b[0m \t -6.345019780580339\n",
            "12     \t [ 5.90128271 -3.02530477]. \t  -331.72389537591124 \t -6.345019780580339\n",
            "13     \t [3.72040235 9.59245682]. \t  -65030.831284512555 \t -6.345019780580339\n",
            "14     \t [0.25700611 2.82953541]. \t  -497.0258177285313 \t -6.345019780580339\n",
            "15     \t [-1.64033764  0.25292841]. \t  -13.225033802884496 \t -6.345019780580339\n",
            "16     \t [-0.28501709 -0.12595054]. \t  \u001b[92m-1.8519226429029625\u001b[0m \t -1.8519226429029625\n",
            "17     \t [5.70959431 2.9145606 ]. \t  -276.6450160353852 \t -1.8519226429029625\n",
            "18     \t [-0.62784579  0.17316635]. \t  -3.5960717791730756 \t -1.8519226429029625\n",
            "19     \t [ 2.16836961 -0.57008323]. \t  -5.97604215688495 \t -1.8519226429029625\n",
            "20     \t [ 4.48271032 -0.19553991]. \t  -50.95914862605588 \t -1.8519226429029625\n",
            "21     \t [ 0.70044559 -0.85632892]. \t  \u001b[92m-1.2637132327559573\u001b[0m \t -1.2637132327559573\n",
            "22     \t [-8.99212837  4.07774151]. \t  -3669.643163979375 \t -1.2637132327559573\n",
            "23     \t [ 1.18527379 -0.64513972]. \t  \u001b[92m-0.28335133715369154\u001b[0m \t -0.28335133715369154\n",
            "24     \t [3.24250352 4.49238034]. \t  -2760.885721187458 \t -0.28335133715369154\n",
            "25     \t [2.422493   0.47120282]. \t  -9.851847348469772 \t -0.28335133715369154\n",
            "26     \t [-5.55019062 -5.63467424]. \t  -9578.516139294848 \t -0.28335133715369154\n",
            "27     \t [ 9.34139514 -2.88519841]. \t  -176.37344287785447 \t -0.28335133715369154\n",
            "28     \t [3.09906289 1.17150039]. \t  -4.6570321287409415 \t -0.28335133715369154\n",
            "29     \t [ 0.31754563 -0.33152858]. \t  -0.484843630077507 \t -0.28335133715369154\n",
            "30     \t [ 5.62709174 -6.36322755]. \t  -11377.932328955048 \t -0.28335133715369154\n",
            "31     \t [ 1.37308106 -1.1656866 ]. \t  -3.7549233993888724 \t -0.28335133715369154\n",
            "32     \t [ 0.19606535 -0.15555428]. \t  -0.6899244143382958 \t -0.28335133715369154\n",
            "33     \t [-2.46627227  1.2703317 ]. \t  -76.85279283651194 \t -0.28335133715369154\n",
            "34     \t [-0.7384491  -1.36187711]. \t  -42.589258572292266 \t -0.28335133715369154\n",
            "35     \t [-9.9948344  -8.27133975]. \t  -43236.022323985 \t -0.28335133715369154\n",
            "36     \t [ 0.14122897 -0.27553603]. \t  -0.737712879417416 \t -0.28335133715369154\n",
            "37     \t [-2.10791836 -0.44648734]. \t  -22.225446686242854 \t -0.28335133715369154\n",
            "38     \t [-8.17618025 -1.323817  ]. \t  -357.10143013363876 \t -0.28335133715369154\n",
            "39     \t [9.01206049 2.92274578]. \t  -194.5341302948754 \t -0.28335133715369154\n",
            "40     \t [-1.63728627 -9.57642363]. \t  -68496.22940321284 \t -0.28335133715369154\n",
            "41     \t [0.72932316 0.01992999]. \t  -1.1347742429880943 \t -0.28335133715369154\n",
            "42     \t [ 2.93620331 -1.8920568 ]. \t  -39.425709718050044 \t -0.28335133715369154\n",
            "43     \t [ 1.15482161 -0.41972634]. \t  -1.3119219049633228 \t -0.28335133715369154\n",
            "44     \t [-3.07110895 -1.76460295]. \t  -189.50765771419532 \t -0.28335133715369154\n",
            "45     \t [ 8.2903885  -9.95230143]. \t  -72105.95142930017 \t -0.28335133715369154\n",
            "46     \t [ 6.23035714 -0.33150544]. \t  -99.61042905144392 \t -0.28335133715369154\n",
            "47     \t [2.88519844 0.50212939]. \t  -14.891634140427103 \t -0.28335133715369154\n",
            "48     \t [6.62649932 9.19341493]. \t  -52786.490281699094 \t -0.28335133715369154\n",
            "49     \t [ 1.09969936 -0.9512198 ]. \t  -1.0179663780971386 \t -0.28335133715369154\n",
            "50     \t [-6.5107799  0.8593455]. \t  -184.01945269076842 \t -0.28335133715369154\n",
            "51     \t [ 0.91774604 -0.55424122]. \t  \u001b[92m-0.1908438035179484\u001b[0m \t -0.1908438035179484\n",
            "52     \t [-4.73836531 -2.7767814 ]. \t  -845.7312696634915 \t -0.1908438035179484\n",
            "53     \t [1.26374272 0.38186474]. \t  -1.959522322902396 \t -0.1908438035179484\n",
            "54     \t [ 0.24536892 -9.18910643]. \t  -56875.4113261257 \t -0.1908438035179484\n",
            "55     \t [3.97051309 1.06646228]. \t  -14.575623335779952 \t -0.1908438035179484\n",
            "56     \t [2.43116934 5.18720372]. \t  -5282.4727117377015 \t -0.1908438035179484\n",
            "57     \t [2.96484077 1.38348256]. \t  -5.350852650320931 \t -0.1908438035179484\n",
            "58     \t [-2.07398088  5.0103825 ]. \t  -5476.232284473293 \t -0.1908438035179484\n",
            "59     \t [-4.11862925  0.50166899]. \t  -68.92563047890268 \t -0.1908438035179484\n",
            "60     \t [ 9.8845375  -3.76667601]. \t  -762.7809199151517 \t -0.1908438035179484\n",
            "61     \t [ 9.7594397  -1.34951802]. \t  -151.56418774222593 \t -0.1908438035179484\n",
            "62     \t [ 3.64445271 -4.63530097]. \t  -3100.30982698243 \t -0.1908438035179484\n",
            "63     \t [-9.07829232  3.54027274]. \t  -2433.382440195059 \t -0.1908438035179484\n",
            "64     \t [ 2.89586453 -3.25500391]. \t  -672.952472255595 \t -0.1908438035179484\n",
            "65     \t [-5.25129345  2.12510715]. \t  -447.11280209366765 \t -0.1908438035179484\n",
            "66     \t [ 8.35331433 -2.73762228]. \t  -142.13990155243454 \t -0.1908438035179484\n",
            "67     \t [0.29177252 3.76797388]. \t  -1580.112963263459 \t -0.1908438035179484\n",
            "68     \t [0.41212217 1.5479375 ]. \t  -38.716131886311715 \t -0.1908438035179484\n",
            "69     \t [-1.00101073  1.07403191]. \t  -25.89109270084497 \t -0.1908438035179484\n",
            "70     \t [ 2.20001352 -6.36777374]. \t  -12450.935743667296 \t -0.1908438035179484\n",
            "71     \t [ 3.01437449 -1.3135087 ]. \t  -4.438307811149564 \t -0.1908438035179484\n",
            "72     \t [ 2.63311249 -1.20104358]. \t  -2.793962503821272 \t -0.1908438035179484\n",
            "73     \t [-4.33233879  7.85144038]. \t  -32603.475042977963 \t -0.1908438035179484\n",
            "74     \t [-8.14316125  1.77998603]. \t  -502.930191558415 \t -0.1908438035179484\n",
            "75     \t [ 2.75759941 -7.14404541]. \t  -19730.88974639189 \t -0.1908438035179484\n",
            "76     \t [ 4.01917162 -7.50134124]. \t  -23562.76031559984 \t -0.1908438035179484\n",
            "77     \t [-2.44688676  3.52765592]. \t  -1506.3510075462314 \t -0.1908438035179484\n",
            "78     \t [1.85426636 1.07683251]. \t  -1.1619795137878044 \t -0.1908438035179484\n",
            "79     \t [2.08019587 0.91971997]. \t  -1.4685729746795335 \t -0.1908438035179484\n",
            "80     \t [ 2.68243292 -0.90086672]. \t  -5.074861063492358 \t -0.1908438035179484\n",
            "81     \t [7.74797399 9.81281296]. \t  -68373.2105317395 \t -0.1908438035179484\n",
            "82     \t [0.519504   5.28291595]. \t  -6116.167104381529 \t -0.1908438035179484\n",
            "83     \t [ 1.21280511 -0.77999727]. \t  \u001b[92m-0.04531779701916971\u001b[0m \t -0.04531779701916971\n",
            "84     \t [3.34558955 0.86833311]. \t  -12.255225877529249 \t -0.04531779701916971\n",
            "85     \t [2.44885313 1.03851361]. \t  -2.269507310332774 \t -0.04531779701916971\n",
            "86     \t [0.65317464 0.84534888]. \t  -1.3248100354198333 \t -0.04531779701916971\n",
            "87     \t [-8.26180241 -0.01564831]. \t  -222.31192709882077 \t -0.04531779701916971\n",
            "88     \t [-0.072469   -3.07692734]. \t  -723.7151885814683 \t -0.04531779701916971\n",
            "89     \t [ 1.33157875 -0.82261696]. \t  -0.11089656701551649 \t -0.04531779701916971\n",
            "90     \t [-2.89804816 -1.01231984]. \t  -64.15288597645016 \t -0.04531779701916971\n",
            "91     \t [6.47232451 0.25390567]. \t  -110.42348666866121 \t -0.04531779701916971\n",
            "92     \t [-4.06431762  0.37047618]. \t  -63.298079846082146 \t -0.04531779701916971\n",
            "93     \t [0.51746225 0.78366197]. \t  -1.243287302612685 \t -0.04531779701916971\n",
            "94     \t [7.47848886 4.32620845]. \t  -1836.4155354062034 \t -0.04531779701916971\n",
            "95     \t [ 4.30144137 -1.21927754]. \t  -14.427564718224374 \t -0.04531779701916971\n",
            "96     \t [ 7.43827626 -8.5017821 ]. \t  -37646.51348997539 \t -0.04531779701916971\n",
            "97     \t [5.4040928  1.08001512]. \t  -38.26090964952843 \t -0.04531779701916971\n",
            "98     \t [ 0.9744419  -0.64215307]. \t  -0.045485839241017643 \t -0.04531779701916971\n",
            "99     \t [-4.26426686 -2.52050317]. \t  -603.6837600478647 \t -0.04531779701916971\n",
            "100    \t [-4.95102259 -9.69287804]. \t  -74421.35628980433 \t -0.04531779701916971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "any0xrgYuvxA",
        "outputId": "6224e600-d03a-4dad-ba0c-094855b7003c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.27896664 -3.67267997]. \t  -815.4627708918207 \t -28.992408538517264\n",
            "init   \t [3.44301557 0.15823335]. \t  -28.992408538517264 \t -28.992408538517264\n",
            "init   \t [ 5.63320853 -4.32636643]. \t  -2044.16089755496 \t -28.992408538517264\n",
            "init   \t [-5.32582432  1.27599373]. \t  -187.32245577722023 \t -28.992408538517264\n",
            "init   \t [7.50048729 4.37885841]. \t  -1945.4933626881252 \t -28.992408538517264\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -28.992408538517264\n",
            "2      \t [-0.94408028  9.32615359]. \t  -61182.65342174163 \t -28.992408538517264\n",
            "3      \t [-9.58291173  9.55394704]. \t  -73946.57346618871 \t -28.992408538517264\n",
            "4      \t [-0.59203298 -8.9817774 ]. \t  -52449.51345826705 \t -28.992408538517264\n",
            "5      \t [-5.79185586 -4.56009135]. \t  -4535.994951327608 \t -28.992408538517264\n",
            "6      \t [  7.96625045 -10.        ]. \t  -73802.4505740722 \t -28.992408538517264\n",
            "7      \t [-0.24515074 -2.86070219]. \t  -553.4930590001303 \t -28.992408538517264\n",
            "8      \t [-10.         -0.6336517]. \t  -354.41086877691544 \t -28.992408538517264\n",
            "9      \t [1.54409189 4.0029452 ]. \t  -1861.1679916738271 \t -28.992408538517264\n",
            "10     \t [9.93199294 8.89866837]. \t  -44149.006619557964 \t -28.992408538517264\n",
            "11     \t [-9.05180591  3.90888298]. \t  -3239.0282701929013 \t -28.992408538517264\n",
            "12     \t [4.47909368 8.55828188]. \t  -40345.377509016274 \t -28.992408538517264\n",
            "13     \t [-3.11783424  4.50268737]. \t  -3830.4343416462425 \t -28.992408538517264\n",
            "14     \t [ 6.970631   -0.05954088]. \t  -132.63023454599448 \t -28.992408538517264\n",
            "15     \t [-1.3637802   0.53897056]. \t  \u001b[92m-13.15162985248233\u001b[0m \t -13.15162985248233\n",
            "16     \t [-9.84338192 -4.82358647]. \t  -6474.391191552187 \t -13.15162985248233\n",
            "17     \t [-5.27945026 -9.39574644]. \t  -66170.67729300029 \t -13.15162985248233\n",
            "18     \t [-3.24066968 -1.20192346]. \t  -93.13486528770798 \t -13.15162985248233\n",
            "19     \t [ 9.80009726 -0.4530983 ]. \t  -253.767174459005 \t -13.15162985248233\n",
            "20     \t [ 3.82103858 -8.10668198]. \t  -32579.413813985007 \t -13.15162985248233\n",
            "21     \t [-5.57630504  7.51111549]. \t  -28085.111994824434 \t -13.15162985248233\n",
            "22     \t [ 1.63660318 -0.31345176]. \t  \u001b[92m-4.553034831889238\u001b[0m \t -4.553034831889238\n",
            "23     \t [ 2.63606259 -1.17735944]. \t  \u001b[92m-2.713849590250892\u001b[0m \t -2.713849590250892\n",
            "24     \t [0.20064501 0.56593902]. \t  \u001b[92m-1.026043360375231\u001b[0m \t -1.026043360375231\n",
            "25     \t [ 9.59677179 -5.40648713]. \t  -4849.174932793484 \t -1.026043360375231\n",
            "26     \t [ 0.03095551 -0.10316497]. \t  \u001b[92m-0.9392342300294892\u001b[0m \t -0.9392342300294892\n",
            "27     \t [-7.4518512  -1.44068434]. \t  -340.6927253870056 \t -0.9392342300294892\n",
            "28     \t [ 2.20764331 -2.7287567 ]. \t  -323.25569280354216 \t -0.9392342300294892\n",
            "29     \t [ 2.28269283 -0.43416486]. \t  -8.908644613925954 \t -0.9392342300294892\n",
            "30     \t [ 3.92072967 -1.31678312]. \t  -8.940887953714476 \t -0.9392342300294892\n",
            "31     \t [ 3.39262659 -1.17470379]. \t  -6.525454196200927 \t -0.9392342300294892\n",
            "32     \t [4.92712691 2.03708852]. \t  -38.16757663631995 \t -0.9392342300294892\n",
            "33     \t [ 0.45410311 -0.10564588]. \t  \u001b[92m-0.6708730902986406\u001b[0m \t -0.6708730902986406\n",
            "34     \t [4.66692272 0.30688142]. \t  -53.56150622486405 \t -0.6708730902986406\n",
            "35     \t [ 2.12214983 -0.83075235]. \t  -2.3599057231439415 \t -0.6708730902986406\n",
            "36     \t [0.62127928 0.20561566]. \t  -0.7195740102886623 \t -0.6708730902986406\n",
            "37     \t [1.59483175 0.96941783]. \t  \u001b[92m-0.5159445172592685\u001b[0m \t -0.5159445172592685\n",
            "38     \t [-0.80034209  1.4197909 ]. \t  -49.936800268480376 \t -0.5159445172592685\n",
            "39     \t [0.68323943 0.44123089]. \t  \u001b[92m-0.27305644178590766\u001b[0m \t -0.27305644178590766\n",
            "40     \t [-2.01832129 -4.82538742]. \t  -4730.512899196404 \t -0.27305644178590766\n",
            "41     \t [1.97054642 0.60234878]. \t  -4.041503963093444 \t -0.27305644178590766\n",
            "42     \t [0.43183101 0.60665271]. \t  -0.5079204899760076 \t -0.27305644178590766\n",
            "43     \t [0.00810804 0.10082911]. \t  -0.984148561613974 \t -0.27305644178590766\n",
            "44     \t [-1.1387931 -0.3369882]. \t  -8.305884544632724 \t -0.27305644178590766\n",
            "45     \t [-2.38542659  1.20209129]. \t  -67.12235528828012 \t -0.27305644178590766\n",
            "46     \t [1.28514168 0.46659762]. \t  -1.5253369190302717 \t -0.27305644178590766\n",
            "47     \t [0.71691291 0.63781005]. \t  \u001b[92m-0.09883636460275282\u001b[0m \t -0.09883636460275282\n",
            "48     \t [2.34732517 0.73669619]. \t  -4.9999805546920495 \t -0.09883636460275282\n",
            "49     \t [4.67432013 1.10826181]. \t  -23.338183015419787 \t -0.09883636460275282\n",
            "50     \t [3.51245086 0.92949094]. \t  -12.681604157983571 \t -0.09883636460275282\n",
            "51     \t [-8.03251586  4.12578886]. \t  -3622.4977333168395 \t -0.09883636460275282\n",
            "52     \t [-3.29673452 -4.93157514]. \t  -5413.4907693308 \t -0.09883636460275282\n",
            "53     \t [1.37358501 0.78855877]. \t  -0.1733320492726176 \t -0.09883636460275282\n",
            "54     \t [ 1.09708419 -0.20178053]. \t  -2.072529108057234 \t -0.09883636460275282\n",
            "55     \t [ 2.43556008 -1.01165455]. \t  -2.362961809344832 \t -0.09883636460275282\n",
            "56     \t [ 3.19805191 -1.62691037]. \t  -13.614702168354732 \t -0.09883636460275282\n",
            "57     \t [-5.87599396  3.20777726]. \t  -1447.0835971801562 \t -0.09883636460275282\n",
            "58     \t [ 8.52377976 -9.26055523]. \t  -53189.38260389485 \t -0.09883636460275282\n",
            "59     \t [-0.43071772  0.1452966 ]. \t  -2.494297520022179 \t -0.09883636460275282\n",
            "60     \t [0.00978633 7.64488062]. \t  -27322.200876269962 \t -0.09883636460275282\n",
            "61     \t [9.39787519 2.18008681]. \t  -70.54749838948561 \t -0.09883636460275282\n",
            "62     \t [-2.03514074  0.04060408]. \t  -17.522539319066965 \t -0.09883636460275282\n",
            "63     \t [-0.94734154 -2.76240333]. \t  -529.2610645527533 \t -0.09883636460275282\n",
            "64     \t [-5.38165649 -0.21669377]. \t  -100.6892483846669 \t -0.09883636460275282\n",
            "65     \t [-1.75567565  9.70653295]. \t  -72351.48731166038 \t -0.09883636460275282\n",
            "66     \t [ 4.69089896 -1.35492396]. \t  -15.700521475302931 \t -0.09883636460275282\n",
            "67     \t [-2.53598717 -7.62877066]. \t  -28302.273204681238 \t -0.09883636460275282\n",
            "68     \t [ 7.17931782 -0.57421152]. \t  -123.20164106679526 \t -0.09883636460275282\n",
            "69     \t [6.33797836 5.1826123 ]. \t  -4518.405148097174 \t -0.09883636460275282\n",
            "70     \t [ 6.23554753 -0.27153034]. \t  -101.54063958936088 \t -0.09883636460275282\n",
            "71     \t [-0.0710583   7.88508369]. \t  -30961.893643346364 \t -0.09883636460275282\n",
            "72     \t [-1.00200229 -1.00765412]. \t  -22.402987782058112 \t -0.09883636460275282\n",
            "73     \t [3.25089107 3.65040972]. \t  -1100.1950384901293 \t -0.09883636460275282\n",
            "74     \t [5.61329352 1.89527823]. \t  -26.217714788747568 \t -0.09883636460275282\n",
            "75     \t [-6.52094594  0.82226348]. \t  -180.5385669132019 \t -0.09883636460275282\n",
            "76     \t [-5.53701779  5.0656976 ]. \t  -6508.76130733897 \t -0.09883636460275282\n",
            "77     \t [9.95284226 4.56920198]. \t  -2102.9350150547402 \t -0.09883636460275282\n",
            "78     \t [-0.10241599  0.11671667]. \t  -1.2489452582909175 \t -0.09883636460275282\n",
            "79     \t [ 9.55122594 -1.83559808]. \t  -88.94248751324137 \t -0.09883636460275282\n",
            "80     \t [8.80959179 1.63789687]. \t  -84.71446823830173 \t -0.09883636460275282\n",
            "81     \t [9.894852   1.79410194]. \t  -103.02352578196435 \t -0.09883636460275282\n",
            "82     \t [ 6.88349109 -5.5330375 ]. \t  -5941.485893656064 \t -0.09883636460275282\n",
            "83     \t [ 8.16371685 -9.0944609 ]. \t  -49509.41347498307 \t -0.09883636460275282\n",
            "84     \t [ 2.52627823 -3.2849107 ]. \t  -728.5154544669196 \t -0.09883636460275282\n",
            "85     \t [ 7.66086451 -2.83415504]. \t  -185.6217173760848 \t -0.09883636460275282\n",
            "86     \t [ 2.47058555 -1.52675396]. \t  -11.766824756528827 \t -0.09883636460275282\n",
            "87     \t [ 1.52087332 -1.74486749]. \t  -42.00915806357894 \t -0.09883636460275282\n",
            "88     \t [-8.10679451 -3.97384878]. \t  -3233.485078639014 \t -0.09883636460275282\n",
            "89     \t [0.61124987 4.40812793]. \t  -2926.5720044905806 \t -0.09883636460275282\n",
            "90     \t [ 1.33338474 -6.7834835 ]. \t  -16452.33364704236 \t -0.09883636460275282\n",
            "91     \t [5.10551579 7.71280412]. \t  -25949.235124425155 \t -0.09883636460275282\n",
            "92     \t [-0.34321087 -7.74449847]. \t  -28944.888935127223 \t -0.09883636460275282\n",
            "93     \t [-7.69801172 -6.19002446]. \t  -14299.028906402948 \t -0.09883636460275282\n",
            "94     \t [-4.58912948  6.6451055 ]. \t  -17293.53875567509 \t -0.09883636460275282\n",
            "95     \t [-3.30205712  8.57469757]. \t  -45230.517092884154 \t -0.09883636460275282\n",
            "96     \t [ 3.00188445 -1.29398595]. \t  -4.248241124859602 \t -0.09883636460275282\n",
            "97     \t [1.90246992 0.88711204]. \t  -1.0303215989484562 \t -0.09883636460275282\n",
            "98     \t [5.05108621 5.33718012]. \t  -5407.76273916 \t -0.09883636460275282\n",
            "99     \t [-7.08194145  8.05431634]. \t  -37508.00702857726 \t -0.09883636460275282\n",
            "100    \t [-6.40214562  9.5152672 ]. \t  -70354.36649607628 \t -0.09883636460275282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLyKt6Quvzx",
        "outputId": "4c51b120-2e95-4c10-c99c-ef3995635b1c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.03648056 4.61039249]. \t  -3120.691750998894 \t -133.8839693070206\n",
            "init   \t [-6.13265275  0.39354476]. \t  -133.8839693070206 \t -133.8839693070206\n",
            "init   \t [3.52766533 6.08889746]. \t  -9981.232217710998 \t -133.8839693070206\n",
            "init   \t [0.67025729 8.16068359]. \t  -35124.935571397844 \t -133.8839693070206\n",
            "init   \t [-1.13037742 -4.8805558 ]. \t  -4761.569630397088 \t -133.8839693070206\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -133.8839693070206\n",
            "2      \t [ 8.81602768 -5.83496092]. \t  -7088.736716131159 \t -133.8839693070206\n",
            "3      \t [-9.20794452  7.56905046]. \t  -30751.632331903027 \t -133.8839693070206\n",
            "4      \t [9.38422178 1.56611414]. \t  \u001b[92m-110.41438012079544\u001b[0m \t -110.41438012079544\n",
            "5      \t [9.65812984 9.94541442]. \t  -70886.65075415972 \t -110.41438012079544\n",
            "6      \t [  3.47299839 -10.        ]. \t  -77251.84044849784 \t -110.41438012079544\n",
            "7      \t [-10.          -3.40661724]. \t  -2326.819089345889 \t -110.41438012079544\n",
            "8      \t [ 4.4016417  -1.09433346]. \t  \u001b[92m-19.623332960589863\u001b[0m \t -19.623332960589863\n",
            "9      \t [ -3.66325775 -10.        ]. \t  -82979.19108755681 \t -19.623332960589863\n",
            "10     \t [-0.51140319  0.80111497]. \t  \u001b[92m-8.728199902643897\u001b[0m \t -8.728199902643897\n",
            "11     \t [ 4.13423737 -4.79738778]. \t  -3520.3096559459145 \t -8.728199902643897\n",
            "12     \t [-3.60023019  4.03284354]. \t  -2631.6101602627455 \t -8.728199902643897\n",
            "13     \t [-10.           2.03101302]. \t  -787.1270293251804 \t -8.728199902643897\n",
            "14     \t [-5.84816337 -4.77204097]. \t  -5329.360442787565 \t -8.728199902643897\n",
            "15     \t [5.92703463 1.33234467]. \t  -35.57355131596512 \t -8.728199902643897\n",
            "16     \t [ 1.42531111 -0.61333311]. \t  \u001b[92m-1.0866293409967647\u001b[0m \t -1.0866293409967647\n",
            "17     \t [8.94935273 4.74793873]. \t  -2674.8842545175467 \t -1.0866293409967647\n",
            "18     \t [1.57294283 0.85636759]. \t  \u001b[92m-0.3508254368416508\u001b[0m \t -0.3508254368416508\n",
            "19     \t [ 7.30247519 -1.4208839 ]. \t  -61.03711323220951 \t -0.3508254368416508\n",
            "20     \t [-4.30231038  9.06824702]. \t  -56993.74001571176 \t -0.3508254368416508\n",
            "21     \t [2.09342659 0.88499568]. \t  -1.7510226016935926 \t -0.3508254368416508\n",
            "22     \t [-2.3985528  -0.86200498]. \t  -41.73129605475442 \t -0.3508254368416508\n",
            "23     \t [1.01373306 1.24584743]. \t  -8.740891808051646 \t -0.3508254368416508\n",
            "24     \t [ 1.66809502 -0.35033549]. \t  -4.49407540119828 \t -0.3508254368416508\n",
            "25     \t [ 0.4726783  -1.37199212]. \t  -21.953207918423796 \t -0.3508254368416508\n",
            "26     \t [ 2.50842327 -1.33453858]. \t  -4.495331461606694 \t -0.3508254368416508\n",
            "27     \t [0.66658766 0.16314915]. \t  -0.8635660329334305 \t -0.3508254368416508\n",
            "28     \t [ 1.6566367  -1.24052429]. \t  -4.470587727825223 \t -0.3508254368416508\n",
            "29     \t [  8.47261654 -10.        ]. \t  -73421.31722952986 \t -0.3508254368416508\n",
            "30     \t [1.60190159 2.07817772]. \t  -99.3656627874179 \t -0.3508254368416508\n",
            "31     \t [-7.2405433  3.6542144]. \t  -2372.7189854361277 \t -0.3508254368416508\n",
            "32     \t [1.94919333 0.37320184]. \t  -6.483004615041654 \t -0.3508254368416508\n",
            "33     \t [0.82028355 0.61312322]. \t  \u001b[92m-0.0416669962015881\u001b[0m \t -0.0416669962015881\n",
            "34     \t [4.53817702 9.84643969]. \t  -71731.91582202556 \t -0.0416669962015881\n",
            "35     \t [3.37773423 1.82837463]. \t  -27.5416416202328 \t -0.0416669962015881\n",
            "36     \t [ 9.1818683  -0.64264977]. \t  -206.58412396390037 \t -0.0416669962015881\n",
            "37     \t [-0.48231183  0.08581847]. \t  -2.6913487448880287 \t -0.0416669962015881\n",
            "38     \t [1.07463109 0.92945709]. \t  -0.8587793235684995 \t -0.0416669962015881\n",
            "39     \t [0.24737613 0.79274542]. \t  -2.6046815679511983 \t -0.0416669962015881\n",
            "40     \t [ 0.79948955 -0.20138059]. \t  -1.07234771750924 \t -0.0416669962015881\n",
            "41     \t [ 6.32547265 -0.18852054]. \t  -106.59551418895283 \t -0.0416669962015881\n",
            "42     \t [4.54058116 2.34787347]. \t  -96.63160051892186 \t -0.0416669962015881\n",
            "43     \t [3.32639687 0.93745772]. \t  -10.334031152986707 \t -0.0416669962015881\n",
            "44     \t [-0.10595981  1.83879071]. \t  -95.56920361695136 \t -0.0416669962015881\n",
            "45     \t [-2.30197516  0.7192549 ]. \t  -33.16924467199134 \t -0.0416669962015881\n",
            "46     \t [ 2.33248022 -1.58905147]. \t  -16.547169773744844 \t -0.0416669962015881\n",
            "47     \t [-4.04097297  0.42283596]. \t  -64.10595368195206 \t -0.0416669962015881\n",
            "48     \t [ 7.1994091  -6.28082487]. \t  -10319.671490122182 \t -0.0416669962015881\n",
            "49     \t [0.78578488 0.57954339]. \t  -0.0719000951764619 \t -0.0416669962015881\n",
            "50     \t [-0.16584395  0.40147206]. \t  -1.8358775983576148 \t -0.0416669962015881\n",
            "51     \t [ 3.31092646 -1.09765283]. \t  -6.964858891123254 \t -0.0416669962015881\n",
            "52     \t [ 2.21161589 -0.98796963]. \t  -1.6026395067875296 \t -0.0416669962015881\n",
            "53     \t [-7.01007846 -1.14467003]. \t  -249.65894016951592 \t -0.0416669962015881\n",
            "54     \t [-1.54936135  0.64692999]. \t  -17.88903575305914 \t -0.0416669962015881\n",
            "55     \t [-1.30577327 -0.65791999]. \t  -14.747334024044921 \t -0.0416669962015881\n",
            "56     \t [ 1.92185207 -6.41993558]. \t  -12964.342570814926 \t -0.0416669962015881\n",
            "57     \t [6.64082385 2.62363355]. \t  -133.38098793646694 \t -0.0416669962015881\n",
            "58     \t [4.25963386 0.65100593]. \t  -33.908924986039835 \t -0.0416669962015881\n",
            "59     \t [ 6.63955905 -2.48961083]. \t  -98.08531591546895 \t -0.0416669962015881\n",
            "60     \t [-4.12826678 -1.43062373]. \t  -161.48969371447112 \t -0.0416669962015881\n",
            "61     \t [8.76679784 4.29391193]. \t  -1640.5056011511847 \t -0.0416669962015881\n",
            "62     \t [-8.98890867  0.60653298]. \t  -288.9168339602197 \t -0.0416669962015881\n",
            "63     \t [ 1.19652657 -0.01385355]. \t  -2.90013754162915 \t -0.0416669962015881\n",
            "64     \t [8.22178716 2.76267094]. \t  -151.35949236953613 \t -0.0416669962015881\n",
            "65     \t [ 3.34476732 -1.38828617]. \t  -6.017949508438166 \t -0.0416669962015881\n",
            "66     \t [5.51938091 5.998601  ]. \t  -8850.844942087717 \t -0.0416669962015881\n",
            "67     \t [-0.99559317  0.16994729]. \t  -6.201515462000504 \t -0.0416669962015881\n",
            "68     \t [ 9.95543278 -2.66549726]. \t  -116.39822867442642 \t -0.0416669962015881\n",
            "69     \t [0.7511544  0.65589036]. \t  -0.08578648214106574 \t -0.0416669962015881\n",
            "70     \t [ 0.3286204  -0.16827199]. \t  -0.5987071132695614 \t -0.0416669962015881\n",
            "71     \t [1.25853039 7.04421599]. \t  -19201.570925244465 \t -0.0416669962015881\n",
            "72     \t [2.9957746  1.25225142]. \t  -4.02259261536377 \t -0.0416669962015881\n",
            "73     \t [-4.05737843  1.88193186]. \t  -273.80806922211553 \t -0.0416669962015881\n",
            "74     \t [-3.02927854 -1.38291775]. \t  -110.19524358375693 \t -0.0416669962015881\n",
            "75     \t [7.71939719 1.3140332 ]. \t  -81.54833457870807 \t -0.0416669962015881\n",
            "76     \t [-9.21247784  5.10503346]. \t  -7628.3109577788855 \t -0.0416669962015881\n",
            "77     \t [-5.41577237  1.00350708]. \t  -151.56674301338865 \t -0.0416669962015881\n",
            "78     \t [-4.97692702  8.28742049]. \t  -40556.84917917206 \t -0.0416669962015881\n",
            "79     \t [9.37697898 0.9479332 ]. \t  -185.08124840416218 \t -0.0416669962015881\n",
            "80     \t [4.92905531 1.29978043]. \t  -20.243697047984963 \t -0.0416669962015881\n",
            "81     \t [1.55605664 5.36534893]. \t  -6276.315307365103 \t -0.0416669962015881\n",
            "82     \t [9.50807662 9.54940402]. \t  -59843.33626052231 \t -0.0416669962015881\n",
            "83     \t [9.63573364 2.38494157]. \t  -80.63220154824702 \t -0.0416669962015881\n",
            "84     \t [-7.85460815  5.72821771]. \t  -10876.896890461632 \t -0.0416669962015881\n",
            "85     \t [9.48298339 8.28908303]. \t  -32806.59307027537 \t -0.0416669962015881\n",
            "86     \t [-3.61736309 -8.07696956]. \t  -35982.776385716104 \t -0.0416669962015881\n",
            "87     \t [-2.22392311  8.04670637]. \t  -34712.23316395252 \t -0.0416669962015881\n",
            "88     \t [ 0.07728894 -0.57940525]. \t  -1.557381264160874 \t -0.0416669962015881\n",
            "89     \t [6.22206129 1.86430453]. \t  -28.333393614984992 \t -0.0416669962015881\n",
            "90     \t [0.07531789 6.78340765]. \t  -16911.902094659115 \t -0.0416669962015881\n",
            "91     \t [ 3.26029884 -6.05289087]. \t  -9809.218906598351 \t -0.0416669962015881\n",
            "92     \t [-4.91703696 -6.17035295]. \t  -13177.608925696788 \t -0.0416669962015881\n",
            "93     \t [ 1.20769166 -0.79984243]. \t  -0.053447504305350696 \t -0.0416669962015881\n",
            "94     \t [ 2.59822471 -1.06594215]. \t  -2.766560568363549 \t -0.0416669962015881\n",
            "95     \t [ 3.71376938 -3.98894139]. \t  -1587.6567265140147 \t -0.0416669962015881\n",
            "96     \t [ 1.22295835 -0.82703993]. \t  -0.09177883407102674 \t -0.0416669962015881\n",
            "97     \t [-1.59531082 -0.99543159]. \t  -32.326625222316835 \t -0.0416669962015881\n",
            "98     \t [6.82112242 0.20006335]. \t  -124.76956260580201 \t -0.0416669962015881\n",
            "99     \t [0.57308471 0.50838026]. \t  -0.18856988701464567 \t -0.0416669962015881\n",
            "100    \t [2.62822905 7.69920127]. \t  -26880.869212000907 \t -0.0416669962015881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3FtfYSuv2u",
        "outputId": "eebb0e70-759b-41d4-82ff-29562232c4e1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [7.46858806 9.37081326]. \t  -56594.51755971677 \t -0.3114572247523004\n",
            "init   \t [7.3838908  0.61711383]. \t  -128.46197097057788 \t -0.3114572247523004\n",
            "init   \t [-5.34543344 -9.77202391]. \t  -77131.46635795479 \t -0.3114572247523004\n",
            "init   \t [-1.39062363 -1.9529728 ]. \t  -168.39363959555394 \t -0.3114572247523004\n",
            "init   \t [ 0.45349343 -0.43216408]. \t  -0.3114572247523004 \t -0.3114572247523004\n",
            "1      \t [1.13550865 0.60170966]. \t  -0.3568618976266401 \t -0.3114572247523004\n",
            "2      \t [1.83220389 0.0366395 ]. \t  -7.386842698364754 \t -0.3114572247523004\n",
            "3      \t [0.12526212 2.11923098]. \t  -157.65869365560948 \t -0.3114572247523004\n",
            "4      \t [-9.29644966  8.17908998]. \t  -41056.3576701472 \t -0.3114572247523004\n",
            "5      \t [ 1.07322998 -0.23853749]. \t  -1.846373363258862 \t -0.3114572247523004\n",
            "6      \t [ 7.36073281 -9.27958416]. \t  -54398.51471425817 \t -0.3114572247523004\n",
            "7      \t [ 0.97137793 -0.0247661 ]. \t  -1.8832059618758707 \t -0.3114572247523004\n",
            "8      \t [ 1.45164392 -1.85486044]. \t  -59.16011234429896 \t -0.3114572247523004\n",
            "9      \t [1.80347686 1.11375332]. \t  -1.5633600766467395 \t -0.3114572247523004\n",
            "10     \t [-9.41285804 -2.80958783]. \t  -1378.5505106215326 \t -0.3114572247523004\n",
            "11     \t [-5.93424134  2.41705351]. \t  -668.9093712135107 \t -0.3114572247523004\n",
            "12     \t [-2.03424237  8.23443927]. \t  -37902.17174791396 \t -0.3114572247523004\n",
            "13     \t [ 0.97978892 -7.66053925]. \t  -27092.30284655015 \t -0.3114572247523004\n",
            "14     \t [ 5.64865449 -3.69755954]. \t  -962.9767410745915 \t -0.3114572247523004\n",
            "15     \t [3.06650381 5.65678758]. \t  -7429.684663573885 \t -0.3114572247523004\n",
            "16     \t [9.09566652 4.11146462]. \t  -1286.9666108532908 \t -0.3114572247523004\n",
            "17     \t [-5.08118966 -3.59599708]. \t  -1951.991113730439 \t -0.3114572247523004\n",
            "18     \t [ 9.87045812 -4.08707035]. \t  -1186.743908762745 \t -0.3114572247523004\n",
            "19     \t [-10.           1.81744048]. \t  -672.5304142514286 \t -0.3114572247523004\n",
            "20     \t [-10. -10.]. \t  -88321.0 \t -0.3114572247523004\n",
            "21     \t [1.61919221 0.31654934]. \t  -4.409302138646603 \t -0.3114572247523004\n",
            "22     \t [2.18772153 9.71406758]. \t  -69594.63199043454 \t -0.3114572247523004\n",
            "23     \t [-2.68740822  3.31020571]. \t  -1224.1464510636056 \t -0.3114572247523004\n",
            "24     \t [ 3.35950931 -0.34619439]. \t  -25.03369043782208 \t -0.3114572247523004\n",
            "25     \t [ 0.59682223 -0.51509253]. \t  \u001b[92m-0.17131232405838087\u001b[0m \t -0.17131232405838087\n",
            "26     \t [ 9.53231741 -0.71981733]. \t  -217.1659484412533 \t -0.17131232405838087\n",
            "27     \t [5.56409041 2.07319553]. \t  -39.21926109698289 \t -0.17131232405838087\n",
            "28     \t [3.81394841 1.8699891 ]. \t  -28.140180488834858 \t -0.17131232405838087\n",
            "29     \t [-3.53369076 -0.17008797]. \t  -46.35282340360144 \t -0.17131232405838087\n",
            "30     \t [-5.33936767  5.17890196]. \t  -6997.80309626622 \t -0.17131232405838087\n",
            "31     \t [-8.25925792 -5.92706195]. \t  -12416.321368051467 \t -0.17131232405838087\n",
            "32     \t [-2.19452081 -4.92525567]. \t  -5153.376987233046 \t -0.17131232405838087\n",
            "33     \t [-6.3251282  -0.15483677]. \t  -134.8897252266102 \t -0.17131232405838087\n",
            "34     \t [ 2.41695856 -3.31759096]. \t  -770.0033113962799 \t -0.17131232405838087\n",
            "35     \t [-0.23258091 -0.36019775]. \t  -2.0035131138745115 \t -0.17131232405838087\n",
            "36     \t [2.23443811 1.1396289 ]. \t  -1.7874770199350236 \t -0.17131232405838087\n",
            "37     \t [-1.44159435  0.29147986]. \t  -11.155346575845451 \t -0.17131232405838087\n",
            "38     \t [4.97931151 0.3637167 ]. \t  -60.29231274375268 \t -0.17131232405838087\n",
            "39     \t [-9.20590147  9.77446249]. \t  -80333.23239983381 \t -0.17131232405838087\n",
            "40     \t [ 1.51388645 -1.29011615]. \t  -6.851896947195383 \t -0.17131232405838087\n",
            "41     \t [ 1.21655964 -0.55600701]. \t  -0.7627569975875946 \t -0.17131232405838087\n",
            "42     \t [2.40945614 2.03170028]. \t  -70.34164411051712 \t -0.17131232405838087\n",
            "43     \t [ 0.96833679 -1.43310867]. \t  -19.71096083527366 \t -0.17131232405838087\n",
            "44     \t [2.73097745 0.159032  ]. \t  -17.365318669824873 \t -0.17131232405838087\n",
            "45     \t [ 1.68620787 -4.81732985]. \t  -4001.5024733636606 \t -0.17131232405838087\n",
            "46     \t [ 4.56340341 -3.89517175]. \t  -1342.0510182032006 \t -0.17131232405838087\n",
            "47     \t [ 6.95087163 -1.34872253]. \t  -57.36171980176296 \t -0.17131232405838087\n",
            "48     \t [3.06064026 0.96533015]. \t  -7.111452599539671 \t -0.17131232405838087\n",
            "49     \t [5.40625162 2.25062429]. \t  -64.05435489191615 \t -0.17131232405838087\n",
            "50     \t [ 2.6910362  -2.00336427]. \t  -59.803273498103515 \t -0.17131232405838087\n",
            "51     \t [ 2.65437388 -1.03915466]. \t  -3.226387490781242 \t -0.17131232405838087\n",
            "52     \t [1.50983142 0.67981972]. \t  -0.9455994537328836 \t -0.17131232405838087\n",
            "53     \t [-9.47411852 -4.2405916 ]. \t  -4239.1767153471865 \t -0.17131232405838087\n",
            "54     \t [ 1.99069356 -0.94383465]. \t  -1.0688740867019946 \t -0.17131232405838087\n",
            "55     \t [2.50544651 0.48761377]. \t  -10.507455765258028 \t -0.17131232405838087\n",
            "56     \t [ 1.00161329 -0.82404378]. \t  -0.2541628975264035 \t -0.17131232405838087\n",
            "57     \t [-8.51544016  6.40685072]. \t  -16511.218510710707 \t -0.17131232405838087\n",
            "58     \t [1.09198943 9.34927193]. \t  -60361.30298145525 \t -0.17131232405838087\n",
            "59     \t [-0.00824533 -0.69636553]. \t  -2.9298992157247876 \t -0.17131232405838087\n",
            "60     \t [ 1.50869737 -0.89793839]. \t  -0.28035899787126795 \t -0.17131232405838087\n",
            "61     \t [ 0.29722125 -0.49526031]. \t  -0.5686620116209077 \t -0.17131232405838087\n",
            "62     \t [-0.9925755  -0.25647382]. \t  -6.497707744326874 \t -0.17131232405838087\n",
            "63     \t [-6.16593836  8.44297235]. \t  -44294.66023866623 \t -0.17131232405838087\n",
            "64     \t [6.28399737 2.2195527 ]. \t  -53.39373818772728 \t -0.17131232405838087\n",
            "65     \t [0.03571064 0.19328363]. \t  -0.9328969867512059 \t -0.17131232405838087\n",
            "66     \t [ 5.80697974 -3.99152321]. \t  -1381.0973566289983 \t -0.17131232405838087\n",
            "67     \t [ 6.04685576 -1.54766819]. \t  -28.627343037256118 \t -0.17131232405838087\n",
            "68     \t [ 5.66563522 -1.74182199]. \t  -22.091766105963604 \t -0.17131232405838087\n",
            "69     \t [-6.18178689  6.26894534]. \t  -14427.260880981674 \t -0.17131232405838087\n",
            "70     \t [-9.40595051 -9.81454609]. \t  -81762.00729734289 \t -0.17131232405838087\n",
            "71     \t [5.25653166 1.21062772]. \t  -28.932034197586226 \t -0.17131232405838087\n",
            "72     \t [ 6.29892548 -2.78502241]. \t  -197.865878505557 \t -0.17131232405838087\n",
            "73     \t [-9.00271179 -9.1175408 ]. \t  -61533.45992394255 \t -0.17131232405838087\n",
            "74     \t [ 5.4594368  -2.01510023]. \t  -34.05715903431576 \t -0.17131232405838087\n",
            "75     \t [5.83770453 1.54845595]. \t  -25.576050633454113 \t -0.17131232405838087\n",
            "76     \t [8.15566945 8.31233999]. \t  -33869.05380235113 \t -0.17131232405838087\n",
            "77     \t [ 0.56975078 -9.60594027]. \t  -67696.30772250493 \t -0.17131232405838087\n",
            "78     \t [-4.30895206  9.99717508]. \t  -83420.17417554371 \t -0.17131232405838087\n",
            "79     \t [ 5.5699057  -1.23084053]. \t  -33.78692196686145 \t -0.17131232405838087\n",
            "80     \t [-0.05327668  1.84279366]. \t  -94.81891040193774 \t -0.17131232405838087\n",
            "81     \t [-0.58991652  7.26217391]. \t  -22503.477832359345 \t -0.17131232405838087\n",
            "82     \t [0.47543206 0.52621066]. \t  -0.28745312214182706 \t -0.17131232405838087\n",
            "83     \t [0.18833227 9.67605813]. \t  -69986.44152893897 \t -0.17131232405838087\n",
            "84     \t [-0.00681842  0.57581594]. \t  -1.9113397684800901 \t -0.17131232405838087\n",
            "85     \t [-0.55084098  0.31185653]. \t  -3.516200864428537 \t -0.17131232405838087\n",
            "86     \t [ 2.98452481 -2.57364942]. \t  -214.5891996912539 \t -0.17131232405838087\n",
            "87     \t [-0.5543762   4.56040096]. \t  -3555.4739534323253 \t -0.17131232405838087\n",
            "88     \t [0.70889644 0.70288721]. \t  -0.24065151125439843 \t -0.17131232405838087\n",
            "89     \t [-2.56543486 -7.77172208]. \t  -30450.44183915338 \t -0.17131232405838087\n",
            "90     \t [-9.23238755 -6.67352048]. \t  -19432.10910463239 \t -0.17131232405838087\n",
            "91     \t [-1.41444092 -3.81254297]. \t  -1864.549814117909 \t -0.17131232405838087\n",
            "92     \t [-5.58080905 -2.34532776]. \t  -593.2280959541445 \t -0.17131232405838087\n",
            "93     \t [-4.25079556 -3.48029923]. \t  -1649.3087842062514 \t -0.17131232405838087\n",
            "94     \t [0.79344463 0.46348984]. \t  -0.3073644911053172 \t -0.17131232405838087\n",
            "95     \t [-6.92277622 -1.48211181]. \t  -318.8780347701338 \t -0.17131232405838087\n",
            "96     \t [ 7.96616065 -5.40487512]. \t  -5140.787007532487 \t -0.17131232405838087\n",
            "97     \t [-2.85345816 -9.83264323]. \t  -77015.66153958983 \t -0.17131232405838087\n",
            "98     \t [-4.75559923  8.75778337]. \t  -50057.95735549469 \t -0.17131232405838087\n",
            "99     \t [ 9.77428042 -6.17916373]. \t  -8945.403788429308 \t -0.17131232405838087\n",
            "100    \t [-0.98210894  5.46957316]. \t  -7400.7526049408825 \t -0.17131232405838087\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YT-CgKvuv4q",
        "outputId": "2ed5e00f-f7f6-4b93-cd8d-db82b1d205f6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.18891153 5.22983981]. \t  -5727.433065247983 \t -79.36780179787098\n",
            "init   \t [-5.10936891  5.43378939]. \t  -8270.721382692287 \t -79.36780179787098\n",
            "init   \t [-2.91055837  5.93394799]. \t  -10771.054116716396 \t -79.36780179787098\n",
            "init   \t [-3.07999955 -1.12251129]. \t  -79.36780179787098 \t -79.36780179787098\n",
            "init   \t [-3.93392821 -1.1577216 ]. \t  -111.8486364354089 \t -79.36780179787098\n",
            "1      \t [ 1.46418724 -8.68671967]. \t  -44673.21769976243 \t -79.36780179787098\n",
            "2      \t [ 9.77192101 -1.83559751]. \t  -95.34580222090166 \t -79.36780179787098\n",
            "3      \t [-10.          -9.79648295]. \t  -81682.26916886112 \t -79.36780179787098\n",
            "4      \t [9.60900479 6.31645299]. \t  -9926.306792942905 \t -79.36780179787098\n",
            "5      \t [ 3.64747973 -1.21169324]. \t  \u001b[92m-8.020414806825936\u001b[0m \t -8.020414806825936\n",
            "6      \t [-9.89217977 -2.47027199]. \t  -1095.164983336398 \t -8.020414806825936\n",
            "7      \t [ 8.32006456 -7.65532824]. \t  -23766.792282452214 \t -8.020414806825936\n",
            "8      \t [-4.82061548 -6.19334743]. \t  -13330.026935776552 \t -8.020414806825936\n",
            "9      \t [6.4276201  1.66457678]. \t  -31.029010817303714 \t -8.020414806825936\n",
            "10     \t [-10.           2.83195117]. \t  -1477.1522527015732 \t -8.020414806825936\n",
            "11     \t [-9.75067738  9.5364486 ]. \t  -73566.11540805444 \t -8.020414806825936\n",
            "12     \t [ 0.31374223 -3.11648257]. \t  -730.9457313737804 \t -8.020414806825936\n",
            "13     \t [4.78533373 8.98851618]. \t  -49187.75733270886 \t -8.020414806825936\n",
            "14     \t [ 5.75202169 -3.49672209]. \t  -722.1194772732323 \t -8.020414806825936\n",
            "15     \t [1.7433783  0.73825717]. \t  \u001b[92m-1.406294065725803\u001b[0m \t -1.406294065725803\n",
            "16     \t [2.5723173  0.92539981]. \t  -3.9499637311384097 \t -1.406294065725803\n",
            "17     \t [9.70250737 1.09497334]. \t  -182.44724067822602 \t -1.406294065725803\n",
            "18     \t [3.28533039e-03 9.54204293e+00]. \t  -66320.270226604 \t -1.406294065725803\n",
            "19     \t [-0.36126124  0.92061534]. \t  -10.309988974240515 \t -1.406294065725803\n",
            "20     \t [ 2.14154828 -0.04705546]. \t  -10.43769496539368 \t -1.406294065725803\n",
            "21     \t [-6.88240882 -0.54616098]. \t  -174.00302479042244 \t -1.406294065725803\n",
            "22     \t [-2.6369292   1.56116529]. \t  -126.06961415853603 \t -1.406294065725803\n",
            "23     \t [3.71287406 1.85030981]. \t  -27.00884732113453 \t -1.406294065725803\n",
            "24     \t [4.77021047 4.16010428]. \t  -1795.3909309623725 \t -1.406294065725803\n",
            "25     \t [ 6.75011944 -0.63882227]. \t  -103.4869635145468 \t -1.406294065725803\n",
            "26     \t [-5.15925848  9.75131885]. \t  -76350.00555886983 \t -1.406294065725803\n",
            "27     \t [ 4.62212744 -0.25358159]. \t  -53.503254721084026 \t -1.406294065725803\n",
            "28     \t [3.23402764 1.41453187]. \t  -6.169830828603477 \t -1.406294065725803\n",
            "29     \t [0.71187799 1.6379167 ]. \t  -43.39619645864269 \t -1.406294065725803\n",
            "30     \t [-1.02357626  0.17889897]. \t  -6.460547273450576 \t -1.406294065725803\n",
            "31     \t [-6.44811119  1.82180537]. \t  -397.9643338459968 \t -1.406294065725803\n",
            "32     \t [-3.35943574 -9.9677076 ]. \t  -81683.43345912996 \t -1.406294065725803\n",
            "33     \t [ 3.03908229 -2.31977431]. \t  -123.46657395998766 \t -1.406294065725803\n",
            "34     \t [-9.11374135 -5.41180735]. \t  -9265.905169818247 \t -1.406294065725803\n",
            "35     \t [ 2.99903421 -0.31207157]. \t  -19.723850948687268 \t -1.406294065725803\n",
            "36     \t [2.8662484  1.52275885]. \t  -9.75817822447908 \t -1.406294065725803\n",
            "37     \t [ 3.28435675 -4.63882065]. \t  -3165.813498679299 \t -1.406294065725803\n",
            "38     \t [2.06983724 1.25795395]. \t  -3.542860264543693 \t -1.406294065725803\n",
            "39     \t [7.87131506 2.52055306]. \t  -93.97058957488775 \t -1.406294065725803\n",
            "40     \t [-0.18777685 -0.14018029]. \t  -1.5139425615306104 \t -1.406294065725803\n",
            "41     \t [-6.1345957  -2.52099597]. \t  -761.2034500023925 \t -1.406294065725803\n",
            "42     \t [ 9.16444022 -3.57509708]. \t  -604.4598798975373 \t -1.406294065725803\n",
            "43     \t [-0.55451056 -0.09011807]. \t  -3.0680212516129677 \t -1.406294065725803\n",
            "44     \t [ 1.43761199 -0.50218138]. \t  -1.9333770041252007 \t -1.406294065725803\n",
            "45     \t [ 0.94783492 -0.19671241]. \t  -1.5180645388613545 \t -1.406294065725803\n",
            "46     \t [ 2.34683144 -1.20059281]. \t  -2.388578573887076 \t -1.406294065725803\n",
            "47     \t [ 2.97060907 -1.53865546]. \t  -10.108895216890229 \t -1.406294065725803\n",
            "48     \t [ 0.64782613 -0.20896758]. \t  \u001b[92m-0.7523272766214226\u001b[0m \t -0.7523272766214226\n",
            "49     \t [ 1.88448705 -0.66680503]. \t  -2.7632794573893857 \t -0.7523272766214226\n",
            "50     \t [1.92865344 0.65127372]. \t  -3.1966598574705167 \t -0.7523272766214226\n",
            "51     \t [-0.12944544  0.09968661]. \t  -1.320240095762612 \t -0.7523272766214226\n",
            "52     \t [ 2.43621186 -1.64059603]. \t  -19.43112998971544 \t -0.7523272766214226\n",
            "53     \t [-0.68150638  0.42620827]. \t  -5.010733684393655 \t -0.7523272766214226\n",
            "54     \t [-1.87572729 -0.24064312]. \t  -16.20231456787598 \t -0.7523272766214226\n",
            "55     \t [-0.90828624 -8.44749568]. \t  -41262.02132907984 \t -0.7523272766214226\n",
            "56     \t [-2.60848898 -2.46389377]. \t  -448.1481996483268 \t -0.7523272766214226\n",
            "57     \t [-3.72646574  0.12794961]. \t  -50.6027670929749 \t -0.7523272766214226\n",
            "58     \t [-1.53378448  4.44504863]. \t  -3376.7398542969686 \t -0.7523272766214226\n",
            "59     \t [9.53325184 9.96976773]. \t  -71710.9705251735 \t -0.7523272766214226\n",
            "60     \t [ 0.97114512 -0.77537001]. \t  \u001b[92m-0.10778773764323518\u001b[0m \t -0.10778773764323518\n",
            "61     \t [0.95205653 0.65289855]. \t  \u001b[92m-0.02210046413617886\u001b[0m \t -0.02210046413617886\n",
            "62     \t [ 1.58479393 -0.8776802 ]. \t  -0.345882183880509 \t -0.02210046413617886\n",
            "63     \t [-5.26775709  4.094441  ]. \t  -3049.6450899565607 \t -0.02210046413617886\n",
            "64     \t [1.17571297 0.88564968]. \t  -0.3398323941865823 \t -0.02210046413617886\n",
            "65     \t [ 3.92801181 -1.52061158]. \t  -9.543498169256162 \t -0.02210046413617886\n",
            "66     \t [-0.12631791 -8.28057783]. \t  -37683.12253195882 \t -0.02210046413617886\n",
            "67     \t [ 8.99129795 -2.75625089]. \t  -140.80384847932044 \t -0.02210046413617886\n",
            "68     \t [1.419442   4.07963932]. \t  -2031.2474747312667 \t -0.02210046413617886\n",
            "69     \t [ 7.94287408 -1.37985042]. \t  -82.39829188515836 \t -0.02210046413617886\n",
            "70     \t [1.14660951 2.80941247]. \t  -428.62139830822866 \t -0.02210046413617886\n",
            "71     \t [4.23866479 1.02746509]. \t  -19.53972420063978 \t -0.02210046413617886\n",
            "72     \t [0.54933937 0.88579328]. \t  -2.2835690711152505 \t -0.02210046413617886\n",
            "73     \t [-2.05073054 -8.33771244]. \t  -39819.615905485785 \t -0.02210046413617886\n",
            "74     \t [ 8.4179015  -2.46283236]. \t  -82.60074836574749 \t -0.02210046413617886\n",
            "75     \t [ 7.55133276 -4.59334745]. \t  -2443.658936169019 \t -0.02210046413617886\n",
            "76     \t [5.95833852 5.32796835]. \t  -5189.148320446915 \t -0.02210046413617886\n",
            "77     \t [2.13216871 1.07422308]. \t  -1.3435762957709059 \t -0.02210046413617886\n",
            "78     \t [-1.24901293 -9.33659734]. \t  -61670.94055074521 \t -0.02210046413617886\n",
            "79     \t [-4.9410944  6.2398932]. \t  -13751.490771490682 \t -0.02210046413617886\n",
            "80     \t [-0.23602593  2.10356679]. \t  -166.63900281264824 \t -0.02210046413617886\n",
            "81     \t [ 0.60136566 -0.52495285]. \t  -0.16395236196072657 \t -0.02210046413617886\n",
            "82     \t [ 7.79848131 -4.21075841]. \t  -1576.6462240394658 \t -0.02210046413617886\n",
            "83     \t [-4.80892289  3.33160919]. \t  -1492.625271016946 \t -0.02210046413617886\n",
            "84     \t [-3.8223155   1.43023961]. \t  -148.5012543771113 \t -0.02210046413617886\n",
            "85     \t [-1.1897494  9.006471 ]. \t  -53418.81098000781 \t -0.02210046413617886\n",
            "86     \t [ 7.80204511 -0.46275552]. \t  -155.01248395107325 \t -0.02210046413617886\n",
            "87     \t [-4.4992704  -8.03524031]. \t  -35743.89855337572 \t -0.02210046413617886\n",
            "88     \t [-0.68370568 -6.59381483]. \t  -15364.547152679263 \t -0.02210046413617886\n",
            "89     \t [-2.79604495  4.4167307 ]. \t  -3510.74028683285 \t -0.02210046413617886\n",
            "90     \t [-2.96166805 -0.6366777 ]. \t  -44.15659149056799 \t -0.02210046413617886\n",
            "91     \t [0.08908324 8.48866671]. \t  -41487.71606296508 \t -0.02210046413617886\n",
            "92     \t [8.88254343 7.4035404 ]. \t  -20360.147479058807 \t -0.02210046413617886\n",
            "93     \t [ 4.31696136 -1.52430922]. \t  -11.22013278578763 \t -0.02210046413617886\n",
            "94     \t [4.33586629 0.94515392]. \t  -24.125196097896414 \t -0.02210046413617886\n",
            "95     \t [-4.56624156 -3.7311091 ]. \t  -2131.6166846930196 \t -0.02210046413617886\n",
            "96     \t [-4.2956356  -9.65005052]. \t  -72640.99768931672 \t -0.02210046413617886\n",
            "97     \t [5.96686349 3.20976704]. \t  -453.2320428473832 \t -0.02210046413617886\n",
            "98     \t [5.8679549  2.10242346]. \t  -41.36747332788051 \t -0.02210046413617886\n",
            "99     \t [1.245858   0.67491496]. \t  -0.2846785747141742 \t -0.02210046413617886\n",
            "100    \t [0.73771969 0.56174033]. \t  -0.09152460632348647 \t -0.02210046413617886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHz_Jg2_uv7E",
        "outputId": "7a034978-f589-409b-d476-9a2749c4b648"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.41034571 1.2905751 ]. \t  -7.547501684041513 \t -7.547501684041513\n",
            "init   \t [-0.23116335 -3.27044493]. \t  -936.6052904493877 \t -7.547501684041513\n",
            "init   \t [-2.48263644  0.64071738]. \t  -33.95727983513621 \t -7.547501684041513\n",
            "init   \t [-8.63787423  1.6905812 ]. \t  -504.96347244129606 \t -7.547501684041513\n",
            "init   \t [-5.24204473 -6.78486837]. \t  -18977.794322697024 \t -7.547501684041513\n",
            "1      \t [ 9.84240059 -0.83299926]. \t  -221.1494177066687 \t -7.547501684041513\n",
            "2      \t [6.73867561 8.36273465]. \t  -35481.183803304404 \t -7.547501684041513\n",
            "3      \t [ 8.38641873 -9.74808405]. \t  -66058.08004040587 \t -7.547501684041513\n",
            "4      \t [-3.83701174  9.3377774 ]. \t  -63551.82998472117 \t -7.547501684041513\n",
            "5      \t [ 1.32929548 -9.72487118]. \t  -70550.51704546863 \t -7.547501684041513\n",
            "6      \t [1.04276396 5.73045913]. \t  -8354.995892790164 \t -7.547501684041513\n",
            "7      \t [-10.           7.24711838]. \t  -26590.070325557634 \t -7.547501684041513\n",
            "8      \t [ 5.43871963 -4.06023599]. \t  -1535.7587534305121 \t -7.547501684041513\n",
            "9      \t [-10.         -3.4146828]. \t  -2341.4604261799846 \t -7.547501684041513\n",
            "10     \t [-10. -10.]. \t  -88321.0 \t -7.547501684041513\n",
            "11     \t [5.74549263 2.18212437]. \t  -51.06386417804457 \t -7.547501684041513\n",
            "12     \t [-5.00114921 -1.90061608]. \t  -334.95573808338173 \t -7.547501684041513\n",
            "13     \t [-4.95486104  3.24959128]. \t  -1395.2244714386204 \t -7.547501684041513\n",
            "14     \t [9.25668229 3.88152593]. \t  -939.7712605560133 \t -7.547501684041513\n",
            "15     \t [ 9.57479367 -4.59830407]. \t  -2213.9396015619786 \t -7.547501684041513\n",
            "16     \t [ 2.96049307 -0.0961983 ]. \t  -21.15408277275224 \t -7.547501684041513\n",
            "17     \t [2.69958708 1.65553965]. \t  -18.368044578264453 \t -7.547501684041513\n",
            "18     \t [ 5.51440894 -0.4957413 ]. \t  -70.83873744507414 \t -7.547501684041513\n",
            "19     \t [0.06457665 1.36208347]. \t  -27.46115421192012 \t -7.547501684041513\n",
            "20     \t [4.0392523  2.83083586]. \t  -296.66187148281966 \t -7.547501684041513\n",
            "21     \t [1.8316052  1.38258697]. \t  -8.623617996548322 \t -7.547501684041513\n",
            "22     \t [6.84782578 1.4798667 ]. \t  -46.37728670161587 \t -7.547501684041513\n",
            "23     \t [0.90849575 0.39606411]. \t  \u001b[92m-0.7158571587203243\u001b[0m \t -0.7158571587203243\n",
            "24     \t [-1.31223843  2.41516325]. \t  -342.21719558269024 \t -0.7158571587203243\n",
            "25     \t [2.89502442 9.87660257]. \t  -73884.91156089552 \t -0.7158571587203243\n",
            "26     \t [0.25445499 0.5103737 ]. \t  \u001b[92m-0.6978900069038445\u001b[0m \t -0.6978900069038445\n",
            "27     \t [2.60579908 1.06351778]. \t  -2.814793636965719 \t -0.6978900069038445\n",
            "28     \t [ 2.8953706  -1.06566316]. \t  -4.3714179900303245 \t -0.6978900069038445\n",
            "29     \t [-3.34092010e+00  2.92929369e-03]. \t  -41.16731091909771 \t -0.6978900069038445\n",
            "30     \t [ 3.08931664 -1.91182023]. \t  -39.99549085027981 \t -0.6978900069038445\n",
            "31     \t [0.63051857 0.56483509]. \t  \u001b[92m-0.13663079379920456\u001b[0m \t -0.13663079379920456\n",
            "32     \t [-0.70684771 -0.27336467]. \t  -4.379842823377958 \t -0.13663079379920456\n",
            "33     \t [ 1.70809185 -0.77666527]. \t  -1.004747636985769 \t -0.13663079379920456\n",
            "34     \t [-0.46424221  0.38160145]. \t  -3.2855099476439045 \t -0.13663079379920456\n",
            "35     \t [-1.68516898 -0.50785562]. \t  -16.89896653218432 \t -0.13663079379920456\n",
            "36     \t [1.48680023 0.21529361]. \t  -4.123990403071713 \t -0.13663079379920456\n",
            "37     \t [-0.42850407  0.04744345]. \t  -2.415611996345086 \t -0.13663079379920456\n",
            "38     \t [ 1.8898908  -5.59449624]. \t  -7371.443809904347 \t -0.13663079379920456\n",
            "39     \t [-0.70606437  6.57817583]. \t  -15228.3354045664 \t -0.13663079379920456\n",
            "40     \t [ 1.45330605 -0.03339622]. \t  -4.416726200215836 \t -0.13663079379920456\n",
            "41     \t [-6.96274315 -0.73826311]. \t  -193.10071179544084 \t -0.13663079379920456\n",
            "42     \t [ 2.36888936 -1.51516148]. \t  -11.75321982855257 \t -0.13663079379920456\n",
            "43     \t [ 1.9609774  -0.61766638]. \t  -3.7936645334384043 \t -0.13663079379920456\n",
            "44     \t [-0.78664609  0.09319311]. \t  -4.4849877300655745 \t -0.13663079379920456\n",
            "45     \t [2.34202676 6.18017505]. \t  -10967.747204003006 \t -0.13663079379920456\n",
            "46     \t [0.67147412 0.81208247]. \t  -0.9463944583765673 \t -0.13663079379920456\n",
            "47     \t [ 7.76653418 -1.27814123]. \t  -86.27238085454454 \t -0.13663079379920456\n",
            "48     \t [0.98253539 2.01151484]. \t  -101.10019762574758 \t -0.13663079379920456\n",
            "49     \t [0.9985624  0.61163087]. \t  \u001b[92m-0.12538011673851154\u001b[0m \t -0.12538011673851154\n",
            "50     \t [-4.42871958 -2.82461813]. \t  -860.6208323518431 \t -0.12538011673851154\n",
            "51     \t [2.91791298 5.48479353]. \t  -6558.3470416845375 \t -0.12538011673851154\n",
            "52     \t [ 3.2700743  -1.09817136]. \t  -6.625955282447199 \t -0.12538011673851154\n",
            "53     \t [6.5622132  3.57487226]. \t  -752.726197458964 \t -0.12538011673851154\n",
            "54     \t [-4.87334423  0.15297164]. \t  -82.91182349844144 \t -0.12538011673851154\n",
            "55     \t [5.06393757 1.49338885]. \t  -17.24405414387582 \t -0.12538011673851154\n",
            "56     \t [ 0.175016   -0.44304169]. \t  -0.7752597263033065 \t -0.12538011673851154\n",
            "57     \t [-1.70044178  7.3703767 ]. \t  -24359.483718936048 \t -0.12538011673851154\n",
            "58     \t [-8.13851557  3.27121202]. \t  -1828.7559536916694 \t -0.12538011673851154\n",
            "59     \t [1.13001729 0.86621904]. \t  -0.2916726196116057 \t -0.12538011673851154\n",
            "60     \t [ 0.98001672 -0.49114353]. \t  -0.49555669428749133 \t -0.12538011673851154\n",
            "61     \t [6.05168537 1.13391243]. \t  -49.74269976139431 \t -0.12538011673851154\n",
            "62     \t [-4.58992143  9.98676407]. \t  -83312.89560596988 \t -0.12538011673851154\n",
            "63     \t [ 5.18131272 -7.17227456]. \t  -19108.740228590796 \t -0.12538011673851154\n",
            "64     \t [2.16671978 5.48761587]. \t  -6743.552269408412 \t -0.12538011673851154\n",
            "65     \t [ 0.95003231 -0.60550008]. \t  \u001b[92m-0.09647664536127974\u001b[0m \t -0.09647664536127974\n",
            "66     \t [2.96980569 1.03041568]. \t  -5.312557232212405 \t -0.09647664536127974\n",
            "67     \t [2.32133094 3.46050155]. \t  -937.3568522973343 \t -0.09647664536127974\n",
            "68     \t [5.25751872 2.0780996 ]. \t  -40.9681974444421 \t -0.09647664536127974\n",
            "69     \t [3.67160354 1.14138453]. \t  -9.410545354710493 \t -0.09647664536127974\n",
            "70     \t [2.38003787 1.17405209]. \t  -2.1883988255586853 \t -0.09647664536127974\n",
            "71     \t [3.32908705 1.35552103]. \t  -5.663784436117526 \t -0.09647664536127974\n",
            "72     \t [ 1.49591564 -1.14954277]. \t  -2.8770654208660518 \t -0.09647664536127974\n",
            "73     \t [-3.07061282 -5.73791022]. \t  -9515.907317149677 \t -0.09647664536127974\n",
            "74     \t [-2.7481949  -9.66548518]. \t  -71903.848684301 \t -0.09647664536127974\n",
            "75     \t [1.8838812  8.48582528]. \t  -40405.25904897171 \t -0.09647664536127974\n",
            "76     \t [-9.62055167  0.30011319]. \t  -304.9030695816312 \t -0.09647664536127974\n",
            "77     \t [ 5.63854004 -4.65830064]. \t  -2873.3007456023993 \t -0.09647664536127974\n",
            "78     \t [-2.49276664  8.22484432]. \t  -37983.75841842815 \t -0.09647664536127974\n",
            "79     \t [ 5.04615731 -2.3000861 ]. \t  -77.63575384876303 \t -0.09647664536127974\n",
            "80     \t [ 1.75936005 -0.04383179]. \t  -6.740311750250135 \t -0.09647664536127974\n",
            "81     \t [1.07297526 0.6995916 ]. \t  \u001b[92m-0.023041952644338656\u001b[0m \t -0.023041952644338656\n",
            "82     \t [-4.05024547  7.19737999]. \t  -23204.618124812292 \t -0.023041952644338656\n",
            "83     \t [ 0.50532017 -6.05037639]. \t  -10573.380617831723 \t -0.023041952644338656\n",
            "84     \t [8.71106375 2.33137669]. \t  -68.78799614598407 \t -0.023041952644338656\n",
            "85     \t [7.90609044 1.67631844]. \t  -58.14570868091185 \t -0.023041952644338656\n",
            "86     \t [-4.17190662 -7.01645764]. \t  -21093.9217020648 \t -0.023041952644338656\n",
            "87     \t [9.79185003 1.30121326]. \t  -159.3584646484842 \t -0.023041952644338656\n",
            "88     \t [-6.98147209  5.98337434]. \t  -12414.282514773455 \t -0.023041952644338656\n",
            "89     \t [4.43339058 0.35737805]. \t  -46.69874398855427 \t -0.023041952644338656\n",
            "90     \t [-7.15846338  8.10771801]. \t  -38502.36355553295 \t -0.023041952644338656\n",
            "91     \t [-0.32014405  3.15326937]. \t  -818.3367131916744 \t -0.023041952644338656\n",
            "92     \t [7.8119471  4.05705686]. \t  -1307.1730922817023 \t -0.023041952644338656\n",
            "93     \t [-0.9094418   3.29480777]. \t  -1027.0614777163194 \t -0.023041952644338656\n",
            "94     \t [ 0.40407557 -0.33970468]. \t  -0.41517578720733866 \t -0.023041952644338656\n",
            "95     \t [-1.3666742   1.16084433]. \t  -38.597476446321835 \t -0.023041952644338656\n",
            "96     \t [7.6995205  9.17775011]. \t  -51734.15813973444 \t -0.023041952644338656\n",
            "97     \t [ 2.27165594 -1.0946704 ]. \t  -1.6483341380192558 \t -0.023041952644338656\n",
            "98     \t [-2.41675697  1.53574705]. \t  -113.4562888412338 \t -0.023041952644338656\n",
            "99     \t [-2.35749728 -0.22135412]. \t  -23.33167556035581 \t -0.023041952644338656\n",
            "100    \t [-3.51159788  9.08679019]. \t  -56906.74807362155 \t -0.023041952644338656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUnhsKpCuv9o",
        "outputId": "ae41971d-94e6-44fc-d507-942af6f54b26"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2072.3098866939545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJG0SLpwuwAL",
        "outputId": "96f85e2d-bb74-49e6-b17e-4c6d481a98c7"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.0859967871325082, -3.0859967871325082)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lA9eZf0uwCx",
        "outputId": "6424af0f-a50a-4b2b-bbbd-fede201f5d35"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.6533479564675457, -3.727986180582751)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glrTGcpAuwFa",
        "outputId": "4ee56174-445b-47d4-ebad-93478b244548"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.3511310417680322, -2.3511310417680322)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaRwfbxeuwIR",
        "outputId": "4fb97eb2-c956-426c-e3fb-27aa4ed165cd"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.431103172818014, -2.3433607562962435)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nb5NkfyuwKp",
        "outputId": "34f384a3-db80-4fec-f205-8fbfe7634e33"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.299753758108693, -3.1070794338455276)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Q-WfXbuwNg",
        "outputId": "d4722264-b90b-454f-e7ea-cfad3b7bd564"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.950268075483134, -4.950268075483134)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqS7VLcuwPy",
        "outputId": "23e56dcb-f8fb-4933-9388-bb4fd42b7969"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.10059458972191, -4.869238516356011)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOi5iX8guwSS",
        "outputId": "b9d30030-93c8-42bf-d663-6859ba38d094"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.9928903937541453, -3.9928903937541453)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVApeazuwU5",
        "outputId": "33f812b1-173f-4e91-e4ff-67f942014980"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.7665131834185988, -3.673835207001593)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g92jk9WJuwXb",
        "outputId": "c2061052-e398-48b9-b971-32669ec99b84"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.396963659577809, -7.724081906157097)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmcF1x-NuwZz",
        "outputId": "c52c8bd6-045b-44c8-847c-8c3324fa1ce9"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.290222586116367, -4.680312786013162)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8axVhb6uwcc",
        "outputId": "f1a89b37-0bea-45e8-91ca-4cd2b85bae79"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.520046221038326, -1.7071248260214649)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzlrL8XFuwfB",
        "outputId": "739fd0eb-5b6e-4955-80d8-f47aa780cd0f"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.9098895187956706, -4.205559697994266)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZlLJ1quwh6",
        "outputId": "a3a246fd-cb98-494d-930b-a0dded229357"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.3026113410376774, -3.090293360199478)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVBcHRiMuwjx",
        "outputId": "37784a2d-8de8-43e5-bf37-0b0a00d1e58f"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.320517307417607, -3.0940554534660842)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8nZ_DrKuwnA",
        "outputId": "8452844d-db6a-4164-cea6-2fa7f3f940d3"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.5921553663694261, -2.3142896791659053)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg6qzzQFuwpU",
        "outputId": "86cd90b5-6870-4e86-9990-50fc457e0c9e"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.9290553347368222, -3.178045921541106)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrRtDLMFuwsB",
        "outputId": "7134c9d4-73f9-4c19-a6a5-29cd4eb85fb3"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.734915867255176, -1.7642669319411506)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkmSu4CUuwuh",
        "outputId": "fd59f5b0-13c3-44fe-b298-5ce78ac1fcde"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.411494127888613, -3.812156669041909)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eymecjwkuwxb",
        "outputId": "14fd2d29-0f91-4586-9b9f-1df6667bb3ff"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.351734840993339, -3.770438696557231)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "84XIzmWD2oba",
        "outputId": "093cc60f-0505-41ac-c552-45d6ddf3a8fc"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Red')\r\n",
        "plt.plot(median_winner, color = 'Yellow')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Yellow', alpha=0.4, label='GP EI Regret IQR: L-BFGS-B')\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP EI Regret IQR: Newton-CG with GP d$^{2}$EI')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show() "
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5dkR2Id2oiu",
        "outputId": "9c3f64f5-ca46-4dbe-80dd-e813565d7e06"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3239.482697248459, 2072.3098866939545)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    }
  ]
}