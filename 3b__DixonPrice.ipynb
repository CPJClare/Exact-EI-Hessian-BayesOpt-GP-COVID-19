{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2b__DixonPrice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Dixon-Price synthetic function:\r\n",
        "\r\n",
        "GP EI: Newton-CG (exact GP EI gradients + exact GP EI Hessian) vs. L-BFGS-B (exact GP EI gradients, without Hessian)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/dixonpr.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "d9ab89f0-e661-452d-a0d0-c7e89205fcd2"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyGPGO in /usr/local/lib/python3.6/dist-packages (0.4.0.dev1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "\r\n",
        "rc('text', usetex=False)\r\n",
        "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\r\n",
        "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "obj_func = 'DixonPrice'\r\n",
        "n_test = 100 # test points\r\n",
        "df = 3 # nu\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dEI_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'DixonPrice': # 2-D\r\n",
        "            \r\n",
        "    def f_syn_polarity(x1_training, x2_training):\r\n",
        "        return  operator * ((x1_training - 1)**2\r\n",
        "                            + 2 * (2 * x2_training ** 2 - x1_training)**2            \r\n",
        "                           )\r\n",
        "\r\n",
        "# Constraints:\r\n",
        "    lb = -10 \r\n",
        "    ub = +10\r\n",
        "    \r\n",
        "# Input array dimension(s):\r\n",
        "    dim = 2\r\n",
        "    max_iter = 100\r\n",
        "\r\n",
        "# 2-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub]),\r\n",
        "             }\r\n",
        "\r\n",
        "    \r\n",
        "# True y bounds:\r\n",
        "    y_lb = 0.00000\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "\r\n",
        "# Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, max_iter) \r\n",
        "    x2_test = np.linspace(lb, ub, max_iter)\r\n",
        "    Xstar_d = np.column_stack((x1_test,x2_test))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 99\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    \r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "        \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r**2-1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "### Acquisition function derivatives:\r\n",
        "\r\n",
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-06, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'EI_GP': self.EI_GP,\r\n",
        "            'dEI_GP': self.dEI_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def EI_GP(self, tau, mean, std):\r\n",
        "        \r\n",
        "        z = (mean - tau - self.eps) / (std + self.eps)\r\n",
        "        return z * (std + self.eps) * norm.cdf(z) + (std + self.eps) * norm.pdf(z)[0]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2s, d2m):\r\n",
        "    \r\n",
        "        z = (mean - tau - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - (dvdv + d2s) / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m -(z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (z * norm.cdf(z) + norm.pdf(z)[0]) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx\r\n",
        "        d2f = (z * norm.cdf(z) + norm.pdf(z)[0]) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2s, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2s, d2m, **self.params)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    sigmaf = 1\r\n",
        "    l = 1\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -np.dot(dv.T, v)\r\n",
        "        d2s = np.dot(d2v.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, self.alpha)\r\n",
        "        d2m = np.dot(d2Kstar, self.alpha)\r\n",
        "        return ds, dm, dvdv, d2s, d2m\r\n",
        "    \r\n",
        "class dtStudentProcess(tStudentProcess):\r\n",
        "    sigmaf = 1\r\n",
        "    l = 1\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        \r\n",
        "        L11 = cholesky(self.K11).T\r\n",
        "        v11 = solve(L11, Kstar.T)\r\n",
        "        dv11 = solve(L11, dKstar.T)\r\n",
        "        d2v11 = solve(L11, d2Kstar.T)\r\n",
        "        alpha11 = solve(L11.T, solve(L11, self.y))\r\n",
        "        \r\n",
        "        ds = -np.dot(dv11.T, v11)\r\n",
        "        d2s = -np.dot(d2v11.T, v11)\r\n",
        "        dvdv = np.dot(dv11.T, dv11)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha11)\r\n",
        "        d2m = np.dot(d2Kstar, self.alpha)\r\n",
        "        return ds, dm"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "### dGPGO - BayesOpt class: \r\n",
        "\r\n",
        "class dGPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: \r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "    p = np.full((n_start,1),1)\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "    \r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 hessp = self.hessp_nonzero,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]     \r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "cfddee4f-c3cd-4375-e0bb-9389f2f5ce61"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1609759137.176067"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "f09209f9-a361-4073-91fb-fd44c937a541"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_1 = d2GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.24340351 -6.61860491]. \t  -14577.093458365041 \t -540.0726783817402\n",
            "init   \t [-1.27881961  5.38524945]. \t  -7033.582264098692 \t -540.0726783817402\n",
            "init   \t [-4.09349391 -7.01674086]. \t  -21064.19892235138 \t -540.0726783817402\n",
            "init   \t [-9.55043351 -1.59551015]. \t  -540.0726783817402 \t -540.0726783817402\n",
            "init   \t [-5.22635718 -3.24687616]. \t  -1423.2804229542423 \t -540.0726783817402\n",
            "1      \t [8.37416725 6.27688756]. \t  -9973.589615047737 \t -540.0726783817402\n",
            "2      \t [-9.76713775  9.96269922]. \t  -86875.28945668103 \t -540.0726783817402\n",
            "3      \t [ 9.98926049 -9.3463478 ]. \t  -54345.61921377557 \t -540.0726783817402\n",
            "4      \t [ 9.44490266 -0.94685252]. \t  \u001b[92m-188.41779181630653\u001b[0m \t -188.41779181630653\n",
            "5      \t [ -5.71524748 -13.96461029]. \t  -313258.9453086423 \t -188.41779181630653\n",
            "6      \t [-10.10913729 -20.06167194]. \t  -1328738.0445942972 \t -188.41779181630653\n",
            "7      \t [-12.38891977  -7.55737348]. \t  -32242.84449462435 \t -188.41779181630653\n",
            "8      \t [3.45685175 0.29515637]. \t  \u001b[92m-27.587271653981432\u001b[0m \t -27.587271653981432\n",
            "9      \t [-7.29911972  3.709041  ]. \t  -2492.777864737907 \t -27.587271653981432\n",
            "10     \t [  1.37566486 -13.1409446 ]. \t  -236662.7642007916 \t -27.587271653981432\n",
            "11     \t [-71.39705254 -75.51792945]. \t  -263462640.68493322 \t -27.587271653981432\n",
            "12     \t [-0.62072552 -0.79850955]. \t  \u001b[92m-9.81608378188993\u001b[0m \t -9.81608378188993\n",
            "13     \t [3.37484376 8.90502033]. \t  -48194.5769897518 \t -9.81608378188993\n",
            "14     \t [ 6.92233961 -4.46391467]. \t  -2203.9398923755048 \t -9.81608378188993\n",
            "15     \t [4.07770053 3.69301877]. \t  -1085.8663725931776 \t -9.81608378188993\n",
            "16     \t [-2.97259508  0.66277349]. \t  -45.443954498313985 \t -9.81608378188993\n",
            "17     \t [-35.39307034 -36.68927567]. \t  -14880916.722261518 \t -9.81608378188993\n",
            "18     \t [-0.9062252  -3.11263535]. \t  -826.4519509478366 \t -9.81608378188993\n",
            "19     \t [0.56454101 1.24982823]. \t  -13.292730763065736 \t -9.81608378188993\n",
            "20     \t [ 0.97902319 -0.15668085]. \t  \u001b[92m-1.7299625847814593\u001b[0m \t -1.7299625847814593\n",
            "21     \t [-3.93330011  8.95461175]. \t  -54015.580312945465 \t -1.7299625847814593\n",
            "22     \t [ 2.10667478 -1.40697019]. \t  -8.087911610910195 \t -1.7299625847814593\n",
            "23     \t [ 0.47070545 -0.19685938]. \t  \u001b[92m-0.5893623930947876\u001b[0m \t -0.5893623930947876\n",
            "24     \t [-0.24582787  0.26776761]. \t  -1.8550821588236268 \t -0.5893623930947876\n",
            "25     \t [6.99377468 1.23487141]. \t  -67.0349740057245 \t -0.5893623930947876\n",
            "26     \t [ 4.22932274 -1.4679427 ]. \t  -10.44145007047248 \t -0.5893623930947876\n",
            "27     \t [-0.8115279  -0.25321924]. \t  -5.047960650296771 \t -0.5893623930947876\n",
            "28     \t [-8.51833569 -4.8138452 ]. \t  -6110.83572388887 \t -0.5893623930947876\n",
            "29     \t [-6.71666219 -0.28074889]. \t  -154.05893299924375 \t -0.5893623930947876\n",
            "30     \t [-0.9954933  1.4494201]. \t  -58.00232597608417 \t -0.5893623930947876\n",
            "31     \t [9.67797817 1.64318147]. \t  -111.9079476716168 \t -0.5893623930947876\n",
            "32     \t [-0.45940609 -0.22987155]. \t  -2.7685149135087403 \t -0.5893623930947876\n",
            "33     \t [ 4.11952878 -2.21211234]. \t  -73.96924555251553 \t -0.5893623930947876\n",
            "34     \t [ 5.31832938 -0.89814586]. \t  -46.101980164629545 \t -0.5893623930947876\n",
            "35     \t [-8.07337685 -9.88184673]. \t  -82805.24270582151 \t -0.5893623930947876\n",
            "36     \t [ 3.55953023 -0.39807718]. \t  -27.580096161751513 \t -0.5893623930947876\n",
            "37     \t [1.74675114 1.18253548]. \t  -2.7627598092038275 \t -0.5893623930947876\n",
            "38     \t [-9.8545825   1.34674201]. \t  -481.3511805040524 \t -0.5893623930947876\n",
            "39     \t [ 5.70091754 -8.14733325]. \t  -32309.142731195105 \t -0.5893623930947876\n",
            "40     \t [0.74961508 0.18998972]. \t  -0.9804961638965406 \t -0.5893623930947876\n",
            "41     \t [109.12141042 108.49864366]. \t  -1098390449.5975165 \t -0.5893623930947876\n",
            "42     \t [-4.88905439  1.65517437]. \t  -249.68254157856282 \t -0.5893623930947876\n",
            "43     \t [ 1.42646113 -1.55525242]. \t  -23.453880797636348 \t -0.5893623930947876\n",
            "44     \t [1.54194991 0.37716152]. \t  -3.4560620365163213 \t -0.5893623930947876\n",
            "45     \t [ 4.04534893 -0.93426908]. \t  -19.850760381658137 \t -0.5893623930947876\n",
            "46     \t [-3.30910877 -0.66231956]. \t  -53.62103126413878 \t -0.5893623930947876\n",
            "47     \t [ 2.75282019 -0.86907665]. \t  -6.158658066190721 \t -0.5893623930947876\n",
            "48     \t [-1.29426966 -0.67473396]. \t  -14.98597242488901 \t -0.5893623930947876\n",
            "49     \t [ 1.96596964 -0.56138311]. \t  -4.501113499485708 \t -0.5893623930947876\n",
            "50     \t [2.63992793 0.18523005]. \t  -15.912608980644505 \t -0.5893623930947876\n",
            "51     \t [0.10739441 0.27421282]. \t  -0.8004411893953339 \t -0.5893623930947876\n",
            "52     \t [9.09015916 9.62156367]. \t  -62058.86386351431 \t -0.5893623930947876\n",
            "53     \t [1.04414074 0.77201658]. \t  \u001b[92m-0.04568447350163922\u001b[0m \t -0.04568447350163922\n",
            "54     \t [ 2.05926448 -6.1821805 ]. \t  -11065.72629953515 \t -0.04568447350163922\n",
            "55     \t [ 2.60426264 -3.59796671]. \t  -1087.0926778505834 \t -0.04568447350163922\n",
            "56     \t [ 0.90438136 -0.6183934 ]. \t  -0.04809723355965922 \t -0.04568447350163922\n",
            "57     \t [1.35287358 0.98018969]. \t  -0.7712910919359555 \t -0.04568447350163922\n",
            "58     \t [ 9.89374551 -4.55284232]. \t  -2071.544755196085 \t -0.04568447350163922\n",
            "59     \t [1.87578473 0.8398646 ]. \t  -1.1995226063309452 \t -0.04568447350163922\n",
            "60     \t [4.99270134 8.41236709]. \t  -37303.99922392121 \t -0.04568447350163922\n",
            "61     \t [ 1.33361978 -0.75382376]. \t  -0.18901415121159465 \t -0.04568447350163922\n",
            "62     \t [ 7.17256087 -0.9033448 ]. \t  -99.49472631670415 \t -0.04568447350163922\n",
            "63     \t [-5.51046832  6.32629507]. \t  -14681.526646814731 \t -0.04568447350163922\n",
            "64     \t [ 2.17849603 -0.87463367]. \t  -2.230029811969959 \t -0.04568447350163922\n",
            "65     \t [-1.05056018  9.72447146]. \t  -72342.02090492527 \t -0.04568447350163922\n",
            "66     \t [-0.28079526 -1.18962417]. \t  -20.999649416284967 \t -0.04568447350163922\n",
            "67     \t [-1.80628276 -0.28973827]. \t  -15.669990723368109 \t -0.04568447350163922\n",
            "68     \t [-9.52462087 -4.31583999]. \t  -4487.0498646714095 \t -0.04568447350163922\n",
            "69     \t [ 0.66852413 -0.85686487]. \t  -1.38959045539405 \t -0.04568447350163922\n",
            "70     \t [ 0.48527097 -0.77956298]. \t  -1.3312304645847437 \t -0.04568447350163922\n",
            "71     \t [6.22407215 1.35304227]. \t  -40.42502717286698 \t -0.04568447350163922\n",
            "72     \t [4.81479802 0.54934477]. \t  -50.02174628725366 \t -0.04568447350163922\n",
            "73     \t [2.37932605 8.30118572]. \t  -36689.91287944836 \t -0.04568447350163922\n",
            "74     \t [-1.34219568 -9.43450538]. \t  -64346.925071400525 \t -0.04568447350163922\n",
            "75     \t [7.05797547 2.79815697]. \t  -184.66686542810012 \t -0.04568447350163922\n",
            "76     \t [-2.54790021 -0.34899218]. \t  -28.172443201245002 \t -0.04568447350163922\n",
            "77     \t [ 2.96475059 -1.30916547]. \t  -4.2891270744889365 \t -0.04568447350163922\n",
            "78     \t [ 0.6548571  -4.73899305]. \t  -3918.236193193717 \t -0.04568447350163922\n",
            "79     \t [-2.11542918 -0.94077489]. \t  -39.90080283256739 \t -0.04568447350163922\n",
            "80     \t [-2.53249873 -8.90909798]. \t  -52032.73112440396 \t -0.04568447350163922\n",
            "81     \t [ 3.2481393  -1.14003583]. \t  -5.895950675392666 \t -0.04568447350163922\n",
            "82     \t [4.40914038 2.89442519]. \t  -316.4822145066299 \t -0.04568447350163922\n",
            "83     \t [4.45233326 1.97971428]. \t  -34.85135964807685 \t -0.04568447350163922\n",
            "84     \t [0.86796068 0.72127877]. \t  -0.0769644466380927 \t -0.04568447350163922\n",
            "85     \t [-8.89096135  3.26803045]. \t  -1928.0780127427365 \t -0.04568447350163922\n",
            "86     \t [ 3.62652117 -8.97584202]. \t  -49622.52268386854 \t -0.04568447350163922\n",
            "87     \t [-1.94956536  6.35408321]. \t  -13686.721634452053 \t -0.04568447350163922\n",
            "88     \t [-5.28205775  0.69650269]. \t  -117.64650369098928 \t -0.04568447350163922\n",
            "89     \t [4.32847395 3.52225331]. \t  -850.2723996448592 \t -0.04568447350163922\n",
            "90     \t [ 5.95507955 -0.58533502]. \t  -80.09535385780426 \t -0.04568447350163922\n",
            "91     \t [1.27144605 1.61000405]. \t  -30.693378009561993 \t -0.04568447350163922\n",
            "92     \t [ 7.16829238 -3.33687695]. \t  -494.14050371366375 \t -0.04568447350163922\n",
            "93     \t [ 6.52661423 -2.56713644]. \t  -119.08863668164818 \t -0.04568447350163922\n",
            "94     \t [-5.71866905  3.75727751]. \t  -2350.7444695066465 \t -0.04568447350163922\n",
            "95     \t [ 8.07429825 -8.26933835]. \t  -33172.07978023832 \t -0.04568447350163922\n",
            "96     \t [4.65582967 2.22391992]. \t  -68.19250159634333 \t -0.04568447350163922\n",
            "97     \t [ 7.54668457 -2.59018668]. \t  -111.80691749774971 \t -0.04568447350163922\n",
            "98     \t [ 8.40402256 -4.17626348]. \t  -1457.0235509451786 \t -0.04568447350163922\n",
            "99     \t [1.67328896 1.86655206]. \t  -56.52195071756115 \t -0.04568447350163922\n",
            "100    \t [-5.34906297  1.65464129]. \t  -274.660530146399 \t -0.04568447350163922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "c6bfb471-ddfb-4799-ed39-3a54494790f5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_2 = d2GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.03970985 -8.52192802]. \t  -38238.719775621794 \t -57.443300302345605\n",
            "init   \t [ 7.89863525 -1.27012905]. \t  -91.24969381804144 \t -57.443300302345605\n",
            "init   \t [-7.44644532  1.51715749]. \t  -361.74642651372136 \t -57.443300302345605\n",
            "init   \t [ 6.80941839 -1.29758898]. \t  -57.443300302345605 \t -57.443300302345605\n",
            "init   \t [3.91821121 3.69276194]. \t  -1099.406544633519 \t -57.443300302345605\n",
            "1      \t [-3.35617937 -7.20418892]. \t  -22984.1595726879 \t -57.443300302345605\n",
            "2      \t [-2.87227721  9.90072912]. \t  -79154.24352053565 \t -57.443300302345605\n",
            "3      \t [9.76713458 9.30216525]. \t  -53406.34593257343 \t -57.443300302345605\n",
            "4      \t [-11.23812303  -4.89840435]. \t  -7165.411996853938 \t -57.443300302345605\n",
            "5      \t [-39.91953012 -49.94905485]. \t  -50598156.63405781 \t -57.443300302345605\n",
            "6      \t [-9.85635154  8.2084324 ]. \t  -41943.744160795155 \t -57.443300302345605\n",
            "7      \t [-12.30945567 -11.76979024]. \t  -167641.78839343725 \t -57.443300302345605\n",
            "8      \t [-0.96925372 -0.61007647]. \t  \u001b[92m-9.751086512664948\u001b[0m \t -9.751086512664948\n",
            "9      \t [-30.46265266 -28.91007651]. \t  -5794922.646553113 \t -9.751086512664948\n",
            "10     \t [ -1.04870664 -13.83856994]. \t  -295009.50557448267 \t -9.751086512664948\n",
            "11     \t [ 2.41067748 -3.9724009 ]. \t  -1701.3487582891516 \t -9.751086512664948\n",
            "12     \t [-62.5361975  -61.78374115]. \t  -118491579.0340713 \t -9.751086512664948\n",
            "13     \t [-2.49942915  3.66946082]. \t  -1744.4151199713574 \t -9.751086512664948\n",
            "14     \t [2.97258899 9.37888261]. \t  -59830.263246461305 \t -9.751086512664948\n",
            "15     \t [9.93364434 3.1610923 ]. \t  -281.86986791399926 \t -9.751086512664948\n",
            "16     \t [-17.22063449 -17.33112139]. \t  -764071.6520115322 \t -9.751086512664948\n",
            "17     \t [-4.36703327 -1.91510992]. \t  -302.6938802106458 \t -9.751086512664948\n",
            "18     \t [-12.96331146   0.85099487]. \t  -610.3680313126094 \t -9.751086512664948\n",
            "19     \t [ 2.36215305 -0.03432663]. \t  -12.992739196644083 \t -9.751086512664948\n",
            "20     \t [ 1.6965072  -8.90268805]. \t  -49185.009340831726 \t -9.751086512664948\n",
            "21     \t [ 9.78932172 -3.79351762]. \t  -798.6617852489943 \t -9.751086512664948\n",
            "22     \t [-7.36275358 -9.92840391]. \t  -83717.93045642696 \t -9.751086512664948\n",
            "23     \t [ 4.50123433 -0.03194624]. \t  -52.74412076676432 \t -9.751086512664948\n",
            "24     \t [-6.75395395 -4.29468347]. \t  -3869.4709541540155 \t -9.751086512664948\n",
            "25     \t [-1.19848282 -2.91446412]. \t  -666.344355168578 \t -9.751086512664948\n",
            "26     \t [0.84426983 1.01439372]. \t  \u001b[92m-2.9704815874398585\u001b[0m \t -2.9704815874398585\n",
            "27     \t [7.4322608 5.624308 ]. \t  -6276.115572689569 \t -2.9704815874398585\n",
            "28     \t [-1.14439597  0.35568025]. \t  -8.503959360211185 \t -2.9704815874398585\n",
            "29     \t [0.15715675 0.0869664 ]. \t  \u001b[92m-0.7507300308865946\u001b[0m \t -0.7507300308865946\n",
            "30     \t [-5.92526949  5.27255267]. \t  -7618.58423470874 \t -0.7507300308865946\n",
            "31     \t [-8.91358749 -0.79821554]. \t  -305.8651612090258 \t -0.7507300308865946\n",
            "32     \t [ 0.0508366  -0.03585975]. \t  -0.905570134789214 \t -0.7507300308865946\n",
            "33     \t [-0.62673533 -0.21716182]. \t  -3.6861051043431754 \t -0.7507300308865946\n",
            "34     \t [0.23272385 0.11518293]. \t  \u001b[92m-0.6737410248369122\u001b[0m \t -0.6737410248369122\n",
            "35     \t [ 5.8550793  -4.45603839]. \t  -2316.2292802856286 \t -0.6737410248369122\n",
            "36     \t [-0.49446034  6.17490108]. \t  -11784.361449314862 \t -0.6737410248369122\n",
            "37     \t [6.34963568 0.53972791]. \t  -95.1357160247394 \t -0.6737410248369122\n",
            "38     \t [-1.83422304 -0.20584179]. \t  -15.397671516899365 \t -0.6737410248369122\n",
            "39     \t [0.98387592 1.40484486]. \t  -17.562580023112382 \t -0.6737410248369122\n",
            "40     \t [0.16308031 2.00231073]. \t  -124.1155613071918 \t -0.6737410248369122\n",
            "41     \t [1.70144483 0.29624375]. \t  -5.148913665659565 \t -0.6737410248369122\n",
            "42     \t [1.30859837 0.91852788]. \t  \u001b[92m-0.3821945218466912\u001b[0m \t -0.3821945218466912\n",
            "43     \t [-0.09649101 -0.46175283]. \t  -1.749188122894419 \t -0.3821945218466912\n",
            "44     \t [0.8390783  0.64851188]. \t  \u001b[92m-0.025904257113482494\u001b[0m \t -0.025904257113482494\n",
            "45     \t [8.47542836 0.86464123]. \t  -153.32895620076943 \t -0.025904257113482494\n",
            "46     \t [-9.12987395  3.04892046]. \t  -1639.600308085574 \t -0.025904257113482494\n",
            "47     \t [-9.39681078  9.86068833]. \t  -83228.4691757223 \t -0.025904257113482494\n",
            "48     \t [ 0.90766099 -0.37734296]. \t  -0.784499367355668 \t -0.025904257113482494\n",
            "49     \t [0.8957417  0.78342188]. \t  -0.23099652665987278 \t -0.025904257113482494\n",
            "50     \t [-8.28999214 -2.10020643]. \t  -671.9263016290016 \t -0.025904257113482494\n",
            "51     \t [2.86550724 0.72834129]. \t  -9.992883851980888 \t -0.025904257113482494\n",
            "52     \t [ 9.98374981 -0.30160668]. \t  -272.8589791020866 \t -0.025904257113482494\n",
            "53     \t [9.76705851 8.21340072]. \t  -31403.330122896183 \t -0.025904257113482494\n",
            "54     \t [-0.03419426  0.8540656 ]. \t  -5.527956398992371 \t -0.025904257113482494\n",
            "55     \t [ 1.38265893 -0.18571954]. \t  -3.5979141742767697 \t -0.025904257113482494\n",
            "56     \t [6.41619118 2.27686027]. \t  -60.57164316360735 \t -0.025904257113482494\n",
            "57     \t [-5.35735181  1.19083881]. \t  -174.68431316623503 \t -0.025904257113482494\n",
            "58     \t [-9.42952584 -4.1515827 ]. \t  -3963.336127203735 \t -0.025904257113482494\n",
            "59     \t [2.45550249 0.77554451]. \t  -5.256320277166324 \t -0.025904257113482494\n",
            "60     \t [1.390785   7.89766594]. \t  -30433.296516509945 \t -0.025904257113482494\n",
            "61     \t [ 3.76049883 -1.15330976]. \t  -10.041462799760914 \t -0.025904257113482494\n",
            "62     \t [2.1678619  1.47347977]. \t  -10.820135442011312 \t -0.025904257113482494\n",
            "63     \t [-2.83471316 -2.43994193]. \t  -449.31961196714644 \t -0.025904257113482494\n",
            "64     \t [-2.39191873  0.372648  ]. \t  -25.75919399676671 \t -0.025904257113482494\n",
            "65     \t [-0.54379441  6.45592031]. \t  -14081.343119217587 \t -0.025904257113482494\n",
            "66     \t [-0.1896273   4.22494727]. \t  -2577.597424934544 \t -0.025904257113482494\n",
            "67     \t [ 0.32557563 -0.61495873]. \t  -0.8259787340296165 \t -0.025904257113482494\n",
            "68     \t [1.75681562 0.88938156]. \t  -0.6338915115745557 \t -0.025904257113482494\n",
            "69     \t [-5.594978  -5.0845089]. \t  -6609.945274306271 \t -0.025904257113482494\n",
            "70     \t [ 1.33605182 -1.02827291]. \t  -1.3254867767738763 \t -0.025904257113482494\n",
            "71     \t [ 0.82300292 -0.73197445]. \t  -0.15490231521035006 \t -0.025904257113482494\n",
            "72     \t [ 7.70530973 -4.0913636 ]. \t  -1373.4771047270685 \t -0.025904257113482494\n",
            "73     \t [ 1.25910626 -0.77446087]. \t  -0.07422297869913888 \t -0.025904257113482494\n",
            "74     \t [-0.31603546 -0.55825071]. \t  -3.4966053616034607 \t -0.025904257113482494\n",
            "75     \t [ 2.22809454 -0.84007332]. \t  -2.8420446459060957 \t -0.025904257113482494\n",
            "76     \t [ 3.23658576 -0.45542641]. \t  -20.92696738143664 \t -0.025904257113482494\n",
            "77     \t [-5.7912885   4.92505611]. \t  -5943.893075288378 \t -0.025904257113482494\n",
            "78     \t [-5.41625753  2.95579503]. \t  -1089.0456511360185 \t -0.025904257113482494\n",
            "79     \t [0.82424761 0.42705916]. \t  -0.4531483648225632 \t -0.025904257113482494\n",
            "80     \t [ 1.38126724 -0.85602978]. \t  -0.15957995887680337 \t -0.025904257113482494\n",
            "81     \t [-5.6520449  -0.54908606]. \t  -122.50064975718254 \t -0.025904257113482494\n",
            "82     \t [ 4.7673075  -1.44449855]. \t  -14.898647045675359 \t -0.025904257113482494\n",
            "83     \t [-4.09708912  0.73311384]. \t  -79.47950472911278 \t -0.025904257113482494\n",
            "84     \t [ 7.96364668 -3.79670713]. \t  -919.2992796888569 \t -0.025904257113482494\n",
            "85     \t [-0.42287358 -3.42135285]. \t  -1138.1612419423348 \t -0.025904257113482494\n",
            "86     \t [ 7.89604122 -2.16448552]. \t  -51.90046479188232 \t -0.025904257113482494\n",
            "87     \t [9.22419433 1.02904472]. \t  -168.63717468394677 \t -0.025904257113482494\n",
            "88     \t [-1.6844257  -7.01140427]. \t  -20008.808295517654 \t -0.025904257113482494\n",
            "89     \t [1.3270714  0.74694865]. \t  -0.19619236803955004 \t -0.025904257113482494\n",
            "90     \t [8.17555757 7.6307228 ]. \t  -23500.736860312965 \t -0.025904257113482494\n",
            "91     \t [-4.78552334  8.83038623]. \t  -51706.21634292386 \t -0.025904257113482494\n",
            "92     \t [-1.39136264 -7.11112906]. \t  -21029.567136461766 \t -0.025904257113482494\n",
            "93     \t [-1.75019318  1.96533379]. \t  -187.12493048878747 \t -0.025904257113482494\n",
            "94     \t [-7.84633632  8.56911237]. \t  -47945.970974422846 \t -0.025904257113482494\n",
            "95     \t [ 6.33238924 -5.55543991]. \t  -6165.2998444628665 \t -0.025904257113482494\n",
            "96     \t [5.71707915 4.2191094 ]. \t  -1808.44012269075 \t -0.025904257113482494\n",
            "97     \t [7.18033546 2.18284709]. \t  -49.23503654820095 \t -0.025904257113482494\n",
            "98     \t [ 5.28574755 -2.76259469]. \t  -217.4930416817421 \t -0.025904257113482494\n",
            "99     \t [6.29326671 1.34097759]. \t  -42.56440207309509 \t -0.025904257113482494\n",
            "100    \t [-3.29500331  4.82568198]. \t  -4992.364448817138 \t -0.025904257113482494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "c9359227-f369-4f90-e4fa-7deec6385b6c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_3 = d2GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.03151214 -7.81497449]. \t  -27448.6338140421 \t -1.7863168283775226\n",
            "init   \t [-0.27757442 -0.00337645]. \t  -1.7863168283775226 \t -1.7863168283775226\n",
            "init   \t [ 3.14219998 -5.28230578]. \t  -5551.437171353593 \t -1.7863168283775226\n",
            "init   \t [ 2.25589781 -7.60695193]. \t  -25754.9725373955 \t -1.7863168283775226\n",
            "init   \t [ 4.24404593 -2.54878916]. \t  -163.60006264419064 \t -1.7863168283775226\n",
            "1      \t [-5.61872775  6.96371276]. \t  -21099.51033227109 \t -1.7863168283775226\n",
            "2      \t [-9.6736302  -6.79831075]. \t  -20965.88636571433 \t -1.7863168283775226\n",
            "3      \t [5.95256263 7.26277879]. \t  -19842.284445028028 \t -1.7863168283775226\n",
            "4      \t [-9.56557533  1.04731122]. \t  -388.19351001051024 \t -1.7863168283775226\n",
            "5      \t [ -3.39252593 -11.90986646]. \t  -164851.86183492068 \t -1.7863168283775226\n",
            "6      \t [-17.53778096 -18.14510616]. \t  -914370.1083636795 \t -1.7863168283775226\n",
            "7      \t [-10.72577999 -13.72831271]. \t  -300696.3696566352 \t -1.7863168283775226\n",
            "8      \t [-15.75961687  -0.86138605]. \t  -875.5674621589994 \t -1.7863168283775226\n",
            "9      \t [ 9.8823633  -0.26074619]. \t  -268.8804640347504 \t -1.7863168283775226\n",
            "10     \t [-3.61529629 -3.88183428]. \t  -2299.77307191449 \t -1.7863168283775226\n",
            "11     \t [0.14156343 4.63871755]. \t  -3680.4995108452363 \t -1.7863168283775226\n",
            "12     \t [-4.5615212   1.34334013]. \t  -164.44945075484523 \t -1.7863168283775226\n",
            "13     \t [-0.40178801  9.68331847]. \t  -70641.16951990491 \t -1.7863168283775226\n",
            "14     \t [4.38399427 2.16486945]. \t  -61.238149216935476 \t -1.7863168283775226\n",
            "15     \t [ 9.98753251 -5.81585278]. \t  -6730.301003660748 \t -1.7863168283775226\n",
            "16     \t [-12.64238701   6.51318807]. \t  -19193.00875552654 \t -1.7863168283775226\n",
            "17     \t [9.91784296 9.46154543]. \t  -57285.27507142265 \t -1.7863168283775226\n",
            "18     \t [-7.68887533 -2.44695272]. \t  -848.8447530237256 \t -1.7863168283775226\n",
            "19     \t [9.59723684 4.12555468]. \t  -1268.8492071186606 \t -1.7863168283775226\n",
            "20     \t [1.43714055 0.06771092]. \t  -4.2692943205388865 \t -1.7863168283775226\n",
            "21     \t [ 1.44480006 -1.40320696]. \t  -12.629734726367031 \t -1.7863168283775226\n",
            "22     \t [6.6059969  0.15454016]. \t  -117.44800396017388 \t -1.7863168283775226\n",
            "23     \t [2.09938963 1.11613158]. \t  \u001b[92m-1.5161576967282764\u001b[0m \t -1.5161576967282764\n",
            "24     \t [-0.44818552 -0.1901319 ]. \t  -2.63905223321422 \t -1.5161576967282764\n",
            "25     \t [1.48645193 0.79904033]. \t  \u001b[92m-0.3244336009783415\u001b[0m \t -0.3244336009783415\n",
            "26     \t [-9.63444314  9.87604258]. \t  -83922.90247770057 \t -0.3244336009783415\n",
            "27     \t [-1.14652624  0.52224942]. \t  -10.333405434850206 \t -0.3244336009783415\n",
            "28     \t [  2.14828898 -17.31233283]. \t  -713501.1286649621 \t -0.3244336009783415\n",
            "29     \t [-0.09367376 -2.20984878]. \t  -195.65650383140525 \t -0.3244336009783415\n",
            "30     \t [ 1.77777187 -1.05371977]. \t  -0.9972124306229588 \t -0.3244336009783415\n",
            "31     \t [-5.43287625 -7.63732574]. \t  -29853.496455237928 \t -0.3244336009783415\n",
            "32     \t [ 6.61314919 -3.80908378]. \t  -1035.4834978923784 \t -0.3244336009783415\n",
            "33     \t [-2.0920441  -0.39531208]. \t  -21.124817727872376 \t -0.3244336009783415\n",
            "34     \t [-6.5209947   2.89952612]. \t  -1145.6563131680173 \t -0.3244336009783415\n",
            "35     \t [2.23416298 1.56274849]. \t  -15.570306831928129 \t -0.3244336009783415\n",
            "36     \t [1.9097893  0.06171165]. \t  -8.064238090568919 \t -0.3244336009783415\n",
            "37     \t [-8.97322249  4.70913343]. \t  -5786.597290457958 \t -0.3244336009783415\n",
            "38     \t [ 9.68142153 -9.46630572]. \t  -57563.3066318167 \t -0.3244336009783415\n",
            "39     \t [ 3.6148632  -0.34420119]. \t  -29.658119449816258 \t -0.3244336009783415\n",
            "40     \t [6.05460441 3.09515728]. \t  -369.05166165593704 \t -0.3244336009783415\n",
            "41     \t [0.95795396 0.20848131]. \t  -1.519137131891804 \t -0.3244336009783415\n",
            "42     \t [3.2184374  0.55594701]. \t  -18.444410357277558 \t -0.3244336009783415\n",
            "43     \t [1.83041787 0.61398597]. \t  -3.0071274883443526 \t -0.3244336009783415\n",
            "44     \t [0.85667398 0.53954777]. \t  \u001b[92m-0.17118838213653775\u001b[0m \t -0.17118838213653775\n",
            "45     \t [1.86690021 0.92982405]. \t  -0.7894686801623526 \t -0.17118838213653775\n",
            "46     \t [ 1.66295637 -0.82982573]. \t  -0.6028000162250124 \t -0.17118838213653775\n",
            "47     \t [-3.06447516  2.93917767]. \t  -844.1143708675695 \t -0.17118838213653775\n",
            "48     \t [-6.01999287 -0.13418035]. \t  -122.63061047599547 \t -0.17118838213653775\n",
            "49     \t [0.92386792 1.1439739 ]. \t  -5.741576565358855 \t -0.17118838213653775\n",
            "50     \t [1.03450793 0.42992548]. \t  -0.8852048554706723 \t -0.17118838213653775\n",
            "51     \t [3.31392295 1.49225496]. \t  -7.952193742191369 \t -0.17118838213653775\n",
            "52     \t [ 4.17652719 -1.68276628]. \t  -14.51193436043231 \t -0.17118838213653775\n",
            "53     \t [-3.99104534 -0.04515812]. \t  -56.83256277106378 \t -0.17118838213653775\n",
            "54     \t [ 4.26648716 -0.5970356 ]. \t  -35.92585890601441 \t -0.17118838213653775\n",
            "55     \t [1.3464218 0.7656643]. \t  -0.18051703725216586 \t -0.17118838213653775\n",
            "56     \t [ 1.83415236 -0.81424352]. \t  -1.212278243855153 \t -0.17118838213653775\n",
            "57     \t [ 3.76469082 -1.66404432]. \t  -13.933383502394797 \t -0.17118838213653775\n",
            "58     \t [-0.82343287 -0.15597039]. \t  -4.845976774569344 \t -0.17118838213653775\n",
            "59     \t [-5.50862208  0.93055068]. \t  -147.21100865555368 \t -0.17118838213653775\n",
            "60     \t [-4.60172925  0.73352925]. \t  -95.85554907913139 \t -0.17118838213653775\n",
            "61     \t [0.5103189 0.7678886]. \t  -1.134874548848486 \t -0.17118838213653775\n",
            "62     \t [ 4.05661397 -1.36296349]. \t  -9.575826277279246 \t -0.17118838213653775\n",
            "63     \t [-8.44339496  0.08126659]. \t  -232.20599304231348 \t -0.17118838213653775\n",
            "64     \t [ 3.03252856 -4.64622883]. \t  -3226.9486716463393 \t -0.17118838213653775\n",
            "65     \t [-1.34895969 -6.60032357]. \t  -15662.054369899819 \t -0.17118838213653775\n",
            "66     \t [ 0.56816815 -0.02748304]. \t  -0.8286802273916385 \t -0.17118838213653775\n",
            "67     \t [ 2.57030794 -2.14737887]. \t  -90.968441299027 \t -0.17118838213653775\n",
            "68     \t [1.30568365 0.85184234]. \t  \u001b[92m-0.13583369496260622\u001b[0m \t -0.13583369496260622\n",
            "69     \t [3.39013569 3.7091123 ]. \t  -1169.7336127707895 \t -0.13583369496260622\n",
            "70     \t [4.5103584  1.08405857]. \t  -21.653750818990083 \t -0.13583369496260622\n",
            "71     \t [4.16510496 1.31722735]. \t  -10.98374243839039 \t -0.13583369496260622\n",
            "72     \t [4.92267066 1.44979391]. \t  -16.420881447194066 \t -0.13583369496260622\n",
            "73     \t [ 5.26578534 -6.19497163]. \t  -10239.711748289283 \t -0.13583369496260622\n",
            "74     \t [ 0.25318592 -3.02381372]. \t  -650.9873297211229 \t -0.13583369496260622\n",
            "75     \t [0.3848995  0.38581899]. \t  -0.39355174081710126 \t -0.13583369496260622\n",
            "76     \t [-0.08028517 -0.34518912]. \t  -1.3700231694505207 \t -0.13583369496260622\n",
            "77     \t [-5.53208922 -3.06693246]. \t  -1227.9521036431554 \t -0.13583369496260622\n",
            "78     \t [-1.57045538 -1.38555967]. \t  -65.14358357094548 \t -0.13583369496260622\n",
            "79     \t [5.50025847 2.631525  ]. \t  -159.68360540471554 \t -0.13583369496260622\n",
            "80     \t [ 7.7878485  -0.83363241]. \t  -127.94273632673325 \t -0.13583369496260622\n",
            "81     \t [ 0.02497789 -0.71864536]. \t  -2.982491401804714 \t -0.13583369496260622\n",
            "82     \t [3.33980119 3.6619608 ]. \t  -1108.1060419131188 \t -0.13583369496260622\n",
            "83     \t [0.82387246 0.76164079]. \t  -0.25724444734025764 \t -0.13583369496260622\n",
            "84     \t [-3.50885816  4.46413975]. \t  -3781.5402603850184 \t -0.13583369496260622\n",
            "85     \t [-1.95345393  1.5456868 ]. \t  -99.35578790571606 \t -0.13583369496260622\n",
            "86     \t [1.82850804 4.73345635]. \t  -3695.7131544669023 \t -0.13583369496260622\n",
            "87     \t [-1.88141974  0.0641797 ]. \t  -15.444193046252067 \t -0.13583369496260622\n",
            "88     \t [-9.65611648  3.01445109]. \t  -1662.5647470123702 \t -0.13583369496260622\n",
            "89     \t [2.71945313 1.24302118]. \t  -3.231430469006281 \t -0.13583369496260622\n",
            "90     \t [-4.66950881 -2.21553784]. \t  -451.87371405420083 \t -0.13583369496260622\n",
            "91     \t [0.33250374 0.11926908]. \t  -0.6304483326609811 \t -0.13583369496260622\n",
            "92     \t [ 2.78182392 -1.27265521]. \t  -3.593469907187097 \t -0.13583369496260622\n",
            "93     \t [ 3.47587984 -4.77822747]. \t  -3565.6219455686582 \t -0.13583369496260622\n",
            "94     \t [-3.91391919 -0.82546241]. \t  -79.8336338275687 \t -0.13583369496260622\n",
            "95     \t [3.1329186  2.03610205]. \t  -57.769679858785196 \t -0.13583369496260622\n",
            "96     \t [ 3.02335012 -1.1616632 ]. \t  -4.304451937194685 \t -0.13583369496260622\n",
            "97     \t [-6.11691153  5.8954905 ]. \t  -11490.603962595726 \t -0.13583369496260622\n",
            "98     \t [-8.88127832 -9.83888317]. \t  -82100.84304076748 \t -0.13583369496260622\n",
            "99     \t [1.39639806 5.72636826]. \t  -8239.890787569202 \t -0.13583369496260622\n",
            "100    \t [ 9.497473   -0.19024079]. \t  -249.87168618536793 \t -0.13583369496260622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "a5c736ac-7077-4150-c264-94610bca3744"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_4 = d2GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.78722095 6.4258993 ]. \t  -11523.905614569041 \t -765.9755148718126\n",
            "init   \t [2.8441433  3.34445237]. \t  -765.9755148718126 \t -765.9755148718126\n",
            "init   \t [-9.21720189 -8.68655223]. \t  -51387.35679532384 \t -765.9755148718126\n",
            "init   \t [-4.4699872  -3.67278993]. \t  -2007.9696367442386 \t -765.9755148718126\n",
            "init   \t [-6.12815176 -6.05814236]. \t  -12700.957148627067 \t -765.9755148718126\n",
            "1      \t [ 5.15020416 -5.71066603]. \t  -7234.8050437839365 \t -765.9755148718126\n",
            "2      \t [-3.97305893  9.21389704]. \t  -60413.181901672484 \t -765.9755148718126\n",
            "3      \t [-8.51899843  2.33131307]. \t  -842.4810529068852 \t -765.9755148718126\n",
            "4      \t [  1.17903038 -11.46989784]. \t  -137223.15007446375 \t -765.9755148718126\n",
            "5      \t [-69.72587698 -67.86198471]. \t  -172250110.28562367 \t -765.9755148718126\n",
            "6      \t [-13.74774616  -1.80552776]. \t  -1039.0478168891748 \t -765.9755148718126\n",
            "7      \t [-2.83803863  2.67646344]. \t  \u001b[92m-604.0016793287856\u001b[0m \t -604.0016793287856\n",
            "8      \t [-15.77348496 -11.71707459]. \t  -168891.26326749465 \t -604.0016793287856\n",
            "9      \t [ 8.97539946 -0.27131946]. \t  \u001b[92m-219.4802014122361\u001b[0m \t -219.4802014122361\n",
            "10     \t [ 0.91647747 -1.68350789]. \t  \u001b[92m-45.16846668192221\u001b[0m \t -45.16846668192221\n",
            "11     \t [  7.62383649 -10.88715893]. \t  -105326.20289873917 \t -45.16846668192221\n",
            "12     \t [ -9.00644367 -15.81372919]. \t  -518576.7493659311 \t -45.16846668192221\n",
            "13     \t [-10.15357019   7.64051398]. \t  -32335.925643605668 \t -45.16846668192221\n",
            "14     \t [-35.19286831 -37.22705085]. \t  -15758679.51748453 \t -45.16846668192221\n",
            "15     \t [1.85662458 8.42140482]. \t  -39191.49075064337 \t -45.16846668192221\n",
            "16     \t [ 0.34392676 -5.94518556]. \t  -9897.700686983879 \t -45.16846668192221\n",
            "17     \t [ -4.48794437 -11.01884174]. \t  -122362.19814850767 \t -45.16846668192221\n",
            "18     \t [-9.18108282 -1.919588  ]. \t  -651.5070438100439 \t -45.16846668192221\n",
            "19     \t [ 4.7111189  -0.56457836]. \t  -46.961186884792355 \t -45.16846668192221\n",
            "20     \t [-1.33383904 -0.79170411]. \t  \u001b[92m-18.8363908155254\u001b[0m \t -18.8363908155254\n",
            "21     \t [ 8.8265108  -4.11307361]. \t  -1312.078224120276 \t -18.8363908155254\n",
            "22     \t [6.13145361 1.66761882]. \t  -26.98058724585679 \t -18.8363908155254\n",
            "23     \t [9.53651703 3.34482853]. \t  -402.5642253885253 \t -18.8363908155254\n",
            "24     \t [0.78996349 0.5646075 ]. \t  \u001b[92m-0.09056700129179428\u001b[0m \t -0.09056700129179428\n",
            "25     \t [8.57973321 9.98456116]. \t  -72869.16695759939 \t -0.09056700129179428\n",
            "26     \t [-5.55999398  0.15955791]. \t  -105.99817546653372 \t -0.09056700129179428\n",
            "27     \t [-6.62352408  4.9130127 ]. \t  -6085.899140443135 \t -0.09056700129179428\n",
            "28     \t [0.10783759 0.16183711]. \t  -0.8021042933517469 \t -0.09056700129179428\n",
            "29     \t [0.28729995 0.16287836]. \t  -0.6176792684089881 \t -0.09056700129179428\n",
            "30     \t [-1.49093561 -2.22100886]. \t  -264.1538630160063 \t -0.09056700129179428\n",
            "31     \t [1.34693447 0.09164139]. \t  -3.658898676206393 \t -0.09056700129179428\n",
            "32     \t [-1.31316384 -0.21393406]. \t  -9.297087794163076 \t -0.09056700129179428\n",
            "33     \t [ 2.83658525 -2.40746082]. \t  -156.67834561625094 \t -0.09056700129179428\n",
            "34     \t [0.65829813 0.45114132]. \t  -0.24300440145194357 \t -0.09056700129179428\n",
            "35     \t [-0.43199815 -0.42367312]. \t  -3.301967937980036 \t -0.09056700129179428\n",
            "36     \t [-1.46996565  5.29625253]. \t  -6634.836575577645 \t -0.09056700129179428\n",
            "37     \t [6.681024 0.464195]. \t  -110.40078377600108 \t -0.09056700129179428\n",
            "38     \t [1.56415682 0.63942284]. \t  -1.4325993920996853 \t -0.09056700129179428\n",
            "39     \t [-2.68420732 -0.58160048]. \t  -36.16233598285577 \t -0.09056700129179428\n",
            "40     \t [6.09202015 2.78638739]. \t  -204.00067974463605 \t -0.09056700129179428\n",
            "41     \t [5.42933754 1.15854669]. \t  -34.6877271773627 \t -0.09056700129179428\n",
            "42     \t [0.78601779 0.65295584]. \t  \u001b[92m-0.054682135157014894\u001b[0m \t -0.054682135157014894\n",
            "43     \t [-0.24532764  0.94987878]. \t  -9.95475067659605 \t -0.054682135157014894\n",
            "44     \t [ 5.57734152 -1.78229998]. \t  -22.155925992111435 \t -0.054682135157014894\n",
            "45     \t [ 6.46670087 -1.95032191]. \t  -32.487714493105415 \t -0.054682135157014894\n",
            "46     \t [ 9.60576125 -7.20033915]. \t  -17777.659722130218 \t -0.054682135157014894\n",
            "47     \t [1.91299589 0.49054712]. \t  -4.933222636279261 \t -0.054682135157014894\n",
            "48     \t [1.28504352 0.77939902]. \t  -0.09108283136151642 \t -0.054682135157014894\n",
            "49     \t [3.56905532 0.54776267]. \t  -24.2295804521989 \t -0.054682135157014894\n",
            "50     \t [-9.91387411 -4.76925574]. \t  -6258.645914932864 \t -0.054682135157014894\n",
            "51     \t [8.81606978 0.28697194]. \t  -210.7831447781824 \t -0.054682135157014894\n",
            "52     \t [0.02389677 1.98620116]. \t  -124.7036246345092 \t -0.054682135157014894\n",
            "53     \t [ 1.27648713 -0.63292458]. \t  -0.5282654422747914 \t -0.054682135157014894\n",
            "54     \t [ 6.49196718 -7.21609816]. \t  -19102.05751112033 \t -0.054682135157014894\n",
            "55     \t [ 1.72456471 -0.23824666]. \t  -5.715905177417754 \t -0.054682135157014894\n",
            "56     \t [ 0.75567059 -0.85408622]. \t  -1.048834726082826 \t -0.054682135157014894\n",
            "57     \t [0.01242725 5.36685212]. \t  -6635.061400079478 \t -0.054682135157014894\n",
            "58     \t [-5.54828522 -2.83642507]. \t  -979.3639882971961 \t -0.054682135157014894\n",
            "59     \t [-9.31011341  2.2251477 ]. \t  -844.5524321080262 \t -0.054682135157014894\n",
            "60     \t [ 0.78374146 -0.25038619]. \t  -0.9136302895674375 \t -0.054682135157014894\n",
            "61     \t [ 7.7589623  -0.11838766]. \t  -165.21816050757536 \t -0.054682135157014894\n",
            "62     \t [0.24305877 0.67589176]. \t  -1.4723702896995015 \t -0.054682135157014894\n",
            "63     \t [-3.36428627 -4.54178341]. \t  -4000.9142702923155 \t -0.054682135157014894\n",
            "64     \t [-9.52691162  7.81135867]. \t  -34727.70400980378 \t -0.054682135157014894\n",
            "65     \t [0.70007031 2.81598637]. \t  -459.71012085825157 \t -0.054682135157014894\n",
            "66     \t [-3.66699095 -1.76817563]. \t  -218.58888550221934 \t -0.054682135157014894\n",
            "67     \t [ 3.74941644 -7.90499041]. \t  -29400.175506072163 \t -0.054682135157014894\n",
            "68     \t [0.56436426 4.51406788]. \t  -3230.541957069111 \t -0.054682135157014894\n",
            "69     \t [-9.71312871 -4.81043275]. \t  -6385.34998145005 \t -0.054682135157014894\n",
            "70     \t [0.29754546 0.39250371]. \t  -0.4936659581205672 \t -0.054682135157014894\n",
            "71     \t [-2.95393482  4.31677071]. \t  -3251.4055782804458 \t -0.054682135157014894\n",
            "72     \t [-4.14324067  0.79371649]. \t  -84.84233316701318 \t -0.054682135157014894\n",
            "73     \t [-1.43321797 -0.44400218]. \t  -12.60001720740824 \t -0.054682135157014894\n",
            "74     \t [-4.11355789 -8.05623696]. \t  -35894.995196675634 \t -0.054682135157014894\n",
            "75     \t [ 7.85982694 -4.90624431]. \t  -3292.4393334292486 \t -0.054682135157014894\n",
            "76     \t [-2.42025704  1.13470729]. \t  -61.605767203668044 \t -0.054682135157014894\n",
            "77     \t [7.03050385 8.26918204]. \t  -33695.19910931697 \t -0.054682135157014894\n",
            "78     \t [8.53890693 8.20984019]. \t  -31942.053167629954 \t -0.054682135157014894\n",
            "79     \t [-0.62588663 -0.08256257]. \t  -3.4614784443388897 \t -0.054682135157014894\n",
            "80     \t [-8.01751757 -8.31904695]. \t  -42965.158021264026 \t -0.054682135157014894\n",
            "81     \t [3.76477698 3.76714555]. \t  -1219.7352679061078 \t -0.054682135157014894\n",
            "82     \t [1.78056241 1.11117737]. \t  -1.558355654716626 \t -0.054682135157014894\n",
            "83     \t [ 0.77818021 -6.51871063]. \t  -14182.359299973818 \t -0.054682135157014894\n",
            "84     \t [7.23246528 9.31484792]. \t  -55350.43524845048 \t -0.054682135157014894\n",
            "85     \t [8.73485241 4.70702164]. \t  -2591.309797284475 \t -0.054682135157014894\n",
            "86     \t [ 3.03157594 -9.76296096]. \t  -70391.07523319387 \t -0.054682135157014894\n",
            "87     \t [9.6329446  2.37617339]. \t  -80.03531643505009 \t -0.054682135157014894\n",
            "88     \t [ 9.77528004 -4.79792497]. \t  -2707.289737859233 \t -0.054682135157014894\n",
            "89     \t [ 9.69860606 -1.93611298]. \t  -85.35929646652941 \t -0.054682135157014894\n",
            "90     \t [ 6.15494206 -2.46099085]. \t  -97.56919143620553 \t -0.054682135157014894\n",
            "91     \t [ 6.18894526 -1.26474752]. \t  -44.80263421598132 \t -0.054682135157014894\n",
            "92     \t [ 5.37857082 -2.58028248]. \t  -145.16840990953483 \t -0.054682135157014894\n",
            "93     \t [5.45426873 5.76928074]. \t  -7489.910291650112 \t -0.054682135157014894\n",
            "94     \t [ 8.79753823 -2.35236101]. \t  -71.1043735607819 \t -0.054682135157014894\n",
            "95     \t [-1.29255613 -2.49606527]. \t  -383.55902441051194 \t -0.054682135157014894\n",
            "96     \t [ 6.62625188 -0.43495995]. \t  -109.72650818812865 \t -0.054682135157014894\n",
            "97     \t [3.86956944 0.02692883]. \t  -38.15911986727106 \t -0.054682135157014894\n",
            "98     \t [-5.98245003  7.30174925]. \t  -25412.370855398018 \t -0.054682135157014894\n",
            "99     \t [-1.28302942  5.55932414]. \t  -7967.22056861976 \t -0.054682135157014894\n",
            "100    \t [ 6.29792024 -4.77555238]. \t  -3119.230411100237 \t -0.054682135157014894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "6cb2a03a-0c94-4fb0-ec72-316030294d9a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_5 = d2GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.14706549 0.83521255]. \t  -0.14473002518929054 \t -0.14473002518929054\n",
            "init   \t [ 1.20847567 -2.35903813]. \t  -196.92158721117866 \t -0.14473002518929054\n",
            "init   \t [-7.61811642  6.91998231]. \t  -21453.432632023236 \t -0.14473002518929054\n",
            "init   \t [4.79483225 8.64715129]. \t  -41920.46290057033 \t -0.14473002518929054\n",
            "init   \t [7.18656013 4.16209252]. \t  -1546.3183076435794 \t -0.14473002518929054\n",
            "1      \t [-1.28443039  2.56483835]. \t  -422.31640133461167 \t -0.14473002518929054\n",
            "2      \t [2.20317475 0.72550179]. \t  -4.094787565175626 \t -0.14473002518929054\n",
            "3      \t [ 9.14614046 -9.26496158]. \t  -52900.23738128742 \t -0.14473002518929054\n",
            "4      \t [-7.13636588 -6.2842414 ]. \t  -14899.421659869966 \t -0.14473002518929054\n",
            "5      \t [  2.24813239 -11.33449926]. \t  -129739.23230941298 \t -0.14473002518929054\n",
            "6      \t [-9.16030993  0.25878873]. \t  -275.99818055407 \t -0.14473002518929054\n",
            "7      \t [ 8.14263041 -2.29282181]. \t  -62.26456075976108 \t -0.14473002518929054\n",
            "8      \t [ -8.44193684 -13.31040162]. \t  -263301.3054326806 \t -0.14473002518929054\n",
            "9      \t [-1.44371435  7.9798997 ]. \t  -33185.528079790965 \t -0.14473002518929054\n",
            "10     \t [-14.8596174   -8.23391111]. \t  -45524.46102467055 \t -0.14473002518929054\n",
            "11     \t [-1.313065   -6.56518595]. \t  -15323.590167002543 \t -0.14473002518929054\n",
            "12     \t [ 5.09488953 -5.79203028]. \t  -7704.852449532565 \t -0.14473002518929054\n",
            "13     \t [-3.97670473 -1.57021667]. \t  -183.46772696285583 \t -0.14473002518929054\n",
            "14     \t [2.65318471 3.92875368]. \t  -1595.1341679538937 \t -0.14473002518929054\n",
            "15     \t [ 4.9185749  -0.04593638]. \t  -63.65699159305458 \t -0.14473002518929054\n",
            "16     \t [1.22734511 0.46138149]. \t  -1.3368088179892832 \t -0.14473002518929054\n",
            "17     \t [9.98706881 0.45188222]. \t  -264.2693865262832 \t -0.14473002518929054\n",
            "18     \t [-4.72044687  2.00523855]. \t  -358.4817374868871 \t -0.14473002518929054\n",
            "19     \t [9.50654069 9.55278718]. \t  -59933.78326922437 \t -0.14473002518929054\n",
            "20     \t [-4.61151212 -9.49836947]. \t  -68518.16630691824 \t -0.14473002518929054\n",
            "21     \t [-1.07929633 -0.68578146]. \t  -12.48337429007286 \t -0.14473002518929054\n",
            "22     \t [ 9.71329142 -4.8427496 ]. \t  -2842.285613658635 \t -0.14473002518929054\n",
            "23     \t [-9.96011897 -2.72487128]. \t  -1351.1930285989793 \t -0.14473002518929054\n",
            "24     \t [1.77158044 0.70495829]. \t  -1.8048094120767724 \t -0.14473002518929054\n",
            "25     \t [-9.83883546  2.71550323]. \t  -1326.4970092254289 \t -0.14473002518929054\n",
            "26     \t [1.29197572 0.60157165]. \t  -0.7309496076958966 \t -0.14473002518929054\n",
            "27     \t [6.98127063 0.46375328]. \t  -121.6103752555632 \t -0.14473002518929054\n",
            "28     \t [ 4.26413247 -1.46734544]. \t  -10.658101044144965 \t -0.14473002518929054\n",
            "29     \t [-5.20061094  9.97113347]. \t  -83309.30701521723 \t -0.14473002518929054\n",
            "30     \t [0.6911461  0.41591498]. \t  -0.333683065651094 \t -0.14473002518929054\n",
            "31     \t [2.17371345 0.11017018]. \t  -10.617775102696527 \t -0.14473002518929054\n",
            "32     \t [ 5.16551346 -1.9124842 ]. \t  -26.593735036871458 \t -0.14473002518929054\n",
            "33     \t [-6.30857334 -0.57449747]. \t  -150.5399120812006 \t -0.14473002518929054\n",
            "34     \t [-1.31519758 -1.54646048]. \t  -79.73811888446424 \t -0.14473002518929054\n",
            "35     \t [0.44948625 0.34367089]. \t  -0.39403092200703094 \t -0.14473002518929054\n",
            "36     \t [1.50050498 1.10531234]. \t  -2.0287232367435513 \t -0.14473002518929054\n",
            "37     \t [ 3.81291412 -1.70491774]. \t  -15.917085658895143 \t -0.14473002518929054\n",
            "38     \t [ 0.12517122 -0.50309095]. \t  -1.055692795715033 \t -0.14473002518929054\n",
            "39     \t [-4.33795198  4.55155866]. \t  -4218.5220253755115 \t -0.14473002518929054\n",
            "40     \t [-0.28103395  0.03961992]. \t  -1.802557068230817 \t -0.14473002518929054\n",
            "41     \t [9.99021943 5.21734739]. \t  -4032.643850034044 \t -0.14473002518929054\n",
            "42     \t [ 6.6258371  -2.42698256]. \t  -84.79090869617234 \t -0.14473002518929054\n",
            "43     \t [-2.92985739  0.59523871]. \t  -41.92078905609694 \t -0.14473002518929054\n",
            "44     \t [-4.08287229 -4.04319449]. \t  -2731.0363175118932 \t -0.14473002518929054\n",
            "45     \t [ 3.52108556 -0.56661794]. \t  -22.93285254484979 \t -0.14473002518929054\n",
            "46     \t [-9.7223639  -9.14283186]. \t  -62705.80704069833 \t -0.14473002518929054\n",
            "47     \t [-0.01241463 -0.13758713]. \t  -1.030038547504486 \t -0.14473002518929054\n",
            "48     \t [3.23638431 0.51412591]. \t  -19.66505508002978 \t -0.14473002518929054\n",
            "49     \t [-0.5993517  5.5202509]. \t  -7578.302277915097 \t -0.14473002518929054\n",
            "50     \t [-1.33272683 -0.24578125]. \t  -9.667192910477752 \t -0.14473002518929054\n",
            "51     \t [-0.763835   -3.11453932]. \t  -816.3288887456149 \t -0.14473002518929054\n",
            "52     \t [ 0.99725083 -0.43475213]. \t  -0.7669040832315646 \t -0.14473002518929054\n",
            "53     \t [-2.13622567 -0.17013324]. \t  -19.464204128740136 \t -0.14473002518929054\n",
            "54     \t [2.43960564 1.12725848]. \t  -2.0931980872224347 \t -0.14473002518929054\n",
            "55     \t [5.09988088 1.63586808]. \t  -16.93628122109199 \t -0.14473002518929054\n",
            "56     \t [-3.15868785  7.96922798]. \t  -33908.812544066655 \t -0.14473002518929054\n",
            "57     \t [-9.12550271 -8.71878829]. \t  -52047.64348572759 \t -0.14473002518929054\n",
            "58     \t [-7.09555014 -1.87065862]. \t  -462.83524980013703 \t -0.14473002518929054\n",
            "59     \t [3.31411436 1.14585081]. \t  -6.302270769510345 \t -0.14473002518929054\n",
            "60     \t [ 7.55197615 -4.15741169]. \t  -1502.675032654747 \t -0.14473002518929054\n",
            "61     \t [ 8.19411257 -1.87895633]. \t  -54.32335346758279 \t -0.14473002518929054\n",
            "62     \t [-1.72679888 -1.20461971]. \t  -50.291013329594755 \t -0.14473002518929054\n",
            "63     \t [-2.98420544 -5.26783978]. \t  -6856.742089050018 \t -0.14473002518929054\n",
            "64     \t [-0.69370063  0.32202411]. \t  -4.492583123757635 \t -0.14473002518929054\n",
            "65     \t [0.87641422 0.06018784]. \t  -1.5261831927636231 \t -0.14473002518929054\n",
            "66     \t [-1.39559589  0.44220152]. \t  -12.123332262121792 \t -0.14473002518929054\n",
            "67     \t [0.6540322  0.45814347]. \t  -0.22943170710098815 \t -0.14473002518929054\n",
            "68     \t [ 2.51281922 -0.53561285]. \t  -9.808505974204905 \t -0.14473002518929054\n",
            "69     \t [-6.24072439  0.690922  ]. \t  -155.97769002463605 \t -0.14473002518929054\n",
            "70     \t [-7.98074325 -9.01144947]. \t  -58148.32966529846 \t -0.14473002518929054\n",
            "71     \t [ 7.19505941 -2.2019923 ]. \t  -50.90358099568845 \t -0.14473002518929054\n",
            "72     \t [ 1.26930407 -0.11307527]. \t  -3.166263438190672 \t -0.14473002518929054\n",
            "73     \t [-8.33248565  0.93367549]. \t  -290.14625559474524 \t -0.14473002518929054\n",
            "74     \t [ 7.03740317 -5.13546999]. \t  -4215.020903821437 \t -0.14473002518929054\n",
            "75     \t [-7.43781506 -0.14536251]. \t  -183.0997841788702 \t -0.14473002518929054\n",
            "76     \t [4.23884251 1.22232799]. \t  -13.618457089658648 \t -0.14473002518929054\n",
            "77     \t [9.54647128 9.218395  ]. \t  -51536.510783866055 \t -0.14473002518929054\n",
            "78     \t [-3.92414716  1.66843292]. \t  -204.42376146817975 \t -0.14473002518929054\n",
            "79     \t [-5.68758126  9.14632012]. \t  -59901.28760036906 \t -0.14473002518929054\n",
            "80     \t [ 0.99525104 -4.28503412]. \t  -2552.949101192402 \t -0.14473002518929054\n",
            "81     \t [ 4.51812308 -2.96445056]. \t  -353.3900566599988 \t -0.14473002518929054\n",
            "82     \t [ 2.73098168 -1.410219  ]. \t  -6.103590738801536 \t -0.14473002518929054\n",
            "83     \t [ 3.5932631  -1.31636857]. \t  -6.757582461633976 \t -0.14473002518929054\n",
            "84     \t [1.25943139 0.81670423]. \t  \u001b[92m-0.0784290628392909\u001b[0m \t -0.0784290628392909\n",
            "85     \t [ 0.9162279  -1.44483782]. \t  -21.24767775102589 \t -0.0784290628392909\n",
            "86     \t [ 5.28731313 -1.26221374]. \t  -27.20900272589987 \t -0.0784290628392909\n",
            "87     \t [-7.69365142  1.92166151]. \t  -530.3451666391649 \t -0.0784290628392909\n",
            "88     \t [ 1.26662928 -1.60145483]. \t  -29.911774845084963 \t -0.0784290628392909\n",
            "89     \t [ 7.25300217 -4.22881872]. \t  -1665.061079179567 \t -0.0784290628392909\n",
            "90     \t [ 3.18895115 -1.56007049]. \t  -10.427498863334982 \t -0.0784290628392909\n",
            "91     \t [ 9.14319148 -0.78701318]. \t  -191.271024830003 \t -0.0784290628392909\n",
            "92     \t [ 8.64980484 -6.85356859]. \t  -14608.311187269232 \t -0.0784290628392909\n",
            "93     \t [ 0.50027978 -0.55060306]. \t  -0.27221251746004616 \t -0.0784290628392909\n",
            "94     \t [-5.34944394  6.9938714 ]. \t  -21331.681049501938 \t -0.0784290628392909\n",
            "95     \t [ 8.44278079 -7.19997955]. \t  -18195.42650662692 \t -0.0784290628392909\n",
            "96     \t [7.94956547 1.96494096]. \t  -48.40004505770033 \t -0.0784290628392909\n",
            "97     \t [ 9.17998145 -4.28924253]. \t  -1592.1130148597142 \t -0.0784290628392909\n",
            "98     \t [-9.98435938  9.97617706]. \t  -87509.89242531608 \t -0.0784290628392909\n",
            "99     \t [-3.64129716  4.08824078]. \t  -2769.7220190796297 \t -0.0784290628392909\n",
            "100    \t [1.1011717 0.6757056]. \t  -0.08093542227958248 \t -0.0784290628392909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "f846378b-f26d-40cf-830e-7697303c9380"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_6 = d2GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.85720303 -3.36040389]. \t  -480.81536490368427 \t -43.91981950896418\n",
            "init   \t [ 6.42458246 -9.16606749]. \t  -52264.36633669738 \t -43.91981950896418\n",
            "init   \t [-7.8468664   1.90104128]. \t  -532.7651696660866 \t -43.91981950896418\n",
            "init   \t [ 0.59634724 -1.62385143]. \t  -43.91981950896418 \t -43.91981950896418\n",
            "init   \t [-3.29184301  2.45038864]. \t  -486.6398213674611 \t -43.91981950896418\n",
            "1      \t [2.90710192 9.80448543]. \t  -71709.54822485936 \t -43.91981950896418\n",
            "2      \t [-9.22684845 -9.49630667]. \t  -71990.69239039435 \t -43.91981950896418\n",
            "3      \t [-7.69566798  9.86414506]. \t  -81924.88656432132 \t -43.91981950896418\n",
            "4      \t [-14.49000459  -1.61254953]. \t  -1015.3825784463077 \t -43.91981950896418\n",
            "5      \t [-1.42589422 -8.37210292]. \t  -40112.73076912865 \t -43.91981950896418\n",
            "6      \t [6.91801255 2.83109097]. \t  -201.085049944819 \t -43.91981950896418\n",
            "7      \t [-19.59825297 -20.37813544]. \t  -1445883.5322964683 \t -43.91981950896418\n",
            "8      \t [9.25715772 8.0152434 ]. \t  -28500.28911684498 \t -43.91981950896418\n",
            "9      \t [-4.90896655 -3.12913489]. \t  -1234.62583398818 \t -43.91981950896418\n",
            "10     \t [2.01537901 3.50509075]. \t  -1018.5721581962224 \t -43.91981950896418\n",
            "11     \t [-1.82649604  6.25302689]. \t  -12816.690262854694 \t -43.91981950896418\n",
            "12     \t [-53.93633628 -56.45952   ]. \t  -82674586.97341692 \t -43.91981950896418\n",
            "13     \t [-9.69959319 -3.64327344]. \t  -2742.095324004481 \t -43.91981950896418\n",
            "14     \t [ 2.88661417 -4.65553919]. \t  -3277.8214178889693 \t -43.91981950896418\n",
            "15     \t [-9.73766146  5.06611173]. \t  -7574.054369880922 \t -43.91981950896418\n",
            "16     \t [ 4.07882326 -0.20600831]. \t  \u001b[92m-41.38233652059988\u001b[0m \t -41.38233652059988\n",
            "17     \t [9.96878176 0.33386517]. \t  -270.402225297586 \t -41.38233652059988\n",
            "18     \t [9.88907131 3.42383547]. \t  -446.5581873408777 \t -41.38233652059988\n",
            "19     \t [-0.309037   0.3724757]. \t  \u001b[92m-2.401573527313572\u001b[0m \t -2.401573527313572\n",
            "20     \t [ 9.94339114 -5.83893736]. \t  -6864.475500501584 \t -2.401573527313572\n",
            "21     \t [-1.40789736 -3.87381463]. \t  -1980.3284163693127 \t -2.401573527313572\n",
            "22     \t [-5.52268268 -6.94329763]. \t  -20826.662449252213 \t -2.401573527313572\n",
            "23     \t [ 6.47365283 -0.10186634]. \t  -113.24069486906149 \t -2.401573527313572\n",
            "24     \t [-0.92096538  0.71057555]. \t  -11.146089933675487 \t -2.401573527313572\n",
            "25     \t [5.88507079 5.6322615 ]. \t  -6650.094762645206 \t -2.401573527313572\n",
            "26     \t [-5.61427526  0.08836312]. \t  -107.13999011846055 \t -2.401573527313572\n",
            "27     \t [-0.52729332 -0.33376025]. \t  -3.45788031331116 \t -2.401573527313572\n",
            "28     \t [-0.02112416 -0.0644817 ]. \t  \u001b[92m-1.0444279594933907\u001b[0m \t -1.0444279594933907\n",
            "29     \t [ 0.67886926 -0.48958899]. \t  \u001b[92m-0.18270511013132607\u001b[0m \t -0.18270511013132607\n",
            "30     \t [ 1.95165392 -0.92110338]. \t  -1.0354821205920615 \t -0.18270511013132607\n",
            "31     \t [2.22604554 0.04247199]. \t  -11.381647173006462 \t -0.18270511013132607\n",
            "32     \t [0.33569442 0.67160449]. \t  -1.0829442104911082 \t -0.18270511013132607\n",
            "33     \t [0.46043721 0.19301025]. \t  -0.5890139329660278 \t -0.18270511013132607\n",
            "34     \t [ 0.50455892 -0.37492265]. \t  -0.3452992605154044 \t -0.18270511013132607\n",
            "35     \t [-2.23936006 -0.75611641]. \t  -33.379910466131996 \t -0.18270511013132607\n",
            "36     \t [ 1.1367388  -0.27255539]. \t  -1.9716412782339923 \t -0.18270511013132607\n",
            "37     \t [ 1.49794773 -1.30191666]. \t  -7.407478668549623 \t -0.18270511013132607\n",
            "38     \t [-5.4178733  5.785831 ]. \t  -10515.893810317048 \t -0.18270511013132607\n",
            "39     \t [-0.44859786  1.71351337]. \t  -82.00482592882513 \t -0.18270511013132607\n",
            "40     \t [ 1.58969771 -0.77234865]. \t  -0.6624103433081502 \t -0.18270511013132607\n",
            "41     \t [-1.37569669 -0.11997504]. \t  -9.58908936230584 \t -0.18270511013132607\n",
            "42     \t [ 4.67892651 -1.93448782]. \t  -29.276831331822052 \t -0.18270511013132607\n",
            "43     \t [-2.06431944  2.74454778]. \t  -596.2230506718907 \t -0.18270511013132607\n",
            "44     \t [ 7.74852284 -9.71120688]. \t  -65470.96414442118 \t -0.18270511013132607\n",
            "45     \t [-0.3398488   1.92037519]. \t  -120.85401780007317 \t -0.18270511013132607\n",
            "46     \t [-9.70531014 -0.41807056]. \t  -316.8047327360364 \t -0.18270511013132607\n",
            "47     \t [5.79106982 3.37893875]. \t  -603.908252005958 \t -0.18270511013132607\n",
            "48     \t [ 2.4933684  -8.68377221]. \t  -44001.31737096439 \t -0.18270511013132607\n",
            "49     \t [ 3.36906149 -0.59688918]. \t  -19.726523072711384 \t -0.18270511013132607\n",
            "50     \t [0.29509901 0.63965042]. \t  -1.0443751101793834 \t -0.18270511013132607\n",
            "51     \t [-3.79388167  7.61725185]. \t  -28745.721149029167 \t -0.18270511013132607\n",
            "52     \t [0.52260077 0.61105445]. \t  -0.32841826338667945 \t -0.18270511013132607\n",
            "53     \t [0.72380351 0.45289169]. \t  -0.27295152970918507 \t -0.18270511013132607\n",
            "54     \t [ 8.02164712 -2.67265804]. \t  -127.7928233851124 \t -0.18270511013132607\n",
            "55     \t [ 6.92146635 -2.07353916]. \t  -40.69286965924218 \t -0.18270511013132607\n",
            "56     \t [ 9.40373491 -1.45238791]. \t  -124.38858904300906 \t -0.18270511013132607\n",
            "57     \t [0.92604478 0.63613875]. \t  \u001b[92m-0.03270704534357148\u001b[0m \t -0.03270704534357148\n",
            "58     \t [4.55449115 1.57638214]. \t  -12.979638220860418 \t -0.03270704534357148\n",
            "59     \t [ 1.48185466 -0.86420782]. \t  -0.23246502420954696 \t -0.03270704534357148\n",
            "60     \t [3.04362669 0.72259344]. \t  -12.171163950885502 \t -0.03270704534357148\n",
            "61     \t [ 7.48668909 -2.11199074]. \t  -46.19168681964074 \t -0.03270704534357148\n",
            "62     \t [ 8.96861704 -2.04719604]. \t  -64.18704220289794 \t -0.03270704534357148\n",
            "63     \t [ 5.89567934 -2.16578438]. \t  -48.26599718027012 \t -0.03270704534357148\n",
            "64     \t [-9.71885462  3.02903771]. \t  -1690.6306426150086 \t -0.03270704534357148\n",
            "65     \t [-9.37022401 -5.04390326]. \t  -7368.183318605593 \t -0.03270704534357148\n",
            "66     \t [7.18587785 1.24681412]. \t  -71.5054688868222 \t -0.03270704534357148\n",
            "67     \t [ 2.94638521 -1.49718838]. \t  -8.511683431768233 \t -0.03270704534357148\n",
            "68     \t [-2.69311824  1.32085629]. \t  -90.08427328645001 \t -0.03270704534357148\n",
            "69     \t [-2.64260059  1.02280298]. \t  -58.10619491706118 \t -0.03270704534357148\n",
            "70     \t [8.73397685 7.35943592]. \t  -19895.599784253176 \t -0.03270704534357148\n",
            "71     \t [-7.45020871 -5.3634945 ]. \t  -8517.338851062503 \t -0.03270704534357148\n",
            "72     \t [8.54124136 6.9162167 ]. \t  -15239.051709360609 \t -0.03270704534357148\n",
            "73     \t [ 1.31820967 -0.04303052]. \t  -3.5571116527695414 \t -0.03270704534357148\n",
            "74     \t [ 1.05122142 -0.74109144]. \t  \u001b[92m-0.00708151029528118\u001b[0m \t -0.00708151029528118\n",
            "75     \t [-5.54358842  5.51304305]. \t  -8842.3887107496 \t -0.00708151029528118\n",
            "76     \t [-1.09631494 -0.62532173]. \t  -11.451080009705871 \t -0.00708151029528118\n",
            "77     \t [4.66219434 9.80236653]. \t  -70333.85057301067 \t -0.00708151029528118\n",
            "78     \t [ 2.24003725 -8.95614477]. \t  -50046.53950081282 \t -0.00708151029528118\n",
            "79     \t [2.08556743 0.6680644 ]. \t  -4.024703356773264 \t -0.00708151029528118\n",
            "80     \t [-0.70520444  6.38911094]. \t  -13564.859667105267 \t -0.00708151029528118\n",
            "81     \t [ 0.76413099 -0.43252571]. \t  -0.3597936617961455 \t -0.00708151029528118\n",
            "82     \t [-8.11969226  2.93973397]. \t  -1373.8712307481312 \t -0.00708151029528118\n",
            "83     \t [ 3.7895797  -1.06406981]. \t  -12.433557590511274 \t -0.00708151029528118\n",
            "84     \t [-8.6698848  -1.14240882]. \t  -347.9871088865486 \t -0.00708151029528118\n",
            "85     \t [-1.97214442 -6.10811556]. \t  -11740.98045602341 \t -0.00708151029528118\n",
            "86     \t [ 3.03353766 -1.29445199]. \t  -4.337109278540841 \t -0.00708151029528118\n",
            "87     \t [8.90445694 8.69235288]. \t  -40509.58705670207 \t -0.00708151029528118\n",
            "88     \t [-3.62428043  3.07093999]. \t  -1032.5906525321786 \t -0.00708151029528118\n",
            "89     \t [-4.53268014  0.81560817]. \t  -99.36274943872641 \t -0.00708151029528118\n",
            "90     \t [-3.02485719 -0.02365569]. \t  -34.51254144719541 \t -0.00708151029528118\n",
            "91     \t [-3.43540112  1.15080945]. \t  -93.70595834481217 \t -0.00708151029528118\n",
            "92     \t [ 7.61381756 -5.9672286 ]. \t  -8134.124752290816 \t -0.00708151029528118\n",
            "93     \t [-2.09951624  0.1038914 ]. \t  -18.605157588421953 \t -0.00708151029528118\n",
            "94     \t [-0.05444533  5.13783057]. \t  -5587.156302415063 \t -0.00708151029528118\n",
            "95     \t [6.57010489 8.77670648]. \t  -43538.24471272127 \t -0.00708151029528118\n",
            "96     \t [-8.81909582 -6.38636263]. \t  -16437.245746249155 \t -0.00708151029528118\n",
            "97     \t [-7.38079581 -5.17206868]. \t  -7483.323761270523 \t -0.00708151029528118\n",
            "98     \t [ 1.30913725 -0.76022489]. \t  -0.1425391002847861 \t -0.00708151029528118\n",
            "99     \t [-3.06810703  6.60137068]. \t  -16297.396964435744 \t -0.00708151029528118\n",
            "100    \t [-7.12382562 -8.06620731]. \t  -37741.79479062148 \t -0.00708151029528118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "e1bc7732-3a11-4c98-c0c8-a6268ed8f591"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_7 = d2GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.9467253  -3.95286781]. \t  -2981.1801714787616 \t -297.17705346549894\n",
            "init   \t [-8.75927171 -0.80279317]. \t  -297.17705346549894 \t -297.17705346549894\n",
            "init   \t [6.70506769 8.53994095]. \t  -38761.39667089972 \t -297.17705346549894\n",
            "init   \t [4.53977969 5.36992444]. \t  -5658.63331787327 \t -297.17705346549894\n",
            "init   \t [-4.61589867  2.88058584]. \t  -931.3893916140455 \t -297.17705346549894\n",
            "1      \t [ 2.44677649 -4.93762112]. \t  -4291.960403637314 \t -297.17705346549894\n",
            "2      \t [-16.04639688  -7.48820171]. \t  -33157.32991792824 \t -297.17705346549894\n",
            "3      \t [ 9.83790456 -9.57130198]. \t  -60200.563023412855 \t -297.17705346549894\n",
            "4      \t [-7.89992531  9.8439324 ]. \t  -81449.7775076618 \t -297.17705346549894\n",
            "5      \t [ 9.0936247  -0.78669084]. \t  \u001b[92m-188.93582542124247\u001b[0m \t -188.93582542124247\n",
            "6      \t [-28.95089568 -20.19941257]. \t  -1428891.057833231 \t -188.93582542124247\n",
            "7      \t [ -0.25006192 -11.32647782]. \t  -131923.07441803042 \t -188.93582542124247\n",
            "8      \t [ -7.36104312 -13.27926308]. \t  -259325.63694486461 \t -188.93582542124247\n",
            "9      \t [0.42210597 0.91487768]. \t  \u001b[92m-3.4684505922616853\u001b[0m \t -3.4684505922616853\n",
            "10     \t [-1.254296   8.6667351]. \t  -45896.790383750355 \t -3.4684505922616853\n",
            "11     \t [-10.4247977   -7.89226304]. \t  -36580.75799832886 \t -3.4684505922616853\n",
            "12     \t [-2.38352764 -1.96717919]. \t  -216.40319553942612 \t -3.4684505922616853\n",
            "13     \t [-9.80709008  3.90771205]. \t  -3372.637444253527 \t -3.4684505922616853\n",
            "14     \t [ 4.71344565 -0.33743311]. \t  -54.033108345663095 \t -3.4684505922616853\n",
            "15     \t [9.8109182  3.99815686]. \t  -1059.72788314995 \t -3.4684505922616853\n",
            "16     \t [-2.26332542 -6.21468906]. \t  -12653.707976389882 \t -3.4684505922616853\n",
            "17     \t [ 6.98259511 -4.18475303]. \t  -1608.4676539918344 \t -3.4684505922616853\n",
            "18     \t [-0.15626683  3.69147149]. \t  -1503.9740253299706 \t -3.4684505922616853\n",
            "19     \t [1.7159579  0.72907851]. \t  \u001b[92m-1.3650140241199245\u001b[0m \t -1.3650140241199245\n",
            "20     \t [ 0.13434345 -0.54955838]. \t  \u001b[92m-1.1905699824511415\u001b[0m \t -1.1905699824511415\n",
            "21     \t [2.69085714 1.60315214]. \t  -14.857495600738728 \t -1.1905699824511415\n",
            "22     \t [-0.28270835  0.18519308]. \t  -1.8921659111897258 \t -1.1905699824511415\n",
            "23     \t [0.50977244 0.19575997]. \t  \u001b[92m-0.6155236750725124\u001b[0m \t -0.6155236750725124\n",
            "24     \t [ 0.37434513 -0.86536852]. \t  -2.915410207849585 \t -0.6155236750725124\n",
            "25     \t [-5.3965394   6.31608627]. \t  -14553.008828648124 \t -0.6155236750725124\n",
            "26     \t [ 3.92501606 -9.38665827]. \t  -59378.78756091592 \t -0.6155236750725124\n",
            "27     \t [2.08254843 0.19597094]. \t  -9.217890656245293 \t -0.6155236750725124\n",
            "28     \t [1.38543728 0.57502435]. \t  -1.1972940569419614 \t -0.6155236750725124\n",
            "29     \t [5.56335407 1.57810364]. \t  -21.502887077306394 \t -0.6155236750725124\n",
            "30     \t [-6.25798902 -8.33121493]. \t  -42146.928669870766 \t -0.6155236750725124\n",
            "31     \t [-5.90317276  0.16378068]. \t  -118.62122676677319 \t -0.6155236750725124\n",
            "32     \t [ 0.0161191  -0.11549953]. \t  -0.968244709594219 \t -0.6155236750725124\n",
            "33     \t [6.76132498 2.00050251]. \t  -36.281450329255904 \t -0.6155236750725124\n",
            "34     \t [-0.02663715 -0.6129153 ]. \t  -2.2644506952403427 \t -0.6155236750725124\n",
            "35     \t [-2.73705227  0.30995725]. \t  -31.125976246648804 \t -0.6155236750725124\n",
            "36     \t [6.85960443 0.39748222]. \t  -119.97289465896506 \t -0.6155236750725124\n",
            "37     \t [ 2.66939544 -0.45037961]. \t  -13.035663810409968 \t -0.6155236750725124\n",
            "38     \t [2.93973629 9.84939109]. \t  -73027.86879967064 \t -0.6155236750725124\n",
            "39     \t [ 1.23186749 -0.74096685]. \t  \u001b[92m-0.089569407601569\u001b[0m \t -0.089569407601569\n",
            "40     \t [1.88203346 1.34558249]. \t  -6.827275651919434 \t -0.089569407601569\n",
            "41     \t [ 1.18905112 -1.08320137]. \t  -2.715812600085744 \t -0.089569407601569\n",
            "42     \t [ 1.25759336 -0.79583816]. \t  \u001b[92m-0.06652081148694637\u001b[0m \t -0.06652081148694637\n",
            "43     \t [ 2.52275686 -1.14237522]. \t  -2.334025939058194 \t -0.06652081148694637\n",
            "44     \t [4.17833734 1.45356911]. \t  -10.106319678245889 \t -0.06652081148694637\n",
            "45     \t [ 9.64217477 -5.21011216]. \t  -4061.6397745707504 \t -0.06652081148694637\n",
            "46     \t [ 2.90744647 -1.50894482]. \t  -9.059502400229494 \t -0.06652081148694637\n",
            "47     \t [ 4.09436519 -1.66270756]. \t  -13.692556912772787 \t -0.06652081148694637\n",
            "48     \t [ 6.26972533 -8.79393908]. \t  -44071.11153816772 \t -0.06652081148694637\n",
            "49     \t [4.04168828 5.71766789]. \t  -7534.875876145788 \t -0.06652081148694637\n",
            "50     \t [6.18605508 2.9315531 ]. \t  -268.98106539191855 \t -0.06652081148694637\n",
            "51     \t [0.09078931 0.49741316]. \t  -1.1531775245016371 \t -0.06652081148694637\n",
            "52     \t [7.59219359 1.98635129]. \t  -43.635805491681175 \t -0.06652081148694637\n",
            "53     \t [6.23459498 1.65460092]. \t  -28.553713076247945 \t -0.06652081148694637\n",
            "54     \t [4.74527053 0.95155461]. \t  -31.247967251331048 \t -0.06652081148694637\n",
            "55     \t [ 1.99180448 -0.76999519]. \t  -2.2830103504715926 \t -0.06652081148694637\n",
            "56     \t [ 0.77891869 -0.38442418]. \t  -0.5161406548588461 \t -0.06652081148694637\n",
            "57     \t [-1.01978609  0.33183321]. \t  -7.154798520552767 \t -0.06652081148694637\n",
            "58     \t [0.44778028 0.61779327]. \t  -0.5040987671120454 \t -0.06652081148694637\n",
            "59     \t [ 4.91041875 -1.98267389]. \t  -32.71493808932828 \t -0.06652081148694637\n",
            "60     \t [ 3.15577593 -0.98434366]. \t  -7.6139844969359824 \t -0.06652081148694637\n",
            "61     \t [ 1.49834958 -1.02360006]. \t  -0.9615634306300178 \t -0.06652081148694637\n",
            "62     \t [-2.04908506  1.24041281]. \t  -61.855498879983806 \t -0.06652081148694637\n",
            "63     \t [ 9.65168305 -3.44245725]. \t  -469.61957102685653 \t -0.06652081148694637\n",
            "64     \t [4.70595523 1.59805356]. \t  -14.05666144741788 \t -0.06652081148694637\n",
            "65     \t [ 9.65399849 -2.64245281]. \t  -112.06311789377975 \t -0.06652081148694637\n",
            "66     \t [-7.35504328 -2.27957323]. \t  -699.7863099239688 \t -0.06652081148694637\n",
            "67     \t [0.96771099 0.97896825]. \t  -1.8024217739758621 \t -0.06652081148694637\n",
            "68     \t [3.58483332 1.70372442]. \t  -16.542785554500107 \t -0.06652081148694637\n",
            "69     \t [ 1.4703293  -0.89485366]. \t  -0.2556348756579331 \t -0.06652081148694637\n",
            "70     \t [-7.09636232  3.45590826]. \t  -1985.4394576424781 \t -0.06652081148694637\n",
            "71     \t [7.5867087  0.21594955]. \t  -155.68802635951303 \t -0.06652081148694637\n",
            "72     \t [-4.27597599 -0.95995858]. \t  -102.72065945152684 \t -0.06652081148694637\n",
            "73     \t [-4.54631698  7.84714993]. \t  -32646.281973896577 \t -0.06652081148694637\n",
            "74     \t [ 0.87445829 -0.61735948]. \t  \u001b[92m-0.04093518822293769\u001b[0m \t -0.04093518822293769\n",
            "75     \t [-3.04732377 -3.62317595]. \t  -1733.6106727819033 \t -0.04093518822293769\n",
            "76     \t [-0.84598952 -1.64004297]. \t  -80.92066587135855 \t -0.04093518822293769\n",
            "77     \t [-5.43939435 -1.20356317]. \t  -180.46102889329177 \t -0.04093518822293769\n",
            "78     \t [-2.90231269 -0.91674015]. \t  -57.23834634706157 \t -0.04093518822293769\n",
            "79     \t [-1.63211376 -0.57282963]. \t  -17.401401070184033 \t -0.04093518822293769\n",
            "80     \t [ 5.13145893 -6.98450545]. \t  -17105.591663517112 \t -0.04093518822293769\n",
            "81     \t [0.99500481 3.75849543]. \t  -1485.9503694431025 \t -0.04093518822293769\n",
            "82     \t [-6.86179769 -6.00034947]. \t  -12502.820114758297 \t -0.04093518822293769\n",
            "83     \t [3.127693   1.03778417]. \t  -6.423264896797338 \t -0.04093518822293769\n",
            "84     \t [ 5.39378092 -7.0924974 ]. \t  -18150.434280823014 \t -0.04093518822293769\n",
            "85     \t [4.32363851 5.40727383]. \t  -5876.267471595092 \t -0.04093518822293769\n",
            "86     \t [3.16517791 4.06931018]. \t  -1799.0994691062533 \t -0.04093518822293769\n",
            "87     \t [-2.10379631 -5.07196877]. \t  -5745.594840224066 \t -0.04093518822293769\n",
            "88     \t [-1.65719838  1.17430137]. \t  -46.04808128627561 \t -0.04093518822293769\n",
            "89     \t [6.63753775 3.99835237]. \t  -1315.6183299249508 \t -0.04093518822293769\n",
            "90     \t [-9.46301857  0.66459733]. \t  -323.5706524987455 \t -0.04093518822293769\n",
            "91     \t [ 4.74676856 -2.01197137]. \t  -36.473748104095634 \t -0.04093518822293769\n",
            "92     \t [1.41348014 1.0338677 ]. \t  -1.2201424340980065 \t -0.04093518822293769\n",
            "93     \t [0.08800363 9.77315282]. \t  -72917.78441875978 \t -0.04093518822293769\n",
            "94     \t [ 5.33331267 -1.38215934]. \t  -23.35341821092686 \t -0.04093518822293769\n",
            "95     \t [-2.55040769 -1.69504259]. \t  -150.27739847390453 \t -0.04093518822293769\n",
            "96     \t [2.32485898 1.07314851]. \t  -1.7561812851922909 \t -0.04093518822293769\n",
            "97     \t [ 1.37175544 -1.3771187 ]. \t  -11.862198666920797 \t -0.04093518822293769\n",
            "98     \t [ 5.61987587 -7.25406049]. \t  -19870.792210242922 \t -0.04093518822293769\n",
            "99     \t [-6.33179118  5.3289402 ]. \t  -8023.791023469346 \t -0.04093518822293769\n",
            "100    \t [-2.64019465  8.15832871]. \t  -36873.09521062235 \t -0.04093518822293769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "4f82e38a-54e5-4667-fd03-b4f43a71450e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_8 = d2GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-7.8100025  -9.15569971]. \t  -61652.61989589963 \t -123.0699213709516\n",
            "init   \t [ 1.9938517 -4.0025859]. \t  -1806.696586806874 \t -123.0699213709516\n",
            "init   \t [ 5.93223564 -2.6760775 ]. \t  -165.12946982275756 \t -123.0699213709516\n",
            "init   \t [ 7.61216688 -4.04316733]. \t  -1301.9580077172761 \t -123.0699213709516\n",
            "init   \t [-3.41793472  1.37433832]. \t  -123.0699213709516 \t -123.0699213709516\n",
            "1      \t [-98.81463622 -92.74238158]. \t  -598667007.054983 \t -123.0699213709516\n",
            "2      \t [7.33725011 6.34690223]. \t  -10765.158736519235 \t -123.0699213709516\n",
            "3      \t [-6.99148152  9.31430956]. \t  -65227.40102136099 \t -123.0699213709516\n",
            "4      \t [0.47559161 8.55656073]. \t  -42605.34024544054 \t -123.0699213709516\n",
            "5      \t [  1.48689111 -10.76449601]. \t  -106041.28301651844 \t -123.0699213709516\n",
            "6      \t [-9.721135    0.63451534]. \t  -336.55101039217925 \t -123.0699213709516\n",
            "7      \t [ 7.86685851 -9.65405122]. \t  -63796.325713701866 \t -123.0699213709516\n",
            "8      \t [-15.02212417  -9.80071859]. \t  -86062.6257008048 \t -123.0699213709516\n",
            "9      \t [-3.84702547 -4.02290713]. \t  -2646.4865780007854 \t -123.0699213709516\n",
            "10     \t [2.55766053 2.09985064]. \t  \u001b[92m-80.8286747929862\u001b[0m \t -80.8286747929862\n",
            "11     \t [-157.47204314 -154.00426315]. \t  -4530040731.778163 \t -80.8286747929862\n",
            "12     \t [-11.76815469  -4.3178455 ]. \t  -4975.955908370903 \t -80.8286747929862\n",
            "13     \t [9.32063891 1.10185427]. \t  -164.2454074862331 \t -80.8286747929862\n",
            "14     \t [-10.93490528   5.15056325]. \t  -8332.268364131181 \t -80.8286747929862\n",
            "15     \t [-5.93291083  4.05033361]. \t  -3050.154259880464 \t -80.8286747929862\n",
            "16     \t [-0.67603058  3.45860831]. \t  -1213.126473428567 \t -80.8286747929862\n",
            "17     \t [-2.97296382 -8.17278991]. \t  -37314.11878500217 \t -80.8286747929862\n",
            "18     \t [5.81963696 1.69341152]. \t  \u001b[92m-23.24313088311752\u001b[0m \t -23.24313088311752\n",
            "19     \t [-7.81452295 -3.02346961]. \t  -1439.8297642178943 \t -23.24313088311752\n",
            "20     \t [ 3.45191722 -0.2698732 ]. \t  -27.874530699759955 \t -23.24313088311752\n",
            "21     \t [5.28988091 9.87566309]. \t  -72041.8584116926 \t -23.24313088311752\n",
            "22     \t [-0.86780242 -1.67731768]. \t  -87.84812617121251 \t -23.24313088311752\n",
            "23     \t [-5.70559701 -0.1133256 ]. \t  -110.66022693567565 \t -23.24313088311752\n",
            "24     \t [ 6.01091204 -0.06423932]. \t  -97.17306129123519 \t -23.24313088311752\n",
            "25     \t [9.96127562 9.88691337]. \t  -68931.10822656454 \t -23.24313088311752\n",
            "26     \t [6.56689966 2.27800672]. \t  -60.04893611096429 \t -23.24313088311752\n",
            "27     \t [ 4.31035597 -6.45856125]. \t  -12529.537238076215 \t -23.24313088311752\n",
            "28     \t [4.51189193 3.22305507]. \t  -541.3867332844394 \t -23.24313088311752\n",
            "29     \t [ 1.21617242 -0.28224862]. \t  \u001b[92m-2.280568347954156\u001b[0m \t -2.280568347954156\n",
            "30     \t [-0.3596398   0.21727254]. \t  \u001b[92m-2.260951379250973\u001b[0m \t -2.260951379250973\n",
            "31     \t [1.28727522 0.13986241]. \t  -3.1982950231029417 \t -2.260951379250973\n",
            "32     \t [-0.8861657   0.18598234]. \t  -5.382987519614326 \t -2.260951379250973\n",
            "33     \t [5.60323033 1.17893089]. \t  -37.13374314994377 \t -2.260951379250973\n",
            "34     \t [9.67357193 3.56869834]. \t  -574.3619453946935 \t -2.260951379250973\n",
            "35     \t [-2.64936754  5.75183021]. \t  -9484.730710461623 \t -2.260951379250973\n",
            "36     \t [-0.11224753  0.1229503 ]. \t  \u001b[92m-1.277696284017129\u001b[0m \t -1.277696284017129\n",
            "37     \t [-1.75997782  0.11864031]. \t  -14.012287256721972 \t -1.277696284017129\n",
            "38     \t [ 9.58787667 -1.49820867]. \t  -125.74344140498607 \t -1.277696284017129\n",
            "39     \t [ 1.63582599 -0.160976  ]. \t  -5.422383185662185 \t -1.277696284017129\n",
            "40     \t [ 9.7669401  -5.86685853]. \t  -7056.125746915859 \t -1.277696284017129\n",
            "41     \t [ 4.72147872 -1.72425438]. \t  -16.84882935049343 \t -1.277696284017129\n",
            "42     \t [ 3.13745005 -0.87479751]. \t  -9.733003784786302 \t -1.277696284017129\n",
            "43     \t [ 0.93529965 -0.25051774]. \t  -1.3156778895830032 \t -1.277696284017129\n",
            "44     \t [0.26388089 0.44404726]. \t  \u001b[92m-0.5759188141122675\u001b[0m \t -0.5759188141122675\n",
            "45     \t [0.19549425 0.15967268]. \t  -0.6889920372295295 \t -0.5759188141122675\n",
            "46     \t [ 0.56341775 -1.07579026]. \t  -6.3242283104089605 \t -0.5759188141122675\n",
            "47     \t [ 0.33371736 -0.56558523]. \t  -0.631273038861805 \t -0.5759188141122675\n",
            "48     \t [ 1.25359233 -0.54627356]. \t  -0.9269836002471781 \t -0.5759188141122675\n",
            "49     \t [2.69184983 5.03424066]. \t  -4609.961593003638 \t -0.5759188141122675\n",
            "50     \t [-3.23428689 -0.57425663]. \t  -48.25298214641066 \t -0.5759188141122675\n",
            "51     \t [5.06834609 5.96825386]. \t  -8773.955635900966 \t -0.5759188141122675\n",
            "52     \t [2.45394383 1.13319004]. \t  -2.140079578398679 \t -0.5759188141122675\n",
            "53     \t [1.00927738 0.78270075]. \t  \u001b[92m-0.09336657669638086\u001b[0m \t -0.09336657669638086\n",
            "54     \t [1.20506414 0.52160345]. \t  -0.9156918883402045 \t -0.09336657669638086\n",
            "55     \t [ 6.13223941 -8.32988771]. \t  -35214.04422492559 \t -0.09336657669638086\n",
            "56     \t [-6.13893734 -1.59182807]. \t  -302.14783823643666 \t -0.09336657669638086\n",
            "57     \t [ 0.46456789 -0.3258581 ]. \t  -0.41389811919616726 \t -0.09336657669638086\n",
            "58     \t [0.39177332 0.47859436]. \t  -0.37873951816249485 \t -0.09336657669638086\n",
            "59     \t [-6.16955622  0.44482737]. \t  -137.60883826786048 \t -0.09336657669638086\n",
            "60     \t [3.21586681 1.25857388]. \t  -4.914645044663584 \t -0.09336657669638086\n",
            "61     \t [ 3.72617784 -1.24365415]. \t  -8.232984507080612 \t -0.09336657669638086\n",
            "62     \t [-7.01656824  4.39875358]. \t  -4243.920950143667 \t -0.09336657669638086\n",
            "63     \t [-0.23611372  2.32528763]. \t  -245.73469272243707 \t -0.09336657669638086\n",
            "64     \t [-0.10455221  1.37739503]. \t  -31.624224864609662 \t -0.09336657669638086\n",
            "65     \t [9.06425658 7.71589085]. \t  -24267.53443867601 \t -0.09336657669638086\n",
            "66     \t [2.41021695 1.00190576]. \t  -2.312863857404778 \t -0.09336657669638086\n",
            "67     \t [-6.69536845  2.63555047]. \t  -906.9189730934511 \t -0.09336657669638086\n",
            "68     \t [-5.39998576 -6.84540931]. \t  -19690.208465023108 \t -0.09336657669638086\n",
            "69     \t [4.31523028 4.55266855]. \t  -2769.505070887002 \t -0.09336657669638086\n",
            "70     \t [ 1.0094345  -0.76651103]. \t  \u001b[92m-0.05496475915320519\u001b[0m \t -0.05496475915320519\n",
            "71     \t [2.63980749 1.18030711]. \t  -2.7318592802003865 \t -0.05496475915320519\n",
            "72     \t [-2.21950213  4.78261049]. \t  -4611.883322732063 \t -0.05496475915320519\n",
            "73     \t [ 2.78685118 -0.15378793]. \t  -18.203102605247672 \t -0.05496475915320519\n",
            "74     \t [1.00373524 1.87651118]. \t  -72.93551000134227 \t -0.05496475915320519\n",
            "75     \t [-4.09492747 -2.35923518]. \t  -489.6757853579257 \t -0.05496475915320519\n",
            "76     \t [ 2.04095505 -0.91198357]. \t  -1.3686406872156445 \t -0.05496475915320519\n",
            "77     \t [3.66025196 0.74747573]. \t  -20.008726306035815 \t -0.05496475915320519\n",
            "78     \t [ 9.87354977 -8.45660868]. \t  -35539.20050149259 \t -0.05496475915320519\n",
            "79     \t [-0.61430943 -9.6560787 ]. \t  -70010.95138703822 \t -0.05496475915320519\n",
            "80     \t [ 0.45204368 -0.60065918]. \t  -0.4455589139961583 \t -0.05496475915320519\n",
            "81     \t [1.13377697 0.69766944]. \t  -0.06928311330863457 \t -0.05496475915320519\n",
            "82     \t [-5.28630706  7.33387952]. \t  -25513.321947840155 \t -0.05496475915320519\n",
            "83     \t [ 7.09723595 -3.04258965]. \t  -297.89342045477264 \t -0.05496475915320519\n",
            "84     \t [ 7.18884261 -1.81663248]. \t  -38.99452072330875 \t -0.05496475915320519\n",
            "85     \t [-1.53502454 -5.06789235]. \t  -5603.688678125964 \t -0.05496475915320519\n",
            "86     \t [ 0.93232909 -0.2382274 ]. \t  -1.3455264914714893 \t -0.05496475915320519\n",
            "87     \t [-4.37081265 -0.80027575]. \t  -92.72894595951811 \t -0.05496475915320519\n",
            "88     \t [1.96352415 0.98137928]. \t  -0.9311633886251407 \t -0.05496475915320519\n",
            "89     \t [ 1.52017907 -1.78452586]. \t  -47.29397676328524 \t -0.05496475915320519\n",
            "90     \t [ 0.66501211 -3.19880746]. \t  -784.1705345699172 \t -0.05496475915320519\n",
            "91     \t [-6.54058379  8.44174   ]. \t  -44498.522389758466 \t -0.05496475915320519\n",
            "92     \t [-0.39591866  9.01209183]. \t  -53030.154493987946 \t -0.05496475915320519\n",
            "93     \t [ 7.5702067  -6.56973839]. \t  -12447.148877440855 \t -0.05496475915320519\n",
            "94     \t [3.22035406 7.00700377]. \t  -18045.75399055018 \t -0.05496475915320519\n",
            "95     \t [-4.18805224  5.0527217 ]. \t  -6131.608945360351 \t -0.05496475915320519\n",
            "96     \t [-9.842156   -3.27093426]. \t  -2069.4497773451208 \t -0.05496475915320519\n",
            "97     \t [ 9.86423471 -7.70892022]. \t  -23836.50570445349 \t -0.05496475915320519\n",
            "98     \t [ 2.08086811 -1.21197818]. \t  -2.636879467086902 \t -0.05496475915320519\n",
            "99     \t [-0.2734736  -5.50592384]. \t  -7420.184072100605 \t -0.05496475915320519\n",
            "100    \t [ 1.36835905 -1.26158467]. \t  -6.722923850036937 \t -0.05496475915320519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "f3216d61-3d3e-4194-bd44-75f8835227df"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_9 = d2GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.44557117 -0.23843202]. \t  -28.18355402366269 \t -28.18355402366269\n",
            "init   \t [ 6.50990348 -9.37107225]. \t  -57236.27240949214 \t -28.18355402366269\n",
            "init   \t [6.16099927 1.31234839]. \t  -41.39446954533859 \t -28.18355402366269\n",
            "init   \t [-4.04755003 -9.06608559]. \t  -56766.42271365435 \t -28.18355402366269\n",
            "init   \t [ 9.81254799 -9.86348534]. \t  -68353.23444364112 \t -28.18355402366269\n",
            "1      \t [-8.12773813  6.26616826]. \t  -15102.348898507695 \t -28.18355402366269\n",
            "2      \t [3.58702664 9.74831565]. \t  -69550.52203005194 \t -28.18355402366269\n",
            "3      \t [-9.43477705 -1.75785015]. \t  -596.5317353689182 \t -28.18355402366269\n",
            "4      \t [-10.38572712 -14.10922672]. \t  -333917.0852592331 \t -28.18355402366269\n",
            "5      \t [-25.12900824 -28.96709672]. \t  -5803242.571646073 \t -28.18355402366269\n",
            "6      \t [-2.43872094 -1.89096098]. \t  -195.7682062944888 \t -28.18355402366269\n",
            "7      \t [-19.88480669 -22.42372734]. \t  -2103867.5398021336 \t -28.18355402366269\n",
            "8      \t [-1.40064521  4.59220821]. \t  -3803.7422899005496 \t -28.18355402366269\n",
            "9      \t [-12.44354077  -7.30301258]. \t  -28555.84010743045 \t -28.18355402366269\n",
            "10     \t [ 9.75128696 -3.02170619]. \t  -221.4296353439725 \t -28.18355402366269\n",
            "11     \t [ 1.73519405 -5.07561161]. \t  -4958.324483215532 \t -28.18355402366269\n",
            "12     \t [9.42812131 5.7332353 ]. \t  -6413.082723731119 \t -28.18355402366269\n",
            "13     \t [-3.07662798  9.01497684]. \t  -54874.09859199814 \t -28.18355402366269\n",
            "14     \t [-5.7676168   1.41660456]. \t  -237.14257591453995 \t -28.18355402366269\n",
            "15     \t [ 6.09954382 -2.89800788]. \t  -254.87217861802392 \t -28.18355402366269\n",
            "16     \t [3.50714316 4.17023192]. \t  -1962.47764474379 \t -28.18355402366269\n",
            "17     \t [  1.07573112 -10.65694261]. \t  -102210.88318237649 \t -28.18355402366269\n",
            "18     \t [9.31253201 0.86788036]. \t  -190.96856354894862 \t -28.18355402366269\n",
            "19     \t [-5.79459426 -4.0505999 ]. \t  -3027.524669080955 \t -28.18355402366269\n",
            "20     \t [ 0.33480144 -0.28157673]. \t  \u001b[92m-0.5046035264236911\u001b[0m \t -0.5046035264236911\n",
            "21     \t [1.5959279  0.65238208]. \t  -1.4643551669147068 \t -0.5046035264236911\n",
            "22     \t [ 0.8458072  -0.12116015]. \t  -1.3569489002395292 \t -0.5046035264236911\n",
            "23     \t [0.83672598 0.72697099]. \t  \u001b[92m-0.12367647103295268\u001b[0m \t -0.12367647103295268\n",
            "24     \t [0.93174698 0.88746431]. \t  -0.8326855293599876 \t -0.12367647103295268\n",
            "25     \t [-1.99377345  1.00092489]. \t  -40.92228735256559 \t -0.12367647103295268\n",
            "26     \t [-9.60473065  2.21670449]. \t  -867.6879617370829 \t -0.12367647103295268\n",
            "27     \t [1.06348619 0.22995532]. \t  -1.838513644398887 \t -0.12367647103295268\n",
            "28     \t [ 0.26527171 -0.06387526]. \t  -0.6720384241412437 \t -0.12367647103295268\n",
            "29     \t [-0.14794848  0.42494163]. \t  -1.8361498491509356 \t -0.12367647103295268\n",
            "30     \t [-8.56128461 -8.9134474 ]. \t  -56177.37173959819 \t -0.12367647103295268\n",
            "31     \t [ 0.64558148 -0.95083686]. \t  -2.8288898956337603 \t -0.12367647103295268\n",
            "32     \t [0.29252615 0.73987384]. \t  -1.7878912861009604 \t -0.12367647103295268\n",
            "33     \t [8.63124405 9.41658793]. \t  -56986.396973058334 \t -0.12367647103295268\n",
            "34     \t [-2.23039425 -4.57833756]. \t  -3909.365044106635 \t -0.12367647103295268\n",
            "35     \t [2.69193457 0.81412992]. \t  -6.596300611156385 \t -0.12367647103295268\n",
            "36     \t [ 7.7534389  -4.87385121]. \t  -3206.5945659035438 \t -0.12367647103295268\n",
            "37     \t [ 2.09538573 -1.23213011]. \t  -2.970468638494177 \t -0.12367647103295268\n",
            "38     \t [ 6.21004765 -0.33326829]. \t  -98.85478029240848 \t -0.12367647103295268\n",
            "39     \t [ 2.6868747  -1.19432516]. \t  -2.9006253887634355 \t -0.12367647103295268\n",
            "40     \t [-4.3517255  -0.64133564]. \t  -82.18872652515466 \t -0.12367647103295268\n",
            "41     \t [-0.5379206  -0.46630239]. \t  -4.257865651761023 \t -0.12367647103295268\n",
            "42     \t [0.92075275 0.52342196]. \t  -0.28425718885340884 \t -0.12367647103295268\n",
            "43     \t [ 2.12475186 -0.63396654]. \t  -4.75475100338776 \t -0.12367647103295268\n",
            "44     \t [-5.26032605  3.72159654]. \t  -2212.031920249907 \t -0.12367647103295268\n",
            "45     \t [6.90622346 2.85151552]. \t  -209.95511965817124 \t -0.12367647103295268\n",
            "46     \t [-9.07984401 -4.51156514]. \t  -5059.349716364759 \t -0.12367647103295268\n",
            "47     \t [ 3.52470214 -1.92862941]. \t  -37.021064453977985 \t -0.12367647103295268\n",
            "48     \t [4.7869921  1.40198461]. \t  -15.806337438195051 \t -0.12367647103295268\n",
            "49     \t [-0.33026258 -0.98056426]. \t  -11.924096019819377 \t -0.12367647103295268\n",
            "50     \t [-9.68754667  9.79829972]. \t  -81480.73107563582 \t -0.12367647103295268\n",
            "51     \t [ 2.40196759 -1.11106934]. \t  -1.9744864412390422 \t -0.12367647103295268\n",
            "52     \t [-1.57694269 -0.01431373]. \t  -11.616715162372255 \t -0.12367647103295268\n",
            "53     \t [0.65487565 0.20414257]. \t  -0.7723976712689451 \t -0.12367647103295268\n",
            "54     \t [-6.75598933 -0.2864638 ]. \t  -155.93128028367303 \t -0.12367647103295268\n",
            "55     \t [-0.91593041  0.15891831]. \t  -5.538803688173616 \t -0.12367647103295268\n",
            "56     \t [ 7.98605608 -0.48736107]. \t  -161.63565597696189 \t -0.12367647103295268\n",
            "57     \t [-2.64801284  0.01406951]. \t  -27.33613532821893 \t -0.12367647103295268\n",
            "58     \t [-7.30446077 -4.45298308]. \t  -4479.931410554043 \t -0.12367647103295268\n",
            "59     \t [3.19764279 4.65963946]. \t  -3241.2264667529867 \t -0.12367647103295268\n",
            "60     \t [4.31259181 2.88175609]. \t  -313.3783601466608 \t -0.12367647103295268\n",
            "61     \t [5.06099678 0.76051018]. \t  -46.977957943352536 \t -0.12367647103295268\n",
            "62     \t [2.43214251 0.10159372]. \t  -13.681696103840789 \t -0.12367647103295268\n",
            "63     \t [1.44999064 1.38309823]. \t  -11.492585562770858 \t -0.12367647103295268\n",
            "64     \t [ 1.35127257 -6.48666679]. \t  -13712.604773039777 \t -0.12367647103295268\n",
            "65     \t [ 1.35981138 -2.11252867]. \t  -114.61041061004717 \t -0.12367647103295268\n",
            "66     \t [ 1.02551281 -0.37702712]. \t  -1.0994470205442355 \t -0.12367647103295268\n",
            "67     \t [2.30532712 5.16428926]. \t  -5210.732025182756 \t -0.12367647103295268\n",
            "68     \t [-8.75020178 -5.68207233]. \t  -10847.325259422847 \t -0.12367647103295268\n",
            "69     \t [-2.72271128 -6.53641866]. \t  -15562.549628196019 \t -0.12367647103295268\n",
            "70     \t [ 6.33822363 -6.12437893]. \t  -9461.780082440946 \t -0.12367647103295268\n",
            "71     \t [ 0.39286627 -0.5332549 ]. \t  -0.43046154222307037 \t -0.12367647103295268\n",
            "72     \t [1.72739661 2.71616151]. \t  -339.96948038768585 \t -0.12367647103295268\n",
            "73     \t [ 1.15990097 -2.97987577]. \t  -551.1069438955279 \t -0.12367647103295268\n",
            "74     \t [0.3050817  0.53268541]. \t  -0.6206460200499715 \t -0.12367647103295268\n",
            "75     \t [ 1.74201547 -1.26711675]. \t  -4.867415409798537 \t -0.12367647103295268\n",
            "76     \t [-4.31190326  4.05092865]. \t  -2785.779603738259 \t -0.12367647103295268\n",
            "77     \t [ 2.51854328 -1.65411194]. \t  -19.753826243047307 \t -0.12367647103295268\n",
            "78     \t [-3.40110708 -5.96485396]. \t  -11137.778595501333 \t -0.12367647103295268\n",
            "79     \t [-2.28071149 -6.41422133]. \t  -14313.303102479305 \t -0.12367647103295268\n",
            "80     \t [ 0.267685   -6.28865747]. \t  -12427.84636356615 \t -0.12367647103295268\n",
            "81     \t [3.90257918 1.03769522]. \t  -14.54266304784133 \t -0.12367647103295268\n",
            "82     \t [-9.04863361  1.2583201 ]. \t  -399.40569125181196 \t -0.12367647103295268\n",
            "83     \t [ 4.7260572  -1.33363726]. \t  -16.61606568569339 \t -0.12367647103295268\n",
            "84     \t [-3.18441793 -4.2986316 ]. \t  -3240.090681636895 \t -0.12367647103295268\n",
            "85     \t [ 4.11636844 -1.29089554]. \t  -10.939640400451674 \t -0.12367647103295268\n",
            "86     \t [8.73188655 3.77820009]. \t  -845.2649612317308 \t -0.12367647103295268\n",
            "87     \t [6.07941098 5.69407462]. \t  -6932.582633423325 \t -0.12367647103295268\n",
            "88     \t [-6.99691523  3.70409887]. \t  -2435.848917227575 \t -0.12367647103295268\n",
            "89     \t [-4.54889154 -3.53520937]. \t  -1776.5217755378185 \t -0.12367647103295268\n",
            "90     \t [5.57518348 7.26651834]. \t  -20032.691716494577 \t -0.12367647103295268\n",
            "91     \t [-5.39542752 -4.86496958]. \t  -5602.078430337968 \t -0.12367647103295268\n",
            "92     \t [-5.34625143 -7.38849223]. \t  -26272.63213176849 \t -0.12367647103295268\n",
            "93     \t [ 4.34379926 -2.21067563]. \t  -70.158921569656 \t -0.12367647103295268\n",
            "94     \t [-6.10772408 -4.44348195]. \t  -4208.653841123318 \t -0.12367647103295268\n",
            "95     \t [-6.31343574 -2.1723769 ]. \t  -549.7296951375912 \t -0.12367647103295268\n",
            "96     \t [ 9.69587589 -7.31644498]. \t  -19035.44955913957 \t -0.12367647103295268\n",
            "97     \t [-5.07031387 -4.65553508]. \t  -4725.5175973484165 \t -0.12367647103295268\n",
            "98     \t [-3.20025038  3.94954699]. \t  -2384.099734581828 \t -0.12367647103295268\n",
            "99     \t [-2.21002255  4.40918558]. \t  -3387.3857635547797 \t -0.12367647103295268\n",
            "100    \t [ 5.85237815 -0.64502844]. \t  -73.95148943738262 \t -0.12367647103295268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "22c1d405-51d7-4e3d-b8a8-3c2c8f592a06"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_10 = d2GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.07179171 -7.69986114]. \t  -26686.60864226137 \t -217.28346804397978\n",
            "init   \t [ 9.00565729 -0.35617197]. \t  -217.28346804397978 \t -217.28346804397978\n",
            "init   \t [ 7.4494907  -5.75334638]. \t  -6945.3080829060855 \t -217.28346804397978\n",
            "init   \t [-9.1858075  -2.05611077]. \t  -726.1597680698058 \t -217.28346804397978\n",
            "init   \t [-5.33735605  6.83481449]. \t  -19549.897628464034 \t -217.28346804397978\n",
            "1      \t [7.70674409 9.05288798]. \t  -48843.620887164754 \t -217.28346804397978\n",
            "2      \t [-15.25611937 -23.05652361]. \t  -2326427.769537942 \t -217.28346804397978\n",
            "3      \t [1.36988239 1.21701407]. \t  \u001b[92m-5.2080597150791546\u001b[0m \t -5.2080597150791546\n",
            "4      \t [-6.09332056 -9.94318576]. \t  -83141.37562406182 \t -5.2080597150791546\n",
            "5      \t [-13.31371093 -10.92036807]. \t  -127034.11450332246 \t -5.2080597150791546\n",
            "6      \t [-12.32578431   5.3295012 ]. \t  -9736.306174208183 \t -5.2080597150791546\n",
            "7      \t [-2.54754658 -3.07258079]. \t  -930.993628751534 \t -5.2080597150791546\n",
            "8      \t [1.24965186 7.03532041]. \t  -19106.98703987238 \t -5.2080597150791546\n",
            "9      \t [  3.3305569  -14.79114937]. \t  -377109.24867886154 \t -5.2080597150791546\n",
            "10     \t [-5.23560523  1.25324889]. \t  -179.22670201667287 \t -5.2080597150791546\n",
            "11     \t [-23.69574042 -22.69901083]. \t  -2223225.809825271 \t -5.2080597150791546\n",
            "12     \t [ 4.22887952 -1.43028888]. \t  -10.463435306194818 \t -5.2080597150791546\n",
            "13     \t [5.51544221 3.75025185]. \t  -1043.1151206682757 \t -5.2080597150791546\n",
            "14     \t [9.54236899 3.4799621 ]. \t  -503.85377059445585 \t -5.2080597150791546\n",
            "15     \t [  8.15882811 -10.95855921]. \t  -107718.93417381076 \t -5.2080597150791546\n",
            "16     \t [-10.58671037  -6.27140169]. \t  -16064.544631141287 \t -5.2080597150791546\n",
            "17     \t [-9.94821397  9.81261826]. \t  -82151.13071974964 \t -5.2080597150791546\n",
            "18     \t [ 1.68385074 -2.42475397]. \t  -203.47942108354388 \t -5.2080597150791546\n",
            "19     \t [-1.61008691 -8.12689249]. \t  -35759.71666213472 \t -5.2080597150791546\n",
            "20     \t [-1.0772153   1.77181792]. \t  -112.53314168297277 \t -5.2080597150791546\n",
            "21     \t [-8.86272635  1.69591828]. \t  -524.4700543909037 \t -5.2080597150791546\n",
            "22     \t [1.47436007 2.08092455]. \t  -103.50605803636999 \t -5.2080597150791546\n",
            "23     \t [1.48500429 0.10930574]. \t  \u001b[92m-4.5049070262237905\u001b[0m \t -4.5049070262237905\n",
            "24     \t [1.31647724 0.60330654]. \t  \u001b[92m-0.7928686392780588\u001b[0m \t -0.7928686392780588\n",
            "25     \t [5.92758943 0.03200877]. \t  -94.50519339523548 \t -0.7928686392780588\n",
            "26     \t [3.14641521 0.89925508]. \t  -9.283366185653744 \t -0.7928686392780588\n",
            "27     \t [1.94141718 0.59174598]. \t  -3.966877967703944 \t -0.7928686392780588\n",
            "28     \t [-5.93047362 -4.08043286]. \t  -3126.0703549650893 \t -0.7928686392780588\n",
            "29     \t [-2.4411987   9.96635689]. \t  -80892.44248158827 \t -0.7928686392780588\n",
            "30     \t [ 4.37461865 -3.04172198]. \t  -410.67509251468584 \t -0.7928686392780588\n",
            "31     \t [ 4.10201641 -0.5376862 ]. \t  -34.45688821842011 \t -0.7928686392780588\n",
            "32     \t [-1.55242068  4.02672546]. \t  -2315.993619530783 \t -0.7928686392780588\n",
            "33     \t [-2.10661505  0.72097627]. \t  -29.448566272699512 \t -0.7928686392780588\n",
            "34     \t [0.88030337 0.94242574]. \t  -1.6200638256496414 \t -0.7928686392780588\n",
            "35     \t [-0.50780535 -0.26290934]. \t  -3.1082329756069855 \t -0.7928686392780588\n",
            "36     \t [-0.64239201  0.3027251 ]. \t  -4.060936518602019 \t -0.7928686392780588\n",
            "37     \t [-0.54113752  0.57541392]. \t  -5.271158655725608 \t -0.7928686392780588\n",
            "38     \t [-1.20376593  0.77635104]. \t  -16.46514859501741 \t -0.7928686392780588\n",
            "39     \t [3.24178211 1.61151147]. \t  -12.647415456204175 \t -0.7928686392780588\n",
            "40     \t [-1.30140785 -0.34567534]. \t  -10.042085244814903 \t -0.7928686392780588\n",
            "41     \t [ 9.8356841  -3.07346653]. \t  -242.11726431745427 \t -0.7928686392780588\n",
            "42     \t [ 3.13526747 -0.79955969]. \t  -11.453859276511249 \t -0.7928686392780588\n",
            "43     \t [ 6.54051464 -1.86523814]. \t  -31.046269103610765 \t -0.7928686392780588\n",
            "44     \t [ 0.37173045 -0.33724089]. \t  \u001b[92m-0.4363489168057352\u001b[0m \t -0.4363489168057352\n",
            "45     \t [ 0.60710758 -0.22506819]. \t  -0.6660240443850638 \t -0.4363489168057352\n",
            "46     \t [-8.4873731  4.8505468]. \t  -6260.055858579166 \t -0.4363489168057352\n",
            "47     \t [-0.3634558   0.08098012]. \t  -2.1426237164615056 \t -0.4363489168057352\n",
            "48     \t [ 1.01870563 -0.71130785]. \t  \u001b[92m-0.000442052499338034\u001b[0m \t -0.000442052499338034\n",
            "49     \t [ 1.65413147 -0.56809754]. \t  -2.4626854038665797 \t -0.000442052499338034\n",
            "50     \t [5.48219774 3.88996803]. \t  -1248.3360607294771 \t -0.000442052499338034\n",
            "51     \t [4.51492599 2.38156528]. \t  -105.61918767682589 \t -0.000442052499338034\n",
            "52     \t [5.80760198 1.38966903]. \t  -30.680969393564517 \t -0.000442052499338034\n",
            "53     \t [5.99859689 2.31612087]. \t  -69.73621458104004 \t -0.000442052499338034\n",
            "54     \t [ 0.72766745 -0.28826936]. \t  -0.7046598830045853 \t -0.000442052499338034\n",
            "55     \t [2.80077054 1.12389109]. \t  -3.393484006632173 \t -0.000442052499338034\n",
            "56     \t [-3.76757267  0.39468033]. \t  -56.00815295654134 \t -0.000442052499338034\n",
            "57     \t [ 6.76050866 -2.05206792]. \t  -38.70433769107767 \t -0.000442052499338034\n",
            "58     \t [4.84023392 1.31613956]. \t  -18.532977520979344 \t -0.000442052499338034\n",
            "59     \t [ 0.3249369  -4.31804827]. \t  -2733.447425327223 \t -0.000442052499338034\n",
            "60     \t [-6.31918975 -0.51484084]. \t  -147.39668670073516 \t -0.000442052499338034\n",
            "61     \t [-8.11865691 -7.58792165]. \t  -30475.006856139753 \t -0.000442052499338034\n",
            "62     \t [1.52650926 0.8961131 ]. \t  -0.2898614485373722 \t -0.000442052499338034\n",
            "63     \t [ 1.51171163 -0.78200951]. \t  -0.4284678478953032 \t -0.000442052499338034\n",
            "64     \t [2.30669218 5.53075925]. \t  -6933.509486673679 \t -0.000442052499338034\n",
            "65     \t [ 2.642255   -1.44058112]. \t  -7.246896560299004 \t -0.000442052499338034\n",
            "66     \t [ 3.6629799  -1.07712952]. \t  -10.696417574329054 \t -0.000442052499338034\n",
            "67     \t [ 1.36710345 -0.84054689]. \t  -0.13898493565453612 \t -0.000442052499338034\n",
            "68     \t [ 2.3753554  -1.17126276]. \t  -2.1629769994058146 \t -0.000442052499338034\n",
            "69     \t [ 7.38047154 -9.25721189]. \t  -53840.23648129998 \t -0.000442052499338034\n",
            "70     \t [ 5.53014797 -1.52113567]. \t  -22.15103832611234 \t -0.000442052499338034\n",
            "71     \t [1.27292744 0.82576303]. \t  -0.09099382248244976 \t -0.000442052499338034\n",
            "72     \t [ 1.50812225 -1.30782685]. \t  -7.575030105411535 \t -0.000442052499338034\n",
            "73     \t [ 4.79010638 -1.18871613]. \t  -22.079610760707066 \t -0.000442052499338034\n",
            "74     \t [9.94231219 3.27114541]. \t  -342.55812679072415 \t -0.000442052499338034\n",
            "75     \t [-8.46680573  8.44266127]. \t  -45706.03566664887 \t -0.000442052499338034\n",
            "76     \t [ 3.25293074 -5.22260964]. \t  -5268.121662610138 \t -0.000442052499338034\n",
            "77     \t [-9.60702549  8.65089734]. \t  -50854.704943960656 \t -0.000442052499338034\n",
            "78     \t [ 1.43472543 -8.73045771]. \t  -45606.45220107105 \t -0.000442052499338034\n",
            "79     \t [5.34550126 0.93888375]. \t  -44.551934510489104 \t -0.000442052499338034\n",
            "80     \t [-9.36547476  2.02581118]. \t  -725.0845732068685 \t -0.000442052499338034\n",
            "81     \t [ 0.20326007 -0.8050308 ]. \t  -3.0236077481688683 \t -0.000442052499338034\n",
            "82     \t [6.82596773 5.53661569]. \t  -5970.574953084083 \t -0.000442052499338034\n",
            "83     \t [ 2.00780911 -1.00237165]. \t  -1.0156849059458342 \t -0.000442052499338034\n",
            "84     \t [-9.84186936 -0.80085221]. \t  -365.05948714640203 \t -0.000442052499338034\n",
            "85     \t [ 8.13288047 -6.31727247]. \t  -10327.7779569444 \t -0.000442052499338034\n",
            "86     \t [ 1.09998098 -8.34432714]. \t  -38173.95512799832 \t -0.000442052499338034\n",
            "87     \t [6.84783286 1.11760082]. \t  -72.03814129887033 \t -0.000442052499338034\n",
            "88     \t [ 7.11878012 -1.20910938]. \t  -72.63365938492225 \t -0.000442052499338034\n",
            "89     \t [-8.17943794 -0.93216136]. \t  -280.967265407088 \t -0.000442052499338034\n",
            "90     \t [3.69377315 9.72531409]. \t  -68805.27204374927 \t -0.000442052499338034\n",
            "91     \t [ 2.25504466 -3.58192906]. \t  -1097.1994125105232 \t -0.000442052499338034\n",
            "92     \t [3.84302816 0.29679479]. \t  -34.97444363718047 \t -0.000442052499338034\n",
            "93     \t [5.81368138 7.99447649]. \t  -29795.869878511236 \t -0.000442052499338034\n",
            "94     \t [ 2.1995583  -9.43301258]. \t  -61787.33215146634 \t -0.000442052499338034\n",
            "95     \t [-1.41958965 -0.15751026]. \t  -10.171562020910802 \t -0.000442052499338034\n",
            "96     \t [-3.95788396  5.24743984]. \t  -6993.457889775188 \t -0.000442052499338034\n",
            "97     \t [-3.87570309  8.8357695 ]. \t  -51234.876069122845 \t -0.000442052499338034\n",
            "98     \t [-3.18917536 -1.71868298]. \t  -183.05701438345855 \t -0.000442052499338034\n",
            "99     \t [-2.92348034 -0.27284857]. \t  -34.2726500274852 \t -0.000442052499338034\n",
            "100    \t [-4.26573344 -0.88641983]. \t  -95.87407536021895 \t -0.000442052499338034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "53f93acb-e2a9-4b38-8877-6a3a08a38dfd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_11 = d2GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.62071325 6.42495699]. \t  -10714.638013792866 \t -9.740000060755142\n",
            "init   \t [ 2.91787228 -1.52623973]. \t  -9.740000060755142 \t -9.740000060755142\n",
            "init   \t [-5.95378042 -0.16186459]. \t  -120.5034756901465 \t -9.740000060755142\n",
            "init   \t [-7.22883337 -0.94957929]. \t  -230.87623811313406 \t -9.740000060755142\n",
            "init   \t [-7.72530745 -9.94151023]. \t  -84448.33746856163 \t -9.740000060755142\n",
            "1      \t [ 9.71879818 -6.86089495]. \t  -14331.165445255861 \t -9.740000060755142\n",
            "2      \t [-0.41525252  9.69422331]. \t  -70969.40865155176 \t -9.740000060755142\n",
            "3      \t [-9.5194484  9.1420568]. \t  -62537.95985021731 \t -9.740000060755142\n",
            "4      \t [ 0.94955298 -9.31626642]. \t  -59606.43954696483 \t -9.740000060755142\n",
            "5      \t [-13.42939903  -2.71608082]. \t  -1796.8369371408419 \t -9.740000060755142\n",
            "6      \t [-30.42518923 -34.96956494]. \t  -12263784.982572172 \t -9.740000060755142\n",
            "7      \t [ 8.49048764 -0.19399542]. \t  -197.73923329838743 \t -9.740000060755142\n",
            "8      \t [-0.93565485  2.80807474]. \t  -561.9424884622073 \t -9.740000060755142\n",
            "9      \t [-11.44062611   2.84834442]. \t  -1685.6681675814116 \t -9.740000060755142\n",
            "10     \t [-1.83453863 -3.36281919]. \t  -1203.8004394574816 \t -9.740000060755142\n",
            "11     \t [3.80479894 5.14337162]. \t  -4830.222305482144 \t -9.740000060755142\n",
            "12     \t [ 4.8631515  -5.28179266]. \t  -5202.962561779686 \t -9.740000060755142\n",
            "13     \t [-4.90683956  5.94357573]. \t  -11453.223121939483 \t -9.740000060755142\n",
            "14     \t [-9.24218668 -5.07902277]. \t  -7506.728797697176 \t -9.740000060755142\n",
            "15     \t [-15.28874095 -18.61700562]. \t  -1004137.6641775732 \t -9.740000060755142\n",
            "16     \t [4.93743172 0.47991413]. \t  -55.58678394606533 \t -9.740000060755142\n",
            "17     \t [5.24640175 9.84084534]. \t  -71035.84612395246 \t -9.740000060755142\n",
            "18     \t [ 1.24247496 -0.14679844]. \t  \u001b[92m-2.935796795634533\u001b[0m \t -2.935796795634533\n",
            "19     \t [  6.07951857 -10.39082816]. \t  -88107.48611138681 \t -2.935796795634533\n",
            "20     \t [-4.99105109 -5.25851406]. \t  -7306.864663801201 \t -2.935796795634533\n",
            "21     \t [1.69220352 0.61311038]. \t  \u001b[92m-2.247830625816612\u001b[0m \t -2.247830625816612\n",
            "22     \t [-3.5787386   0.06074801]. \t  -46.685349674750675 \t -2.247830625816612\n",
            "23     \t [6.73920919 2.57971789]. \t  -119.2861815459801 \t -2.247830625816612\n",
            "24     \t [-8.09405974  3.165073  ]. \t  -1665.2320345594542 \t -2.247830625816612\n",
            "25     \t [1.80824395 0.86910591]. \t  \u001b[92m-0.830334799634687\u001b[0m \t -0.830334799634687\n",
            "26     \t [1.87079499 1.19411038]. \t  -2.683022385928261 \t -0.830334799634687\n",
            "27     \t [2.41698728 0.35137205]. \t  -11.426196762445578 \t -0.830334799634687\n",
            "28     \t [ 2.11103866 -1.799357  ]. \t  -39.329204245967176 \t -0.830334799634687\n",
            "29     \t [ 5.14182964 -1.39003463]. \t  -20.418443676855386 \t -0.830334799634687\n",
            "30     \t [ 0.11511127 -0.18986349]. \t  \u001b[92m-0.7867286354195575\u001b[0m \t -0.7867286354195575\n",
            "31     \t [-9.94522097 -0.65158699]. \t  -352.83394027924084 \t -0.7867286354195575\n",
            "32     \t [ 3.33727079 -1.36661083]. \t  -5.779610152409278 \t -0.7867286354195575\n",
            "33     \t [-4.41086062  1.67943638]. \t  -231.3577428847595 \t -0.7867286354195575\n",
            "34     \t [ 0.20107465 -0.59627987]. \t  -1.1585321581818193 \t -0.7867286354195575\n",
            "35     \t [ 9.36615136 -3.14755016]. \t  -288.3135879525186 \t -0.7867286354195575\n",
            "36     \t [-3.42738622 -8.83248316]. \t  -50870.05885384415 \t -0.7867286354195575\n",
            "37     \t [-0.34840984 -0.39820313]. \t  -2.7040995724777024 \t -0.7867286354195575\n",
            "38     \t [2.25145657 1.19414518]. \t  -2.2873652842691294 \t -0.7867286354195575\n",
            "39     \t [-5.25358867  9.68630861]. \t  -74462.0396231826 \t -0.7867286354195575\n",
            "40     \t [ 4.07954436 -1.59769352]. \t  -11.58773412206113 \t -0.7867286354195575\n",
            "41     \t [-2.53182558 -0.79020428]. \t  -41.06074099944064 \t -0.7867286354195575\n",
            "42     \t [-4.54756724 -1.76204974]. \t  -262.2104568959321 \t -0.7867286354195575\n",
            "43     \t [ 6.4025268  -1.69787445]. \t  -29.998761195228397 \t -0.7867286354195575\n",
            "44     \t [ 0.47502241 -0.36080302]. \t  \u001b[92m-0.36776339919787215\u001b[0m \t -0.36776339919787215\n",
            "45     \t [-1.7687581  0.5638483]. \t  -19.230299812700427 \t -0.36776339919787215\n",
            "46     \t [ 0.38019515 -0.75350278]. \t  -1.5252282444465597 \t -0.36776339919787215\n",
            "47     \t [ 0.67758561 -0.50797742]. \t  \u001b[92m-0.15611780076293735\u001b[0m \t -0.15611780076293735\n",
            "48     \t [-2.82695935 -5.75623563]. \t  -9563.009858686777 \t -0.15611780076293735\n",
            "49     \t [9.47038186 2.22423336]. \t  -72.10699928438305 \t -0.15611780076293735\n",
            "50     \t [-0.38202811 -0.08194416]. \t  -2.222775432715206 \t -0.15611780076293735\n",
            "51     \t [ 1.26913823 -3.74176847]. \t  -1429.3281577514304 \t -0.15611780076293735\n",
            "52     \t [ 1.53432015 -1.01035666]. \t  -0.8002472121981175 \t -0.15611780076293735\n",
            "53     \t [-2.36433395 -0.2277325 ]. \t  -23.501364856204273 \t -0.15611780076293735\n",
            "54     \t [ 4.31002081 -1.08189392]. \t  -18.710411026093894 \t -0.15611780076293735\n",
            "55     \t [ 5.53284339 -8.2539833 ]. \t  -34197.885286768586 \t -0.15611780076293735\n",
            "56     \t [-0.23000497 -8.42508835]. \t  -40439.91226306904 \t -0.15611780076293735\n",
            "57     \t [-0.88657591  2.10122396]. \t  -192.39391201053718 \t -0.15611780076293735\n",
            "58     \t [4.09868526 1.58938879]. \t  -11.420663851959533 \t -0.15611780076293735\n",
            "59     \t [0.67519118 0.69616347]. \t  -0.27848565286067795 \t -0.15611780076293735\n",
            "60     \t [0.17256898 1.25036524]. \t  -18.139916707281074 \t -0.15611780076293735\n",
            "61     \t [-7.87187674 -4.59603992]. \t  -5102.547071025583 \t -0.15611780076293735\n",
            "62     \t [-0.50260382  0.36668725]. \t  -3.4483133808556765 \t -0.15611780076293735\n",
            "63     \t [3.22732899 1.01580301]. \t  -7.669005702117132 \t -0.15611780076293735\n",
            "64     \t [-1.18815574  1.020311  ]. \t  -26.176766031005062 \t -0.15611780076293735\n",
            "65     \t [ 0.93914859 -0.12971748]. \t  -1.6435465122151858 \t -0.15611780076293735\n",
            "66     \t [1.97432265 2.53632665]. \t  -238.20247058643878 \t -0.15611780076293735\n",
            "67     \t [0.16099158 0.89012971]. \t  -4.757608938674867 \t -0.15611780076293735\n",
            "68     \t [ 2.10734891 -0.67162117]. \t  -4.13123039575514 \t -0.15611780076293735\n",
            "69     \t [ 3.35980611 -1.0829861 ]. \t  -7.625435078763864 \t -0.15611780076293735\n",
            "70     \t [1.0003115  0.90210071]. \t  -0.7869100675756836 \t -0.15611780076293735\n",
            "71     \t [-1.2800157  -0.45151021]. \t  -10.89539509195281 \t -0.15611780076293735\n",
            "72     \t [1.26688474 0.82203192]. \t  \u001b[92m-0.08553779603923299\u001b[0m \t -0.08553779603923299\n",
            "73     \t [ 2.32782963 -5.9831748 ]. \t  -9598.132944240877 \t -0.08553779603923299\n",
            "74     \t [4.63518173 0.35127548]. \t  -51.730529059530966 \t -0.08553779603923299\n",
            "75     \t [ 2.0286068  -1.08649317]. \t  -1.278915759527782 \t -0.08553779603923299\n",
            "76     \t [-0.05913795  0.47833897]. \t  -1.6558431734022214 \t -0.08553779603923299\n",
            "77     \t [ 9.67852138 -2.76893952]. \t  -139.28678989564332 \t -0.08553779603923299\n",
            "78     \t [ 1.05483998 -0.73713101]. \t  \u001b[92m-0.005040638158741799\u001b[0m \t -0.005040638158741799\n",
            "79     \t [-0.69810267 -7.36598808]. \t  -23858.13215841738 \t -0.005040638158741799\n",
            "80     \t [ 9.96461037 -1.9507269 ]. \t  -91.44630152846665 \t -0.005040638158741799\n",
            "81     \t [ 1.39442669 -4.26455993]. \t  -2447.1487452458628 \t -0.005040638158741799\n",
            "82     \t [2.74498008 6.90529164]. \t  -17160.390579779105 \t -0.005040638158741799\n",
            "83     \t [8.75404465 3.15419183]. \t  -308.4941042225309 \t -0.005040638158741799\n",
            "84     \t [ 8.32326456 -5.33167961]. \t  -4764.020014189273 \t -0.005040638158741799\n",
            "85     \t [0.38153504 0.26450757]. \t  -0.4992463373231544 \t -0.005040638158741799\n",
            "86     \t [9.92326249 1.50576109]. \t  -137.69927087117713 \t -0.005040638158741799\n",
            "87     \t [-9.81419467  4.16506142]. \t  -4079.167887251405 \t -0.005040638158741799\n",
            "88     \t [-4.3145491  -3.64523851]. \t  -1936.638108318239 \t -0.005040638158741799\n",
            "89     \t [9.49820215 9.93517295]. \t  -70697.90824820579 \t -0.005040638158741799\n",
            "90     \t [-2.07678822  8.62162833]. \t  -45455.60988660681 \t -0.005040638158741799\n",
            "91     \t [-1.13533016 -4.2962465 ]. \t  -2900.286107739129 \t -0.005040638158741799\n",
            "92     \t [-1.36834628  4.22089697]. \t  -2743.651788682231 \t -0.005040638158741799\n",
            "93     \t [ 3.08294269 -8.08402201]. \t  -32578.006703472063 \t -0.005040638158741799\n",
            "94     \t [7.61503009 3.48124151]. \t  -596.4105271155338 \t -0.005040638158741799\n",
            "95     \t [5.56075894 2.21270684]. \t  -56.609745690134375 \t -0.005040638158741799\n",
            "96     \t [3.4338222  1.40468789]. \t  -6.448749544123398 \t -0.005040638158741799\n",
            "97     \t [ 7.47631853 -0.76898239]. \t  -121.16277987722734 \t -0.005040638158741799\n",
            "98     \t [-0.21372332  5.24303817]. \t  -6093.924112011358 \t -0.005040638158741799\n",
            "99     \t [6.84973548 1.46790401]. \t  -47.12515676067582 \t -0.005040638158741799\n",
            "100    \t [ 9.75240598 -3.36757363]. \t  -410.9070903804599 \t -0.005040638158741799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "55ac7cc0-266c-4338-b569-adb1261f2c71"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_12 = d2GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.16961099  2.44217542]. \t  -706.482335023616 \t -706.482335023616\n",
            "init   \t [-1.24544522  5.70717167]. \t  -8820.05719953671 \t -706.482335023616\n",
            "init   \t [ 5.59951616 -4.54814789]. \t  -2580.3963775443385 \t -706.482335023616\n",
            "init   \t [-4.4707149   6.03744355]. \t  -12002.832292900235 \t -706.482335023616\n",
            "init   \t [9.16278707 7.51865269]. \t  -21656.00779050222 \t -706.482335023616\n",
            "1      \t [-8.80381554 -6.31425832]. \t  -15776.028882530063 \t -706.482335023616\n",
            "2      \t [-2.10622833 -9.37900803]. \t  -63404.573438939275 \t -706.482335023616\n",
            "3      \t [-83.49822185 -73.61332398]. \t  -238558525.23009148 \t -706.482335023616\n",
            "4      \t [-21.65008419 -14.3878703 ]. \t  -380132.7002112593 \t -706.482335023616\n",
            "5      \t [-12.50100416   0.61250738]. \t  \u001b[92m-533.4728585449687\u001b[0m \t -533.4728585449687\n",
            "6      \t [-0.91777906 -1.83475346]. \t  \u001b[92m-120.73571628803775\u001b[0m \t -120.73571628803775\n",
            "7      \t [-11.94113053 -12.7024045 ]. \t  -224139.73656201683 \t -120.73571628803775\n",
            "8      \t [  7.39047826 -10.97607442]. \t  -109139.4620138441 \t -120.73571628803775\n",
            "9      \t [-21.27263264 -22.73633954]. \t  -2227199.812784501 \t -120.73571628803775\n",
            "10     \t [-11.1942074   7.6712936]. \t  -33374.83851113447 \t -120.73571628803775\n",
            "11     \t [4.56993219 2.3320827 ]. \t  \u001b[92m-92.30816127407111\u001b[0m \t -92.30816127407111\n",
            "12     \t [8.90595546 0.41689246]. \t  -208.99507419761974 \t -92.30816127407111\n",
            "13     \t [3.81060723 7.05653583]. \t  -18355.05216300961 \t -92.30816127407111\n",
            "14     \t [-5.01962349 -2.86264476]. \t  -952.9344376453503 \t -92.30816127407111\n",
            "15     \t [-34.27656754 -35.59135544]. \t  -13188086.138495158 \t -92.30816127407111\n",
            "16     \t [-16.69940899 -18.57535964]. \t  -999410.0716183687 \t -92.30816127407111\n",
            "17     \t [-27.16754368 -27.74758715]. \t  -4911926.411650477 \t -92.30816127407111\n",
            "18     \t [-15.92235095  -6.47610692]. \t  -20207.353236490333 \t -92.30816127407111\n",
            "19     \t [ 2.59951443 -1.15942121]. \t  \u001b[92m-2.574288631529033\u001b[0m \t -2.574288631529033\n",
            "20     \t [ 0.83525533 -4.82262436]. \t  -4173.380457961054 \t -2.574288631529033\n",
            "21     \t [ 9.80385012 -3.66204039]. \t  -656.6799799228362 \t -2.574288631529033\n",
            "22     \t [0.41170918 1.40771597]. \t  -25.574085911310572 \t -2.574288631529033\n",
            "23     \t [ 3.02736634 -8.60907648]. \t  -42173.11120502375 \t -2.574288631529033\n",
            "24     \t [-8.76053262 -1.24693435]. \t  -377.0723940853243 \t -2.574288631529033\n",
            "25     \t [-2.79976767  1.32577054]. \t  -94.1992792109429 \t -2.574288631529033\n",
            "26     \t [-9.71378829  3.36471435]. \t  -2208.657680014171 \t -2.574288631529033\n",
            "27     \t [ 3.84635456 -0.75894838]. \t  -22.62077042786802 \t -2.574288631529033\n",
            "28     \t [-2.1550919   9.79142082]. \t  -75203.54889502253 \t -2.574288631529033\n",
            "29     \t [ 1.30303054 -0.68242502]. \t  \u001b[92m-0.36803440147714755\u001b[0m \t -0.36803440147714755\n",
            "30     \t [8.30726276 3.35485216]. \t  -456.835330498602 \t -0.36803440147714755\n",
            "31     \t [2.38560175 0.61238765]. \t  -7.2700345066352465 \t -0.36803440147714755\n",
            "32     \t [0.47081769 0.28791741]. \t  -0.4661142043144742 \t -0.36803440147714755\n",
            "33     \t [6.27237399 0.34585011]. \t  -100.59571092523811 \t -0.36803440147714755\n",
            "34     \t [1.55684435 2.61619308]. \t  -294.6851990140005 \t -0.36803440147714755\n",
            "35     \t [-4.55889339 -5.94489775]. \t  -11353.772515528482 \t -0.36803440147714755\n",
            "36     \t [1.51399418 0.05866178]. \t  -4.806961787314738 \t -0.36803440147714755\n",
            "37     \t [3.2611797  0.44444286]. \t  -21.54223029871821 \t -0.36803440147714755\n",
            "38     \t [-0.69103619  0.67198608]. \t  -7.942338781930578 \t -0.36803440147714755\n",
            "39     \t [-6.86405958  9.58526159]. \t  -72732.69823547796 \t -0.36803440147714755\n",
            "40     \t [ 8.3255585  -6.62808577]. \t  -12706.086385646855 \t -0.36803440147714755\n",
            "41     \t [ 2.03023734 -1.24810203]. \t  -3.4170544277389445 \t -0.36803440147714755\n",
            "42     \t [-6.82062611 -9.65198533]. \t  -74669.03325393294 \t -0.36803440147714755\n",
            "43     \t [-0.4269311   0.93074995]. \t  -11.36320310339419 \t -0.36803440147714755\n",
            "44     \t [ 9.6233783  -6.74246078]. \t  -13293.14962611658 \t -0.36803440147714755\n",
            "45     \t [ 6.38818665 -1.39574055]. \t  -41.45271613405559 \t -0.36803440147714755\n",
            "46     \t [ 0.16106039 -0.32524712]. \t  -0.7089223925176974 \t -0.36803440147714755\n",
            "47     \t [ 0.39359025 -0.50753292]. \t  -0.3973005863171114 \t -0.36803440147714755\n",
            "48     \t [ 1.23966921 -0.6307238 ]. \t  -0.45179180458815715 \t -0.36803440147714755\n",
            "49     \t [ 3.72603451 -1.53019535]. \t  -9.26281331271517 \t -0.36803440147714755\n",
            "50     \t [ 0.42410574 -0.16121444]. \t  -0.608609047374506 \t -0.36803440147714755\n",
            "51     \t [-2.30560633 -0.39849111]. \t  -24.689354688555653 \t -0.36803440147714755\n",
            "52     \t [ 3.12362364 -1.40164354]. \t  -5.8077136376483365 \t -0.36803440147714755\n",
            "53     \t [-0.24277336  0.08835568]. \t  -1.6780131098522815 \t -0.36803440147714755\n",
            "54     \t [ 2.1335826  -1.06749967]. \t  -1.32736659080495 \t -0.36803440147714755\n",
            "55     \t [2.58485884 0.17125506]. \t  -15.275173187265553 \t -0.36803440147714755\n",
            "56     \t [ 8.87043805 -6.82608291]. \t  -14281.795986853876 \t -0.36803440147714755\n",
            "57     \t [1.5756336 0.5361869]. \t  -2.33391809291586 \t -0.36803440147714755\n",
            "58     \t [-0.8996596  -0.41278593]. \t  -6.686109513706473 \t -0.36803440147714755\n",
            "59     \t [-5.13610495  4.50565333]. \t  -4221.568063430106 \t -0.36803440147714755\n",
            "60     \t [-4.98991934 -0.07850062]. \t  -85.924024481949 \t -0.36803440147714755\n",
            "61     \t [-1.24885828  0.27865609]. \t  -9.000675746779773 \t -0.36803440147714755\n",
            "62     \t [3.3416869  8.35157673]. \t  -37082.38566533463 \t -0.36803440147714755\n",
            "63     \t [ 4.79446834 -1.39027191]. \t  -16.123166796405208 \t -0.36803440147714755\n",
            "64     \t [ 6.85791138 -6.5587892 ]. \t  -12572.4745327583 \t -0.36803440147714755\n",
            "65     \t [0.42545809 0.84415819]. \t  -2.329090627837269 \t -0.36803440147714755\n",
            "66     \t [ 0.65681684 -0.64620393]. \t  \u001b[92m-0.1813865615295248\u001b[0m \t -0.1813865615295248\n",
            "67     \t [0.99390865 0.40569914]. \t  -0.8837559307466968 \t -0.1813865615295248\n",
            "68     \t [-3.81265908 -4.99503952]. \t  -5793.441585001492 \t -0.1813865615295248\n",
            "69     \t [5.75408802 3.95036   ]. \t  -1318.6783370451064 \t -0.1813865615295248\n",
            "70     \t [3.28012838 3.6359738 ]. \t  -1078.0148941175532 \t -0.1813865615295248\n",
            "71     \t [5.05829336 1.21602579]. \t  -25.296936162885622 \t -0.1813865615295248\n",
            "72     \t [3.71758773 1.46561664]. \t  -8.05455327038622 \t -0.1813865615295248\n",
            "73     \t [-3.60580754  8.70584072]. \t  -48188.54724637447 \t -0.1813865615295248\n",
            "74     \t [4.38671998 1.52971845]. \t  -11.641989004998031 \t -0.1813865615295248\n",
            "75     \t [3.43072363 6.91320283]. \t  -16990.636376821025 \t -0.1813865615295248\n",
            "76     \t [6.63283614 3.72989125]. \t  -929.8747398348743 \t -0.1813865615295248\n",
            "77     \t [ 2.01271661 -6.01062813]. \t  -9869.066925274641 \t -0.1813865615295248\n",
            "78     \t [ 4.62301327 -5.15861912]. \t  -4736.985788552286 \t -0.1813865615295248\n",
            "79     \t [ 1.07596146 -0.63551883]. \t  \u001b[92m-0.1496252200584844\u001b[0m \t -0.1496252200584844\n",
            "80     \t [6.26398562 3.76637446]. \t  -1005.1632549001689 \t -0.1496252200584844\n",
            "81     \t [-0.28452549 -7.4087819 ]. \t  -24230.092896333244 \t -0.1496252200584844\n",
            "82     \t [3.05383398 0.86897819]. \t  -8.983560494324518 \t -0.1496252200584844\n",
            "83     \t [5.71979017 1.40724964]. \t  -28.46519373905457 \t -0.1496252200584844\n",
            "84     \t [ 1.17286731 -8.54177696]. \t  -41905.756991193644 \t -0.1496252200584844\n",
            "85     \t [5.37190763 1.68734372]. \t  -19.32139545757848 \t -0.1496252200584844\n",
            "86     \t [9.2733085  8.70085541]. \t  -40473.99489624034 \t -0.1496252200584844\n",
            "87     \t [ 6.6781321  -2.58651124]. \t  -122.07341521384448 \t -0.1496252200584844\n",
            "88     \t [-9.54284769 -3.55890536]. \t  -2543.6078971587644 \t -0.1496252200584844\n",
            "89     \t [9.88080675 3.8493994 ]. \t  -859.384405902526 \t -0.1496252200584844\n",
            "90     \t [-7.64447184  4.58158647]. \t  -5000.274323967172 \t -0.1496252200584844\n",
            "91     \t [-3.17673625 -2.51350093]. \t  -517.4907760011075 \t -0.1496252200584844\n",
            "92     \t [ 6.01484388 -3.34460012]. \t  -560.3075707518249 \t -0.1496252200584844\n",
            "93     \t [-2.84232082 -7.60664852]. \t  -28129.85622966966 \t -0.1496252200584844\n",
            "94     \t [ 8.14601407 -1.24864916]. \t  -101.62235127458007 \t -0.1496252200584844\n",
            "95     \t [ 7.38654134 -1.79812914]. \t  -42.48072689108267 \t -0.1496252200584844\n",
            "96     \t [ 5.11526129 -0.09034429]. \t  -68.93369481205232 \t -0.1496252200584844\n",
            "97     \t [4.90338108 9.29475615]. \t  -56383.70145988153 \t -0.1496252200584844\n",
            "98     \t [-6.1461776   5.99112053]. \t  -12198.24388708779 \t -0.1496252200584844\n",
            "99     \t [-4.9205641  -9.43264269]. \t  -66917.97104986853 \t -0.1496252200584844\n",
            "100    \t [-2.10756868 -2.72354809]. \t  -583.7872836272707 \t -0.1496252200584844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "9d80d81c-74e3-4ff3-f8a5-4e0d520976ed"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_13 = d2GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.21288618 -1.72798696]. \t  -20.120935450808645 \t -20.120935450808645\n",
            "init   \t [ 3.36205116 -5.40113156]. \t  -6051.706374022101 \t -20.120935450808645\n",
            "init   \t [6.15356672 2.73636929]. \t  -182.2099268583363 \t -20.120935450808645\n",
            "init   \t [-6.55612296 -4.79228264]. \t  -5567.089701718418 \t -20.120935450808645\n",
            "init   \t [ 8.30639981 -0.74368981]. \t  -157.07069920616067 \t -20.120935450808645\n",
            "1      \t [-7.5961809  7.6177101]. \t  -30655.10899619148 \t -20.120935450808645\n",
            "2      \t [9.21200838 9.90573549]. \t  -70031.75643065354 \t -20.120935450808645\n",
            "3      \t [-0.35482891  6.84457911]. \t  -17693.150381619864 \t -20.120935450808645\n",
            "4      \t [  1.95769685 -12.19506591]. \t  -174619.63161434853 \t -20.120935450808645\n",
            "5      \t [-11.9643305  -15.59332468]. \t  -496709.8331133628 \t -20.120935450808645\n",
            "6      \t [-22.77369066 -26.42302672]. \t  -4028404.509187324 \t -20.120935450808645\n",
            "7      \t [ 9.52302423 -8.71140556]. \t  -40545.127961554834 \t -20.120935450808645\n",
            "8      \t [-10.69102118   1.07360333]. \t  -474.48601337183226 \t -20.120935450808645\n",
            "9      \t [-13.35917654  -7.01077014]. \t  -25142.531490921698 \t -20.120935450808645\n",
            "10     \t [-2.31617433  0.30042063]. \t  -23.4638285305372 \t -20.120935450808645\n",
            "11     \t [ -5.50256985 -11.63794115]. \t  -152820.63484744873 \t -20.120935450808645\n",
            "12     \t [-5.93071029  2.22222066]. \t  -547.7723959592485 \t -20.120935450808645\n",
            "13     \t [-1.83352863 -6.66349998]. \t  -16438.52027852523 \t -20.120935450808645\n",
            "14     \t [1.97386237 1.54260487]. \t  \u001b[92m-16.465283083000134\u001b[0m \t -16.465283083000134\n",
            "15     \t [4.71638698 7.04000935]. \t  -17839.20369311654 \t -16.465283083000134\n",
            "16     \t [-10.46673457  -2.93656527]. \t  -1667.5694532874793 \t -16.465283083000134\n",
            "17     \t [ 0.19753514 -1.15056144]. \t  \u001b[92m-12.649421222072922\u001b[0m \t -12.649421222072922\n",
            "18     \t [-49.80406432 -48.94226173]. \t  -46863345.97069412 \t -12.649421222072922\n",
            "19     \t [-2.971555   -2.20495059]. \t  -338.1078904538001 \t -12.649421222072922\n",
            "20     \t [9.32525675 4.27502511]. \t  -1551.8660816935121 \t -12.649421222072922\n",
            "21     \t [ 7.34447518 -3.72975327]. \t  -878.9201542551558 \t -12.649421222072922\n",
            "22     \t [-0.99270955  2.1767774 ]. \t  -223.18879035525 \t -12.649421222072922\n",
            "23     \t [ 3.28985042 -0.06806035]. \t  -26.7679039328027 \t -12.649421222072922\n",
            "24     \t [-10.44592762 -10.34925717]. \t  -101075.39821844807 \t -12.649421222072922\n",
            "25     \t [-7.74567573 -0.99745382]. \t  -266.0469191386042 \t -12.649421222072922\n",
            "26     \t [2.7548341  2.97582273]. \t  -450.455744911671 \t -12.649421222072922\n",
            "27     \t [1.01457089 0.87483281]. \t  \u001b[92m-0.532918375028389\u001b[0m \t -0.532918375028389\n",
            "28     \t [1.89130708 0.43928521]. \t  -5.326670367036791 \t -0.532918375028389\n",
            "29     \t [-9.17852254  3.71331485]. \t  -2805.600104448162 \t -0.532918375028389\n",
            "30     \t [-0.10149059 -2.95025057]. \t  -614.3747839984946 \t -0.532918375028389\n",
            "31     \t [-0.249501   -0.74885867]. \t  -5.320971299908408 \t -0.532918375028389\n",
            "32     \t [1.96575651 0.7240668 ]. \t  -2.6152379244249984 \t -0.532918375028389\n",
            "33     \t [ 5.34822395 -0.19269434]. \t  -74.53639617323107 \t -0.532918375028389\n",
            "34     \t [-3.41906934  3.59694844]. \t  -1735.9392767867666 \t -0.532918375028389\n",
            "35     \t [ 2.06508456 -0.56175521]. \t  -5.246811581894747 \t -0.532918375028389\n",
            "36     \t [ 5.63294844 -7.94344199]. \t  -29092.624121161647 \t -0.532918375028389\n",
            "37     \t [ 1.36210559 -1.02819482]. \t  -1.2629214291826965 \t -0.532918375028389\n",
            "38     \t [1.08516894 1.09124532]. \t  -3.368890413283811 \t -0.532918375028389\n",
            "39     \t [-2.7772763   9.65998633]. \t  -71765.00933891739 \t -0.532918375028389\n",
            "40     \t [ 5.82415001 -9.03224803]. \t  -49534.30542867251 \t -0.532918375028389\n",
            "41     \t [ 0.4830715  -0.18168467]. \t  -0.6150812445968619 \t -0.532918375028389\n",
            "42     \t [ 1.58713924 -0.05125146]. \t  -5.349457986508344 \t -0.532918375028389\n",
            "43     \t [ 0.46004222 -0.80472513]. \t  -1.686414736882948 \t -0.532918375028389\n",
            "44     \t [1.49779498 0.70156839]. \t  -0.7749560014307908 \t -0.532918375028389\n",
            "45     \t [ 2.3717882  -1.06308583]. \t  -1.9066607910579674 \t -0.532918375028389\n",
            "46     \t [-5.13967359 -0.96374632]. \t  -135.61965735675838 \t -0.532918375028389\n",
            "47     \t [1.27814783 0.45382512]. \t  -1.5780866676474714 \t -0.532918375028389\n",
            "48     \t [9.3826568  0.90601465]. \t  -190.1129819863197 \t -0.532918375028389\n",
            "49     \t [1.67255024 1.11552115]. \t  -1.7847691179324499 \t -0.532918375028389\n",
            "50     \t [ 1.93084158 -1.36181104]. \t  -7.190577679575107 \t -0.532918375028389\n",
            "51     \t [-3.03008776 -8.62582546]. \t  -46126.901882453436 \t -0.532918375028389\n",
            "52     \t [-1.06968479 -9.60467806]. \t  -68876.2628835377 \t -0.532918375028389\n",
            "53     \t [-0.19256959  0.6314155 ]. \t  -3.3821872186628372 \t -0.532918375028389\n",
            "54     \t [3.61105275 0.61496235]. \t  -23.116167764375554 \t -0.532918375028389\n",
            "55     \t [-0.28535136 -0.45466743]. \t  -2.6287606917904114 \t -0.532918375028389\n",
            "56     \t [2.06155584 9.71293459]. \t  -69655.6541345046 \t -0.532918375028389\n",
            "57     \t [-2.41865885 -4.25492326]. \t  -2995.8397045771317 \t -0.532918375028389\n",
            "58     \t [-0.28680741  0.46609942]. \t  -2.6964343801871378 \t -0.532918375028389\n",
            "59     \t [2.28988634 4.81890543]. \t  -3900.782175509248 \t -0.532918375028389\n",
            "60     \t [4.48241068 2.44398572]. \t  -123.54146915409495 \t -0.532918375028389\n",
            "61     \t [-9.9903605  -0.01517048]. \t  -320.42102403555316 \t -0.532918375028389\n",
            "62     \t [0.63733264 0.65791808]. \t  \u001b[92m-0.23584225442787377\u001b[0m \t -0.23584225442787377\n",
            "63     \t [0.26448304 0.34802002]. \t  -0.5419750656223005 \t -0.23584225442787377\n",
            "64     \t [0.79942715 0.55188731]. \t  \u001b[92m-0.11263324661875097\u001b[0m \t -0.11263324661875097\n",
            "65     \t [ 0.77876368 -0.82187594]. \t  -0.7037630616891044 \t -0.11263324661875097\n",
            "66     \t [ 5.96303625 -8.00820816]. \t  -29939.094089609996 \t -0.11263324661875097\n",
            "67     \t [4.08981309 0.7279362 ]. \t  -27.909119138463204 \t -0.11263324661875097\n",
            "68     \t [4.80742649 1.22440502]. \t  -21.042118328385396 \t -0.11263324661875097\n",
            "69     \t [-1.18667684 -0.12297926]. \t  -7.74336669155783 \t -0.11263324661875097\n",
            "70     \t [ 9.99087551 -4.86280159]. \t  -2863.8340680637184 \t -0.11263324661875097\n",
            "71     \t [3.86563751 8.40336539]. \t  -37747.851242460165 \t -0.11263324661875097\n",
            "72     \t [-3.98859714  0.66553368]. \t  -72.40696445989835 \t -0.11263324661875097\n",
            "73     \t [6.47280141 0.89138348]. \t  -77.6520672146135 \t -0.11263324661875097\n",
            "74     \t [0.8754449  0.66329311]. \t  \u001b[92m-0.015553945469003573\u001b[0m \t -0.015553945469003573\n",
            "75     \t [-0.2850654  -2.28991978]. \t  -233.74622481526887 \t -0.015553945469003573\n",
            "76     \t [ 5.40287929 -3.75820845]. \t  -1063.208321874586 \t -0.015553945469003573\n",
            "77     \t [ 0.47597082 -0.58416281]. \t  -0.35990888928450027 \t -0.015553945469003573\n",
            "78     \t [ 4.00702346 -4.21814416]. \t  -2003.4400713894202 \t -0.015553945469003573\n",
            "79     \t [ 1.46228444 -6.62887934]. \t  -14937.668100017143 \t -0.015553945469003573\n",
            "80     \t [2.01928411 9.20045794]. \t  -55964.60885655887 \t -0.015553945469003573\n",
            "81     \t [ 5.67102364 -3.82147844]. \t  -1129.740002070439 \t -0.015553945469003573\n",
            "82     \t [ 1.64310826 -0.82825349]. \t  -0.5605792656315828 \t -0.015553945469003573\n",
            "83     \t [-7.57356718  6.86182203]. \t  -20776.68867790153 \t -0.015553945469003573\n",
            "84     \t [-1.10128033 -1.17135281]. \t  -33.98981420862948 \t -0.015553945469003573\n",
            "85     \t [-6.34781947  4.59890045]. \t  -4787.166495711154 \t -0.015553945469003573\n",
            "86     \t [-8.69033094  3.93211189]. \t  -3232.3357421883297 \t -0.015553945469003573\n",
            "87     \t [ 1.17030668 -0.73391793]. \t  -0.04631561698678811 \t -0.015553945469003573\n",
            "88     \t [-1.84270439  9.92074067]. \t  -78959.45602882205 \t -0.015553945469003573\n",
            "89     \t [ 5.35086292 -4.73006518]. \t  -3123.046485571771 \t -0.015553945469003573\n",
            "90     \t [ 5.51750874 -2.52415985]. \t  -124.81656827996554 \t -0.015553945469003573\n",
            "91     \t [0.82574087 0.64960229]. \t  -0.03103057252125801 \t -0.015553945469003573\n",
            "92     \t [ 8.76963296 -9.60728067]. \t  -61892.78078871723 \t -0.015553945469003573\n",
            "93     \t [ 4.04346237 -2.9327525 ]. \t  -355.5608081303744 \t -0.015553945469003573\n",
            "94     \t [ 2.71201342 -1.00824714]. \t  -3.852770028128274 \t -0.015553945469003573\n",
            "95     \t [-0.17249246  1.00137998]. \t  -10.862247659516049 \t -0.015553945469003573\n",
            "96     \t [-7.06018259  3.18491234]. \t  -1560.7397815242407 \t -0.015553945469003573\n",
            "97     \t [-4.1757566   7.11125473]. \t  -22209.55836836178 \t -0.015553945469003573\n",
            "98     \t [-5.39054826  7.86566842]. \t  -33388.93420504962 \t -0.015553945469003573\n",
            "99     \t [ 5.99666688 -0.69597377]. \t  -75.526395702937 \t -0.015553945469003573\n",
            "100    \t [9.84428029 6.27909718]. \t  -9602.938005260863 \t -0.015553945469003573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "9e7362c8-a4bd-479e-8283-beb4f28d8e1b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_14 = d2GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.95102099 0.14299376]. \t  -20.744156010505872 \t -20.744156010505872\n",
            "init   \t [0.5668276  7.92570408]. \t  -31283.56791285067 \t -20.744156010505872\n",
            "init   \t [3.99982381 4.28594204]. \t  -2152.65365220131 \t -20.744156010505872\n",
            "init   \t [ 4.34676764 -5.54361076]. \t  -6535.783782469756 \t -20.744156010505872\n",
            "init   \t [-6.49690953 -0.86317023]. \t  -183.78911629570374 \t -20.744156010505872\n",
            "1      \t [-3.82909823 -9.9518425 ]. \t  -81556.54609796016 \t -20.744156010505872\n",
            "2      \t [-26.06651609 -28.6934826 ]. \t  -5596572.497420569 \t -20.744156010505872\n",
            "3      \t [-8.75957512  8.21832577]. \t  -41475.906975640784 \t -20.744156010505872\n",
            "4      \t [  3.90475719 -12.48795166]. \t  -189727.95697983992 \t -20.744156010505872\n",
            "5      \t [-10.97756517 -11.6266194 ]. \t  -158441.2390006013 \t -20.744156010505872\n",
            "6      \t [8.42986383 9.7389967 ]. \t  -65770.10932310983 \t -20.744156010505872\n",
            "7      \t [-12.08738063  -3.93567084]. \t  -3880.7075116888527 \t -20.744156010505872\n",
            "8      \t [8.47629598 0.03320735]. \t  -199.51542183217234 \t -20.744156010505872\n",
            "9      \t [ 9.93412348 -8.73957701]. \t  -40878.51622921913 \t -20.744156010505872\n",
            "10     \t [-1.51986379 -3.40109471]. \t  -1222.0635260913602 \t -20.744156010505872\n",
            "11     \t [-4.04855107  3.73708648]. \t  -2070.951930397764 \t -20.744156010505872\n",
            "12     \t [-11.34472164   2.22267455]. \t  -1053.4176824372353 \t -20.744156010505872\n",
            "13     \t [9.29077045 4.26738451]. \t  -1540.850903126652 \t -20.744156010505872\n",
            "14     \t [-7.75022868 -5.14349578]. \t  -7436.159541013263 \t -20.744156010505872\n",
            "15     \t [-0.27698467  1.07452841]. \t  \u001b[92m-15.007626072495281\u001b[0m \t -15.007626072495281\n",
            "16     \t [ 8.24794658 -3.29737402]. \t  -416.89257111012483 \t -15.007626072495281\n",
            "17     \t [-3.84518681  9.43716885]. \t  -66246.36120432989 \t -15.007626072495281\n",
            "18     \t [ 0.71976447 -7.80599006]. \t  -29353.365235377667 \t -15.007626072495281\n",
            "19     \t [ 4.93462122 -0.58925448]. \t  -51.43948921692635 \t -15.007626072495281\n",
            "20     \t [0.0206396  2.76110062]. \t  -464.6647646905044 \t -15.007626072495281\n",
            "21     \t [-1.17185112  0.59624905]. \t  \u001b[92m-11.80738873932642\u001b[0m \t -11.80738873932642\n",
            "22     \t [ 0.50131895 -0.47781854]. \t  \u001b[92m-0.2526785797869709\u001b[0m \t -0.2526785797869709\n",
            "23     \t [-0.88813303  0.23649444]. \t  -5.565015412361721 \t -0.2526785797869709\n",
            "24     \t [ 2.27968192 -1.77412231]. \t  -33.88346506846342 \t -0.2526785797869709\n",
            "25     \t [-7.94035663  1.18678472]. \t  -311.36780316751907 \t -0.2526785797869709\n",
            "26     \t [-2.86990943  0.33672408]. \t  -34.15499915685047 \t -0.2526785797869709\n",
            "27     \t [1.27794013 0.21785427]. \t  -2.8763193968011858 \t -0.2526785797869709\n",
            "28     \t [ 0.70801262 -0.05226983]. \t  -1.072404996191551 \t -0.2526785797869709\n",
            "29     \t [0.5283986 0.0404275]. \t  -0.7739305609058844 \t -0.2526785797869709\n",
            "30     \t [-7.72621745  4.29234338]. \t  -4049.9440807169954 \t -0.2526785797869709\n",
            "31     \t [ 0.58951108 -0.70364728]. \t  -0.4896668791270167 \t -0.2526785797869709\n",
            "32     \t [5.6873449  1.34849608]. \t  -30.379987455612657 \t -0.2526785797869709\n",
            "33     \t [ 1.35261235 -0.74064282]. \t  -0.25490492907314577 \t -0.2526785797869709\n",
            "34     \t [-4.12909203 -0.52282149]. \t  -70.03335985327645 \t -0.2526785797869709\n",
            "35     \t [ 5.34586013 -1.82885466]. \t  -22.496799744657586 \t -0.2526785797869709\n",
            "36     \t [ 0.62923186 -0.36561634]. \t  -0.39938507686093977 \t -0.2526785797869709\n",
            "37     \t [ 1.19424952 -0.31428534]. \t  -2.024550545279648 \t -0.2526785797869709\n",
            "38     \t [-0.06061798  0.49695564]. \t  -1.73995704089928 \t -0.2526785797869709\n",
            "39     \t [2.21243124 0.62390212]. \t  -5.582262981047484 \t -0.2526785797869709\n",
            "40     \t [-1.08640402 -0.3534857 ]. \t  -7.924521510214021 \t -0.2526785797869709\n",
            "41     \t [-9.271154   -1.43618489]. \t  -464.42410367310896 \t -0.2526785797869709\n",
            "42     \t [7.00037581 1.56548397]. \t  -44.81523591915629 \t -0.2526785797869709\n",
            "43     \t [ 1.98427218 -0.50928043]. \t  -5.26440119480513 \t -0.2526785797869709\n",
            "44     \t [ 1.45304025 -1.47538326]. \t  -17.030712921107778 \t -0.2526785797869709\n",
            "45     \t [4.37574021 7.78749613]. \t  -27349.3736907906 \t -0.2526785797869709\n",
            "46     \t [-4.75306107 -2.73532456]. \t  -810.6227739528122 \t -0.2526785797869709\n",
            "47     \t [0.04416618 0.55856979]. \t  -1.5860337751806841 \t -0.2526785797869709\n",
            "48     \t [ 3.77285617 -1.8905101 ]. \t  -30.472691179992328 \t -0.2526785797869709\n",
            "49     \t [4.43027072 0.97124894]. \t  -24.706780058562657 \t -0.2526785797869709\n",
            "50     \t [0.77392253 0.55509967]. \t  \u001b[92m-0.10081884565326728\u001b[0m \t -0.10081884565326728\n",
            "51     \t [-9.78985189  3.04528552]. \t  -1722.435073549983 \t -0.10081884565326728\n",
            "52     \t [ 0.89472625 -0.61788589]. \t  \u001b[92m-0.045488607899963125\u001b[0m \t -0.045488607899963125\n",
            "53     \t [0.55612782 0.46089339]. \t  -0.2314926430225663 \t -0.045488607899963125\n",
            "54     \t [3.1979886  1.12182317]. \t  -5.758714471612511 \t -0.045488607899963125\n",
            "55     \t [-7.84198973 -5.30413911]. \t  -8298.306829346673 \t -0.045488607899963125\n",
            "56     \t [2.75543878 1.19495299]. \t  -3.1017202041493457 \t -0.045488607899963125\n",
            "57     \t [-7.76871008 -3.18398393]. \t  -1649.8465300153564 \t -0.045488607899963125\n",
            "58     \t [-4.2717033   5.34641832]. \t  -7577.557966132889 \t -0.045488607899963125\n",
            "59     \t [-4.85850499  0.88508561]. \t  -116.88996371597301 \t -0.045488607899963125\n",
            "60     \t [-3.70409777 -5.27977129]. \t  -7092.176487241333 \t -0.045488607899963125\n",
            "61     \t [-4.80405407 -9.43966596]. \t  -67025.33281313878 \t -0.045488607899963125\n",
            "62     \t [2.46348913 1.07851701]. \t  -2.1793884622273807 \t -0.045488607899963125\n",
            "63     \t [-8.14075251  2.79073766]. \t  -1208.563170401845 \t -0.045488607899963125\n",
            "64     \t [ 0.74047536 -0.6718284 ]. \t  -0.11999111469854173 \t -0.045488607899963125\n",
            "65     \t [8.5032172  2.33748767]. \t  -68.05447515116728 \t -0.045488607899963125\n",
            "66     \t [-6.75387576 -3.05541974]. \t  -1352.9894685659553 \t -0.045488607899963125\n",
            "67     \t [ 6.52116839 -9.49089424]. \t  -60327.305906501824 \t -0.045488607899963125\n",
            "68     \t [8.08369547 1.50633416]. \t  -75.32144567329101 \t -0.045488607899963125\n",
            "69     \t [-8.34277372 -3.1991881 ]. \t  -1747.5942162311192 \t -0.045488607899963125\n",
            "70     \t [6.77549093 2.35990157]. \t  -71.42399333431575 \t -0.045488607899963125\n",
            "71     \t [6.16516321 0.61811416]. \t  -85.0212252663612 \t -0.045488607899963125\n",
            "72     \t [9.96549504 1.77069083]. \t  -107.68323953330102 \t -0.045488607899963125\n",
            "73     \t [-9.8685891   8.29621224]. \t  -43644.11708412386 \t -0.045488607899963125\n",
            "74     \t [4.12031716 0.18517141]. \t  -42.56957734983289 \t -0.045488607899963125\n",
            "75     \t [-3.28487492 -7.62804324]. \t  -28654.8945934056 \t -0.045488607899963125\n",
            "76     \t [ 8.24492868 -8.27259918]. \t  -33142.230176738776 \t -0.045488607899963125\n",
            "77     \t [ 3.1867796 -9.2083516]. \t  -55383.167089094495 \t -0.045488607899963125\n",
            "78     \t [-8.28717958 -9.40295244]. \t  -68623.7541473987 \t -0.045488607899963125\n",
            "79     \t [ 6.74504647 -8.46425389]. \t  -37320.518206723325 \t -0.045488607899963125\n",
            "80     \t [-5.78481188  3.27956315]. \t  -1536.1638856835025 \t -0.045488607899963125\n",
            "81     \t [-7.29670082  2.55242434]. \t  -895.163642157233 \t -0.045488607899963125\n",
            "82     \t [-5.18300448 -5.94565128]. \t  -11555.15717335925 \t -0.045488607899963125\n",
            "83     \t [-2.35585868 -5.29245568]. \t  -6826.784218325494 \t -0.045488607899963125\n",
            "84     \t [ 6.79908003 -6.61180915]. \t  -13036.977935772078 \t -0.045488607899963125\n",
            "85     \t [ 3.3611301  -7.53643968]. \t  -24308.960123070992 \t -0.045488607899963125\n",
            "86     \t [-2.53790586  2.45106559]. \t  -436.1168249295706 \t -0.045488607899963125\n",
            "87     \t [-2.16686506  0.02720457]. \t  -19.432476406526817 \t -0.045488607899963125\n",
            "88     \t [8.16694511 2.96724987]. \t  -229.67532676975765 \t -0.045488607899963125\n",
            "89     \t [ 9.89443542 -1.46004536]. \t  -142.52663895626415 \t -0.045488607899963125\n",
            "90     \t [ 8.15236323 -4.41999009]. \t  -1963.2814553179464 \t -0.045488607899963125\n",
            "91     \t [ 1.41598313 -3.59687813]. \t  -1196.6663659639614 \t -0.045488607899963125\n",
            "92     \t [ 8.71660806 -2.36680358]. \t  -71.9154860123935 \t -0.045488607899963125\n",
            "93     \t [2.13956651 1.21966504]. \t  -2.695063632186856 \t -0.045488607899963125\n",
            "94     \t [ 6.48143849 -2.35955314]. \t  -73.3571027698803 \t -0.045488607899963125\n",
            "95     \t [-2.66782198 -0.61507657]. \t  -36.90677547748368 \t -0.045488607899963125\n",
            "96     \t [ 0.28359708 -0.42166557]. \t  -0.523603048363225 \t -0.045488607899963125\n",
            "97     \t [ 6.34341167 -5.35530953]. \t  -5233.667329281431 \t -0.045488607899963125\n",
            "98     \t [ 9.7014008  -1.18928938]. \t  -170.17915159284428 \t -0.045488607899963125\n",
            "99     \t [ 8.99303271 -1.29363549]. \t  -127.64426897721775 \t -0.045488607899963125\n",
            "100    \t [-3.29970925 -9.55423711]. \t  -69111.27565440805 \t -0.045488607899963125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "c49772b1-1e86-4d2c-c1ba-2f667e334791"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_15 = d2GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.55763717 -4.88997339]. \t  -4281.404311682752 \t -3267.0472724202245\n",
            "init   \t [-4.94946261  4.19808709]. \t  -3267.0472724202245 \t -3267.0472724202245\n",
            "init   \t [-1.04895274 -5.46114084]. \t  -7372.467127638806 \t -3267.0472724202245\n",
            "init   \t [-1.96580869  7.6451547 ]. \t  -28265.42637457811 \t -3267.0472724202245\n",
            "init   \t [-1.25573066  7.56850362]. \t  -26833.737231776184 \t -3267.0472724202245\n",
            "1      \t [-9.76533982 -4.68487047]. \t  -5874.976174155925 \t -3267.0472724202245\n",
            "2      \t [8.28335803 6.88744078]. \t  -15048.8154067911 \t -3267.0472724202245\n",
            "3      \t [ 9.8362843  -6.65190316]. \t  -12452.665647642647 \t -3267.0472724202245\n",
            "4      \t [ -7.51309763 -15.06647996]. \t  -426056.7889400076 \t -3267.0472724202245\n",
            "5      \t [  6.30126693 -12.74130618]. \t  -202760.38799762737 \t -3267.0472724202245\n",
            "6      \t [-11.43331086   2.11634209]. \t  \u001b[92m-986.1826473273671\u001b[0m \t -986.1826473273671\n",
            "7      \t [3.11660642 2.04785404]. \t  \u001b[92m-60.042812109163265\u001b[0m \t -60.042812109163265\n",
            "8      \t [ -0.99208407 -12.01114778]. \t  -167656.22909151213 \t -60.042812109163265\n",
            "9      \t [-394.874255   -394.96858932]. \t  -195181737862.27136 \t -60.042812109163265\n",
            "10     \t [9.34011063 0.13447336]. \t  -242.68421009840233 \t -60.042812109163265\n",
            "11     \t [-1.31331264  0.26183701]. \t  \u001b[92m-9.558909094313758\u001b[0m \t -9.558909094313758\n",
            "12     \t [-9.43319146  9.39489306]. \t  -69272.00418704003 \t -9.558909094313758\n",
            "13     \t [-18.78693924 -17.94078855]. \t  -878285.4505896639 \t -9.558909094313758\n",
            "14     \t [-5.1354557  -1.68794517]. \t  -272.3851117882557 \t -9.558909094313758\n",
            "15     \t [-6.21236802 -9.18113137]. \t  -61161.19930509341 \t -9.558909094313758\n",
            "16     \t [ 5.92511739 -2.82220197]. \t  -224.43804183091714 \t -9.558909094313758\n",
            "17     \t [3.41287497 6.92277872]. \t  -17094.966500815648 \t -9.558909094313758\n",
            "18     \t [ 5.01310713 -7.60769602]. \t  -24543.230092565893 \t -9.558909094313758\n",
            "19     \t [-0.42302743  2.88510736]. \t  -584.8437263443703 \t -9.558909094313758\n",
            "20     \t [-0.16470989 -1.10145798]. \t  -14.784450893194698 \t -9.558909094313758\n",
            "21     \t [6.05803867 1.05523586]. \t  -54.936773239936926 \t -9.558909094313758\n",
            "22     \t [-2.53282851  0.22573429]. \t  -26.364591710511416 \t -9.558909094313758\n",
            "23     \t [-1.2302025  -0.28339211]. \t  \u001b[92m-8.84258979830038\u001b[0m \t -8.84258979830038\n",
            "24     \t [-8.65061276  0.06284463]. \t  -243.07397492319214 \t -8.84258979830038\n",
            "25     \t [ 2.12406673 -0.93845658]. \t  \u001b[92m-1.5265781348713974\u001b[0m \t -1.5265781348713974\n",
            "26     \t [ 9.1944841  -2.44941832]. \t  -82.88355600745071 \t -1.5265781348713974\n",
            "27     \t [ 2.55959379 -0.87484459]. \t  -4.5495524754281735 \t -1.5265781348713974\n",
            "28     \t [-9.17200044  4.92120769]. \t  -6740.9644090024985 \t -1.5265781348713974\n",
            "29     \t [6.61800799 3.16753108]. \t  -393.2862247841722 \t -1.5265781348713974\n",
            "30     \t [ 4.76769694 -0.21789049]. \t  -57.864620777424264 \t -1.5265781348713974\n",
            "31     \t [ 1.77231288 -0.66430961]. \t  -2.1795935255634977 \t -1.5265781348713974\n",
            "32     \t [9.94034579 3.01432936]. \t  -215.46199536446963 \t -1.5265781348713974\n",
            "33     \t [ 1.72066799 -0.88136658]. \t  \u001b[92m-0.5751763577586422\u001b[0m \t -0.5751763577586422\n",
            "34     \t [-4.58915243 -4.97926089]. \t  -5901.1498158668055 \t -0.5751763577586422\n",
            "35     \t [-1.47488689 -1.75213823]. \t  -122.09736089145511 \t -0.5751763577586422\n",
            "36     \t [ 0.13088774 -0.3750274 ]. \t  -0.8005984719525697 \t -0.5751763577586422\n",
            "37     \t [ 0.07548293 -0.56950331]. \t  -1.5118141254390878 \t -0.5751763577586422\n",
            "38     \t [ 1.19764072 -1.00125792]. \t  -1.3428323483178457 \t -0.5751763577586422\n",
            "39     \t [-0.81077909 -0.01382482]. \t  -4.59488637802519 \t -0.5751763577586422\n",
            "40     \t [ 2.38815579 -1.26345171]. \t  -3.22130319108295 \t -0.5751763577586422\n",
            "41     \t [ 0.41462043 -0.60444439]. \t  \u001b[92m-0.5424894601756779\u001b[0m \t -0.5424894601756779\n",
            "42     \t [-3.55222476  1.31838855]. \t  -119.52297848892475 \t -0.5424894601756779\n",
            "43     \t [ 0.92803719 -0.64651076]. \t  \u001b[92m-0.022137895347558854\u001b[0m \t -0.022137895347558854\n",
            "44     \t [0.85033931 3.49628115]. \t  -1113.7181578937912 \t -0.022137895347558854\n",
            "45     \t [2.72532957 0.28393154]. \t  -16.12593184759468 \t -0.022137895347558854\n",
            "46     \t [5.50934152 9.97080643]. \t  -74769.15171027063 \t -0.022137895347558854\n",
            "47     \t [ 0.3794039  -0.42480457]. \t  -0.3858229843734759 \t -0.022137895347558854\n",
            "48     \t [ 6.33124059 -0.34756025]. \t  -102.58966238895482 \t -0.022137895347558854\n",
            "49     \t [4.43689229 1.87440423]. \t  -25.227290333061198 \t -0.022137895347558854\n",
            "50     \t [-5.04355946  9.7782733 ]. \t  -77082.56208637712 \t -0.022137895347558854\n",
            "51     \t [ 0.91774604 -0.55424122]. \t  -0.1908438035179484 \t -0.022137895347558854\n",
            "52     \t [-4.73836531 -2.7767814 ]. \t  -845.7312696634915 \t -0.022137895347558854\n",
            "53     \t [-5.19661439  0.6196679 ]. \t  -109.55072131728093 \t -0.022137895347558854\n",
            "54     \t [ 3.58843631 -1.90789745]. \t  -33.95743430081127 \t -0.022137895347558854\n",
            "55     \t [3.97051309 1.06646228]. \t  -14.575623335779952 \t -0.022137895347558854\n",
            "56     \t [2.43116934 5.18720372]. \t  -5282.4727117377015 \t -0.022137895347558854\n",
            "57     \t [-8.41074348 -8.70603717]. \t  -51289.14438183774 \t -0.022137895347558854\n",
            "58     \t [-2.07398088  5.0103825 ]. \t  -5476.232284473293 \t -0.022137895347558854\n",
            "59     \t [-4.38452908 -0.14070639]. \t  -68.13892913132634 \t -0.022137895347558854\n",
            "60     \t [ 9.8845375  -3.76667601]. \t  -762.7809199151517 \t -0.022137895347558854\n",
            "61     \t [2.42559477 1.49682716]. \t  -10.481562535705507 \t -0.022137895347558854\n",
            "62     \t [1.31630156 0.51295289]. \t  -1.3484369840770294 \t -0.022137895347558854\n",
            "63     \t [1.36356184 0.61125589]. \t  -0.891814564927714 \t -0.022137895347558854\n",
            "64     \t [ 2.89586453 -3.25500391]. \t  -672.952472255595 \t -0.022137895347558854\n",
            "65     \t [1.72495396 0.97499318]. \t  -0.5877000694652834 \t -0.022137895347558854\n",
            "66     \t [3.33598621 2.31905593]. \t  -115.57125351130023 \t -0.022137895347558854\n",
            "67     \t [0.29177252 3.76797388]. \t  -1580.112963263459 \t -0.022137895347558854\n",
            "68     \t [-0.60408676  1.56069986]. \t  -62.538686514977236 \t -0.022137895347558854\n",
            "69     \t [ 3.64714609 -1.32715244]. \t  -7.038372402690053 \t -0.022137895347558854\n",
            "70     \t [ 7.65289543 -2.37467238]. \t  -70.54578285242621 \t -0.022137895347558854\n",
            "71     \t [ 8.64749813 -1.60833237]. \t  -82.62202585955133 \t -0.022137895347558854\n",
            "72     \t [4.93270146 4.87445417]. \t  -3642.9255103394166 \t -0.022137895347558854\n",
            "73     \t [-4.33233879  7.85144038]. \t  -32603.475042977963 \t -0.022137895347558854\n",
            "74     \t [-9.32629639  3.54705748]. \t  -2485.687738481979 \t -0.022137895347558854\n",
            "75     \t [ 2.93521859 -1.14713795]. \t  -3.9291348533950647 \t -0.022137895347558854\n",
            "76     \t [ 4.01917162 -7.50134124]. \t  -23562.76031559984 \t -0.022137895347558854\n",
            "77     \t [5.15533938 1.51242473]. \t  -17.9407646245595 \t -0.022137895347558854\n",
            "78     \t [ 9.83384425 -0.69132109]. \t  -235.67437894663698 \t -0.022137895347558854\n",
            "79     \t [1.38278424 1.07406967]. \t  -1.8558024260156791 \t -0.022137895347558854\n",
            "80     \t [4.41412612 1.54129151]. \t  -11.88343948408625 \t -0.022137895347558854\n",
            "81     \t [7.74797399 9.81281296]. \t  -68373.2105317395 \t -0.022137895347558854\n",
            "82     \t [0.519504   5.28291595]. \t  -6116.167104381529 \t -0.022137895347558854\n",
            "83     \t [ 1.21280511 -0.77999727]. \t  -0.04531779701916971 \t -0.022137895347558854\n",
            "84     \t [ 0.55410946 -0.16344366]. \t  -0.7001829035173883 \t -0.022137895347558854\n",
            "85     \t [2.78277336 1.18352047]. \t  -3.1789778460108558 \t -0.022137895347558854\n",
            "86     \t [0.82692303 1.02223155]. \t  -3.2202514883390627 \t -0.022137895347558854\n",
            "87     \t [-8.26180241 -0.01564831]. \t  -222.31192709882077 \t -0.022137895347558854\n",
            "88     \t [-3.35697144 -0.49094299]. \t  -48.45937206509758 \t -0.022137895347558854\n",
            "89     \t [-1.97101032 -4.06030242]. \t  -2450.870425270952 \t -0.022137895347558854\n",
            "90     \t [-2.89804816 -1.01231984]. \t  -64.15288597645016 \t -0.022137895347558854\n",
            "91     \t [6.47232451 0.25390567]. \t  -110.42348666866121 \t -0.022137895347558854\n",
            "92     \t [-4.06431762  0.37047618]. \t  -63.298079846082146 \t -0.022137895347558854\n",
            "93     \t [0.51746225 0.78366197]. \t  -1.243287302612685 \t -0.022137895347558854\n",
            "94     \t [7.47848886 4.32620845]. \t  -1836.4155354062034 \t -0.022137895347558854\n",
            "95     \t [5.21339778 7.16195836]. \t  -18981.104424719997 \t -0.022137895347558854\n",
            "96     \t [6.19567443 1.94765314]. \t  -30.864967761607573 \t -0.022137895347558854\n",
            "97     \t [-0.46237735  0.78239592]. \t  -7.828219065210108 \t -0.022137895347558854\n",
            "98     \t [0.84397922 0.75161594]. \t  -0.18779018159473626 \t -0.022137895347558854\n",
            "99     \t [-4.26426686 -2.52050317]. \t  -603.6837600478647 \t -0.022137895347558854\n",
            "100    \t [-1.30453102  0.88388112]. \t  -21.750501454050546 \t -0.022137895347558854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "7bc2a83f-7b2e-48b3-e9bd-b8195a5845cc"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_16 = d2GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.27896664 -3.67267997]. \t  -815.4627708918207 \t -28.992408538517264\n",
            "init   \t [3.44301557 0.15823335]. \t  -28.992408538517264 \t -28.992408538517264\n",
            "init   \t [ 5.63320853 -4.32636643]. \t  -2044.16089755496 \t -28.992408538517264\n",
            "init   \t [-5.32582432  1.27599373]. \t  -187.32245577722023 \t -28.992408538517264\n",
            "init   \t [7.50048729 4.37885841]. \t  -1945.4933626881252 \t -28.992408538517264\n",
            "1      \t [-2.44633796 -8.71755625]. \t  -47714.014485289874 \t -28.992408538517264\n",
            "2      \t [-0.94408028  9.32615359]. \t  -61182.65342174163 \t -28.992408538517264\n",
            "3      \t [-9.58291173  9.55394704]. \t  -73946.57346618871 \t -28.992408538517264\n",
            "4      \t [-9.96784472 -3.20585558]. \t  -1983.5841560879148 \t -28.992408538517264\n",
            "5      \t [-12.07466105  -9.68777512]. \t  -79995.55058438538 \t -28.992408538517264\n",
            "6      \t [  5.36487823 -11.09395012]. \t  -115975.406719157 \t -28.992408538517264\n",
            "7      \t [-1.00187217 -2.41726175]. \t  -325.987359586068 \t -28.992408538517264\n",
            "8      \t [-12.67901686 -16.99291113]. \t  -696852.2878803565 \t -28.992408538517264\n",
            "9      \t [-10.30945384   2.48615157]. \t  -1155.8842484877978 \t -28.992408538517264\n",
            "10     \t [2.61479957e-03 3.14594750e+00]. \t  -784.3903644241187 \t -28.992408538517264\n",
            "11     \t [8.87535711 9.17115881]. \t  -50843.647954551314 \t -28.992408538517264\n",
            "12     \t [ 9.83171549 -7.92230725]. \t  -26848.289891580538 \t -28.992408538517264\n",
            "13     \t [-5.4193772   5.71201839]. \t  -10030.748009177907 \t -28.992408538517264\n",
            "14     \t [-4.98143955 -3.92257851]. \t  -2592.572457088352 \t -28.992408538517264\n",
            "15     \t [7.90708084 0.0832154 ]. \t  -172.31396434180684 \t -28.992408538517264\n",
            "16     \t [2.96807012 6.08391022]. \t  -10102.87986903581 \t -28.992408538517264\n",
            "17     \t [ 1.58776498 -5.50892905]. \t  -6988.053884985168 \t -28.992408538517264\n",
            "18     \t [-7.21381515 -8.3774682 ]. \t  -43625.858420012475 \t -28.992408538517264\n",
            "19     \t [-2.39123278  0.39915672]. \t  \u001b[92m-26.18741269329688\u001b[0m \t -26.18741269329688\n",
            "20     \t [4.85166618 1.75768794]. \t  \u001b[92m-18.358610867762607\u001b[0m \t -18.358610867762607\n",
            "21     \t [ 2.40127997 -1.86557906]. \t  -43.54149213746919 \t -18.358610867762607\n",
            "22     \t [-6.8214451  -0.93193248]. \t  -207.66884082649068 \t -18.358610867762607\n",
            "23     \t [-3.17980515 -0.24943833]. \t  -39.306829323052 \t -18.358610867762607\n",
            "24     \t [ 9.94693951 -2.70869861]. \t  -124.73974959393753 \t -18.358610867762607\n",
            "25     \t [4.88740934 0.09540586]. \t  -62.53026178720407 \t -18.358610867762607\n",
            "26     \t [ 0.03095551 -0.10316497]. \t  \u001b[92m-0.9392342300294892\u001b[0m \t -0.9392342300294892\n",
            "27     \t [2.71800746 2.29317025]. \t  -124.60821707074462 \t -0.9392342300294892\n",
            "28     \t [-9.44126646  5.41888513]. \t  -9403.290179141928 \t -0.9392342300294892\n",
            "29     \t [ 1.001862   -0.83198532]. \t  \u001b[92m-0.2926728162280416\u001b[0m \t -0.2926728162280416\n",
            "30     \t [ 3.96180743 -1.29116039]. \t  -9.560109736461621 \t -0.2926728162280416\n",
            "31     \t [9.61926857 1.57518535]. \t  -117.66430894956987 \t -0.2926728162280416\n",
            "32     \t [-4.97834856  9.77045296]. \t  -76790.810880623 \t -0.2926728162280416\n",
            "33     \t [0.77393442 0.19706055]. \t  -1.020685854736246 \t -0.2926728162280416\n",
            "34     \t [-7.3547023   1.77997483]. \t  -444.705705149057 \t -0.2926728162280416\n",
            "35     \t [ 2.13926091 -0.81780946]. \t  -2.583156892710142 \t -0.2926728162280416\n",
            "36     \t [-3.43632514  1.34585004]. \t  -119.33852461529034 \t -0.2926728162280416\n",
            "37     \t [ 0.42556786 -0.1561308 ]. \t  -0.6139501756576893 \t -0.2926728162280416\n",
            "38     \t [ 0.62800132 -0.47460437]. \t  \u001b[92m-0.20139744124299974\u001b[0m \t -0.20139744124299974\n",
            "39     \t [ 1.32460687 -9.68401501]. \t  -69367.56720518795 \t -0.20139744124299974\n",
            "40     \t [ 0.36977173 -9.90508545]. \t  -76716.14558306814 \t -0.20139744124299974\n",
            "41     \t [-1.2770428   0.27378714]. \t  -9.25736274573276 \t -0.20139744124299974\n",
            "42     \t [7.39423722 9.80328097]. \t  -68353.61004999385 \t -0.20139744124299974\n",
            "43     \t [1.37489416 0.10534609]. \t  -3.8001323949640127 \t -0.20139744124299974\n",
            "44     \t [ 8.19858837 -9.66164772]. \t  -63773.66709109646 \t -0.20139744124299974\n",
            "45     \t [-2.99252903 -8.06234898]. \t  -35391.53099516104 \t -0.20139744124299974\n",
            "46     \t [0.82950982 0.01096577]. \t  -1.4044421387685726 \t -0.20139744124299974\n",
            "47     \t [5.60056357 1.828265  ]. \t  -23.51764895890401 \t -0.20139744124299974\n",
            "48     \t [ 2.43890832 -0.14598987]. \t  -13.554794344136067 \t -0.20139744124299974\n",
            "49     \t [4.67432013 1.10826181]. \t  -23.338183015419787 \t -0.20139744124299974\n",
            "50     \t [ 3.34524984 -0.67491752]. \t  -17.35107527644133 \t -0.20139744124299974\n",
            "51     \t [-8.03251586  4.12578886]. \t  -3622.4977333168395 \t -0.20139744124299974\n",
            "52     \t [ 3.83209313 -1.62122812]. \t  -12.08010992927644 \t -0.20139744124299974\n",
            "53     \t [-6.21234888 -9.10408776]. \t  -59207.06821301126 \t -0.20139744124299974\n",
            "54     \t [ 1.41724058 -0.75774225]. \t  -0.31869762107958144 \t -0.20139744124299974\n",
            "55     \t [ 0.71733069 -0.46317606]. \t  -0.24609716901289924 \t -0.20139744124299974\n",
            "56     \t [ 0.22004071 -0.03815501]. \t  -0.7026265894274512 \t -0.20139744124299974\n",
            "57     \t [-5.87599396  3.20777726]. \t  -1447.0835971801562 \t -0.20139744124299974\n",
            "58     \t [ 8.52377976 -9.26055523]. \t  -53189.38260389485 \t -0.20139744124299974\n",
            "59     \t [-1.78942802 -0.09691785]. \t  -14.320185603340352 \t -0.20139744124299974\n",
            "60     \t [ 1.54529583 -0.42017554]. \t  -3.1400333306013914 \t -0.20139744124299974\n",
            "61     \t [3.00913684 9.91331595]. \t  -74918.36128909086 \t -0.20139744124299974\n",
            "62     \t [ 6.61537771 -1.9690889 ]. \t  -34.12822279685744 \t -0.20139744124299974\n",
            "63     \t [ 5.9626285  -1.42803205]. \t  -31.727176942282654 \t -0.20139744124299974\n",
            "64     \t [ 7.61775892 -1.63593355]. \t  -54.05701102065142 \t -0.20139744124299974\n",
            "65     \t [-1.75567565  9.70653295]. \t  -72351.48731166038 \t -0.20139744124299974\n",
            "66     \t [-7.61918752 -8.44388807]. \t  -45204.992501737106 \t -0.20139744124299974\n",
            "67     \t [-2.53598717 -7.62877066]. \t  -28302.273204681238 \t -0.20139744124299974\n",
            "68     \t [ 7.17931782 -0.57421152]. \t  -123.20164106679526 \t -0.20139744124299974\n",
            "69     \t [6.33797836 5.1826123 ]. \t  -4518.405148097174 \t -0.20139744124299974\n",
            "70     \t [ 6.23554753 -0.27153034]. \t  -101.54063958936088 \t -0.20139744124299974\n",
            "71     \t [-0.0710583   7.88508369]. \t  -30961.893643346364 \t -0.20139744124299974\n",
            "72     \t [9.75480775 4.36809253]. \t  -1690.409359333059 \t -0.20139744124299974\n",
            "73     \t [3.25089107 3.65040972]. \t  -1100.1950384901293 \t -0.20139744124299974\n",
            "74     \t [3.73819659 1.41966354]. \t  -7.66905843336126 \t -0.20139744124299974\n",
            "75     \t [2.17539147 1.07904357]. \t  -1.4285337460124834 \t -0.20139744124299974\n",
            "76     \t [2.52932073 1.22033602]. \t  -2.7422381655846433 \t -0.20139744124299974\n",
            "77     \t [-6.22501894  3.52131694]. \t  -1977.2233309434237 \t -0.20139744124299974\n",
            "78     \t [-0.10241599  0.11671667]. \t  -1.2489452582909175 \t -0.20139744124299974\n",
            "79     \t [ 9.55122594 -1.83559808]. \t  -88.94248751324137 \t -0.20139744124299974\n",
            "80     \t [9.33855769 4.80413045]. \t  -2781.0713822845873 \t -0.20139744124299974\n",
            "81     \t [-9.64089881 -9.7796186 ]. \t  -80853.1547843824 \t -0.20139744124299974\n",
            "82     \t [ 6.88349109 -5.5330375 ]. \t  -5941.485893656064 \t -0.20139744124299974\n",
            "83     \t [ 8.16371685 -9.0944609 ]. \t  -49509.41347498307 \t -0.20139744124299974\n",
            "84     \t [9.72920925 2.42401936]. \t  -84.38035313543975 \t -0.20139744124299974\n",
            "85     \t [ 7.84801605 -2.83981008]. \t  -184.04612616242872 \t -0.20139744124299974\n",
            "86     \t [1.08954768 0.73596483]. \t  \u001b[92m-0.00809714241241969\u001b[0m \t -0.00809714241241969\n",
            "87     \t [ 2.54599126 -8.46775776]. \t  -39685.38564442981 \t -0.00809714241241969\n",
            "88     \t [-8.10679451 -3.97384878]. \t  -3233.485078639014 \t -0.00809714241241969\n",
            "89     \t [0.61124987 4.40812793]. \t  -2926.5720044905806 \t -0.00809714241241969\n",
            "90     \t [ 4.40378496 -1.63600701]. \t  -13.387914139576013 \t -0.00809714241241969\n",
            "91     \t [-2.05843435  2.31714723]. \t  -336.86901750125134 \t -0.00809714241241969\n",
            "92     \t [1.60382435 1.26762827]. \t  -5.548407898119247 \t -0.00809714241241969\n",
            "93     \t [0.59222959 0.97291003]. \t  -3.55084523485797 \t -0.00809714241241969\n",
            "94     \t [-4.58912948  6.6451055 ]. \t  -17293.53875567509 \t -0.00809714241241969\n",
            "95     \t [-3.30205712  8.57469757]. \t  -45230.517092884154 \t -0.00809714241241969\n",
            "96     \t [ 3.00188445 -1.29398595]. \t  -4.248241124859602 \t -0.00809714241241969\n",
            "97     \t [-5.68991653  6.92891146]. \t  -20734.417945070247 \t -0.00809714241241969\n",
            "98     \t [5.05108621 5.33718012]. \t  -5407.76273916 \t -0.00809714241241969\n",
            "99     \t [-7.08194145  8.05431634]. \t  -37508.00702857726 \t -0.00809714241241969\n",
            "100    \t [-6.40214562  9.5152672 ]. \t  -70354.36649607628 \t -0.00809714241241969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "034b4dc4-bb0d-4bef-eded-66271c3943ee"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_17 = d2GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.03648056 4.61039249]. \t  -3120.691750998894 \t -133.8839693070206\n",
            "init   \t [-6.13265275  0.39354476]. \t  -133.8839693070206 \t -133.8839693070206\n",
            "init   \t [3.52766533 6.08889746]. \t  -9981.232217710998 \t -133.8839693070206\n",
            "init   \t [0.67025729 8.16068359]. \t  -35124.935571397844 \t -133.8839693070206\n",
            "init   \t [-1.13037742 -4.8805558 ]. \t  -4761.569630397088 \t -133.8839693070206\n",
            "1      \t [ 9.47756828 -1.18144623]. \t  -161.27269462973322 \t -133.8839693070206\n",
            "2      \t [-9.85290042 -8.55285713]. \t  -48886.9473889917 \t -133.8839693070206\n",
            "3      \t [-9.20794452  7.56905046]. \t  -30751.632331903027 \t -133.8839693070206\n",
            "4      \t [ 9.29095451 -9.9002349 ]. \t  -69811.16084951065 \t -133.8839693070206\n",
            "5      \t [9.65812984 9.94541442]. \t  -70886.65075415972 \t -133.8839693070206\n",
            "6      \t [  2.32802534 -12.12842359]. \t  -170377.13124064243 \t -133.8839693070206\n",
            "7      \t [-12.21855525  -1.35547702]. \t  -679.9172523504294 \t -133.8839693070206\n",
            "8      \t [ -4.50004644 -12.10420029]. \t  -177070.56850062392 \t -133.8839693070206\n",
            "9      \t [-24.69577122 -18.96020191]. \t  -1106763.1545213743 \t -133.8839693070206\n",
            "10     \t [ 3.73996695 -1.00135042]. \t  \u001b[92m-13.524827059253356\u001b[0m \t -13.524827059253356\n",
            "11     \t [-1.77704389  2.72980879]. \t  -564.2072089927992 \t -13.524827059253356\n",
            "12     \t [-12.08898324 -15.40062802]. \t  -473433.992585326 \t -13.524827059253356\n",
            "13     \t [-48.06356373 -51.80764923]. \t  -58671305.28830586 \t -13.524827059253356\n",
            "14     \t [ 4.37958906 -5.03368884]. \t  -4298.145604370071 \t -13.524827059253356\n",
            "15     \t [9.10259516 3.76068583]. \t  -801.6209395632309 \t -13.524827059253356\n",
            "16     \t [-6.72742494 -3.83957523]. \t  -2682.3455894633826 \t -13.524827059253356\n",
            "17     \t [-4.14762975  6.09236623]. \t  -12313.8116109825 \t -13.524827059253356\n",
            "18     \t [-9.29176093  2.06018579]. \t  -738.2121256501493 \t -13.524827059253356\n",
            "19     \t [5.97783219 0.84276511]. \t  -66.31725600831446 \t -13.524827059253356\n",
            "20     \t [ 1.214519   -0.57448845]. \t  \u001b[92m-0.6608370322265602\u001b[0m \t -0.6608370322265602\n",
            "21     \t [2.09342659 0.88499568]. \t  -1.7510226016935926 \t -0.6608370322265602\n",
            "22     \t [1.05351068 0.02102328]. \t  -2.2189094119435655 \t -0.6608370322265602\n",
            "23     \t [1.44577925 1.67789363]. \t  -35.225073284875805 \t -0.6608370322265602\n",
            "24     \t [ 1.66809502 -0.35033549]. \t  -4.494075401198297 \t -0.6608370322265602\n",
            "25     \t [ 0.78837037 -1.05630005]. \t  -4.210261778071582 \t -0.6608370322265602\n",
            "26     \t [ 8.36052857 -5.63741378]. \t  -6148.3323853774355 \t -0.6608370322265602\n",
            "27     \t [3.81849311 1.43789322]. \t  -8.14435013828235 \t -0.6608370322265602\n",
            "28     \t [-4.70679914 -7.03119119]. \t  -21491.06968409885 \t -0.6608370322265602\n",
            "29     \t [-1.24108251 -0.89945302]. \t  -21.371516102570585 \t -0.6608370322265602\n",
            "30     \t [-4.77948706  9.9722683 ]. \t  -82997.77178128301 \t -0.6608370322265602\n",
            "31     \t [-2.84127285 -1.31221161]. \t  -93.75962365964345 \t -0.6608370322265602\n",
            "32     \t [ 1.63018101 -1.60282106]. \t  -25.00770793771384 \t -0.6608370322265602\n",
            "33     \t [2.57308957 1.12777627]. \t  -2.4763314142857045 \t -0.6608370322265602\n",
            "34     \t [4.53817702 9.84643969]. \t  -71731.91582202556 \t -0.6608370322265602\n",
            "35     \t [-0.48462041 -8.5446659 ]. \t  -42930.953323449256 \t -0.6608370322265602\n",
            "36     \t [2.80349425 0.47333055]. \t  -14.348509999428376 \t -0.6608370322265602\n",
            "37     \t [ 6.21518174 -1.33888527]. \t  -41.031438615625575 \t -0.6608370322265602\n",
            "38     \t [0.00411651 0.12813953]. \t  -0.9934339480147786 \t -0.6608370322265602\n",
            "39     \t [-0.68256731 -0.58645986]. \t  -6.587231388977374 \t -0.6608370322265602\n",
            "40     \t [ 0.79948955 -0.20138059]. \t  -1.07234771750924 \t -0.6608370322265602\n",
            "41     \t [4.21911297 1.80139989]. \t  -20.67729944878713 \t -0.6608370322265602\n",
            "42     \t [-9.09251394 -1.73514767]. \t  -558.7241520724862 \t -0.6608370322265602\n",
            "43     \t [-4.83149124  2.3268064 ]. \t  -524.4491322431182 \t -0.6608370322265602\n",
            "44     \t [4.10684867 0.71992231]. \t  -28.505653933528745 \t -0.6608370322265602\n",
            "45     \t [2.93032952 1.50979216]. \t  -9.030947224062329 \t -0.6608370322265602\n",
            "46     \t [-0.0747283  -0.08948061]. \t  -1.1715090907208028 \t -0.6608370322265602\n",
            "47     \t [7.49418335 6.24942136]. \t  -10015.512564562698 \t -0.6608370322265602\n",
            "48     \t [-0.60229142 -0.15853239]. \t  -3.4189976733578664 \t -0.6608370322265602\n",
            "49     \t [3.32741633 1.17715336]. \t  -6.035219426396386 \t -0.6608370322265602\n",
            "50     \t [ 3.91126304 -1.25481989]. \t  -9.637097551155371 \t -0.6608370322265602\n",
            "51     \t [ 3.18630467 -1.72904504]. \t  -20.380384441871648 \t -0.6608370322265602\n",
            "52     \t [-8.87122825 -6.11949031]. \t  -14131.446577500716 \t -0.6608370322265602\n",
            "53     \t [-0.82920668  6.66522266]. \t  -16088.204723007937 \t -0.6608370322265602\n",
            "54     \t [7.70320643 1.15764082]. \t  -95.39286661319782 \t -0.6608370322265602\n",
            "55     \t [1.57473787 0.07283317]. \t  -5.223319725728929 \t -0.6608370322265602\n",
            "56     \t [ 4.77071925 -8.18989699]. \t  -33491.60698036469 \t -0.6608370322265602\n",
            "57     \t [-9.31116261  4.76144485]. \t  -6080.41257509405 \t -0.6608370322265602\n",
            "58     \t [-6.83505354  6.06352726]. \t  -12979.34794213537 \t -0.6608370322265602\n",
            "59     \t [ 0.03828497 -4.77979984]. \t  -4169.625990517897 \t -0.6608370322265602\n",
            "60     \t [-5.50634066  4.88851737]. \t  -5724.441837423282 \t -0.6608370322265602\n",
            "61     \t [8.76679784 4.29391193]. \t  -1640.5056011511847 \t -0.6608370322265602\n",
            "62     \t [ 0.53814252 -0.82267738]. \t  -1.543241617824508 \t -0.6608370322265602\n",
            "63     \t [4.67105362 7.62456185]. \t  -24921.18303646411 \t -0.6608370322265602\n",
            "64     \t [ 5.11998917 -1.5820477 ]. \t  -17.000411990404455 \t -0.6608370322265602\n",
            "65     \t [0.34576108 0.29031239]. \t  \u001b[92m-0.4908271938161843\u001b[0m \t -0.4908271938161843\n",
            "66     \t [9.18016933 2.20926011]. \t  -67.5914342215606 \t -0.4908271938161843\n",
            "67     \t [0.31687549 0.60476593]. \t  -0.8104589458355942 \t -0.4908271938161843\n",
            "68     \t [-9.0776245  -0.18304074]. \t  -268.8071160142377 \t -0.4908271938161843\n",
            "69     \t [0.7511544  0.65589036]. \t  \u001b[92m-0.08578648214106574\u001b[0m \t -0.08578648214106574\n",
            "70     \t [9.9686442  1.68139598]. \t  -117.66569757080475 \t -0.08578648214106574\n",
            "71     \t [1.25853039 7.04421599]. \t  -19201.570925244465 \t -0.08578648214106574\n",
            "72     \t [8.06193148 8.60590626]. \t  -39284.21404271839 \t -0.08578648214106574\n",
            "73     \t [-4.05737843  1.88193186]. \t  -273.80806922211553 \t -0.08578648214106574\n",
            "74     \t [ 4.98816825 -0.84975274]. \t  -41.025483068887105 \t -0.08578648214106574\n",
            "75     \t [7.71939719 1.3140332 ]. \t  -81.54833457870807 \t -0.08578648214106574\n",
            "76     \t [6.47399209 2.40499038]. \t  -81.86155655740683 \t -0.08578648214106574\n",
            "77     \t [-5.41577237  1.00350708]. \t  -151.56674301338865 \t -0.08578648214106574\n",
            "78     \t [-4.97692702  8.28742049]. \t  -40556.84917917206 \t -0.08578648214106574\n",
            "79     \t [-2.8043882   0.52423234]. \t  -36.97236779218163 \t -0.08578648214106574\n",
            "80     \t [4.92905531 1.29978043]. \t  -20.243697047984963 \t -0.08578648214106574\n",
            "81     \t [-3.73993072 -0.14963314]. \t  -51.11501595676817 \t -0.08578648214106574\n",
            "82     \t [9.50807662 9.54940402]. \t  -59843.33626052231 \t -0.08578648214106574\n",
            "83     \t [ 3.67276286 -6.86781614]. \t  -16445.989871299862 \t -0.08578648214106574\n",
            "84     \t [ 7.10299514 -2.25212398]. \t  -55.74348896402328 \t -0.08578648214106574\n",
            "85     \t [9.48298339 8.28908303]. \t  -32806.59307027537 \t -0.08578648214106574\n",
            "86     \t [-3.61736309 -8.07696956]. \t  -35982.776385716104 \t -0.08578648214106574\n",
            "87     \t [-2.22392311  8.04670637]. \t  -34712.23316395252 \t -0.08578648214106574\n",
            "88     \t [-2.77815484 -3.57040748]. \t  -1613.0870410767286 \t -0.08578648214106574\n",
            "89     \t [6.22206129 1.86430453]. \t  -28.333393614984992 \t -0.08578648214106574\n",
            "90     \t [0.07531789 6.78340765]. \t  -16911.902094659115 \t -0.08578648214106574\n",
            "91     \t [ 3.26029884 -6.05289087]. \t  -9809.218906598351 \t -0.08578648214106574\n",
            "92     \t [-4.91703696 -6.17035295]. \t  -13177.608925696788 \t -0.08578648214106574\n",
            "93     \t [0.78199885 1.18467404]. \t  -8.24801580874012 \t -0.08578648214106574\n",
            "94     \t [-1.44728277  3.12767244]. \t  -888.9934981790411 \t -0.08578648214106574\n",
            "95     \t [ 3.71376938 -3.98894139]. \t  -1587.6567265140147 \t -0.08578648214106574\n",
            "96     \t [-2.85792593  1.3685304 ]. \t  -102.10068794850362 \t -0.08578648214106574\n",
            "97     \t [-1.65224248 -0.31499164]. \t  -13.884437512439666 \t -0.08578648214106574\n",
            "98     \t [6.82112242 0.20006335]. \t  -124.76956260580201 \t -0.08578648214106574\n",
            "99     \t [0.57308471 0.50838026]. \t  -0.18856988701464567 \t -0.08578648214106574\n",
            "100    \t [-1.38351393  0.59240365]. \t  -14.378908963010282 \t -0.08578648214106574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "6f7fb484-538f-460f-e2a4-213b3dab9b58"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_18 = d2GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [7.46858806 9.37081326]. \t  -56594.51755971677 \t -0.3114572247523004\n",
            "init   \t [7.3838908  0.61711383]. \t  -128.46197097057788 \t -0.3114572247523004\n",
            "init   \t [-5.34543344 -9.77202391]. \t  -77131.46635795479 \t -0.3114572247523004\n",
            "init   \t [-1.39062363 -1.9529728 ]. \t  -168.39363959555394 \t -0.3114572247523004\n",
            "init   \t [ 0.45349343 -0.43216408]. \t  -0.3114572247523004 \t -0.3114572247523004\n",
            "1      \t [1.13550865 0.60170966]. \t  -0.3568618976266401 \t -0.3114572247523004\n",
            "2      \t [ 3.31875215 -1.02891994]. \t  -8.26333388374366 \t -0.3114572247523004\n",
            "3      \t [ 1.55677101 -0.46918709]. \t  -2.8031293078482102 \t -0.3114572247523004\n",
            "4      \t [0.21033772 1.30032928]. \t  -20.73880231778645 \t -0.3114572247523004\n",
            "5      \t [ 1.16777959 -0.15129036]. \t  -2.5459272005116174 \t -0.3114572247523004\n",
            "6      \t [2.47418429 2.01618701]. \t  -66.15017672068348 \t -0.3114572247523004\n",
            "7      \t [ 2.3986449  -2.43386978]. \t  -180.51582543439307 \t -0.3114572247523004\n",
            "8      \t [ 2.8066687  -0.25783649]. \t  -17.561495661537435 \t -0.3114572247523004\n",
            "9      \t [ 5.34191809 -1.06970722]. \t  -37.49840150918408 \t -0.3114572247523004\n",
            "10     \t [-9.1578455   9.85877625]. \t  -82967.39651747962 \t -0.3114572247523004\n",
            "11     \t [-9.22340659  0.51462485]. \t  -294.76333972189946 \t -0.3114572247523004\n",
            "12     \t [-21.16813699 -17.29836454]. \t  -768386.4143655088 \t -0.3114572247523004\n",
            "13     \t [ 5.89067148 -9.86224618]. \t  -71191.85079453432 \t -0.3114572247523004\n",
            "14     \t [-11.90179462  -5.87352535]. \t  -13255.55680041153 \t -0.3114572247523004\n",
            "15     \t [ 3.7418106  -0.77277539]. \t  -20.496497630920068 \t -0.3114572247523004\n",
            "16     \t [-0.77262632  9.74750417]. \t  -72812.65224704825 \t -0.3114572247523004\n",
            "17     \t [ -7.07258753 -16.68164957]. \t  -635416.9768064647 \t -0.3114572247523004\n",
            "18     \t [ 9.69766491 -4.25986356]. \t  -1490.259789195166 \t -0.3114572247523004\n",
            "19     \t [-4.37849887  3.77454061]. \t  -2190.171859509989 \t -0.3114572247523004\n",
            "20     \t [-0.48911719 -6.53584901]. \t  -14768.003171246892 \t -0.3114572247523004\n",
            "21     \t [-6.55669095 -3.70132276]. \t  -2363.1602936124054 \t -0.3114572247523004\n",
            "22     \t [3.37022022 5.88558812]. \t  -8693.893848658876 \t -0.3114572247523004\n",
            "23     \t [-8.88998834  4.48323256]. \t  -4917.220028134189 \t -0.3114572247523004\n",
            "24     \t [9.46019098 2.63080788]. \t  -109.98059450134956 \t -0.3114572247523004\n",
            "25     \t [ 0.59682223 -0.51509253]. \t  \u001b[92m-0.17131232405838087\u001b[0m \t -0.17131232405838087\n",
            "26     \t [-0.37831524  3.6850814 ]. \t  -1518.5787899816103 \t -0.17131232405838087\n",
            "27     \t [ 4.11975061 -4.92642019]. \t  -3955.913450696437 \t -0.17131232405838087\n",
            "28     \t [-5.15521607  0.24507706]. \t  -93.54514275367359 \t -0.17131232405838087\n",
            "29     \t [-13.91172327 -13.7992318 ]. \t  -311876.4100792852 \t -0.17131232405838087\n",
            "30     \t [-4.87004106  7.59084834]. \t  -28888.241628955268 \t -0.17131232405838087\n",
            "31     \t [-1.217343    0.67312202]. \t  -13.935365099102407 \t -0.17131232405838087\n",
            "32     \t [-0.50881339  0.56400909]. \t  -4.8986864133896715 \t -0.17131232405838087\n",
            "33     \t [0.22999854 0.75389091]. \t  -2.2371281743289773 \t -0.17131232405838087\n",
            "34     \t [1.11285054 0.84149311]. \t  -0.19680289569945517 \t -0.17131232405838087\n",
            "35     \t [ 9.86818064 -7.68525607]. \t  -23518.294802650667 \t -0.17131232405838087\n",
            "36     \t [7.21149617 4.49581812]. \t  -2244.8246127227094 \t -0.17131232405838087\n",
            "37     \t [ 9.65645761 -0.18911848]. \t  -258.67587150647734 \t -0.17131232405838087\n",
            "38     \t [ 1.92345373 -0.12662603]. \t  -8.007444417089769 \t -0.17131232405838087\n",
            "39     \t [3.22811239 9.7546806 ]. \t  -70002.42429157911 \t -0.17131232405838087\n",
            "40     \t [-9.28805671 -9.10572996]. \t  -61437.55629437586 \t -0.17131232405838087\n",
            "41     \t [4.45745628 1.13954437]. \t  -18.875685787636087 \t -0.17131232405838087\n",
            "42     \t [ 6.89852286 -1.86618797]. \t  -34.801494287717986 \t -0.17131232405838087\n",
            "43     \t [5.42951099 1.55966529]. \t  -20.25766097753674 \t -0.17131232405838087\n",
            "44     \t [-3.19007326 -2.76255405]. \t  -698.618787838704 \t -0.17131232405838087\n",
            "45     \t [ 1.68620787 -4.81732985]. \t  -4001.5024733636606 \t -0.17131232405838087\n",
            "46     \t [-1.65081486 -0.00553277]. \t  -12.477603048643417 \t -0.17131232405838087\n",
            "47     \t [-0.53176951 -0.45915755]. \t  -4.164340375486594 \t -0.17131232405838087\n",
            "48     \t [-2.57780182  0.32081243]. \t  -28.298003201575032 \t -0.17131232405838087\n",
            "49     \t [ 0.66043971 -1.40834511]. \t  -21.98028855234697 \t -0.17131232405838087\n",
            "50     \t [-9.55775544 -1.70014287]. \t  -582.0192920150815 \t -0.17131232405838087\n",
            "51     \t [ 2.65437388 -1.03915466]. \t  -3.226387490781242 \t -0.17131232405838087\n",
            "52     \t [ 1.53800134 -1.12122053]. \t  -2.195650186198717 \t -0.17131232405838087\n",
            "53     \t [ 1.05453902 -0.81730151]. \t  \u001b[92m-0.16137401866352513\u001b[0m \t -0.16137401866352513\n",
            "54     \t [ 0.7975791  -0.58592851]. \t  \u001b[92m-0.06559609396288771\u001b[0m \t -0.06559609396288771\n",
            "55     \t [5.24783304 0.6785167 ]. \t  -55.49103795519363 \t -0.06559609396288771\n",
            "56     \t [3.9676847 1.9219404]. \t  -32.2002960368292 \t -0.06559609396288771\n",
            "57     \t [-8.51544016  6.40685072]. \t  -16511.218510710707 \t -0.06559609396288771\n",
            "58     \t [-0.5225587  -0.08089583]. \t  -2.8920203337768227 \t -0.06559609396288771\n",
            "59     \t [1.60841033 0.5582593 ]. \t  -2.3110206603774945 \t -0.06559609396288771\n",
            "60     \t [ 1.76199316 -9.65225237]. \t  -68132.71966320611 \t -0.06559609396288771\n",
            "61     \t [ 0.26694286 -0.5255387 ]. \t  -0.7003236070594647 \t -0.06559609396288771\n",
            "62     \t [ 3.00439812 -0.20606656]. \t  -21.06423677740036 \t -0.06559609396288771\n",
            "63     \t [-6.16593836  8.44297235]. \t  -44294.66023866623 \t -0.06559609396288771\n",
            "64     \t [ 2.91741373 -6.11095378]. \t  -10305.570540281722 \t -0.06559609396288771\n",
            "65     \t [-6.47398976 -8.29725134]. \t  -41621.6570495819 \t -0.06559609396288771\n",
            "66     \t [ 5.80697974 -3.99152321]. \t  -1381.0973566289983 \t -0.06559609396288771\n",
            "67     \t [9.85029576 5.76014896]. \t  -6464.711867952875 \t -0.06559609396288771\n",
            "68     \t [6.12280465 7.31494899]. \t  -20385.503213300974 \t -0.06559609396288771\n",
            "69     \t [-6.18178689  6.26894534]. \t  -14427.260880981674 \t -0.06559609396288771\n",
            "70     \t [ 7.88897555 -1.62661354]. \t  -60.94921596857296 \t -0.06559609396288771\n",
            "71     \t [-7.17442331  1.04855605]. \t  -242.54106077202937 \t -0.06559609396288771\n",
            "72     \t [-4.11978008  9.49501985]. \t  -68055.49271965375 \t -0.06559609396288771\n",
            "73     \t [-9.00271179 -9.1175408 ]. \t  -61533.45992394255 \t -0.06559609396288771\n",
            "74     \t [-3.84115924 -5.4095626 ]. \t  -7802.945286291192 \t -0.06559609396288771\n",
            "75     \t [1.61856633 4.40196963]. \t  -2758.5635071437023 \t -0.06559609396288771\n",
            "76     \t [2.4398866  0.90198281]. \t  -3.3943681171170725 \t -0.06559609396288771\n",
            "77     \t [2.75282387 1.19886267]. \t  -3.1020228009022337 \t -0.06559609396288771\n",
            "78     \t [-4.30895206  9.99717508]. \t  -83420.17417554371 \t -0.06559609396288771\n",
            "79     \t [-2.12642766 -9.85576909]. \t  -77154.74896888451 \t -0.06559609396288771\n",
            "80     \t [-0.05327668  1.84279366]. \t  -94.81891040193774 \t -0.06559609396288771\n",
            "81     \t [-0.58991652  7.26217391]. \t  -22503.477832359345 \t -0.06559609396288771\n",
            "82     \t [0.27489919 0.32648808]. \t  -0.5333874950701858 \t -0.06559609396288771\n",
            "83     \t [1.477502   1.20913117]. \t  -4.412700106999839 \t -0.06559609396288771\n",
            "84     \t [5.89672057 3.89720334]. \t  -1222.4860890315838 \t -0.06559609396288771\n",
            "85     \t [ 6.7607555  -0.95608528]. \t  -81.84654812126797 \t -0.06559609396288771\n",
            "86     \t [-1.29841773  1.12056778]. \t  -34.311297811671636 \t -0.06559609396288771\n",
            "87     \t [-0.5543762   4.56040096]. \t  -3555.4739534323253 \t -0.06559609396288771\n",
            "88     \t [0.83684724 0.83076622]. \t  -0.6173984752961477 \t -0.06559609396288771\n",
            "89     \t [-2.56543486 -7.77172208]. \t  -30450.44183915338 \t -0.06559609396288771\n",
            "90     \t [-9.23238755 -6.67352048]. \t  -19432.10910463239 \t -0.06559609396288771\n",
            "91     \t [-1.41444092 -3.81254297]. \t  -1864.549814117909 \t -0.06559609396288771\n",
            "92     \t [-5.58080905 -2.34532776]. \t  -593.2280959541445 \t -0.06559609396288771\n",
            "93     \t [-2.096222   -0.61984963]. \t  -25.999019961128703 \t -0.06559609396288771\n",
            "94     \t [7.72771931 8.68302332]. \t  -40978.77317930908 \t -0.06559609396288771\n",
            "95     \t [-1.46501658 -2.1326753 ]. \t  -229.1721286101595 \t -0.06559609396288771\n",
            "96     \t [ 7.96616065 -5.40487512]. \t  -5140.787007532487 \t -0.06559609396288771\n",
            "97     \t [-2.85345816 -9.83264323]. \t  -77015.66153958983 \t -0.06559609396288771\n",
            "98     \t [ 2.34074929 -0.94213818]. \t  -2.4371905298183547 \t -0.06559609396288771\n",
            "99     \t [ 9.77428042 -6.17916373]. \t  -8945.403788429308 \t -0.06559609396288771\n",
            "100    \t [-0.98210894  5.46957316]. \t  -7400.7526049408825 \t -0.06559609396288771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "72fb8c0f-6ea4-49cb-a3ff-cf345c4c9b89"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_19 = d2GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.18891153 5.22983981]. \t  -5727.433065247983 \t -79.36780179787098\n",
            "init   \t [-5.10936891  5.43378939]. \t  -8270.721382692287 \t -79.36780179787098\n",
            "init   \t [-2.91055837  5.93394799]. \t  -10771.054116716396 \t -79.36780179787098\n",
            "init   \t [-3.07999955 -1.12251129]. \t  -79.36780179787098 \t -79.36780179787098\n",
            "init   \t [-3.93392821 -1.1577216 ]. \t  -111.8486364354089 \t -79.36780179787098\n",
            "1      \t [ 1.46418724 -8.68671967]. \t  -44673.21769976243 \t -79.36780179787098\n",
            "2      \t [ 9.77192101 -1.83559751]. \t  -95.34580222090166 \t -79.36780179787098\n",
            "3      \t [-9.83688426 -8.54529444]. \t  -48715.20932349114 \t -79.36780179787098\n",
            "4      \t [9.60900479 6.31645299]. \t  -9926.306792942905 \t -79.36780179787098\n",
            "5      \t [-19.89378227 -17.68129152]. \t  -832873.393547256 \t -79.36780179787098\n",
            "6      \t [-11.03676257   1.68012826]. \t  -701.4902303882396 \t -79.36780179787098\n",
            "7      \t [-221.40622812 -220.02527179]. \t  -18834988186.84332 \t -79.36780179787098\n",
            "8      \t [ 3.54230836 -1.67159161]. \t  \u001b[92m-14.83661695017993\u001b[0m \t -14.83661695017993\n",
            "9      \t [ 9.69261719 -9.77242937]. \t  -65820.84067993904 \t -14.83661695017993\n",
            "10     \t [6.1714239  1.74841076]. \t  -26.75024761279674 \t -14.83661695017993\n",
            "11     \t [-11.13575487   8.15137111]. \t  -41633.940943751186 \t -14.83661695017993\n",
            "12     \t [-10.82382197 -15.63602698]. \t  -499728.88972019585 \t -14.83661695017993\n",
            "13     \t [-4.0006694  -6.33080221]. \t  -14190.408102991041 \t -14.83661695017993\n",
            "14     \t [5.25069229 9.97986436]. \t  -75247.1563887796 \t -14.83661695017993\n",
            "15     \t [ 6.04018878 -4.75696195]. \t  -3101.3787215228685 \t -14.83661695017993\n",
            "16     \t [-9.71366175 -2.87887312]. \t  -1497.0563453997547 \t -14.83661695017993\n",
            "17     \t [-0.01867416 -2.7871405 ]. \t  -484.9524075499412 \t -14.83661695017993\n",
            "18     \t [3.28533039e-03 9.54204293e+00]. \t  -66320.270226604 \t -14.83661695017993\n",
            "19     \t [-6.07511758  9.87539755]. \t  -80950.23212547012 \t -14.83661695017993\n",
            "20     \t [1.87033353 0.88174238]. \t  \u001b[92m-0.9564275448657357\u001b[0m \t -0.9564275448657357\n",
            "21     \t [-1.99485964  1.54871836]. \t  -101.22945157110775 \t -0.9564275448657357\n",
            "22     \t [8.29922861 0.49047679]. \t  -175.5239149537315 \t -0.9564275448657357\n",
            "23     \t [5.32281973 4.50262384]. \t  -2500.2067375240285 \t -0.9564275448657357\n",
            "24     \t [-7.09003623  1.39325757]. \t  -306.2344721363766 \t -0.9564275448657357\n",
            "25     \t [ 5.87833288 -0.87175509]. \t  -61.789764091542935 \t -0.9564275448657357\n",
            "26     \t [1.62234106 0.54419943]. \t  -2.509252731391051 \t -0.9564275448657357\n",
            "27     \t [ 2.94846742 -0.54942807]. \t  -14.791996007475372 \t -0.9564275448657357\n",
            "28     \t [1.74816211 1.01282851]. \t  \u001b[92m-0.7439480786677081\u001b[0m \t -0.7439480786677081\n",
            "29     \t [3.36099777 1.23128568]. \t  -5.790619961472802 \t -0.7439480786677081\n",
            "30     \t [4.07823914 0.5127498 ]. \t  -34.714852575284745 \t -0.7439480786677081\n",
            "31     \t [1.19985657 1.0576507 ]. \t  -2.1923130056958544 \t -0.7439480786677081\n",
            "32     \t [1.54039677 1.50735894]. \t  -18.338440491069356 \t -0.7439480786677081\n",
            "33     \t [1.58433972 0.70915428]. \t  -1.0108702715776599 \t -0.7439480786677081\n",
            "34     \t [ 2.79740563 -1.63587076]. \t  -16.28406693367883 \t -0.7439480786677081\n",
            "35     \t [-6.1337001  -2.42919645]. \t  -694.2676943617337 \t -0.7439480786677081\n",
            "36     \t [0.53026037 0.64221252]. \t  \u001b[92m-0.3942495161125849\u001b[0m \t -0.3942495161125849\n",
            "37     \t [ 5.60982676 -8.11857633]. \t  -31880.560182608337 \t -0.3942495161125849\n",
            "38     \t [2.06983724 1.25795395]. \t  -3.542860264543693 \t -0.3942495161125849\n",
            "39     \t [1.28759636 0.63032908]. \t  -0.5687443438123919 \t -0.3942495161125849\n",
            "40     \t [ 2.3689013  -4.57423611]. \t  -3118.957383045698 \t -0.3942495161125849\n",
            "41     \t [8.99849094 2.58768937]. \t  -102.586491285242 \t -0.3942495161125849\n",
            "42     \t [-5.49923748 -9.87419648]. \t  -80441.73748078574 \t -0.3942495161125849\n",
            "43     \t [ 2.55675355 -0.64092622]. \t  -8.445185721805005 \t -0.3942495161125849\n",
            "44     \t [ 1.43761199 -0.50218138]. \t  -1.9333770041252007 \t -0.3942495161125849\n",
            "45     \t [-8.48702601  4.41312955]. \t  -4590.8169129396865 \t -0.3942495161125849\n",
            "46     \t [ 3.15690964 -1.1965707 ]. \t  -4.824363825315868 \t -0.3942495161125849\n",
            "47     \t [-3.78724396  0.99030986]. \t  -89.01214575220554 \t -0.3942495161125849\n",
            "48     \t [ 0.64782613 -0.20896758]. \t  -0.7523272766214226 \t -0.3942495161125849\n",
            "49     \t [0.12559158 0.68074226]. \t  -2.048524233365259 \t -0.3942495161125849\n",
            "50     \t [0.04101544 0.51743679]. \t  -1.4086450926190945 \t -0.3942495161125849\n",
            "51     \t [24.4065475  23.69089171]. \t  -2412239.682257594 \t -0.3942495161125849\n",
            "52     \t [-4.84351285 -1.7428499 ]. \t  -272.57673817873376 \t -0.3942495161125849\n",
            "53     \t [ 9.74263245 -5.38350273]. \t  -4727.073427523028 \t -0.3942495161125849\n",
            "54     \t [7.33480785 0.61386146]. \t  -126.75302120090174 \t -0.3942495161125849\n",
            "55     \t [1.2371514  0.27655733]. \t  -2.4071484652923534 \t -0.3942495161125849\n",
            "56     \t [-1.20818584 -0.18681003]. \t  -8.142559704612122 \t -0.3942495161125849\n",
            "57     \t [-0.06133028 -0.3718988 ]. \t  -1.3548392920328975 \t -0.3942495161125849\n",
            "58     \t [3.320003   1.83995359]. \t  -29.199220551963297 \t -0.3942495161125849\n",
            "59     \t [9.53325184 9.96976773]. \t  -71710.9705251735 \t -0.3942495161125849\n",
            "60     \t [-5.5804551  -9.03194172]. \t  -56984.546470424066 \t -0.3942495161125849\n",
            "61     \t [ 0.05612942 -0.24302856]. \t  -0.8985787646544279 \t -0.3942495161125849\n",
            "62     \t [-1.05453229 -0.75294912]. \t  -13.799266189206374 \t -0.3942495161125849\n",
            "63     \t [-5.26775709  4.094441  ]. \t  -3049.6450899565607 \t -0.3942495161125849\n",
            "64     \t [1.12948776 0.83942447]. \t  \u001b[92m-0.1733198012372158\u001b[0m \t -0.1733198012372158\n",
            "65     \t [-8.51581064 -6.35642805]. \t  -16048.163840611456 \t -0.1733198012372158\n",
            "66     \t [-0.91762384 -0.22114277]. \t  -5.739485879516737 \t -0.1733198012372158\n",
            "67     \t [ 8.99129795 -2.75625089]. \t  -140.80384847932044 \t -0.1733198012372158\n",
            "68     \t [1.419442   4.07963932]. \t  -2031.2474747312667 \t -0.1733198012372158\n",
            "69     \t [ 3.78253068 -1.31721328]. \t  -7.937700772204609 \t -0.1733198012372158\n",
            "70     \t [2.52066105 1.16435442]. \t  -2.3852050944006593 \t -0.1733198012372158\n",
            "71     \t [0.5532274  9.31466232]. \t  -59839.26864189083 \t -0.1733198012372158\n",
            "72     \t [1.53649089 2.94276619]. \t  -498.5100831823912 \t -0.1733198012372158\n",
            "73     \t [5.5072197  0.97956616]. \t  -46.06423921695451 \t -0.1733198012372158\n",
            "74     \t [0.78443953 0.56512418]. \t  \u001b[92m-0.08892845324023205\u001b[0m \t -0.08892845324023205\n",
            "75     \t [ 4.60460021 -1.77106085]. \t  -18.56234810152845 \t -0.08892845324023205\n",
            "76     \t [5.95833852 5.32796835]. \t  -5189.148320446915 \t -0.08892845324023205\n",
            "77     \t [4.82855898 2.47155562]. \t  -123.84113983285569 \t -0.08892845324023205\n",
            "78     \t [ 2.84727866 -1.36750759]. \t  -5.00689133042058 \t -0.08892845324023205\n",
            "79     \t [-4.9410944  6.2398932]. \t  -13751.490771490682 \t -0.08892845324023205\n",
            "80     \t [-0.23602593  2.10356679]. \t  -166.63900281264824 \t -0.08892845324023205\n",
            "81     \t [ 7.84946974 -1.97522348]. \t  -46.91955168187221 \t -0.08892845324023205\n",
            "82     \t [ 7.79848131 -4.21075841]. \t  -1576.6462240394658 \t -0.08892845324023205\n",
            "83     \t [-4.80892289  3.33160919]. \t  -1492.625271016946 \t -0.08892845324023205\n",
            "84     \t [-4.08148904  2.1486598 ]. \t  -380.3982183242623 \t -0.08892845324023205\n",
            "85     \t [-2.83616533  0.11576938]. \t  -31.109363736465365 \t -0.08892845324023205\n",
            "86     \t [-4.08319065  0.27059241]. \t  -61.61838661017162 \t -0.08892845324023205\n",
            "87     \t [-4.4992704  -8.03524031]. \t  -35743.89855337572 \t -0.08892845324023205\n",
            "88     \t [ 1.8961735  -1.32638718]. \t  -6.067700625202037 \t -0.08892845324023205\n",
            "89     \t [ 2.27665772 -1.26445249]. \t  -3.3264198310195807 \t -0.08892845324023205\n",
            "90     \t [-2.5565048   1.26777645]. \t  -79.2580482941634 \t -0.08892845324023205\n",
            "91     \t [ 1.49281686 -0.97523396]. \t  -0.5779962492910728 \t -0.08892845324023205\n",
            "92     \t [2.82521735 0.67212372]. \t  -10.717408959965091 \t -0.08892845324023205\n",
            "93     \t [ 4.31696136 -1.52430922]. \t  -11.22013278578763 \t -0.08892845324023205\n",
            "94     \t [-2.7276963  -2.51108819]. \t  -484.45476619146893 \t -0.08892845324023205\n",
            "95     \t [-4.56624156 -3.7311091 ]. \t  -2131.6166846930196 \t -0.08892845324023205\n",
            "96     \t [-4.2956356  -9.65005052]. \t  -72640.99768931672 \t -0.08892845324023205\n",
            "97     \t [5.96686349 3.20976704]. \t  -453.2320428473832 \t -0.08892845324023205\n",
            "98     \t [-7.3883821   2.35828361]. \t  -755.707722767639 \t -0.08892845324023205\n",
            "99     \t [-9.88634809  7.60304415]. \t  -31618.46472055067 \t -0.08892845324023205\n",
            "100    \t [1.05089398 0.78511245]. \t  \u001b[92m-0.06877206463151544\u001b[0m \t -0.06877206463151544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "a9330520-e3ff-411d-b125-85be89057978"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_20 = d2GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.41034571 1.2905751 ]. \t  -7.547501684041513 \t -7.547501684041513\n",
            "init   \t [-0.23116335 -3.27044493]. \t  -936.6052904493877 \t -7.547501684041513\n",
            "init   \t [-2.48263644  0.64071738]. \t  -33.95727983513621 \t -7.547501684041513\n",
            "init   \t [-8.63787423  1.6905812 ]. \t  -504.96347244129606 \t -7.547501684041513\n",
            "init   \t [-5.24204473 -6.78486837]. \t  -18977.794322697024 \t -7.547501684041513\n",
            "1      \t [ 9.84240059 -0.83299926]. \t  -221.1494177066687 \t -7.547501684041513\n",
            "2      \t [6.73867561 8.36273465]. \t  -35481.183803304404 \t -7.547501684041513\n",
            "3      \t [ 8.38641873 -9.74808405]. \t  -66058.08004040587 \t -7.547501684041513\n",
            "4      \t [-3.83701174  9.3377774 ]. \t  -63551.82998472117 \t -7.547501684041513\n",
            "5      \t [-12.19903967  -8.2977456 ]. \t  -45116.75524392016 \t -7.547501684041513\n",
            "6      \t [  1.26673862 -11.47144426]. \t  -137205.63550960255 \t -7.547501684041513\n",
            "7      \t [-10.97478399   7.91175596]. \t  -37226.053497944995 \t -7.547501684041513\n",
            "8      \t [ 5.47512615 -4.02382947]. \t  -1468.0303063345998 \t -7.547501684041513\n",
            "9      \t [5.63829286 2.23576808]. \t  -59.51595761489296 \t -7.547501684041513\n",
            "10     \t [1.19303264 6.15434344]. \t  -11118.082615949374 \t -7.547501684041513\n",
            "11     \t [-9.54463926 -3.22235832]. \t  -1948.8029558851763 \t -7.547501684041513\n",
            "12     \t [-5.01120975 -1.68702485]. \t  -265.2565659465258 \t -7.547501684041513\n",
            "13     \t [-4.42201964  3.78243267]. \t  -2212.101808761339 \t -7.547501684041513\n",
            "14     \t [9.25668229 3.88152593]. \t  -939.7712605560133 \t -7.547501684041513\n",
            "15     \t [ 9.57479367 -4.59830407]. \t  -2213.9396015619786 \t -7.547501684041513\n",
            "16     \t [ 3.59151328 -0.2476054 ]. \t  -30.782425777837926 \t -7.547501684041513\n",
            "17     \t [ 3.13436835 -7.03540989]. \t  -18382.68756134267 \t -7.547501684041513\n",
            "18     \t [ 6.80318444 -0.80413179]. \t  -94.39557526986036 \t -7.547501684041513\n",
            "19     \t [0.07353678 1.37104359]. \t  -28.031294430127247 \t -7.547501684041513\n",
            "20     \t [2.24537612 1.87345321]. \t  -47.13841828913171 \t -7.547501684041513\n",
            "21     \t [ 1.56907163 -0.75434789]. \t  \u001b[92m-0.6953475680428638\u001b[0m \t -0.6953475680428638\n",
            "22     \t [-7.91645344  4.98784765]. \t  -6732.014903639481 \t -0.6953475680428638\n",
            "23     \t [0.97356652 0.45807815]. \t  \u001b[92m-0.6142988143004436\u001b[0m \t -0.6142988143004436\n",
            "24     \t [2.04126339 0.2563074 ]. \t  -8.379485400996032 \t -0.6142988143004436\n",
            "25     \t [2.89502442 9.87660257]. \t  -73884.91156089552 \t -0.6142988143004436\n",
            "26     \t [1.10918762 0.1949488 ]. \t  -2.1468336119284688 \t -0.6142988143004436\n",
            "27     \t [ 2.35187261 -0.88310616]. \t  -3.08246657976607 \t -0.6142988143004436\n",
            "28     \t [-1.51682124 -6.5001646 ]. \t  -14805.593989173782 \t -0.6142988143004436\n",
            "29     \t [4.75441337e+00 2.94046931e-03]. \t  -59.304183931531455 \t -0.6142988143004436\n",
            "30     \t [ 3.08937579 -1.91176107]. \t  -39.987102000861455 \t -0.6142988143004436\n",
            "31     \t [0.88675372 0.81948702]. \t  \u001b[92m-0.429361323646086\u001b[0m \t -0.429361323646086\n",
            "32     \t [1.08758492 1.09581677]. \t  -3.4610936757242396 \t -0.429361323646086\n",
            "33     \t [ 2.133047   -0.35171012]. \t  -8.395124681097528 \t -0.429361323646086\n",
            "34     \t [-0.46424221  0.38160145]. \t  -3.2855099476439045 \t -0.429361323646086\n",
            "35     \t [1.29907932 0.92614417]. \t  -0.4362375627798162 \t -0.429361323646086\n",
            "36     \t [-0.46140817 -0.15979036]. \t  -2.6609732203354115 \t -0.429361323646086\n",
            "37     \t [-0.31083107  0.17096781]. \t  -1.9910298573398262 \t -0.429361323646086\n",
            "38     \t [1.2794607  0.49242837]. \t  -1.3405248153936646 \t -0.429361323646086\n",
            "39     \t [4.15468728 3.76019348]. \t  -1173.8311036297891 \t -0.429361323646086\n",
            "40     \t [6.6924181  1.71294556]. \t  -33.761750809407594 \t -0.429361323646086\n",
            "41     \t [5.38047634 1.39580166]. \t  -23.592798871500698 \t -0.429361323646086\n",
            "42     \t [-8.46193236 -9.39442377]. \t  -68519.05264027971 \t -0.429361323646086\n",
            "43     \t [ 1.23830095 -0.64916763]. \t  \u001b[92m-0.3695704673718105\u001b[0m \t -0.3695704673718105\n",
            "44     \t [-0.95285717 -0.07301797]. \t  -5.670394327203896 \t -0.3695704673718105\n",
            "45     \t [-5.31984363  0.58723395]. \t  -112.16935051451406 \t -0.3695704673718105\n",
            "46     \t [-1.79830301  1.68982602]. \t  -120.61048256335266 \t -0.3695704673718105\n",
            "47     \t [ 0.04686235 -0.1308756 ]. \t  -0.9087891819257023 \t -0.3695704673718105\n",
            "48     \t [7.41942349 4.26504765]. \t  -1718.785273908734 \t -0.3695704673718105\n",
            "49     \t [1.12538811 0.73616564]. \t  \u001b[92m-0.019168075639020333\u001b[0m \t -0.019168075639020333\n",
            "50     \t [-7.40554879 -0.92130892]. \t  -236.3886239345398 \t -0.019168075639020333\n",
            "51     \t [-1.50998149 -0.59902231]. \t  -16.224739730985476 \t -0.019168075639020333\n",
            "52     \t [9.55322663 6.75971883]. \t  -13466.88613990843 \t -0.019168075639020333\n",
            "53     \t [ 8.36030492 -9.70579298]. \t  -64886.2395979656 \t -0.019168075639020333\n",
            "54     \t [ 7.54899001 -6.5777006 ]. \t  -12519.612770632544 \t -0.019168075639020333\n",
            "55     \t [-2.88497728  0.00483808]. \t  -31.739776471545348 \t -0.019168075639020333\n",
            "56     \t [ 0.175016   -0.44304169]. \t  -0.7752597263033065 \t -0.019168075639020333\n",
            "57     \t [-1.70044178  7.3703767 ]. \t  -24359.483718936048 \t -0.019168075639020333\n",
            "58     \t [-8.13851557  3.27121202]. \t  -1828.7559536916694 \t -0.019168075639020333\n",
            "59     \t [-4.85954037  1.18420917]. \t  -151.8154572199726 \t -0.019168075639020333\n",
            "60     \t [ 0.98001672 -0.49114353]. \t  -0.49555669428749133 \t -0.019168075639020333\n",
            "61     \t [0.18353621 0.59372552]. \t  -1.2105038064661753 \t -0.019168075639020333\n",
            "62     \t [-4.58992143  9.98676407]. \t  -83312.89560596988 \t -0.019168075639020333\n",
            "63     \t [1.49201765 0.84289766]. \t  -0.2521817566324601 \t -0.019168075639020333\n",
            "64     \t [ 1.70320171 -1.40155187]. \t  -10.400136237342533 \t -0.019168075639020333\n",
            "65     \t [8.29375657 1.08428206]. \t  -123.82362914870536 \t -0.019168075639020333\n",
            "66     \t [-4.44737865 -6.16367525]. \t  -12967.374706732573 \t -0.019168075639020333\n",
            "67     \t [0.40128712 2.84430967]. \t  -498.30632354137344 \t -0.019168075639020333\n",
            "68     \t [5.62072092 2.03270808]. \t  -35.32284101790895 \t -0.019168075639020333\n",
            "69     \t [2.26004594 0.99505905]. \t  -1.7442481176453555 \t -0.019168075639020333\n",
            "70     \t [0.02052282 1.1664541 ]. \t  -15.547017694471727 \t -0.019168075639020333\n",
            "71     \t [2.92186663 4.80081643]. \t  -3731.649350686981 \t -0.019168075639020333\n",
            "72     \t [9.87482425 3.93322583]. \t  -966.2904962785457 \t -0.019168075639020333\n",
            "73     \t [-3.07061282 -5.73791022]. \t  -9515.907317149677 \t -0.019168075639020333\n",
            "74     \t [7.42857409 3.56574496]. \t  -689.3625783898757 \t -0.019168075639020333\n",
            "75     \t [1.8838812  8.48582528]. \t  -40405.25904897171 \t -0.019168075639020333\n",
            "76     \t [7.93614006 2.36866575]. \t  -69.69268355218986 \t -0.019168075639020333\n",
            "77     \t [ 5.63854004 -4.65830064]. \t  -2873.3007456023993 \t -0.019168075639020333\n",
            "78     \t [-2.49276664  8.22484432]. \t  -37983.75841842815 \t -0.019168075639020333\n",
            "79     \t [ 5.04853492 -2.29886935]. \t  -77.35496716677031 \t -0.019168075639020333\n",
            "80     \t [ 3.8563348  -3.17181931]. \t  -537.2292181586561 \t -0.019168075639020333\n",
            "81     \t [ 5.42135206 -2.77774505]. \t  -219.96389294977982 \t -0.019168075639020333\n",
            "82     \t [ 2.03856719 -1.30084846]. \t  -4.701226034498959 \t -0.019168075639020333\n",
            "83     \t [7.11600182 2.33073398]. \t  -65.51008128581083 \t -0.019168075639020333\n",
            "84     \t [ 5.19754341 -1.9160305 ]. \t  -26.819724951313702 \t -0.019168075639020333\n",
            "85     \t [-5.14931039  8.85200777]. \t  -52438.625272453966 \t -0.019168075639020333\n",
            "86     \t [ 4.79820984 -0.96405619]. \t  -31.706556542628896 \t -0.019168075639020333\n",
            "87     \t [6.39721098 0.9530691 ]. \t  -71.09238850821002 \t -0.019168075639020333\n",
            "88     \t [-6.98147209  5.98337434]. \t  -12414.282514773455 \t -0.019168075639020333\n",
            "89     \t [6.09896331 1.92557693]. \t  -29.466981204619696 \t -0.019168075639020333\n",
            "90     \t [ 4.33423879 -1.17338163]. \t  -16.113677219719783 \t -0.019168075639020333\n",
            "91     \t [3.9288047  0.96962721]. \t  -16.97019880376162 \t -0.019168075639020333\n",
            "92     \t [7.8119471  4.05705686]. \t  -1307.1730922817023 \t -0.019168075639020333\n",
            "93     \t [-0.9094418   3.29480777]. \t  -1027.0614777163194 \t -0.019168075639020333\n",
            "94     \t [ 0.40407557 -0.33970468]. \t  -0.41517578720733866 \t -0.019168075639020333\n",
            "95     \t [ 2.66525043 -1.10672471]. \t  -2.866000909859744 \t -0.019168075639020333\n",
            "96     \t [7.6995205  9.17775011]. \t  -51734.15813973444 \t -0.019168075639020333\n",
            "97     \t [2.67310973 0.43658823]. \t  -13.304826326080933 \t -0.019168075639020333\n",
            "98     \t [-9.7307755  -8.74354945]. \t  -53012.24150789965 \t -0.019168075639020333\n",
            "99     \t [ 7.44174365 -0.70432951]. \t  -124.69031671498486 \t -0.019168075639020333\n",
            "100    \t [ 4.71125637 -1.47491966]. \t  -14.033316054559677 \t -0.019168075639020333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "2fb833d8-c7ba-4410-b188-8422396d3a44"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1609762602.8784375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "2e59cae3-1af3-4db7-cd3b-7e65ec8ccc71"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.24340351 -6.61860491]. \t  -14577.093458365041 \t -540.0726783817402\n",
            "init   \t [-1.27881961  5.38524945]. \t  -7033.582264098692 \t -540.0726783817402\n",
            "init   \t [-4.09349391 -7.01674086]. \t  -21064.19892235138 \t -540.0726783817402\n",
            "init   \t [-9.55043351 -1.59551015]. \t  -540.0726783817402 \t -540.0726783817402\n",
            "init   \t [-5.22635718 -3.24687616]. \t  -1423.2804229542423 \t -540.0726783817402\n",
            "1      \t [8.37416725 6.27688756]. \t  -9973.589615047737 \t -540.0726783817402\n",
            "2      \t [-9.76713775  9.96269922]. \t  -86875.28945668103 \t -540.0726783817402\n",
            "3      \t [ 9.98926049 -9.3463478 ]. \t  -54345.61921377557 \t -540.0726783817402\n",
            "4      \t [ 9.44490266 -0.94685252]. \t  \u001b[92m-188.41779181630653\u001b[0m \t -188.41779181630653\n",
            "5      \t [3.33951676 0.29151535]. \t  \u001b[92m-25.565488361329496\u001b[0m \t -25.565488361329496\n",
            "6      \t [-10. -10.]. \t  -88321.0 \t -25.565488361329496\n",
            "7      \t [3.1213907  9.77966003]. \t  -70814.45944220333 \t -25.565488361329496\n",
            "8      \t [-8.39356713  3.79485161]. \t  -2855.2293931078266 \t -25.565488361329496\n",
            "9      \t [-1.06492015 -0.23704844]. \t  \u001b[92m-7.0359850202847944\u001b[0m \t -7.0359850202847944\n",
            "10     \t [3.73729516 3.87282622]. \t  -1386.6964129825742 \t -7.0359850202847944\n",
            "11     \t [ 6.5546133  -3.95181909]. \t  -1248.9731373320117 \t -7.0359850202847944\n",
            "12     \t [-3.45829526  9.5441169 ]. \t  -68943.27824726085 \t -7.0359850202847944\n",
            "13     \t [-4.9256546   1.42566147]. \t  -196.77788657540822 \t -7.0359850202847944\n",
            "14     \t [-0.70944507 -2.87780327]. \t  -599.631698293957 \t -7.0359850202847944\n",
            "15     \t [-1.86355015  0.422577  ]. \t  -18.06287242505496 \t -7.0359850202847944\n",
            "16     \t [0.43130959 1.18920014]. \t  -11.815435679642627 \t -7.0359850202847944\n",
            "17     \t [6.29095794 0.13011026]. \t  -106.2968511789855 \t -7.0359850202847944\n",
            "18     \t [9.81087286 1.95202726]. \t  -87.22413591234051 \t -7.0359850202847944\n",
            "19     \t [ 5.00168139 -9.59215212]. \t  -64110.24551483478 \t -7.0359850202847944\n",
            "20     \t [-8.47290185 -5.42941577]. \t  -9183.349203811867 \t -7.0359850202847944\n",
            "21     \t [ 9.63529606 -4.70636043]. \t  -2477.8041716782013 \t -7.0359850202847944\n",
            "22     \t [ 2.10667478 -1.40697019]. \t  -8.087911610910195 \t -7.0359850202847944\n",
            "23     \t [ 3.12657351 -2.287699  ]. \t  -112.289953376554 \t -7.0359850202847944\n",
            "24     \t [-0.42543673  0.07870163]. \t  \u001b[92m-2.415250681997074\u001b[0m \t -2.415250681997074\n",
            "25     \t [ 2.68125121 -0.05700443]. \t  -17.135204222732877 \t -2.415250681997074\n",
            "26     \t [ 1.71930958 -1.45904151]. \t  -13.403286277244282 \t -2.415250681997074\n",
            "27     \t [-5.18427868  5.17207526]. \t  -6926.104605471746 \t -2.415250681997074\n",
            "28     \t [-1.34483863  0.86349999]. \t  -21.585229676053643 \t -2.415250681997074\n",
            "29     \t [ 2.07131228 -1.22298534]. \t  -2.8407823837670105 \t -2.415250681997074\n",
            "30     \t [6.96535621 2.62145806]. \t  -127.4877947494711 \t -2.415250681997074\n",
            "31     \t [0.53704974 2.33728369]. \t  -216.0661741304401 \t -2.415250681997074\n",
            "32     \t [0.22021602 0.44357982]. \t  \u001b[92m-0.6681358241231322\u001b[0m \t -0.6681358241231322\n",
            "33     \t [7.43336651 9.91035573]. \t  -71481.07096276061 \t -0.6681358241231322\n",
            "34     \t [-1.77820937 -9.92177327]. \t  -78940.4055435768 \t -0.6681358241231322\n",
            "35     \t [-0.67284472  0.07520596]. \t  -3.7345499250685545 \t -0.6681358241231322\n",
            "36     \t [1.47370503 0.48830577]. \t  -2.2116965852242347 \t -0.6681358241231322\n",
            "37     \t [-3.36962251 -0.18173171]. \t  -42.70133096956171 \t -0.6681358241231322\n",
            "38     \t [-7.17646816  0.17192206]. \t  -171.56194147368984 \t -0.6681358241231322\n",
            "39     \t [ 4.31794567 -0.79687577]. \t  -29.588440926738635 \t -0.6681358241231322\n",
            "40     \t [1.14697865 0.64308355]. \t  \u001b[92m-0.22623091172615428\u001b[0m \t -0.22623091172615428\n",
            "41     \t [1.91296055 0.55388432]. \t  -4.210299085552981 \t -0.22623091172615428\n",
            "42     \t [0.39956306 6.00813304]. \t  -10309.62344575111 \t -0.22623091172615428\n",
            "43     \t [-2.35116103  1.16846887]. \t  -62.87966349738941 \t -0.22623091172615428\n",
            "44     \t [0.04977028 0.76301413]. \t  -3.387651194979536 \t -0.22623091172615428\n",
            "45     \t [3.62928088 9.60519888]. \t  -65449.59401056733 \t -0.22623091172615428\n",
            "46     \t [-9.81678463 -3.62781302]. \t  -2729.0342417360357 \t -0.22623091172615428\n",
            "47     \t [-5.91223484  8.2221967 ]. \t  -39878.20582274619 \t -0.22623091172615428\n",
            "48     \t [-6.51551697  3.36011556]. \t  -1749.6688622490576 \t -0.22623091172615428\n",
            "49     \t [ 1.85341951 -0.67393323]. \t  -2.5145544308680803 \t -0.22623091172615428\n",
            "50     \t [-9.67701872  0.80035905]. \t  -354.1616178156454 \t -0.22623091172615428\n",
            "51     \t [-2.11069268  6.54093577]. \t  -15384.670965355263 \t -0.22623091172615428\n",
            "52     \t [5.96478258 5.29373115]. \t  -5041.14190295565 \t -0.22623091172615428\n",
            "53     \t [1.04414074 0.77201658]. \t  \u001b[92m-0.04568447350163922\u001b[0m \t -0.04568447350163922\n",
            "54     \t [-2.48030052 -0.53597222]. \t  -30.776500629086115 \t -0.04568447350163922\n",
            "55     \t [-2.53389748 -8.241353  ]. \t  -38307.04787877346 \t -0.04568447350163922\n",
            "56     \t [ 0.90438136 -0.6183934 ]. \t  -0.04809723355965922 \t -0.04568447350163922\n",
            "57     \t [1.0322886  0.53246056]. \t  -0.43397648085025103 \t -0.04568447350163922\n",
            "58     \t [3.93131005 2.43882584]. \t  -135.4569623453088 \t -0.04568447350163922\n",
            "59     \t [2.33210091 0.83415605]. \t  -3.543454021125002 \t -0.04568447350163922\n",
            "60     \t [4.10018482 1.62876189]. \t  -12.51782697917652 \t -0.04568447350163922\n",
            "61     \t [3.78231751 1.69937333]. \t  -15.688752569211065 \t -0.04568447350163922\n",
            "62     \t [2.26472704 0.93582625]. \t  -2.1262531800001403 \t -0.04568447350163922\n",
            "63     \t [-5.51046832  6.32629507]. \t  -14681.526646814731 \t -0.04568447350163922\n",
            "64     \t [-4.91725821  5.69084557]. \t  -9748.045106821997 \t -0.04568447350163922\n",
            "65     \t [4.79519875 2.2851141 ]. \t  -78.20998672455718 \t -0.04568447350163922\n",
            "66     \t [-0.28079526 -1.18962417]. \t  -20.999649416284967 \t -0.04568447350163922\n",
            "67     \t [-0.77028155 -0.51272335]. \t  -6.493398423722132 \t -0.04568447350163922\n",
            "68     \t [-0.26622942 -0.29549967]. \t  -1.9920688394491264 \t -0.04568447350163922\n",
            "69     \t [ 0.66852413 -0.85686487]. \t  -1.38959045539405 \t -0.04568447350163922\n",
            "70     \t [ 0.51594469 -0.74481167]. \t  -0.93889885425577 \t -0.04568447350163922\n",
            "71     \t [4.22934121 1.90578876]. \t  -28.84770061870176 \t -0.04568447350163922\n",
            "72     \t [4.55988372 0.70787427]. \t  -37.98739814257962 \t -0.04568447350163922\n",
            "73     \t [ 1.28946043 -0.6540748 ]. \t  -0.46020906587404087 \t -0.04568447350163922\n",
            "74     \t [-1.34219568 -9.43450538]. \t  -64346.925071400525 \t -0.04568447350163922\n",
            "75     \t [-1.21685943  1.94656391]. \t  -159.62138470584105 \t -0.04568447350163922\n",
            "76     \t [1.8731478  0.91505657]. \t  -0.8411842341706863 \t -0.04568447350163922\n",
            "77     \t [ 1.05409182 -2.81709776]. \t  -439.1484486340524 \t -0.04568447350163922\n",
            "78     \t [5.02926629 9.79752204]. \t  -69919.53944352253 \t -0.04568447350163922\n",
            "79     \t [ 4.96550438 -1.71912249]. \t  -17.512257560827877 \t -0.04568447350163922\n",
            "80     \t [-2.53249873 -8.90909798]. \t  -52032.73112440396 \t -0.04568447350163922\n",
            "81     \t [0.81808368 0.84061101]. \t  -0.741548339282459 \t -0.04568447350163922\n",
            "82     \t [4.40914038 2.89442519]. \t  -316.4822145066299 \t -0.04568447350163922\n",
            "83     \t [ 0.82431349 -0.75352205]. \t  -0.22465309077004153 \t -0.04568447350163922\n",
            "84     \t [5.81021894 8.07018827]. \t  -30996.582036601674 \t -0.04568447350163922\n",
            "85     \t [-8.89096135  3.26803045]. \t  -1928.0780127427365 \t -0.04568447350163922\n",
            "86     \t [ 3.62652117 -8.97584202]. \t  -49622.52268386854 \t -0.04568447350163922\n",
            "87     \t [0.68010171 0.23286554]. \t  -0.7559000468715673 \t -0.04568447350163922\n",
            "88     \t [3.86958984 0.8940705 ]. \t  -18.548208159848564 \t -0.04568447350163922\n",
            "89     \t [4.32847395 3.52225331]. \t  -850.2723996448592 \t -0.04568447350163922\n",
            "90     \t [2.19316819 1.30234213]. \t  -4.298957151724024 \t -0.04568447350163922\n",
            "91     \t [1.27144605 1.61000405]. \t  -30.693378009561993 \t -0.04568447350163922\n",
            "92     \t [ 7.16829238 -3.33687695]. \t  -494.14050371366375 \t -0.04568447350163922\n",
            "93     \t [ 7.77358787 -0.96265358]. \t  -115.97865102430211 \t -0.04568447350163922\n",
            "94     \t [-5.71866905  3.75727751]. \t  -2350.7444695066465 \t -0.04568447350163922\n",
            "95     \t [ 8.07429825 -8.26933835]. \t  -33172.07978023832 \t -0.04568447350163922\n",
            "96     \t [-8.07480962 -7.10576937]. \t  -23869.963676107633 \t -0.04568447350163922\n",
            "97     \t [ 7.54668457 -2.59018668]. \t  -111.80691749774971 \t -0.04568447350163922\n",
            "98     \t [ 8.40402256 -4.17626348]. \t  -1457.0235509451786 \t -0.04568447350163922\n",
            "99     \t [1.67328896 1.86655206]. \t  -56.52195071756115 \t -0.04568447350163922\n",
            "100    \t [-5.34906297  1.65464129]. \t  -274.660530146399 \t -0.04568447350163922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "1b62bf29-89a4-42a4-bca4-a280a34745a7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.03970985 -8.52192802]. \t  -38238.719775621794 \t -57.443300302345605\n",
            "init   \t [ 7.89863525 -1.27012905]. \t  -91.24969381804144 \t -57.443300302345605\n",
            "init   \t [-7.44644532  1.51715749]. \t  -361.74642651372136 \t -57.443300302345605\n",
            "init   \t [ 6.80941839 -1.29758898]. \t  -57.443300302345605 \t -57.443300302345605\n",
            "init   \t [3.91821121 3.69276194]. \t  -1099.406544633519 \t -57.443300302345605\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -57.443300302345605\n",
            "2      \t [-2.87227721  9.90072912]. \t  -79154.24352053565 \t -57.443300302345605\n",
            "3      \t [-1.3885243  -5.74216696]. \t  -9073.302042724144 \t -57.443300302345605\n",
            "4      \t [9.87440114 9.90717168]. \t  -69590.82131163792 \t -57.443300302345605\n",
            "5      \t [-9.80908004  8.2473246 ]. \t  -42658.83446871175 \t -57.443300302345605\n",
            "6      \t [-1.55889664  0.86514165]. \t  \u001b[92m-25.224228993551957\u001b[0m \t -25.224228993551957\n",
            "7      \t [-10.          -3.46054654]. \t  -2426.308851254873 \t -25.224228993551957\n",
            "8      \t [9.55883762 3.47985245]. \t  -503.0795369750957 \t -25.224228993551957\n",
            "9      \t [ 2.64204429 -1.94000997]. \t  -50.42731402367399 \t -25.224228993551957\n",
            "10     \t [3.57379202 9.11909317]. \t  -52976.494874433454 \t -25.224228993551957\n",
            "11     \t [ -4.33842781 -10.        ]. \t  -83536.88497243836 \t -25.224228993551957\n",
            "12     \t [ 1.56872435 -9.95745516]. \t  -77408.15056369216 \t -25.224228993551957\n",
            "13     \t [-1.18817533  4.63982673]. \t  -3919.8794147456333 \t -25.224228993551957\n",
            "14     \t [-5.5529339  -2.13420902]. \t  -472.92638986789166 \t -25.224228993551957\n",
            "15     \t [ 5.85332745 -4.1460442 ]. \t  -1651.0244287625735 \t -25.224228993551957\n",
            "16     \t [-5.70845475  4.88694372]. \t  -5763.703838600713 \t -25.224228993551957\n",
            "17     \t [ 5.3558315  -0.31563863]. \t  -72.15382046400919 \t -25.224228993551957\n",
            "18     \t [ 9.99041516 -4.36197466]. \t  -1655.9174046015619 \t -25.224228993551957\n",
            "19     \t [1.07177734 0.64291694]. \t  \u001b[92m-0.12529308633715452\u001b[0m \t -0.12529308633715452\n",
            "20     \t [1.138948   0.79862699]. \t  \u001b[92m-0.056659621348073536\u001b[0m \t -0.056659621348073536\n",
            "21     \t [6.85598088 6.33790454]. \t  -10833.526750495294 \t -0.056659621348073536\n",
            "22     \t [-0.50758919 -1.33990215]. \t  -35.86437249349787 \t -0.056659621348073536\n",
            "23     \t [-5.86919609 -5.11240596]. \t  -6808.306331484121 \t -0.056659621348073536\n",
            "24     \t [9.72250327 0.50940577]. \t  -245.49143252935806 \t -0.056659621348073536\n",
            "25     \t [6.87793692 1.39642928]. \t  -52.28600779234054 \t -0.056659621348073536\n",
            "26     \t [-4.06893245  0.85512085]. \t  -86.88675444822144 \t -0.056659621348073536\n",
            "27     \t [2.07262921 0.01085187]. \t  -9.7401645670528 \t -0.056659621348073536\n",
            "28     \t [-2.51662921 -2.09177896]. \t  -266.2891512710654 \t -0.056659621348073536\n",
            "29     \t [0.82380382 0.75361347]. \t  -0.22581134127282743 \t -0.056659621348073536\n",
            "30     \t [-9.72122709  3.55353099]. \t  -2561.6407137521574 \t -0.056659621348073536\n",
            "31     \t [1.7497237  0.56984027]. \t  -2.983352257081203 \t -0.056659621348073536\n",
            "32     \t [ 2.4849051  -5.23908941]. \t  -5496.074919485441 \t -0.056659621348073536\n",
            "33     \t [0.58975652 1.12552854]. \t  -7.7255800472495375 \t -0.056659621348073536\n",
            "34     \t [0.23272385 0.11518293]. \t  -0.6737410248369122 \t -0.056659621348073536\n",
            "35     \t [ 4.04330231 -0.61293341]. \t  -30.935263350021415 \t -0.056659621348073536\n",
            "36     \t [-0.60854477  0.27124232]. \t  -3.7295496879471584 \t -0.056659621348073536\n",
            "37     \t [-9.30549929 -0.21950445]. \t  -282.9933972414611 \t -0.056659621348073536\n",
            "38     \t [ 2.91326097 -0.04129658]. \t  -20.59502335940865 \t -0.056659621348073536\n",
            "39     \t [ 0.75508995 -1.46818765]. \t  -25.351106056591572 \t -0.056659621348073536\n",
            "40     \t [-3.95348592 -3.1273378 ]. \t  -1130.3505347278744 \t -0.056659621348073536\n",
            "41     \t [5.03739579 3.24304436]. \t  -528.1263482536671 \t -0.056659621348073536\n",
            "42     \t [1.12658692 0.73651644]. \t  \u001b[92m-0.01949769299578454\u001b[0m \t -0.01949769299578454\n",
            "43     \t [-3.69818796  1.71305488]. \t  -205.1395047257916 \t -0.01949769299578454\n",
            "44     \t [0.58685565 0.40072366]. \t  -0.3118777751028433 \t -0.01949769299578454\n",
            "45     \t [-2.70827301 -0.16005325]. \t  -28.981047564501424 \t -0.01949769299578454\n",
            "46     \t [-0.08012039  0.47081473]. \t  -1.7146670029266076 \t -0.01949769299578454\n",
            "47     \t [-9.39681078  9.86068833]. \t  -83228.4691757223 \t -0.01949769299578454\n",
            "48     \t [-3.86201655 -7.50572326]. \t  -27183.88286113719 \t -0.01949769299578454\n",
            "49     \t [-2.7849979 -6.5175139]. \t  -15411.28286027935 \t -0.01949769299578454\n",
            "50     \t [-8.28999214 -2.10020643]. \t  -671.9263016290016 \t -0.01949769299578454\n",
            "51     \t [9.87931521 1.11187389]. \t  -188.56325899224535 \t -0.01949769299578454\n",
            "52     \t [-7.41448581  7.87189908]. \t  -34575.4376890135 \t -0.01949769299578454\n",
            "53     \t [9.76705851 8.21340072]. \t  -31403.330122896183 \t -0.01949769299578454\n",
            "54     \t [-2.80091004  1.81608432]. \t  -191.06295292016443 \t -0.01949769299578454\n",
            "55     \t [-4.1069211   2.83806047]. \t  -843.4616834917406 \t -0.01949769299578454\n",
            "56     \t [-1.59186141 -0.12771658]. \t  -11.995645064568627 \t -0.01949769299578454\n",
            "57     \t [9.60190153 1.30557436]. \t  -150.69555963189242 \t -0.01949769299578454\n",
            "58     \t [-9.42952584 -4.1515827 ]. \t  -3963.336127203735 \t -0.01949769299578454\n",
            "59     \t [-6.46618709  1.68563166]. \t  -350.9352624725696 \t -0.01949769299578454\n",
            "60     \t [1.390785   7.89766594]. \t  -30433.296516509945 \t -0.01949769299578454\n",
            "61     \t [7.67868447 0.02907516]. \t  -162.47729186586284 \t -0.01949769299578454\n",
            "62     \t [9.06782253 7.72878671]. \t  -24441.615352562665 \t -0.01949769299578454\n",
            "63     \t [-2.83471316 -2.43994193]. \t  -449.31961196714644 \t -0.01949769299578454\n",
            "64     \t [8.33220336 7.60650159]. \t  -23117.065066203402 \t -0.01949769299578454\n",
            "65     \t [-0.54379441  6.45592031]. \t  -14081.343119217587 \t -0.01949769299578454\n",
            "66     \t [ 0.41045765 -9.86409628]. \t  -75420.11863943869 \t -0.01949769299578454\n",
            "67     \t [ 2.97072111 -4.8998992 ]. \t  -4062.4027889644326 \t -0.01949769299578454\n",
            "68     \t [-0.22640268 -0.37184844]. \t  -2.0099712932390794 \t -0.01949769299578454\n",
            "69     \t [1.56267888 1.38428708]. \t  -10.620796638206087 \t -0.01949769299578454\n",
            "70     \t [0.72006032 0.58500891]. \t  -0.08089944663980621 \t -0.01949769299578454\n",
            "71     \t [ 0.82300292 -0.73197445]. \t  -0.15490231521035006 \t -0.01949769299578454\n",
            "72     \t [ 7.70530973 -4.0913636 ]. \t  -1373.4771047270685 \t -0.01949769299578454\n",
            "73     \t [ 1.25910626 -0.77446087]. \t  -0.07422297869913888 \t -0.01949769299578454\n",
            "74     \t [-0.31603546 -0.55825071]. \t  -3.4966053616034607 \t -0.01949769299578454\n",
            "75     \t [ 0.67079457 -0.51664122]. \t  -0.14589134626262357 \t -0.01949769299578454\n",
            "76     \t [8.63905631 5.4974074 ]. \t  -5425.6474319473755 \t -0.01949769299578454\n",
            "77     \t [-5.7912885   4.92505611]. \t  -5943.893075288378 \t -0.01949769299578454\n",
            "78     \t [ 2.9166844  -3.25884828]. \t  -675.1749727182538 \t -0.01949769299578454\n",
            "79     \t [-5.04643942  6.02138366]. \t  -12067.840893946795 \t -0.01949769299578454\n",
            "80     \t [ 1.38126724 -0.85602978]. \t  -0.15957995887680337 \t -0.01949769299578454\n",
            "81     \t [ 2.23402298 -0.77238818]. \t  -3.689575083058641 \t -0.01949769299578454\n",
            "82     \t [-5.20499438  9.18626805]. \t  -60576.60751066257 \t -0.01949769299578454\n",
            "83     \t [ 2.58859446 -1.24947244]. \t  -3.093449568554198 \t -0.01949769299578454\n",
            "84     \t [ 7.96364668 -3.79670713]. \t  -919.2992796888569 \t -0.01949769299578454\n",
            "85     \t [-0.42287358 -3.42135285]. \t  -1138.1612419423348 \t -0.01949769299578454\n",
            "86     \t [ 7.89604122 -2.16448552]. \t  -51.90046479188232 \t -0.01949769299578454\n",
            "87     \t [9.22419433 1.02904472]. \t  -168.63717468394677 \t -0.01949769299578454\n",
            "88     \t [-1.6844257  -7.01140427]. \t  -20008.808295517654 \t -0.01949769299578454\n",
            "89     \t [ 6.95948576 -4.63130315]. \t  -2618.660143559438 \t -0.01949769299578454\n",
            "90     \t [2.10769287 0.98405062]. \t  -1.2854529222587505 \t -0.01949769299578454\n",
            "91     \t [-4.78552334  8.83038623]. \t  -51706.21634292386 \t -0.01949769299578454\n",
            "92     \t [-0.07962577 -0.60678907]. \t  -2.497342076920563 \t -0.01949769299578454\n",
            "93     \t [-1.75019318  1.96533379]. \t  -187.12493048878747 \t -0.01949769299578454\n",
            "94     \t [ 1.60902345 -0.96752657]. \t  -0.5094494834204859 \t -0.01949769299578454\n",
            "95     \t [ 6.33238924 -5.55543991]. \t  -6165.2998444628665 \t -0.01949769299578454\n",
            "96     \t [6.7658984  2.51231206]. \t  -101.86679158076527 \t -0.01949769299578454\n",
            "97     \t [-2.65518918  7.78839684]. \t  -30752.187944470195 \t -0.01949769299578454\n",
            "98     \t [-5.90291294  3.79305508]. \t  -2452.7008239852353 \t -0.01949769299578454\n",
            "99     \t [ 5.087094   -4.05750602]. \t  -1566.7927954166332 \t -0.01949769299578454\n",
            "100    \t [-3.29500331  4.82568198]. \t  -4992.364448817138 \t -0.01949769299578454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "c32fd9e8-7dd2-41b3-9bf0-9c35aa60a171"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.03151214 -7.81497449]. \t  -27448.6338140421 \t -1.7863168283775226\n",
            "init   \t [-0.27757442 -0.00337645]. \t  -1.7863168283775226 \t -1.7863168283775226\n",
            "init   \t [ 3.14219998 -5.28230578]. \t  -5551.437171353593 \t -1.7863168283775226\n",
            "init   \t [ 2.25589781 -7.60695193]. \t  -25754.9725373955 \t -1.7863168283775226\n",
            "init   \t [ 4.24404593 -2.54878916]. \t  -163.60006264419064 \t -1.7863168283775226\n",
            "1      \t [-5.61872775  6.96371276]. \t  -21099.51033227109 \t -1.7863168283775226\n",
            "2      \t [-9.6736302  -6.79831075]. \t  -20965.88636571433 \t -1.7863168283775226\n",
            "3      \t [5.95256263 7.26277879]. \t  -19842.284445028028 \t -1.7863168283775226\n",
            "4      \t [-7.00510536 -0.20855881]. \t  -164.67744560983954 \t -1.7863168283775226\n",
            "5      \t [ 9.92174856 -1.24037905]. \t  -173.29656257293826 \t -1.7863168283775226\n",
            "6      \t [-3.49504171 -4.56372325]. \t  -4097.283187715339 \t -1.7863168283775226\n",
            "7      \t [-0.48474573  3.89995895]. \t  -1912.3319256842417 \t -1.7863168283775226\n",
            "8      \t [ -4.65018639 -10.        ]. \t  -83795.3221828279 \t -1.7863168283775226\n",
            "9      \t [-10.           3.40113132]. \t  -2316.907941393545 \t -1.7863168283775226\n",
            "10     \t [0.18037714 9.89102847]. \t  -76429.06005443657 \t -1.7863168283775226\n",
            "11     \t [6.01735262 1.64710049]. \t  -25.873506990552773 \t -1.7863168283775226\n",
            "12     \t [2.14917723 0.24623328]. \t  -9.545491484296612 \t -1.7863168283775226\n",
            "13     \t [ 9.97923216 -7.68882125]. \t  -23519.65817380143 \t -1.7863168283775226\n",
            "14     \t [9.78265058 3.98023095]. \t  -1036.5149354350756 \t -1.7863168283775226\n",
            "15     \t [-1.84020244 -0.44754306]. \t  -18.109047607222724 \t -1.7863168283775226\n",
            "16     \t [-9.99669574  9.15887933]. \t  -63323.023962983476 \t -1.7863168283775226\n",
            "17     \t [9.91784296 9.46154543]. \t  -57285.27507142265 \t -1.7863168283775226\n",
            "18     \t [ 0.66632259 -1.9268354 ]. \t  -91.48130689575467 \t -1.7863168283775226\n",
            "19     \t [3.27938067 1.87588064]. \t  -33.44785478560142 \t -1.7863168283775226\n",
            "20     \t [-10.          -1.94126044]. \t  -736.0916308709075 \t -1.7863168283775226\n",
            "21     \t [-4.24837428  1.86067568]. \t  -277.19951495538396 \t -1.7863168283775226\n",
            "22     \t [ 6.52378563 -1.63599301]. \t  -33.25393751607127 \t -1.7863168283775226\n",
            "23     \t [7.03828403 0.05606828]. \t  -135.35883001977373 \t -1.7863168283775226\n",
            "24     \t [5.35071469 2.593197  ]. \t  -150.10422637826207 \t -1.7863168283775226\n",
            "25     \t [ 7.35261408 -3.54619422]. \t  -673.9198536369116 \t -1.7863168283775226\n",
            "26     \t [2.22429758 5.75664184]. \t  -8207.214220471775 \t -1.7863168283775226\n",
            "27     \t [0.5609216  0.28871852]. \t  \u001b[92m-0.5035847435912526\u001b[0m \t -0.5035847435912526\n",
            "28     \t [-0.73479683  0.16673361]. \t  -4.258974926798736 \t -0.5035847435912526\n",
            "29     \t [-6.42221255 -2.72666258]. \t  -961.7528626157076 \t -0.5035847435912526\n",
            "30     \t [ 4.86002528 -0.22520264]. \t  -60.18820645780106 \t -0.5035847435912526\n",
            "31     \t [1.04923622 0.60084884]. \t  \u001b[92m-0.21654070751345567\u001b[0m \t -0.21654070751345567\n",
            "32     \t [-1.40718109 -1.37403551]. \t  -59.52415789379336 \t -0.21654070751345567\n",
            "33     \t [-4.29466991 -0.77921445]. \t  -88.73213688990937 \t -0.21654070751345567\n",
            "34     \t [-6.5209947   2.89952612]. \t  -1145.6563131680173 \t -0.21654070751345567\n",
            "35     \t [-0.16727031  0.57425474]. \t  -2.729740683262485 \t -0.21654070751345567\n",
            "36     \t [0.05198653 0.19354467]. \t  -0.8997813509809395 \t -0.21654070751345567\n",
            "37     \t [1.97889245 1.18719164]. \t  -2.3692810691417105 \t -0.21654070751345567\n",
            "38     \t [6.95731136 1.67015437]. \t  -39.289973389546255 \t -0.21654070751345567\n",
            "39     \t [1.62724434 1.45528745]. \t  -14.001758554347363 \t -0.21654070751345567\n",
            "40     \t [8.00016058 6.64169452]. \t  -12920.799139913539 \t -0.21654070751345567\n",
            "41     \t [0.95795396 0.20848131]. \t  -1.519137131891804 \t -0.21654070751345567\n",
            "42     \t [-2.19247135  0.07530166]. \t  -19.905448225930172 \t -0.21654070751345567\n",
            "43     \t [1.83041787 0.61398597]. \t  -3.0071274883443526 \t -0.21654070751345567\n",
            "44     \t [0.78985295 0.4746786 ]. \t  -0.2742932510760383 \t -0.21654070751345567\n",
            "45     \t [9.01104933 1.18023653]. \t  -141.6814680220478 \t -0.21654070751345567\n",
            "46     \t [ 2.89676858 -0.65534873]. \t  -11.903026670277796 \t -0.21654070751345567\n",
            "47     \t [3.10384242 0.02415575]. \t  -23.679342430315295 \t -0.21654070751345567\n",
            "48     \t [ 0.02108405 -0.16518215]. \t  -0.9605190909478537 \t -0.21654070751345567\n",
            "49     \t [-9.40807885  0.81353567]. \t  -338.6694263984088 \t -0.21654070751345567\n",
            "50     \t [1.03450793 0.42992548]. \t  -0.8852048554706723 \t -0.21654070751345567\n",
            "51     \t [-7.55261153 -6.98049741]. \t  -22126.213147022947 \t -0.21654070751345567\n",
            "52     \t [ 6.01925209 -1.32695283]. \t  -37.66934703510435 \t -0.21654070751345567\n",
            "53     \t [-7.65280328  8.39240761]. \t  -44189.95329155957 \t -0.21654070751345567\n",
            "54     \t [0.75388536 0.64178518]. \t  \u001b[92m-0.070341946193454\u001b[0m \t -0.070341946193454\n",
            "55     \t [1.43599877 0.85524126]. \t  -0.1915396155782667 \t -0.070341946193454\n",
            "56     \t [ 1.83415236 -0.81424352]. \t  -1.212278243855153 \t -0.070341946193454\n",
            "57     \t [-6.79058801  3.04631228]. \t  -1346.001643784961 \t -0.070341946193454\n",
            "58     \t [4.27113026 9.60126228]. \t  -64880.80724541189 \t -0.070341946193454\n",
            "59     \t [-5.50862208  0.93055068]. \t  -147.21100865555368 \t -0.070341946193454\n",
            "60     \t [ 1.80197773 -0.50232335]. \t  -4.009247795780853 \t -0.070341946193454\n",
            "61     \t [ 2.19679384 -1.48183025]. \t  -11.067030257112325 \t -0.070341946193454\n",
            "62     \t [-1.97006732 -0.32444149]. \t  -18.331261772302526 \t -0.070341946193454\n",
            "63     \t [-8.44339496  0.08126659]. \t  -232.20599304231348 \t -0.070341946193454\n",
            "64     \t [ 3.03252856 -4.64622883]. \t  -3226.9486716463393 \t -0.070341946193454\n",
            "65     \t [-5.92098836 -9.43033498]. \t  -67600.59403596063 \t -0.070341946193454\n",
            "66     \t [ 1.99507481 -0.83823932]. \t  -1.68586542216448 \t -0.070341946193454\n",
            "67     \t [-6.30677823  1.02322762]. \t  -194.53480580590767 \t -0.070341946193454\n",
            "68     \t [-0.18665639 -4.42654547]. \t  -3102.231289721199 \t -0.070341946193454\n",
            "69     \t [4.73898245 3.74417699]. \t  -1099.64257608463 \t -0.070341946193454\n",
            "70     \t [4.35490102 1.71732416]. \t  -16.020167119629722 \t -0.070341946193454\n",
            "71     \t [3.99742129 1.14954368]. \t  -12.653982822717568 \t -0.070341946193454\n",
            "72     \t [4.92267066 1.44979391]. \t  -16.420881447194066 \t -0.070341946193454\n",
            "73     \t [5.89575926 2.11179948]. \t  -42.25319438508458 \t -0.070341946193454\n",
            "74     \t [ 0.25318592 -3.02381372]. \t  -650.9873297211229 \t -0.070341946193454\n",
            "75     \t [ 6.79081091 -9.25907725]. \t  -54266.09092018643 \t -0.070341946193454\n",
            "76     \t [-1.45810925  0.31429353]. \t  -11.524788054095056 \t -0.070341946193454\n",
            "77     \t [2.3110438  1.06174645]. \t  -1.7252051704626943 \t -0.070341946193454\n",
            "78     \t [-0.7973397 -0.612444 ]. \t  -8.020035398703488 \t -0.070341946193454\n",
            "79     \t [5.50025847 2.631525  ]. \t  -159.68360540471554 \t -0.070341946193454\n",
            "80     \t [ 7.7878485  -0.83363241]. \t  -127.94273632673325 \t -0.070341946193454\n",
            "81     \t [-1.38890174 -0.55778568]. \t  -13.796311972686956 \t -0.070341946193454\n",
            "82     \t [3.33980119 3.6619608 ]. \t  -1108.1060419131188 \t -0.070341946193454\n",
            "83     \t [4.95962777 1.75251714]. \t  -18.47765320683075 \t -0.070341946193454\n",
            "84     \t [-3.50885816  4.46413975]. \t  -3781.5402603850184 \t -0.070341946193454\n",
            "85     \t [-8.65102928  9.910253  ]. \t  -84206.5061584383 \t -0.070341946193454\n",
            "86     \t [5.05410229 0.67823622]. \t  -50.61720443025642 \t -0.070341946193454\n",
            "87     \t [ 0.65459451 -1.51311626]. \t  -30.92187508731874 \t -0.070341946193454\n",
            "88     \t [ 0.96332409 -0.89723156]. \t  -0.8378511675492627 \t -0.070341946193454\n",
            "89     \t [-0.07448706 -0.84900626]. \t  -5.751703363218832 \t -0.070341946193454\n",
            "90     \t [-4.66950881 -2.21553784]. \t  -451.87371405420083 \t -0.070341946193454\n",
            "91     \t [1.07126881 0.95117134]. \t  -1.094913543298511 \t -0.070341946193454\n",
            "92     \t [ 2.78182392 -1.27265521]. \t  -3.593469907187097 \t -0.070341946193454\n",
            "93     \t [ 3.47587984 -4.77822747]. \t  -3565.6219455686582 \t -0.070341946193454\n",
            "94     \t [-3.33938631 -3.56105429]. \t  -1666.394147176159 \t -0.070341946193454\n",
            "95     \t [-3.19099219 -1.52078498]. \t  -139.76182719415408 \t -0.070341946193454\n",
            "96     \t [ 0.87443712 -0.7639833 ]. \t  -0.18735135815346665 \t -0.070341946193454\n",
            "97     \t [-6.11691153  5.8954905 ]. \t  -11490.603962595726 \t -0.070341946193454\n",
            "98     \t [-8.88127832 -9.83888317]. \t  -82100.84304076748 \t -0.070341946193454\n",
            "99     \t [1.39639806 5.72636826]. \t  -8239.890787569202 \t -0.070341946193454\n",
            "100    \t [ 9.497473   -0.19024079]. \t  -249.87168618536793 \t -0.070341946193454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "bdadc83b-e5ed-44f4-bf2d-05d154cec472"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.78722095 6.4258993 ]. \t  -11523.905614569041 \t -765.9755148718126\n",
            "init   \t [2.8441433  3.34445237]. \t  -765.9755148718126 \t -765.9755148718126\n",
            "init   \t [-9.21720189 -8.68655223]. \t  -51387.35679532384 \t -765.9755148718126\n",
            "init   \t [-4.4699872  -3.67278993]. \t  -2007.9696367442386 \t -765.9755148718126\n",
            "init   \t [-6.12815176 -6.05814236]. \t  -12700.957148627067 \t -765.9755148718126\n",
            "1      \t [ 5.15020416 -5.71066603]. \t  -7234.8050437839365 \t -765.9755148718126\n",
            "2      \t [-3.97305893  9.21389704]. \t  -60413.181901672484 \t -765.9755148718126\n",
            "3      \t [-8.51899843  2.33131307]. \t  -842.4810529068852 \t -765.9755148718126\n",
            "4      \t [ -0.08816825 -10.        ]. \t  -80071.7342591979 \t -765.9755148718126\n",
            "5      \t [8.27826207 0.32767616]. \t  \u001b[92m-183.01376768554263\u001b[0m \t -183.01376768554263\n",
            "6      \t [ 9.67802381 -9.83797028]. \t  -67708.7958293111 \t -183.01376768554263\n",
            "7      \t [-2.83803863  2.67646344]. \t  -604.0016776781533 \t -183.01376768554263\n",
            "8      \t [-10.           7.83183069]. \t  -35326.38761526067 \t -183.01376768554263\n",
            "9      \t [ 0.75202853 -1.7859465 ]. \t  \u001b[92m-63.39182813815439\u001b[0m \t -63.39182813815439\n",
            "10     \t [1.85893393 9.46393719]. \t  -62852.36972535914 \t -63.39182813815439\n",
            "11     \t [-10.          -3.00093023]. \t  -1690.2506727519876 \t -63.39182813815439\n",
            "12     \t [-0.43476566 -4.81699878]. \t  -4390.352048984317 \t -63.39182813815439\n",
            "13     \t [ 4.12450281 -0.68838776]. \t  \u001b[92m-29.945965721981686\u001b[0m \t -29.945965721981686\n",
            "14     \t [ 7.38444103 -2.37879275]. \t  -71.69600161633967 \t -29.945965721981686\n",
            "15     \t [9.86518382 3.28980966]. \t  -356.1523837990128 \t -29.945965721981686\n",
            "16     \t [-6.0952986   4.98949329]. \t  -6296.695804740864 \t -29.945965721981686\n",
            "17     \t [9.19083537 9.88939013]. \t  -69563.89849609062 \t -29.945965721981686\n",
            "18     \t [0.01700021 0.89149127]. \t  \u001b[92m-5.911883957167637\u001b[0m \t -5.911883957167637\n",
            "19     \t [-5.46209276  0.06431105]. \t  -101.60842004361601 \t -5.911883957167637\n",
            "20     \t [-2.14453349 -0.60144885]. \t  -26.33911009065814 \t -5.911883957167637\n",
            "21     \t [ 8.8265108  -4.11307361]. \t  -1312.078224120276 \t -5.911883957167637\n",
            "22     \t [ 6.27895618 -1.68115615]. \t  -28.652092677998294 \t -5.911883957167637\n",
            "23     \t [1.32383852 0.29600318]. \t  \u001b[92m-2.7434479628471324\u001b[0m \t -2.7434479628471324\n",
            "24     \t [0.78355696 0.55820097]. \t  \u001b[92m-0.09829127401064257\u001b[0m \t -0.09829127401064257\n",
            "25     \t [5.2730853 1.4697123]. \t  -20.075587573179618 \t -0.09829127401064257\n",
            "26     \t [ 3.44247018 -2.01645131]. \t  -49.95188680167296 \t -0.09829127401064257\n",
            "27     \t [-0.33857912  4.46069147]. \t  -3223.286325311457 \t -0.09829127401064257\n",
            "28     \t [0.10783759 0.16183711]. \t  -0.8021042933517469 \t -0.09829127401064257\n",
            "29     \t [0.28729995 0.16287836]. \t  -0.6176792684089881 \t -0.09829127401064257\n",
            "30     \t [ 4.51316977 -0.00578921]. \t  -53.078554567915205 \t -0.09829127401064257\n",
            "31     \t [6.56738654 2.53878052]. \t  -110.96723860821487 \t -0.09829127401064257\n",
            "32     \t [ 1.67581283 -0.14772669]. \t  -5.784657796609966 \t -0.09829127401064257\n",
            "33     \t [ 3.02908829 -0.90178523]. \t  -8.052081803742315 \t -0.09829127401064257\n",
            "34     \t [0.615912   0.40957902]. \t  -0.3047742231662339 \t -0.09829127401064257\n",
            "35     \t [-0.43199815 -0.42367312]. \t  -3.301967937980036 \t -0.09829127401064257\n",
            "36     \t [ 4.93353927 -9.42015033]. \t  -59558.9750999321 \t -0.09829127401064257\n",
            "37     \t [-4.66553059 -9.24547315]. \t  -61719.02942258552 \t -0.09829127401064257\n",
            "38     \t [1.56415682 0.63942284]. \t  -1.4325993920996853 \t -0.09829127401064257\n",
            "39     \t [-0.76538432 -0.97728436]. \t  -17.433757641452484 \t -0.09829127401064257\n",
            "40     \t [-7.52040435 -0.40475527]. \t  -195.78131945034403 \t -0.09829127401064257\n",
            "41     \t [-1.24675416 -4.65154341]. \t  -3969.191768420114 \t -0.09829127401064257\n",
            "42     \t [0.87078742 0.73772548]. \t  -0.11147406073222062 \t -0.09829127401064257\n",
            "43     \t [ 5.19604065 -4.67146654]. \t  -2974.2823669943177 \t -0.09829127401064257\n",
            "44     \t [ 5.49572533 -3.44560664]. \t  -686.2405513026092 \t -0.09829127401064257\n",
            "45     \t [ 6.00679573 -0.7878787 ]. \t  -70.48398187619578 \t -0.09829127401064257\n",
            "46     \t [ 8.24489025 -1.50244286]. \t  -80.31753445412974 \t -0.09829127401064257\n",
            "47     \t [1.88512221 0.46600164]. \t  -4.993124081743773 \t -0.09829127401064257\n",
            "48     \t [ 6.90434935 -1.53073357]. \t  -44.70091097466366 \t -0.09829127401064257\n",
            "49     \t [ 2.64092556 -2.61533876]. \t  -246.41468407480542 \t -0.09829127401064257\n",
            "50     \t [ 9.76733397 -1.30497578]. \t  -157.8012295434315 \t -0.09829127401064257\n",
            "51     \t [ 3.69430396 -1.4311797 ]. \t  -7.582878668006006 \t -0.09829127401064257\n",
            "52     \t [ 0.44699959 -0.27742485]. \t  -0.47759008467274966 \t -0.09829127401064257\n",
            "53     \t [ 1.27648713 -0.63292458]. \t  -0.5282654422747914 \t -0.09829127401064257\n",
            "54     \t [ 6.49196718 -7.21609816]. \t  -19102.05751112033 \t -0.09829127401064257\n",
            "55     \t [0.59590251 0.11718733]. \t  -0.8095355024495225 \t -0.09829127401064257\n",
            "56     \t [ 0.75567059 -0.85408622]. \t  -1.048834726082826 \t -0.09829127401064257\n",
            "57     \t [3.20122798 0.02553411]. \t  -25.324431859395908 \t -0.09829127401064257\n",
            "58     \t [-5.54828522 -2.83642507]. \t  -979.3639882971961 \t -0.09829127401064257\n",
            "59     \t [-1.38572057 -0.02231777]. \t  -9.537629265829219 \t -0.09829127401064257\n",
            "60     \t [ 0.78374146 -0.25038619]. \t  -0.9136302895674375 \t -0.09829127401064257\n",
            "61     \t [ 7.7589623  -0.11838766]. \t  -165.21816050757536 \t -0.09829127401064257\n",
            "62     \t [ 5.10272363 -8.37402313]. \t  -36545.6085694846 \t -0.09829127401064257\n",
            "63     \t [-3.36428627 -4.54178341]. \t  -4000.9142702923155 \t -0.09829127401064257\n",
            "64     \t [-9.52691162  7.81135867]. \t  -34727.70400980378 \t -0.09829127401064257\n",
            "65     \t [0.70007031 2.81598637]. \t  -459.71012085825157 \t -0.09829127401064257\n",
            "66     \t [-3.66699095 -1.76817563]. \t  -218.58888550221934 \t -0.09829127401064257\n",
            "67     \t [ 3.74941644 -7.90499041]. \t  -29400.175506072163 \t -0.09829127401064257\n",
            "68     \t [0.56436426 4.51406788]. \t  -3230.541957069111 \t -0.09829127401064257\n",
            "69     \t [0.64968919 1.78754069]. \t  -66.03891083229493 \t -0.09829127401064257\n",
            "70     \t [0.29754546 0.39250371]. \t  -0.4936659581205672 \t -0.09829127401064257\n",
            "71     \t [-0.35825835  0.57811025]. \t  -3.9530146075919443 \t -0.09829127401064257\n",
            "72     \t [0.66141394 4.89445901]. \t  -4465.2479659832015 \t -0.09829127401064257\n",
            "73     \t [-3.30036674 -0.10506223]. \t  -40.570407569071875 \t -0.09829127401064257\n",
            "74     \t [-4.11355789 -8.05623696]. \t  -35894.995196675634 \t -0.09829127401064257\n",
            "75     \t [ 7.85982694 -4.90624431]. \t  -3292.4393334292486 \t -0.09829127401064257\n",
            "76     \t [0.44655222 0.85244886]. \t  -2.333540136191661 \t -0.09829127401064257\n",
            "77     \t [7.03050385 8.26918204]. \t  -33695.19910931697 \t -0.09829127401064257\n",
            "78     \t [8.53890693 8.20984019]. \t  -31942.053167629954 \t -0.09829127401064257\n",
            "79     \t [ 2.12312966 -8.41193406]. \t  -38864.9440071836 \t -0.09829127401064257\n",
            "80     \t [ 0.58835811 -0.86428074]. \t  -1.8096872183502524 \t -0.09829127401064257\n",
            "81     \t [3.76477698 3.76714555]. \t  -1219.7352679061078 \t -0.09829127401064257\n",
            "82     \t [1.11275264 1.12080428]. \t  -3.93076369019918 \t -0.09829127401064257\n",
            "83     \t [ 2.40932222 -0.51328481]. \t  -9.0730459066795 \t -0.09829127401064257\n",
            "84     \t [7.23246528 9.31484792]. \t  -55350.43524845048 \t -0.09829127401064257\n",
            "85     \t [8.73485241 4.70702164]. \t  -2591.309797284475 \t -0.09829127401064257\n",
            "86     \t [ 3.03157594 -9.76296096]. \t  -70391.07523319387 \t -0.09829127401064257\n",
            "87     \t [-9.74162944  5.69879592]. \t  -11273.823476747046 \t -0.09829127401064257\n",
            "88     \t [ 9.77528004 -4.79792497]. \t  -2707.289737859233 \t -0.09829127401064257\n",
            "89     \t [-3.70262294 -8.69285128]. \t  -47969.21868999177 \t -0.09829127401064257\n",
            "90     \t [ 8.63087524 -2.72917549]. \t  -136.75382542645792 \t -0.09829127401064257\n",
            "91     \t [9.59066697 1.62142869]. \t  -111.34249031964976 \t -0.09829127401064257\n",
            "92     \t [ 5.37857082 -2.58028248]. \t  -145.16840990953483 \t -0.09829127401064257\n",
            "93     \t [ 5.18160093 -2.11211452]. \t  -45.46778762752092 \t -0.09829127401064257\n",
            "94     \t [0.08161118 1.76828527]. \t  -77.03194911698674 \t -0.09829127401064257\n",
            "95     \t [ 0.70846827 -0.76932106]. \t  -0.5366997564626311 \t -0.09829127401064257\n",
            "96     \t [ 6.62625188 -0.43495995]. \t  -109.72650818812865 \t -0.09829127401064257\n",
            "97     \t [5.684713   1.29474753]. \t  -32.82271019417604 \t -0.09829127401064257\n",
            "98     \t [-5.98245003  7.30174925]. \t  -25412.370855398018 \t -0.09829127401064257\n",
            "99     \t [-0.24093743 -0.70878098]. \t  -4.643355030458981 \t -0.09829127401064257\n",
            "100    \t [ 6.29792024 -4.77555238]. \t  -3119.230411100237 \t -0.09829127401064257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "fe984ae8-98de-4233-d72c-4dff199443cd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.14706549 0.83521255]. \t  -0.14473002518929054 \t -0.14473002518929054\n",
            "init   \t [ 1.20847567 -2.35903813]. \t  -196.92158721117866 \t -0.14473002518929054\n",
            "init   \t [-7.61811642  6.91998231]. \t  -21453.432632023236 \t -0.14473002518929054\n",
            "init   \t [4.79483225 8.64715129]. \t  -41920.46290057033 \t -0.14473002518929054\n",
            "init   \t [7.18656013 4.16209252]. \t  -1546.3183076435794 \t -0.14473002518929054\n",
            "1      \t [-0.89169678  2.93285572]. \t  -658.4352762822963 \t -0.14473002518929054\n",
            "2      \t [2.20317475 0.72550179]. \t  -4.094787565175626 \t -0.14473002518929054\n",
            "3      \t [ 9.14614046 -9.26496158]. \t  -52900.23738128742 \t -0.14473002518929054\n",
            "4      \t [-7.13636588 -6.2842414 ]. \t  -14899.421659869966 \t -0.14473002518929054\n",
            "5      \t [-0.18902547 -8.31395818]. \t  -38328.708807119125 \t -0.14473002518929054\n",
            "6      \t [-9.17522226  0.24387639]. \t  -276.2984757917325 \t -0.14473002518929054\n",
            "7      \t [ 7.29938814 -2.28804897]. \t  -59.792113899411696 \t -0.14473002518929054\n",
            "8      \t [-3.85312917 -1.46856833]. \t  -156.93679922786626 \t -0.14473002518929054\n",
            "9      \t [1.09388354 0.53086462]. \t  -0.5711422063366313 \t -0.14473002518929054\n",
            "10     \t [-1.84291699  8.9668659 ]. \t  -52919.61385971303 \t -0.14473002518929054\n",
            "11     \t [-5.10980906  1.90641206]. \t  -343.79037850105027 \t -0.14473002518929054\n",
            "12     \t [1.50871241 0.91126932]. \t  -0.3050639206002509 \t -0.14473002518929054\n",
            "13     \t [ 4.11108752 -5.95881487]. \t  -8961.932289135033 \t -0.14473002518929054\n",
            "14     \t [-10. -10.]. \t  -88321.0 \t -0.14473002518929054\n",
            "15     \t [4.20090529 0.4171124 ]. \t  -39.93608450598814 \t -0.14473002518929054\n",
            "16     \t [9.8423579  8.93336417]. \t  -44938.875045174995 \t -0.14473002518929054\n",
            "17     \t [9.98706881 0.45188222]. \t  -264.2693865262832 \t -0.14473002518929054\n",
            "18     \t [2.9471352  3.90374086]. \t  -1519.730813314867 \t -0.14473002518929054\n",
            "19     \t [-1.70764276 -4.05345889]. \t  -2397.321474079005 \t -0.14473002518929054\n",
            "20     \t [ 9.46855045 -3.74605477]. \t  -763.4356468459944 \t -0.14473002518929054\n",
            "21     \t [-4.16523024 -9.39020572]. \t  -65199.56776118098 \t -0.14473002518929054\n",
            "22     \t [ 4.48310826 -1.68217338]. \t  -14.899436149451605 \t -0.14473002518929054\n",
            "23     \t [-10.         -2.9818166]. \t  -1664.7302193768944 \t -0.14473002518929054\n",
            "24     \t [1.63565161 0.58134596]. \t  -2.246198494282085 \t -0.14473002518929054\n",
            "25     \t [0.28203088 0.0848536 ]. \t  -0.6587319495480773 \t -0.14473002518929054\n",
            "26     \t [-9.78778202  3.01457798]. \t  -1680.250958795534 \t -0.14473002518929054\n",
            "27     \t [6.98127063 0.46375328]. \t  -121.6103752555632 \t -0.14473002518929054\n",
            "28     \t [-2.48374738  0.19235335]. \t  -25.220635218348715 \t -0.14473002518929054\n",
            "29     \t [-6.84306666 -2.33215207]. \t  -689.5766453446273 \t -0.14473002518929054\n",
            "30     \t [1.41274688 0.73645163]. \t  -0.38556061627576815 \t -0.14473002518929054\n",
            "31     \t [-3.97942325  4.13866992]. \t  -2948.869911722159 \t -0.14473002518929054\n",
            "32     \t [ 3.18265453 -0.82755009]. \t  -11.337746322818317 \t -0.14473002518929054\n",
            "33     \t [-0.27780683 -0.24833815]. \t  -1.9546337286343718 \t -0.14473002518929054\n",
            "34     \t [5.3628     7.62474042]. \t  -24621.32671371917 \t -0.14473002518929054\n",
            "35     \t [0.38587219 0.28005683]. \t  -0.4820427847523505 \t -0.14473002518929054\n",
            "36     \t [ 5.56243585 -1.28141199]. \t  -31.1980566795664 \t -0.14473002518929054\n",
            "37     \t [ 3.81291412 -1.70491774]. \t  -15.917085658895143 \t -0.14473002518929054\n",
            "38     \t [ 6.12699048 -3.01767919]. \t  -318.4184248715372 \t -0.14473002518929054\n",
            "39     \t [-1.96747031 -0.5378139 ]. \t  -21.76968329731455 \t -0.14473002518929054\n",
            "40     \t [ 8.12087618 -1.62928465]. \t  -66.5186328280176 \t -0.14473002518929054\n",
            "41     \t [ 4.58175875 -9.57591342]. \t  -63962.06340312636 \t -0.14473002518929054\n",
            "42     \t [1.21505183 0.67809029]. \t  -0.22081563759203085 \t -0.14473002518929054\n",
            "43     \t [-0.5537127   0.08392089]. \t  -3.0588125779110795 \t -0.14473002518929054\n",
            "44     \t [-7.13427636  8.61588551]. \t  -48489.65974397891 \t -0.14473002518929054\n",
            "45     \t [ 4.05974659 -0.67802698]. \t  -29.085085330285466 \t -0.14473002518929054\n",
            "46     \t [-1.41683764 -0.5840477 ]. \t  -14.653218942559906 \t -0.14473002518929054\n",
            "47     \t [1.27714608 0.95309506]. \t  -0.6592203207825715 \t -0.14473002518929054\n",
            "48     \t [-5.00223563  4.62773213]. \t  -4612.218532478034 \t -0.14473002518929054\n",
            "49     \t [ 2.48697812 -0.95100632]. \t  -3.1308844161424236 \t -0.14473002518929054\n",
            "50     \t [5.50806031 6.92633429]. \t  -16379.165169186766 \t -0.14473002518929054\n",
            "51     \t [-0.763835   -3.11453932]. \t  -816.3288887456149 \t -0.14473002518929054\n",
            "52     \t [-6.19530758 -1.30673651]. \t  -236.49311107238395 \t -0.14473002518929054\n",
            "53     \t [-6.64422699e+00  2.30256913e-03]. \t  -146.7259927020121 \t -0.14473002518929054\n",
            "54     \t [5.09791047 1.46819062]. \t  -18.03079954411408 \t -0.14473002518929054\n",
            "55     \t [ 4.0160458  -5.10868357]. \t  -4651.962191180231 \t -0.14473002518929054\n",
            "56     \t [ 2.51770504 -3.2432221 ]. \t  -688.2304537739782 \t -0.14473002518929054\n",
            "57     \t [9.87825041 3.42382287]. \t  -446.9435586938674 \t -0.14473002518929054\n",
            "58     \t [-7.09555014 -1.87065862]. \t  -462.83524980013703 \t -0.14473002518929054\n",
            "59     \t [ 1.88190063 -0.89300238]. \t  -0.9424799759299856 \t -0.14473002518929054\n",
            "60     \t [ 7.55197615 -4.15741169]. \t  -1502.675032654747 \t -0.14473002518929054\n",
            "61     \t [ 6.83304328 -1.84982541]. \t  -34.02462135949341 \t -0.14473002518929054\n",
            "62     \t [-4.75469008 -7.1150874 ]. \t  -22506.654565598223 \t -0.14473002518929054\n",
            "63     \t [ 0.52143475 -1.6030104 ]. \t  -42.87810013311959 \t -0.14473002518929054\n",
            "64     \t [ 2.59912835 -9.37357639]. \t  -59949.67873668477 \t -0.14473002518929054\n",
            "65     \t [ 7.36731358 -1.841798  ]. \t  -41.22216601423741 \t -0.14473002518929054\n",
            "66     \t [4.83577875 1.65862836]. \t  -15.601156239351546 \t -0.14473002518929054\n",
            "67     \t [0.6540322  0.45814347]. \t  -0.22943170710098815 \t -0.14473002518929054\n",
            "68     \t [ 2.51281922 -0.53561285]. \t  -9.808505974204905 \t -0.14473002518929054\n",
            "69     \t [-6.24072439  0.690922  ]. \t  -155.97769002463605 \t -0.14473002518929054\n",
            "70     \t [ 4.49799941 -2.94966073]. \t  -345.21068692693746 \t -0.14473002518929054\n",
            "71     \t [ 7.19505941 -2.2019923 ]. \t  -50.90358099568845 \t -0.14473002518929054\n",
            "72     \t [ 3.71435421 -0.95314308]. \t  -14.567902081003062 \t -0.14473002518929054\n",
            "73     \t [-8.33248565  0.93367549]. \t  -290.14625559474524 \t -0.14473002518929054\n",
            "74     \t [ 7.03740317 -5.13546999]. \t  -4215.020903821437 \t -0.14473002518929054\n",
            "75     \t [-7.43781506 -0.14536251]. \t  -183.0997841788702 \t -0.14473002518929054\n",
            "76     \t [ 1.25417438 -8.4556558 ]. \t  -40181.68519036071 \t -0.14473002518929054\n",
            "77     \t [9.54647128 9.218395  ]. \t  -51536.510783866055 \t -0.14473002518929054\n",
            "78     \t [ 1.63573994 -0.8071676 ]. \t  -0.6255449923300134 \t -0.14473002518929054\n",
            "79     \t [-5.68758126  9.14632012]. \t  -59901.28760036906 \t -0.14473002518929054\n",
            "80     \t [ 0.99525104 -4.28503412]. \t  -2552.949101192402 \t -0.14473002518929054\n",
            "81     \t [ 4.36834194 -1.10413313]. \t  -18.79646936114178 \t -0.14473002518929054\n",
            "82     \t [0.55435797 0.23579338]. \t  -0.5913800432835926 \t -0.14473002518929054\n",
            "83     \t [1.9516029  9.10809664]. \t  -53768.79758512499 \t -0.14473002518929054\n",
            "84     \t [1.25943139 0.81670423]. \t  \u001b[92m-0.0784290628392909\u001b[0m \t -0.0784290628392909\n",
            "85     \t [-4.56399038  2.07938376]. \t  -380.05412095102264 \t -0.0784290628392909\n",
            "86     \t [2.56789062 4.34321879]. \t  -2474.798140262368 \t -0.0784290628392909\n",
            "87     \t [ 1.16502666 -1.4035056 ]. \t  -15.424368891965159 \t -0.0784290628392909\n",
            "88     \t [5.40894244 1.89054408]. \t  -25.48959941243422 \t -0.0784290628392909\n",
            "89     \t [2.65931419 2.22862947]. \t  -108.58317104397753 \t -0.0784290628392909\n",
            "90     \t [1.2900814  0.79116915]. \t  -0.08706327757258457 \t -0.0784290628392909\n",
            "91     \t [ 9.14319148 -0.78701318]. \t  -191.271024830003 \t -0.0784290628392909\n",
            "92     \t [ 8.64980484 -6.85356859]. \t  -14608.311187269232 \t -0.0784290628392909\n",
            "93     \t [4.66563608 2.8060112 ]. \t  -259.04776831127634 \t -0.0784290628392909\n",
            "94     \t [-5.34944394  6.9938714 ]. \t  -21331.681049501938 \t -0.0784290628392909\n",
            "95     \t [ 8.44278079 -7.19997955]. \t  -18195.42650662692 \t -0.0784290628392909\n",
            "96     \t [7.94956547 1.96494096]. \t  -48.40004505770033 \t -0.0784290628392909\n",
            "97     \t [ 1.22211804 -0.29142523]. \t  -2.2638416335246645 \t -0.0784290628392909\n",
            "98     \t [-8.86077216  7.52949317]. \t  -29986.042979117836 \t -0.0784290628392909\n",
            "99     \t [-3.64129716  4.08824078]. \t  -2769.7220190796297 \t -0.0784290628392909\n",
            "100    \t [1.17088366 0.7487506 ]. \t  \u001b[92m-0.034127250187303546\u001b[0m \t -0.034127250187303546\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "8ee10bef-1c4b-4994-b092-3884b3c4aad3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.85720303 -3.36040389]. \t  -480.81536490368427 \t -43.91981950896418\n",
            "init   \t [ 6.42458246 -9.16606749]. \t  -52264.36633669738 \t -43.91981950896418\n",
            "init   \t [-7.8468664   1.90104128]. \t  -532.7651696660866 \t -43.91981950896418\n",
            "init   \t [ 0.59634724 -1.62385143]. \t  -43.91981950896418 \t -43.91981950896418\n",
            "init   \t [-3.29184301  2.45038864]. \t  -486.6398213674611 \t -43.91981950896418\n",
            "1      \t [-10.          -9.08379396]. \t  -61392.439547084185 \t -43.91981950896418\n",
            "2      \t [7.02990054 8.4753693 ]. \t  -37373.988071206615 \t -43.91981950896418\n",
            "3      \t [-7.69566798  9.86414506]. \t  -81924.88656432132 \t -43.91981950896418\n",
            "4      \t [-1.71570199 -9.52643263]. \t  -67147.64008994015 \t -43.91981950896418\n",
            "5      \t [0.12987728 7.74382654]. \t  -28706.668528284812 \t -43.91981950896418\n",
            "6      \t [-5.8138096  -3.70808956]. \t  -2266.0286734791575 \t -43.91981950896418\n",
            "7      \t [5.18193691 1.8785775 ]. \t  \u001b[92m-24.52862347442945\u001b[0m \t -24.52862347442945\n",
            "8      \t [ 2.16735142 -5.29303535]. \t  -5804.25963330402 \t -24.52862347442945\n",
            "9      \t [1.40352754 2.27618595]. \t  -160.67290344848865 \t -24.52862347442945\n",
            "10     \t [7.84511055 0.73655756]. \t  -138.252805876501 \t -24.52862347442945\n",
            "11     \t [-9.73988323 -3.24085104]. \t  -2005.9921605076943 \t -24.52862347442945\n",
            "12     \t [ 4.14363926 -1.05584602]. \t  \u001b[92m-17.209394872567565\u001b[0m \t -17.209394872567565\n",
            "13     \t [-2.49108876 -1.44499955]. \t  -101.08911043319897 \t -17.209394872567565\n",
            "14     \t [-9.60354959  5.10967351]. \t  -7756.126250501723 \t -17.209394872567565\n",
            "15     \t [3.56051751 4.57228876]. \t  -2932.8543802022127 \t -17.209394872567565\n",
            "16     \t [-4.26590437  5.84803192]. \t  -10588.093214096301 \t -17.209394872567565\n",
            "17     \t [9.24917075 4.02536331]. \t  -1140.6281518901199 \t -17.209394872567565\n",
            "18     \t [ 2.72652304 -0.25019312]. \t  \u001b[92m-16.51471571056421\u001b[0m \t -16.51471571056421\n",
            "19     \t [-1.9235455  -4.84086174]. \t  -4769.754441311758 \t -16.51471571056421\n",
            "20     \t [ 9.94339114 -5.83893736]. \t  -6864.475500501584 \t -16.51471571056421\n",
            "21     \t [4.59832225 0.71684608]. \t  -38.446086554663424 \t -16.51471571056421\n",
            "22     \t [-5.99771949 -7.26760927]. \t  -24973.27176562287 \t -16.51471571056421\n",
            "23     \t [ 9.96499222 -0.71356254]. \t  -240.45615016727385 \t -16.51471571056421\n",
            "24     \t [ 3.89066217 -1.67046061]. \t  \u001b[92m-14.069582211209266\u001b[0m \t -14.069582211209266\n",
            "25     \t [5.88957161 2.25657238]. \t  -60.79622676746749 \t -14.069582211209266\n",
            "26     \t [ 4.87559878 -2.29210452]. \t  -78.45657901504396 \t -14.069582211209266\n",
            "27     \t [-0.52729332 -0.33376025]. \t  \u001b[92m-3.45788031331116\u001b[0m \t -3.45788031331116\n",
            "28     \t [-0.87031007  0.4752706 ]. \t  -6.993820956482738 \t -3.45788031331116\n",
            "29     \t [-5.26160141 -0.66418989]. \t  -114.70261340481355 \t -3.45788031331116\n",
            "30     \t [ 1.95165392 -0.92110338]. \t  \u001b[92m-1.0354821205920615\u001b[0m \t -1.0354821205920615\n",
            "31     \t [-0.09522157  0.76541955]. \t  -4.409863937399255 \t -1.0354821205920615\n",
            "32     \t [-0.42019684  3.3457463 ]. \t  -1042.447981117573 \t -1.0354821205920615\n",
            "33     \t [ 0.17159355 -0.09583341]. \t  \u001b[92m-0.7332133365035686\u001b[0m \t -0.7332133365035686\n",
            "34     \t [ 2.38209451 -9.36475898]. \t  -59870.51695466323 \t -0.7332133365035686\n",
            "35     \t [2.50656106 0.80738462]. \t  -5.1632839508269495 \t -0.7332133365035686\n",
            "36     \t [-0.76796774  0.21914679]. \t  -4.618765393626002 \t -0.7332133365035686\n",
            "37     \t [-9.54461185 -6.92456773]. \t  -22348.016015363617 \t -0.7332133365035686\n",
            "38     \t [3.41810998 1.35589004]. \t  -5.98117516174452 \t -0.7332133365035686\n",
            "39     \t [ 5.94554724 -8.19734584]. \t  -33021.93297494271 \t -0.7332133365035686\n",
            "40     \t [ 1.42375998 -0.0946624 ]. \t  -4.13233369619334 \t -0.7332133365035686\n",
            "41     \t [1.18761478 3.40904926]. \t  -972.9358498398087 \t -0.7332133365035686\n",
            "42     \t [0.96598734 1.4171127 ]. \t  -18.611396869489596 \t -0.7332133365035686\n",
            "43     \t [0.01530393 0.0557642 ]. \t  -0.9697914120824674 \t -0.7332133365035686\n",
            "44     \t [1.96318373 1.5661275 ]. \t  -18.24229893131104 \t -0.7332133365035686\n",
            "45     \t [-0.3398488   1.92037519]. \t  -120.85401780007317 \t -0.7332133365035686\n",
            "46     \t [ 2.08814131 -1.30608399]. \t  -4.687723806207211 \t -0.7332133365035686\n",
            "47     \t [3.31422003 9.89486942]. \t  -74119.90509944806 \t -0.7332133365035686\n",
            "48     \t [ 1.57994074 -0.77129901]. \t  \u001b[92m-0.6407441038056153\u001b[0m \t -0.6407441038056153\n",
            "49     \t [-0.16775205 -0.14193455]. \t  -1.4502085426725528 \t -0.6407441038056153\n",
            "50     \t [ 5.8856001  -0.65100397]. \t  -74.63172985332676 \t -0.6407441038056153\n",
            "51     \t [ 0.19359014 -0.03363999]. \t  -0.7235087793786997 \t -0.6407441038056153\n",
            "52     \t [-0.4451848  1.4532218]. \t  -45.685663811473155 \t -0.6407441038056153\n",
            "53     \t [-0.70291207  4.05680639]. \t  -2263.2758340612963 \t -0.6407441038056153\n",
            "54     \t [-0.14881298  0.11154469]. \t  -1.3801128441938684 \t -0.6407441038056153\n",
            "55     \t [3.30092204 0.73465885]. \t  -15.164142804779893 \t -0.6407441038056153\n",
            "56     \t [-6.40760609  9.9423816 ]. \t  -83376.25651179242 \t -0.6407441038056153\n",
            "57     \t [ 3.2188219  -1.39082617]. \t  -5.768100386170028 \t -0.6407441038056153\n",
            "58     \t [4.55449115 1.57638214]. \t  -12.979638220860418 \t -0.6407441038056153\n",
            "59     \t [ 5.79325672 -1.75684026]. \t  -23.26368252914183 \t -0.6407441038056153\n",
            "60     \t [-6.55560926 -6.37654577]. \t  -15501.56884801357 \t -0.6407441038056153\n",
            "61     \t [6.93219279 7.74582913]. \t  -25601.921290181013 \t -0.6407441038056153\n",
            "62     \t [6.7686097  4.92442389]. \t  -3516.278259609777 \t -0.6407441038056153\n",
            "63     \t [ 4.38161345 -0.57842972]. \t  -38.99990287655739 \t -0.6407441038056153\n",
            "64     \t [ 1.74122751 -0.50389113]. \t  -3.592043188981885 \t -0.6407441038056153\n",
            "65     \t [-9.37022401 -5.04390326]. \t  -7368.183318605593 \t -0.6407441038056153\n",
            "66     \t [7.18587785 1.24681412]. \t  -71.5054688868222 \t -0.6407441038056153\n",
            "67     \t [2.37092546 2.83696544]. \t  -378.6757268110483 \t -0.6407441038056153\n",
            "68     \t [2.31633054 0.33183228]. \t  -10.520040207276743 \t -0.6407441038056153\n",
            "69     \t [3.09460965 1.09727333]. \t  -5.330207074463902 \t -0.6407441038056153\n",
            "70     \t [-1.45956134 -0.46728669]. \t  -13.241160027846089 \t -0.6407441038056153\n",
            "71     \t [0.58710761 0.32434624]. \t  \u001b[92m-0.4542959206008764\u001b[0m \t -0.4542959206008764\n",
            "72     \t [-9.77025633 -0.07356597]. \t  -307.3374821557221 \t -0.4542959206008764\n",
            "73     \t [ 1.31820967 -0.04303052]. \t  -3.5571116527695414 \t -0.4542959206008764\n",
            "74     \t [ 1.9347878  -7.36349954]. \t  -22688.55318229337 \t -0.4542959206008764\n",
            "75     \t [-5.54358842  5.51304305]. \t  -8842.3887107496 \t -0.4542959206008764\n",
            "76     \t [7.78650154 3.89916615]. \t  -1069.4298990570135 \t -0.4542959206008764\n",
            "77     \t [4.66219434 9.80236653]. \t  -70333.85057301067 \t -0.4542959206008764\n",
            "78     \t [7.41697042 2.43771962]. \t  -81.10326257400561 \t -0.4542959206008764\n",
            "79     \t [-2.66403716 -9.86890604]. \t  -77990.1026981319 \t -0.4542959206008764\n",
            "80     \t [-0.70520444  6.38911094]. \t  -13564.859667105267 \t -0.4542959206008764\n",
            "81     \t [5.70377261 4.76196462]. \t  -3166.186738470946 \t -0.4542959206008764\n",
            "82     \t [-8.11969226  2.93973397]. \t  -1373.8712307481312 \t -0.4542959206008764\n",
            "83     \t [ 0.69209831 -0.35324136]. \t  -0.4864856806988134 \t -0.4542959206008764\n",
            "84     \t [ 0.76886647 -0.16580291]. \t  -1.072687043986526 \t -0.4542959206008764\n",
            "85     \t [6.59465701 1.87785881]. \t  -31.719807380703 \t -0.4542959206008764\n",
            "86     \t [-7.47026008  0.3508535 ]. \t  -190.8327059739325 \t -0.4542959206008764\n",
            "87     \t [8.90445694 8.69235288]. \t  -40509.58705670207 \t -0.4542959206008764\n",
            "88     \t [-3.62428043  3.07093999]. \t  -1032.5906525321786 \t -0.4542959206008764\n",
            "89     \t [-1.86605941  1.36311135]. \t  -70.53631045718335 \t -0.4542959206008764\n",
            "90     \t [-3.11039594  0.94287514]. \t  -64.68871383509138 \t -0.4542959206008764\n",
            "91     \t [-8.21078768 -9.43408712]. \t  -69436.73252782355 \t -0.4542959206008764\n",
            "92     \t [0.93235309 0.50392032]. \t  \u001b[92m-0.36494554574717264\u001b[0m \t -0.36494554574717264\n",
            "93     \t [0.40793521 0.55139638]. \t  -0.43065334131480626 \t -0.36494554574717264\n",
            "94     \t [-0.05444533  5.13783057]. \t  -5587.156302415063 \t -0.36494554574717264\n",
            "95     \t [6.57010489 8.77670648]. \t  -43538.24471272127 \t -0.36494554574717264\n",
            "96     \t [-8.81909582 -6.38636263]. \t  -16437.245746249155 \t -0.36494554574717264\n",
            "97     \t [-7.38079581 -5.17206868]. \t  -7483.323761270523 \t -0.36494554574717264\n",
            "98     \t [ 1.30913725 -0.76022489]. \t  \u001b[92m-0.1425391002847861\u001b[0m \t -0.1425391002847861\n",
            "99     \t [-3.06810703  6.60137068]. \t  -16297.396964435744 \t -0.1425391002847861\n",
            "100    \t [-7.12382562 -8.06620731]. \t  -37741.79479062148 \t -0.1425391002847861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "6cab8130-3ba1-4ab6-c71c-514244fc8f9b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.9467253  -3.95286781]. \t  -2981.1801714787616 \t -297.17705346549894\n",
            "init   \t [-8.75927171 -0.80279317]. \t  -297.17705346549894 \t -297.17705346549894\n",
            "init   \t [6.70506769 8.53994095]. \t  -38761.39667089972 \t -297.17705346549894\n",
            "init   \t [4.53977969 5.36992444]. \t  -5658.63331787327 \t -297.17705346549894\n",
            "init   \t [-4.61589867  2.88058584]. \t  -931.3893916140455 \t -297.17705346549894\n",
            "1      \t [ 2.44677649 -4.93762112]. \t  -4291.960403637314 \t -297.17705346549894\n",
            "2      \t [-8.63046085  9.55793493]. \t  -73313.74290460398 \t -297.17705346549894\n",
            "3      \t [ 9.83790456 -9.57130198]. \t  -60200.563023412855 \t -297.17705346549894\n",
            "4      \t [-2.47559954 -9.95929554]. \t  -80694.11504672318 \t -297.17705346549894\n",
            "5      \t [ 9.0936247  -0.78669084]. \t  \u001b[92m-188.93582542124247\u001b[0m \t -188.93582542124247\n",
            "6      \t [-10. -10.]. \t  -88321.0 \t -188.93582542124247\n",
            "7      \t [0.48374205 0.70472029]. \t  \u001b[92m-0.7857421467280545\u001b[0m \t -0.7857421467280545\n",
            "8      \t [-1.0244395   6.99962392]. \t  -19611.60688532351 \t -0.7857421467280545\n",
            "9      \t [-2.07414676 -1.70219855]. \t  -133.29605489623904 \t -0.7857421467280545\n",
            "10     \t [3.05469412 0.30873471]. \t  -20.62744595774866 \t -0.7857421467280545\n",
            "11     \t [-10.           3.12855034]. \t  -1870.4386767115468 \t -0.7857421467280545\n",
            "12     \t [ 7.07786567 -4.73559299]. \t  -2890.662850586515 \t -0.7857421467280545\n",
            "13     \t [  3.18463391 -10.        ]. \t  -77477.3492872562 \t -0.7857421467280545\n",
            "14     \t [9.31931645 4.02601834]. \t  -1136.276803741466 \t -0.7857421467280545\n",
            "15     \t [5.83730126 0.68833772]. \t  -71.2174953599234 \t -0.7857421467280545\n",
            "16     \t [0.29152787 1.49337249]. \t  -35.25963503432326 \t -0.7857421467280545\n",
            "17     \t [0.30444579 0.07320479]. \t  \u001b[92m-0.6563478242332337\u001b[0m \t -0.6563478242332337\n",
            "18     \t [1.82864427 1.32465555]. \t  -6.336696611999095 \t -0.6563478242332337\n",
            "19     \t [ 0.75624065 -0.14979814]. \t  -1.0714895083492593 \t -0.6563478242332337\n",
            "20     \t [-2.95022289 -5.17322093]. \t  -6394.377021036696 \t -0.6563478242332337\n",
            "21     \t [-6.22633622  5.75282287]. \t  -10540.454617234245 \t -0.6563478242332337\n",
            "22     \t [-5.60280791 -0.25058201]. \t  -109.22598995052391 \t -0.6563478242332337\n",
            "23     \t [0.50977244 0.19575997]. \t  \u001b[92m-0.6155236750725124\u001b[0m \t -0.6155236750725124\n",
            "24     \t [ 0.57576004 -1.11450562]. \t  -7.464613400131354 \t -0.6155236750725124\n",
            "25     \t [-1.02674408  0.52359176]. \t  -9.069198213110887 \t -0.6155236750725124\n",
            "26     \t [ 5.05441017 -0.86454058]. \t  -41.779024573082665 \t -0.6155236750725124\n",
            "27     \t [ 1.24717153 -0.63940596]. \t  \u001b[92m-0.4300197657140152\u001b[0m \t -0.4300197657140152\n",
            "28     \t [1.32147502 0.5110621 ]. \t  -1.3804872619243285 \t -0.4300197657140152\n",
            "29     \t [2.21916102 9.71346211]. \t  -69553.70128712986 \t -0.4300197657140152\n",
            "30     \t [ 3.35635613 -0.94804479]. \t  -10.411993704469197 \t -0.4300197657140152\n",
            "31     \t [-9.74444092 -5.531269  ]. \t  -10178.79694441008 \t -0.4300197657140152\n",
            "32     \t [ 0.0161191  -0.11549953]. \t  -0.968244709594219 \t -0.4300197657140152\n",
            "33     \t [ 2.18378448 -1.06049389]. \t  -1.4099288405088621 \t -0.4300197657140152\n",
            "34     \t [ 2.39509947 -1.37738335]. \t  -5.8622174390034285 \t -0.4300197657140152\n",
            "35     \t [ 1.33632763 -0.07436351]. \t  -3.625785579881615 \t -0.4300197657140152\n",
            "36     \t [ 1.06265215 -0.75733626]. \t  \u001b[92m-0.018193713930163783\u001b[0m \t -0.018193713930163783\n",
            "37     \t [-4.43300249  9.32219148]. \t  -63568.16363615236 \t -0.018193713930163783\n",
            "38     \t [2.67933709 1.99879666]. \t  -59.234445819995145 \t -0.018193713930163783\n",
            "39     \t [7.09843495 2.26196132]. \t  -56.841127520184145 \t -0.018193713930163783\n",
            "40     \t [0.92306378 1.08478666]. \t  -4.09835311959922 \t -0.018193713930163783\n",
            "41     \t [-1.46902068  2.63930342]. \t  -480.4693947990232 \t -0.018193713930163783\n",
            "42     \t [ 1.1990243  -0.85440721]. \t  -0.17585170166126357 \t -0.018193713930163783\n",
            "43     \t [-0.59975546 -0.148982  ]. \t  -3.389067531322259 \t -0.018193713930163783\n",
            "44     \t [ 6.88140161 -1.16772672]. \t  -69.10614249137741 \t -0.018193713930163783\n",
            "45     \t [1.9295137  0.81538814]. \t  -1.5835111228497787 \t -0.018193713930163783\n",
            "46     \t [-6.14770153 -8.23941711]. \t  -40335.7572759959 \t -0.018193713930163783\n",
            "47     \t [ 4.09436519 -1.66270756]. \t  -13.692556912772787 \t -0.018193713930163783\n",
            "48     \t [ 6.26972533 -8.79393908]. \t  -44071.11153816772 \t -0.018193713930163783\n",
            "49     \t [-2.2996865   0.23662201]. \t  -22.520201134909605 \t -0.018193713930163783\n",
            "50     \t [-4.00980044 -1.47901875]. \t  -165.70762062066325 \t -0.018193713930163783\n",
            "51     \t [0.09078931 0.49741316]. \t  -1.1531775245016371 \t -0.018193713930163783\n",
            "52     \t [ 0.70456914 -0.49721143]. \t  -0.17558924019398864 \t -0.018193713930163783\n",
            "53     \t [1.23882389 2.85155616]. \t  -451.4934511663499 \t -0.018193713930163783\n",
            "54     \t [ 9.53331578 -7.25925807]. \t  -18451.22689564219 \t -0.018193713930163783\n",
            "55     \t [1.94422577 1.03734043]. \t  -0.9780275703246446 \t -0.018193713930163783\n",
            "56     \t [1.75642414 0.78885332]. \t  -1.0961481144252332 \t -0.018193713930163783\n",
            "57     \t [-2.28693964  0.27262581]. \t  -22.668162728585386 \t -0.018193713930163783\n",
            "58     \t [ 4.86078149 -5.17912561]. \t  -4775.03424588411 \t -0.018193713930163783\n",
            "59     \t [ 2.19640599 -0.3680336 ]. \t  -8.84655348799142 \t -0.018193713930163783\n",
            "60     \t [-2.37426724  7.82643393]. \t  -31201.6159004122 \t -0.018193713930163783\n",
            "61     \t [ 8.3389707  -1.14735903]. \t  -118.9797646176284 \t -0.018193713930163783\n",
            "62     \t [ 9.77604093 -2.69274201]. \t  -121.68296222185278 \t -0.018193713930163783\n",
            "63     \t [ 9.65168305 -3.44245725]. \t  -469.61957102685653 \t -0.018193713930163783\n",
            "64     \t [-7.28602508  1.04653499]. \t  -248.26616680129047 \t -0.018193713930163783\n",
            "65     \t [-7.72097818  1.7868922 ]. \t  -474.06729302232503 \t -0.018193713930163783\n",
            "66     \t [-7.35504328 -2.27957323]. \t  -699.7863099239688 \t -0.018193713930163783\n",
            "67     \t [ 6.34112375 -3.83553244]. \t  -1094.038378129007 \t -0.018193713930163783\n",
            "68     \t [ 9.84758304 -2.01508323]. \t  -84.24106920945124 \t -0.018193713930163783\n",
            "69     \t [ 6.22759308 -2.85061059]. \t  -228.30365097801536 \t -0.018193713930163783\n",
            "70     \t [-0.54503271  0.17393669]. \t  -3.1204850221013887 \t -0.018193713930163783\n",
            "71     \t [7.5867087  0.21594955]. \t  -155.68802635951303 \t -0.018193713930163783\n",
            "72     \t [-0.60794976 -6.81148879]. \t  -17449.970281161743 \t -0.018193713930163783\n",
            "73     \t [-4.54631698  7.84714993]. \t  -32646.281973896577 \t -0.018193713930163783\n",
            "74     \t [ 3.82993686 -1.20880945]. \t  -9.655641684822255 \t -0.018193713930163783\n",
            "75     \t [-3.04732377 -3.62317595]. \t  -1733.6106727819033 \t -0.018193713930163783\n",
            "76     \t [ 8.99131684 -3.36503093]. \t  -436.8092084364122 \t -0.018193713930163783\n",
            "77     \t [-5.43939435 -1.20356317]. \t  -180.46102889329177 \t -0.018193713930163783\n",
            "78     \t [1.07329673 0.74406266]. \t  \u001b[92m-0.007679210655349976\u001b[0m \t -0.007679210655349976\n",
            "79     \t [ 3.50269434 -1.48339176]. \t  -7.877033812407978 \t -0.007679210655349976\n",
            "80     \t [ 0.51543473 -0.48765227]. \t  -0.23797560073195387 \t -0.007679210655349976\n",
            "81     \t [0.99500481 3.75849543]. \t  -1485.9503694431025 \t -0.007679210655349976\n",
            "82     \t [-6.86179769 -6.00034947]. \t  -12502.820114758297 \t -0.007679210655349976\n",
            "83     \t [3.127693   1.03778417]. \t  -6.423264896797338 \t -0.007679210655349976\n",
            "84     \t [ 5.39378092 -7.0924974 ]. \t  -18150.434280823014 \t -0.007679210655349976\n",
            "85     \t [-1.65286483  0.17285563]. \t  -12.903846651642251 \t -0.007679210655349976\n",
            "86     \t [-5.64153068 -1.58224571]. \t  -270.8924671331364 \t -0.007679210655349976\n",
            "87     \t [3.87232763 1.02597281]. \t  -14.495460383520177 \t -0.007679210655349976\n",
            "88     \t [-1.65719838  1.17430137]. \t  -46.04808128627561 \t -0.007679210655349976\n",
            "89     \t [6.63753775 3.99835237]. \t  -1315.6183299249508 \t -0.007679210655349976\n",
            "90     \t [-9.46301857  0.66459733]. \t  -323.5706524987455 \t -0.007679210655349976\n",
            "91     \t [ 4.74676856 -2.01197137]. \t  -36.473748104095634 \t -0.007679210655349976\n",
            "92     \t [ 0.71210475 -0.51965567]. \t  -0.14206593561364175 \t -0.007679210655349976\n",
            "93     \t [7.5759217  2.09830419]. \t  -46.26775520229464 \t -0.007679210655349976\n",
            "94     \t [ 0.75676517 -4.31883246]. \t  -2671.551655887892 \t -0.007679210655349976\n",
            "95     \t [-2.55040769 -1.69504259]. \t  -150.27739847390453 \t -0.007679210655349976\n",
            "96     \t [7.12134543 2.06780058]. \t  -41.562117675120604 \t -0.007679210655349976\n",
            "97     \t [ 1.37175544 -1.3771187 ]. \t  -11.862198666920797 \t -0.007679210655349976\n",
            "98     \t [ 5.61987587 -7.25406049]. \t  -19870.792210242922 \t -0.007679210655349976\n",
            "99     \t [-6.33179118  5.3289402 ]. \t  -8023.791023469346 \t -0.007679210655349976\n",
            "100    \t [-7.27090799  3.58797785]. \t  -2248.7948831675008 \t -0.007679210655349976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "2aaeee54-d076-48a0-fbe5-f1ea1c2a885e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-7.8100025  -9.15569971]. \t  -61652.61989589963 \t -123.0699213709516\n",
            "init   \t [ 1.9938517 -4.0025859]. \t  -1806.696586806874 \t -123.0699213709516\n",
            "init   \t [ 5.93223564 -2.6760775 ]. \t  -165.12946982275756 \t -123.0699213709516\n",
            "init   \t [ 7.61216688 -4.04316733]. \t  -1301.9580077172761 \t -123.0699213709516\n",
            "init   \t [-3.41793472  1.37433832]. \t  -123.0699213709516 \t -123.0699213709516\n",
            "1      \t [6.56161104 9.13365295]. \t  -51413.89230031877 \t -123.0699213709516\n",
            "2      \t [-9.10684612  8.65627415]. \t  -50644.440793557515 \t -123.0699213709516\n",
            "3      \t [-10.         -1.0665418]. \t  -422.3523707348583 \t -123.0699213709516\n",
            "4      \t [-0.33239204  7.74857709]. \t  -29000.49408219279 \t -123.0699213709516\n",
            "5      \t [  4.82194421 -10.        ]. \t  -76203.55418218843 \t -123.0699213709516\n",
            "6      \t [4.00600666 2.63786598]. \t  -205.47872409303372 \t -123.0699213709516\n",
            "7      \t [9.9050887  3.11370436]. \t  -259.2394377182353 \t -123.0699213709516\n",
            "8      \t [-3.84313377 -4.115115  ]. \t  -2867.766906413125 \t -123.0699213709516\n",
            "9      \t [-1.55087801 -9.50196081]. \t  -66345.82717309322 \t -123.0699213709516\n",
            "10     \t [-7.64821324  3.1348157 ]. \t  -1565.6284617538104 \t -123.0699213709516\n",
            "11     \t [-0.09115134  0.42203595]. \t  \u001b[92m-1.590908999091245\u001b[0m \t -1.590908999091245\n",
            "12     \t [-0.27471837  2.92275501]. \t  -604.3441107345412 \t -1.590908999091245\n",
            "13     \t [-0.16616717 -1.14807649]. \t  -17.066012420053486 \t -1.590908999091245\n",
            "14     \t [ 2.33873718 -0.2520255 ]. \t  -11.575481765529673 \t -1.590908999091245\n",
            "15     \t [ 9.84333107 -8.2419934 ]. \t  -31839.06504341117 \t -1.590908999091245\n",
            "16     \t [ 0.47114081 -0.40063569]. \t  \u001b[92m-0.32476581065786236\u001b[0m \t -0.32476581065786236\n",
            "17     \t [7.71936416 0.13888266]. \t  -163.13884431893757 \t -0.32476581065786236\n",
            "18     \t [-0.25793126 -0.22061096]. \t  -1.8348241205470743 \t -0.32476581065786236\n",
            "19     \t [-3.96485885  4.77327805]. \t  -4931.730670201599 \t -0.32476581065786236\n",
            "20     \t [-10.          -5.09747177]. \t  -7801.174337940759 \t -0.32476581065786236\n",
            "21     \t [9.96547899 6.16124452]. \t  -8780.870351741778 \t -0.32476581065786236\n",
            "22     \t [-5.9364371  -0.84115811]. \t  -156.20417713112454 \t -0.32476581065786236\n",
            "23     \t [6.42256851 3.78006133]. \t  -1011.1063774843094 \t -0.32476581065786236\n",
            "24     \t [4.60808273 0.40159656]. \t  -49.749678074431955 \t -0.32476581065786236\n",
            "25     \t [-4.20138848  9.31820197]. \t  -63294.8191254469 \t -0.32476581065786236\n",
            "26     \t [-0.4692321   0.01873558]. \t  -2.600319184299668 \t -0.32476581065786236\n",
            "27     \t [ 0.24332737 -0.26341393]. \t  -0.5944163631245485 \t -0.32476581065786236\n",
            "28     \t [0.6361033  0.37474443]. \t  -0.38480679582271965 \t -0.32476581065786236\n",
            "29     \t [ 1.07210502 -0.40706938]. \t  -1.1024545026340804 \t -0.32476581065786236\n",
            "30     \t [3.80192223 5.30372313]. \t  -5511.332008917288 \t -0.32476581065786236\n",
            "31     \t [1.28727522 0.13986241]. \t  -3.1982950231029417 \t -0.32476581065786236\n",
            "32     \t [2.0374998  1.40578502]. \t  -8.410574465691145 \t -0.32476581065786236\n",
            "33     \t [ 4.99079925 -5.77435322]. \t  -7628.598605526773 \t -0.32476581065786236\n",
            "34     \t [1.15585495 0.80508808]. \t  \u001b[92m-0.06375929072157648\u001b[0m \t -0.06375929072157648\n",
            "35     \t [ 9.51741271 -0.14869969]. \t  -252.02895826208731 \t -0.06375929072157648\n",
            "36     \t [-0.11224753  0.1229503 ]. \t  -1.277696284017129 \t -0.06375929072157648\n",
            "37     \t [-1.75997782  0.11864031]. \t  -14.012287256721972 \t -0.06375929072157648\n",
            "38     \t [1.09892792 0.68534179]. \t  \u001b[92m-0.06069351798149166\u001b[0m \t -0.06069351798149166\n",
            "39     \t [-1.82314968 -0.58008768]. \t  -20.431734797124786 \t -0.06069351798149166\n",
            "40     \t [ 5.74898858 -1.2505808 ]. \t  -36.29305403216088 \t -0.06069351798149166\n",
            "41     \t [ 4.72147872 -1.72425438]. \t  -16.84882935049343 \t -0.06069351798149166\n",
            "42     \t [ 3.13745005 -0.87479751]. \t  -9.733003784786302 \t -0.06069351798149166\n",
            "43     \t [ 0.93529965 -0.25051774]. \t  -1.3156778895830032 \t -0.06069351798149166\n",
            "44     \t [0.26388089 0.44404726]. \t  -0.5759188141122675 \t -0.06069351798149166\n",
            "45     \t [ 3.90502091 -1.15095709]. \t  -11.592292007673878 \t -0.06069351798149166\n",
            "46     \t [-5.2530543   9.29050083]. \t  -63321.594022155434 \t -0.06069351798149166\n",
            "47     \t [ 0.33371736 -0.56558523]. \t  -0.631273038861805 \t -0.06069351798149166\n",
            "48     \t [-6.78208322 -2.61962127]. \t  -901.6278942682517 \t -0.06069351798149166\n",
            "49     \t [4.19396467 1.66803015]. \t  -13.958962388530542 \t -0.06069351798149166\n",
            "50     \t [-3.23428689 -0.57425663]. \t  -48.25298214641066 \t -0.06069351798149166\n",
            "51     \t [5.06834609 5.96825386]. \t  -8773.955635900966 \t -0.06069351798149166\n",
            "52     \t [-5.70811187  0.43499748]. \t  -119.09112874880427 \t -0.06069351798149166\n",
            "53     \t [1.00927738 0.78270075]. \t  -0.09336657669638086 \t -0.06069351798149166\n",
            "54     \t [-5.76574288  8.570498  ]. \t  -46663.62987881991 \t -0.06069351798149166\n",
            "55     \t [ 6.13223941 -8.32988771]. \t  -35214.04422492559 \t -0.06069351798149166\n",
            "56     \t [-6.13893734 -1.59182807]. \t  -302.14783823643666 \t -0.06069351798149166\n",
            "57     \t [3.13283848 1.26747228]. \t  -4.5618427417618035 \t -0.06069351798149166\n",
            "58     \t [4.30514143 8.85815479]. \t  -46601.94656579291 \t -0.06069351798149166\n",
            "59     \t [-6.16955622  0.44482737]. \t  -137.60883826786048 \t -0.06069351798149166\n",
            "60     \t [-5.26021872  0.57031597]. \t  -109.06401744077854 \t -0.06069351798149166\n",
            "61     \t [-1.31521669 -1.20963095]. \t  -41.343091088664025 \t -0.06069351798149166\n",
            "62     \t [-7.01656824  4.39875358]. \t  -4243.920950143667 \t -0.06069351798149166\n",
            "63     \t [4.34856894 1.2782591 ]. \t  -13.548636417950672 \t -0.06069351798149166\n",
            "64     \t [ 7.72355912 -9.4819064 ]. \t  -59274.82494528694 \t -0.06069351798149166\n",
            "65     \t [-9.88556797  1.86668964]. \t  -686.6525866765733 \t -0.06069351798149166\n",
            "66     \t [-9.87418281 -2.53173674]. \t  -1148.243606507633 \t -0.06069351798149166\n",
            "67     \t [3.31352881 1.53087528]. \t  -9.126131078748006 \t -0.06069351798149166\n",
            "68     \t [-5.39998576 -6.84540931]. \t  -19690.208465023108 \t -0.06069351798149166\n",
            "69     \t [4.31523028 4.55266855]. \t  -2769.505070887002 \t -0.06069351798149166\n",
            "70     \t [ 5.14449148 -0.25238526]. \t  -67.51929105981897 \t -0.06069351798149166\n",
            "71     \t [ 9.42253238 -0.3995322 ]. \t  -236.67848034395837 \t -0.06069351798149166\n",
            "72     \t [-2.21950213  4.78261049]. \t  -4611.883322732063 \t -0.06069351798149166\n",
            "73     \t [ 3.84551887 -0.07237014]. \t  -37.5121025003141 \t -0.06069351798149166\n",
            "74     \t [-8.84729879 -5.24014396]. \t  -8229.051424410012 \t -0.06069351798149166\n",
            "75     \t [3.885948   1.31961911]. \t  -8.653769855358489 \t -0.06069351798149166\n",
            "76     \t [ 8.86426913 -4.30594698]. \t  -1654.3678980790066 \t -0.06069351798149166\n",
            "77     \t [-5.91462627e+00 -2.10471570e-03]. \t  -117.77787374168585 \t -0.06069351798149166\n",
            "78     \t [ 9.87354977 -8.45660868]. \t  -35539.20050149259 \t -0.06069351798149166\n",
            "79     \t [-0.61430943 -9.6560787 ]. \t  -70010.95138703822 \t -0.06069351798149166\n",
            "80     \t [ 2.1454274  -0.78001985]. \t  -3.0364716378073604 \t -0.06069351798149166\n",
            "81     \t [1.13377697 0.69766944]. \t  -0.06928311330863457 \t -0.06069351798149166\n",
            "82     \t [-5.28630706  7.33387952]. \t  -25513.321947840155 \t -0.06069351798149166\n",
            "83     \t [ 7.09723595 -3.04258965]. \t  -297.89342045477264 \t -0.06069351798149166\n",
            "84     \t [ 7.13666322 -1.84147436]. \t  -37.910128600593815 \t -0.06069351798149166\n",
            "85     \t [-1.53502454 -5.06789235]. \t  -5603.688678125964 \t -0.06069351798149166\n",
            "86     \t [ 0.93232909 -0.2382274 ]. \t  -1.3455264914714893 \t -0.06069351798149166\n",
            "87     \t [1.27632162 2.44985546]. \t  -230.22465187294836 \t -0.06069351798149166\n",
            "88     \t [1.96352415 0.98137928]. \t  -0.9311633886251407 \t -0.06069351798149166\n",
            "89     \t [ 1.52017907 -1.78452586]. \t  -47.29397676328524 \t -0.06069351798149166\n",
            "90     \t [ 0.66501211 -3.19880746]. \t  -784.1705345699172 \t -0.06069351798149166\n",
            "91     \t [-6.54058379  8.44174   ]. \t  -44498.522389758466 \t -0.06069351798149166\n",
            "92     \t [-0.39591866  9.01209183]. \t  -53030.154493987946 \t -0.06069351798149166\n",
            "93     \t [ 2.97050752 -1.4050746 ]. \t  -5.795718183824131 \t -0.06069351798149166\n",
            "94     \t [ 4.0504259  -1.84565091]. \t  -24.567122228735258 \t -0.06069351798149166\n",
            "95     \t [-4.18805224  5.0527217 ]. \t  -6131.608945360351 \t -0.06069351798149166\n",
            "96     \t [-9.842156   -3.27093426]. \t  -2069.4497773451208 \t -0.06069351798149166\n",
            "97     \t [ 9.86423471 -7.70892022]. \t  -23836.50570445349 \t -0.06069351798149166\n",
            "98     \t [ 2.08086811 -1.21197818]. \t  -2.636879467086902 \t -0.06069351798149166\n",
            "99     \t [-0.2734736  -5.50592384]. \t  -7420.184072100605 \t -0.06069351798149166\n",
            "100    \t [ 0.84601829 -1.36380733]. \t  -16.542572189622856 \t -0.06069351798149166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "3917ecdd-5b88-4744-bbbf-8dc05fbbb0b6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.44557117 -0.23843202]. \t  -28.18355402366269 \t -28.18355402366269\n",
            "init   \t [ 6.50990348 -9.37107225]. \t  -57236.27240949214 \t -28.18355402366269\n",
            "init   \t [6.16099927 1.31234839]. \t  -41.39446954533859 \t -28.18355402366269\n",
            "init   \t [-4.04755003 -9.06608559]. \t  -56766.42271365435 \t -28.18355402366269\n",
            "init   \t [ 9.81254799 -9.86348534]. \t  -68353.23444364112 \t -28.18355402366269\n",
            "1      \t [-8.12773813  6.26616826]. \t  -15102.348898507695 \t -28.18355402366269\n",
            "2      \t [3.58702664 9.74831565]. \t  -69550.52203005194 \t -28.18355402366269\n",
            "3      \t [-9.43477705 -1.75785015]. \t  -596.5317353689182 \t -28.18355402366269\n",
            "4      \t [-2.2390099  -1.73641554]. \t  -147.25342571928326 \t -28.18355402366269\n",
            "5      \t [-1.00367067  4.27897404]. \t  -2834.9811598612127 \t -28.18355402366269\n",
            "6      \t [ 9.27370946 -3.20211659]. \t  -320.83245363494626 \t -28.18355402366269\n",
            "7      \t [8.89638698 5.527643  ]. \t  -5514.807675944536 \t -28.18355402366269\n",
            "8      \t [ 1.36987692 -5.71796218]. \t  -8197.335962250589 \t -28.18355402366269\n",
            "9      \t [-10.          -7.38196391]. \t  -28436.708713333977 \t -28.18355402366269\n",
            "10     \t [-2.75093026  9.62298821]. \t  -70668.03394031047 \t -28.18355402366269\n",
            "11     \t [-5.8147762   1.45564221]. \t  -248.54928705285988 \t -28.18355402366269\n",
            "12     \t [9.87513368 9.90474429]. \t  -69518.58177407873 \t -28.18355402366269\n",
            "13     \t [-6.02203414 -3.8627435 ]. \t  -2621.7058488784805 \t -28.18355402366269\n",
            "14     \t [ 5.31234748 -3.04115197]. \t  -366.2775739616652 \t -28.18355402366269\n",
            "15     \t [3.83695092 3.30455624]. \t  -656.281086102878 \t -28.18355402366269\n",
            "16     \t [ 0.52414869 -0.1072242 ]. \t  \u001b[92m-0.7287464016565732\u001b[0m \t -0.7287464016565732\n",
            "17     \t [ 0.83492738 -0.42282106]. \t  \u001b[92m-0.48301715749584134\u001b[0m \t -0.48301715749584134\n",
            "18     \t [9.31253201 0.86788036]. \t  -190.96856354894862 \t -0.48301715749584134\n",
            "19     \t [ 0.57664004 -0.89594019]. \t  -2.2960003794156894 \t -0.48301715749584134\n",
            "20     \t [-0.40773134  0.39363215]. \t  -3.0116755985543158 \t -0.48301715749584134\n",
            "21     \t [ 0.87987632 -0.0636695 ]. \t  -1.5343910723074392 \t -0.48301715749584134\n",
            "22     \t [ 0.44528415 -0.28429176]. \t  \u001b[92m-0.46861358509810086\u001b[0m \t -0.46861358509810086\n",
            "23     \t [ 2.01277923 -9.55754188]. \t  -65291.86440518609 \t -0.46861358509810086\n",
            "24     \t [-10.           1.88319662]. \t  -705.3316599057688 \t -0.46861358509810086\n",
            "25     \t [ 1.1465174  -1.52825582]. \t  -24.867278945912584 \t -0.46861358509810086\n",
            "26     \t [ 5.86890583 -0.40519695]. \t  -85.10133798247173 \t -0.46861358509810086\n",
            "27     \t [-2.64357647  0.85147194]. \t  -46.79053217013646 \t -0.46861358509810086\n",
            "28     \t [0.78674205 0.45759508]. \t  \u001b[92m-0.31626149785962837\u001b[0m \t -0.31626149785962837\n",
            "29     \t [-0.14794848  0.42494163]. \t  -1.8361498491509356 \t -0.31626149785962837\n",
            "30     \t [-2.40402673 -5.44219706]. \t  -7610.331000610567 \t -0.31626149785962837\n",
            "31     \t [1.91406395 0.53132709]. \t  -4.4775273112542715 \t -0.31626149785962837\n",
            "32     \t [0.38439037 0.83173806]. \t  -2.3757206652801393 \t -0.31626149785962837\n",
            "33     \t [-4.50809061  4.88518664]. \t  -5487.995544544278 \t -0.31626149785962837\n",
            "34     \t [ 7.57892659 -5.18912668]. \t  -4326.060330460954 \t -0.31626149785962837\n",
            "35     \t [1.97194873 5.95761444]. \t  -9526.916957846863 \t -0.31626149785962837\n",
            "36     \t [1.02523531 1.14361819]. \t  -5.0599526580928 \t -0.31626149785962837\n",
            "37     \t [7.28286031 1.91169004]. \t  -39.475712508551204 \t -0.31626149785962837\n",
            "38     \t [ 2.77461248 -1.02101885]. \t  -4.10049332447318 \t -0.31626149785962837\n",
            "39     \t [ 2.1479141  -0.49235396]. \t  -6.849438590481957 \t -0.31626149785962837\n",
            "40     \t [-4.3517255  -0.64133564]. \t  -82.18872652515466 \t -0.31626149785962837\n",
            "41     \t [-2.92168769 -5.05486212]. \t  -5852.770046987735 \t -0.31626149785962837\n",
            "42     \t [0.92075275 0.52342196]. \t  \u001b[92m-0.28425718885340884\u001b[0m \t -0.28425718885340884\n",
            "43     \t [-9.2651553   9.70867601]. \t  -78340.761065285 \t -0.28425718885340884\n",
            "44     \t [4.84022995 4.91379184]. \t  -3790.6341503370104 \t -0.28425718885340884\n",
            "45     \t [ 7.39616664 -0.73667976]. \t  -120.5626468185589 \t -0.28425718885340884\n",
            "46     \t [3.73054145 1.70938678]. \t  -16.38932404844521 \t -0.28425718885340884\n",
            "47     \t [4.28737905 1.12026767]. \t  -17.12501863145796 \t -0.28425718885340884\n",
            "48     \t [3.12787377 9.51196001]. \t  -63249.335349295434 \t -0.28425718885340884\n",
            "49     \t [0.98498249 0.79425871]. \t  \u001b[92m-0.15336383505731177\u001b[0m \t -0.15336383505731177\n",
            "50     \t [-0.61879016  0.70754953]. \t  -7.869559056713758 \t -0.15336383505731177\n",
            "51     \t [ 3.15553514 -1.82589276]. \t  -29.317901276308568 \t -0.15336383505731177\n",
            "52     \t [2.84967697 1.79962463]. \t  -29.740567598378856 \t -0.15336383505731177\n",
            "53     \t [-7.54571784  4.77818483]. \t  -5735.1731847284855 \t -0.15336383505731177\n",
            "54     \t [-5.06554453  3.76103289]. \t  -2262.073843733766 \t -0.15336383505731177\n",
            "55     \t [-8.55978671 -9.9309001 ]. \t  -84803.06583085285 \t -0.15336383505731177\n",
            "56     \t [ 7.98605608 -0.48736107]. \t  -161.63565597696189 \t -0.15336383505731177\n",
            "57     \t [-2.64801284  0.01406951]. \t  -27.33613532821893 \t -0.15336383505731177\n",
            "58     \t [-7.30446077 -4.45298308]. \t  -4479.931410554043 \t -0.15336383505731177\n",
            "59     \t [3.19764279 4.65963946]. \t  -3241.2264667529867 \t -0.15336383505731177\n",
            "60     \t [1.49521071 3.85263359]. \t  -1589.6380746331806 \t -0.15336383505731177\n",
            "61     \t [ 9.01563964 -8.25265235]. \t  -32422.354289695904 \t -0.15336383505731177\n",
            "62     \t [ 7.01098383 -5.27149603]. \t  -4753.513997951199 \t -0.15336383505731177\n",
            "63     \t [2.28876159 9.60659356]. \t  -66456.95186784181 \t -0.15336383505731177\n",
            "64     \t [ 1.35127257 -6.48666679]. \t  -13712.604773039777 \t -0.15336383505731177\n",
            "65     \t [7.82567942 8.46717967]. \t  -36799.93362102439 \t -0.15336383505731177\n",
            "66     \t [ 0.89919885 -2.90745419]. \t  -512.4826454303164 \t -0.15336383505731177\n",
            "67     \t [ 1.32890142 -1.14151099]. \t  -3.3706212636673056 \t -0.15336383505731177\n",
            "68     \t [-8.75020178 -5.68207233]. \t  -10847.325259422847 \t -0.15336383505731177\n",
            "69     \t [-2.72271128 -6.53641866]. \t  -15562.549628196019 \t -0.15336383505731177\n",
            "70     \t [ 2.75394985 -3.95416581]. \t  -1629.5040380870096 \t -0.15336383505731177\n",
            "71     \t [ 3.75929949 -1.62624911]. \t  -12.29597963833867 \t -0.15336383505731177\n",
            "72     \t [1.72739661 2.71616151]. \t  -339.96948038768585 \t -0.15336383505731177\n",
            "73     \t [1.79604589 0.32932879]. \t  -5.620998353343156 \t -0.15336383505731177\n",
            "74     \t [2.34143443 0.56188592]. \t  -7.647665782760731 \t -0.15336383505731177\n",
            "75     \t [ 0.67615036 -0.89514612]. \t  -1.8213969141604853 \t -0.15336383505731177\n",
            "76     \t [-4.31190326  4.05092865]. \t  -2785.779603738259 \t -0.15336383505731177\n",
            "77     \t [-3.45000029  2.43933765]. \t  -491.09273325550805 \t -0.15336383505731177\n",
            "78     \t [-3.40110708 -5.96485396]. \t  -11137.778595501333 \t -0.15336383505731177\n",
            "79     \t [-2.28071149 -6.41422133]. \t  -14313.303102479305 \t -0.15336383505731177\n",
            "80     \t [-0.61382331 -4.51937018]. \t  -3441.004449642181 \t -0.15336383505731177\n",
            "81     \t [ 1.02527191 -0.69346302]. \t  \u001b[92m-0.008700624276078569\u001b[0m \t -0.008700624276078569\n",
            "82     \t [-9.04863361  1.2583201 ]. \t  -399.40569125181196 \t -0.008700624276078569\n",
            "83     \t [ 2.08158224 -1.087064  ]. \t  -1.3286809932511714 \t -0.008700624276078569\n",
            "84     \t [-2.23416867  0.48900376]. \t  -25.174270082102176 \t -0.008700624276078569\n",
            "85     \t [-6.91943809 -0.22521392]. \t  -161.3030317594138 \t -0.008700624276078569\n",
            "86     \t [8.73188655 3.77820009]. \t  -845.2649612317308 \t -0.008700624276078569\n",
            "87     \t [8.49147411 2.59623271]. \t  -105.90989941353337 \t -0.008700624276078569\n",
            "88     \t [-6.99691523  3.70409887]. \t  -2435.848917227575 \t -0.008700624276078569\n",
            "89     \t [-4.54889154 -3.53520937]. \t  -1776.5217755378185 \t -0.008700624276078569\n",
            "90     \t [5.57518348 7.26651834]. \t  -20032.691716494577 \t -0.008700624276078569\n",
            "91     \t [ 3.02522329 -1.55940012]. \t  -10.859739127352245 \t -0.008700624276078569\n",
            "92     \t [-5.34625143 -7.38849223]. \t  -26272.63213176849 \t -0.008700624276078569\n",
            "93     \t [ 4.34379926 -2.21067563]. \t  -70.158921569656 \t -0.008700624276078569\n",
            "94     \t [-6.10772408 -4.44348195]. \t  -4208.653841123318 \t -0.008700624276078569\n",
            "95     \t [-6.31343574 -2.1723769 ]. \t  -549.7296951375912 \t -0.008700624276078569\n",
            "96     \t [ 9.69587589 -7.31644498]. \t  -19035.44955913957 \t -0.008700624276078569\n",
            "97     \t [-5.07031387 -4.65553508]. \t  -4725.5175973484165 \t -0.008700624276078569\n",
            "98     \t [-3.20025038  3.94954699]. \t  -2384.099734581828 \t -0.008700624276078569\n",
            "99     \t [-2.21002255  4.40918558]. \t  -3387.3857635547797 \t -0.008700624276078569\n",
            "100    \t [ 5.85237815 -0.64502844]. \t  -73.95148943738262 \t -0.008700624276078569\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "5e92894b-0408-49c7-b9e9-8279129c9a06"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.07179171 -7.69986114]. \t  -26686.60864226137 \t -217.28346804397978\n",
            "init   \t [ 9.00565729 -0.35617197]. \t  -217.28346804397978 \t -217.28346804397978\n",
            "init   \t [ 7.4494907  -5.75334638]. \t  -6945.3080829060855 \t -217.28346804397978\n",
            "init   \t [-9.1858075  -2.05611077]. \t  -726.1597680698058 \t -217.28346804397978\n",
            "init   \t [-5.33735605  6.83481449]. \t  -19549.897628464034 \t -217.28346804397978\n",
            "1      \t [7.70674409 9.05288798]. \t  -48843.620887164754 \t -217.28346804397978\n",
            "2      \t [-6.08246046 -9.42114812]. \t  -67466.99755823516 \t -217.28346804397978\n",
            "3      \t [1.36988239 1.21701407]. \t  \u001b[92m-5.2080597150791546\u001b[0m \t -5.2080597150791546\n",
            "4      \t [-2.33103765 -3.01515082]. \t  -852.6873063346333 \t -5.2080597150791546\n",
            "5      \t [1.36021695 6.56398187]. \t  -14386.11050211786 \t -5.2080597150791546\n",
            "6      \t [-5.3168764   1.14656483]. \t  -166.1838817179945 \t -5.2080597150791546\n",
            "7      \t [ 3.90189504 -1.60420965]. \t  -11.521453948710668 \t -5.2080597150791546\n",
            "8      \t [-10.           3.27383229]. \t  -2097.4386242382016 \t -5.2080597150791546\n",
            "9      \t [7.29815912 3.47480791]. \t  -607.5401820018474 \t -5.2080597150791546\n",
            "10     \t [-1.2556756   0.76701744]. \t  -16.92030816742675 \t -5.2080597150791546\n",
            "11     \t [-9.94883988 -6.38730751]. \t  -16880.570769860402 \t -5.2080597150791546\n",
            "12     \t [3.87774235 1.26117693]. \t  -9.251926078417746 \t -5.2080597150791546\n",
            "13     \t [ 1.35185837 -1.41277265]. \t  -14.062948909902332 \t -5.2080597150791546\n",
            "14     \t [  7.66990468 -10.        ]. \t  -74026.21876210123 \t -5.2080597150791546\n",
            "15     \t [-1.33825741 -9.59555515]. \t  -68816.77771125651 \t -5.2080597150791546\n",
            "16     \t [-1.95131758  3.14450512]. \t  -952.847714293763 \t -5.2080597150791546\n",
            "17     \t [-0.8500533  0.0424886]. \t  \u001b[92m-4.880181192490879\u001b[0m \t -4.880181192490879\n",
            "18     \t [ 5.79331481 -1.07432324]. \t  -47.26595410056805 \t -4.880181192490879\n",
            "19     \t [ 3.00182226 -3.22102733]. \t  -634.0053376048869 \t -4.880181192490879\n",
            "20     \t [-6.03676686 -3.17558662]. \t  -1422.9691551631163 \t -4.880181192490879\n",
            "21     \t [ 3.68008862 -0.36853836]. \t  -30.417904811719662 \t -4.880181192490879\n",
            "22     \t [-0.90225761  0.308671  ]. \t  -6.007065317273369 \t -4.880181192490879\n",
            "23     \t [ 0.40289532 -0.96482385]. \t  \u001b[92m-4.613165339935522\u001b[0m \t -4.613165339935522\n",
            "24     \t [-1.80921015  0.10306691]. \t  -14.592798035572091 \t -4.613165339935522\n",
            "25     \t [2.52041342 1.9080957 ]. \t  -47.65056393839543 \t -4.613165339935522\n",
            "26     \t [4.46512411 1.53813952]. \t  -12.149259973235344 \t -4.613165339935522\n",
            "27     \t [ 4.07592139 -1.60650369]. \t  -11.819158409676309 \t -4.613165339935522\n",
            "28     \t [0.92833066 0.03662026]. \t  \u001b[92m-1.7187870647584766\u001b[0m \t -1.7187870647584766\n",
            "29     \t [-1.71968977  9.80761426]. \t  -75355.68505306277 \t -1.7187870647584766\n",
            "30     \t [ 2.51777832 -0.81385429]. \t  -5.150438747980516 \t -1.7187870647584766\n",
            "31     \t [0.94153694 0.95380297]. \t  \u001b[92m-1.5449866484431458\u001b[0m \t -1.5449866484431458\n",
            "32     \t [1.2071865  0.42740202]. \t  \u001b[92m-1.460320558629877\u001b[0m \t -1.460320558629877\n",
            "33     \t [ 2.96634672 -0.98978405]. \t  -5.894624630680301 \t -1.460320558629877\n",
            "34     \t [-10.           7.79289985]. \t  -34683.71448893819 \t -1.460320558629877\n",
            "35     \t [-0.50780535 -0.26290934]. \t  -3.1082329756069855 \t -1.460320558629877\n",
            "36     \t [1.02876708 1.06533766]. \t  -3.0815931333688757 \t -1.460320558629877\n",
            "37     \t [ 2.06268737 -0.83742815]. \t  -2.0008095785834388 \t -1.460320558629877\n",
            "38     \t [-0.21437476  0.08112315]. \t  -1.578251946777794 \t -1.460320558629877\n",
            "39     \t [ 1.40245179 -0.45698773]. \t  -2.101535850085867 \t -1.460320558629877\n",
            "40     \t [0.19785985 0.56419291]. \t  \u001b[92m-1.028462542336865\u001b[0m \t -1.028462542336865\n",
            "41     \t [ 9.8356841  -3.07346653]. \t  -242.11726431745427 \t -1.028462542336865\n",
            "42     \t [0.25932599 0.20052508]. \t  \u001b[92m-0.6126122540927241\u001b[0m \t -0.6126122540927241\n",
            "43     \t [3.56556248 9.60408146]. \t  -65464.30595808899 \t -0.6126122540927241\n",
            "44     \t [9.68386147 4.7620457 ]. \t  -2620.1496214916774 \t -0.6126122540927241\n",
            "45     \t [5.14224547 5.09010154]. \t  -4374.4602398029 \t -0.6126122540927241\n",
            "46     \t [-7.34838001  0.3677042 ]. \t  -185.7874556022051 \t -0.6126122540927241\n",
            "47     \t [0.51247083 0.05040804]. \t  -0.7525716583239963 \t -0.6126122540927241\n",
            "48     \t [ 0.98039469 -0.74961879]. \t  \u001b[92m-0.041547044413976206\u001b[0m \t -0.041547044413976206\n",
            "49     \t [4.62838795 0.96614098]. \t  -28.41730768155222 \t -0.041547044413976206\n",
            "50     \t [5.48219774 3.88996803]. \t  -1248.3360607294771 \t -0.041547044413976206\n",
            "51     \t [4.99274959 2.7724122 ]. \t  -231.4220996091821 \t -0.041547044413976206\n",
            "52     \t [ 6.8336958  -0.56445403]. \t  -110.82471313913433 \t -0.041547044413976206\n",
            "53     \t [-4.20074676 -5.40453848]. \t  -7869.280770844423 \t -0.041547044413976206\n",
            "54     \t [ 0.64488077 -4.39600998]. \t  -2888.8750128842175 \t -0.041547044413976206\n",
            "55     \t [-7.30615915  7.14135777]. \t  -23963.782673163754 \t -0.041547044413976206\n",
            "56     \t [-3.50399327  0.63833839]. \t  -57.5925284229707 \t -0.041547044413976206\n",
            "57     \t [ 6.76050866 -2.05206792]. \t  -38.70433769107767 \t -0.041547044413976206\n",
            "58     \t [-4.24352113  0.81128115]. \t  -89.31894166189932 \t -0.041547044413976206\n",
            "59     \t [6.41689229 9.67624699]. \t  -65437.44856780594 \t -0.041547044413976206\n",
            "60     \t [ 9.55393667 -6.86338487]. \t  -14407.180303689185 \t -0.041547044413976206\n",
            "61     \t [ 1.17713286 -0.91698323]. \t  -0.5405853280566585 \t -0.041547044413976206\n",
            "62     \t [ 8.96078027 -0.3822016 ]. \t  -213.66411157121183 \t -0.041547044413976206\n",
            "63     \t [-8.87812201  6.34310448]. \t  -16063.732834410694 \t -0.041547044413976206\n",
            "64     \t [4.31708402 1.28301872]. \t  -13.103517275827803 \t -0.041547044413976206\n",
            "65     \t [3.61861458 1.533298  ]. \t  -9.204614157764116 \t -0.041547044413976206\n",
            "66     \t [ 3.5869595 -1.1267761]. \t  -8.887755028921921 \t -0.041547044413976206\n",
            "67     \t [ 1.36710345 -0.84054689]. \t  -0.13898493565453612 \t -0.041547044413976206\n",
            "68     \t [-1.70729211 -9.56820662]. \t  -68315.6542930769 \t -0.041547044413976206\n",
            "69     \t [5.76529991 0.76276297]. \t  -65.05909677248964 \t -0.041547044413976206\n",
            "70     \t [ 0.23797604 -0.43970884]. \t  -0.6249108354130698 \t -0.041547044413976206\n",
            "71     \t [2.67886227 0.76802552]. \t  -7.313395276669981 \t -0.041547044413976206\n",
            "72     \t [ 1.50812225 -1.30782685]. \t  -7.575030105411535 \t -0.041547044413976206\n",
            "73     \t [ 7.97926101 -7.98257397]. \t  -28591.867752221548 \t -0.041547044413976206\n",
            "74     \t [9.94231219 3.27114541]. \t  -342.55812679072415 \t -0.041547044413976206\n",
            "75     \t [8.69896914 1.90388584]. \t  -63.47568460097385 \t -0.041547044413976206\n",
            "76     \t [-6.09949566  3.16087152]. \t  -1410.9143618624946 \t -0.041547044413976206\n",
            "77     \t [9.80867585 2.18260274]. \t  -77.75087926844343 \t -0.041547044413976206\n",
            "78     \t [ 1.43472543 -8.73045771]. \t  -45606.45220107105 \t -0.041547044413976206\n",
            "79     \t [1.71818045 9.78017345]. \t  -71885.7673970747 \t -0.041547044413976206\n",
            "80     \t [-9.36547476  2.02581118]. \t  -725.0845732068685 \t -0.041547044413976206\n",
            "81     \t [ 9.86970464 -8.11441696]. \t  -29757.834494110662 \t -0.041547044413976206\n",
            "82     \t [6.82596773 5.53661569]. \t  -5970.574953084083 \t -0.041547044413976206\n",
            "83     \t [-5.71077353  6.87092161]. \t  -20097.02860416015 \t -0.041547044413976206\n",
            "84     \t [-9.84186936 -0.80085221]. \t  -365.05948714640203 \t -0.041547044413976206\n",
            "85     \t [ 8.13288047 -6.31727247]. \t  -10327.7779569444 \t -0.041547044413976206\n",
            "86     \t [ 1.09998098 -8.34432714]. \t  -38173.95512799832 \t -0.041547044413976206\n",
            "87     \t [-5.05129396  0.33177034]. \t  -92.1942562736133 \t -0.041547044413976206\n",
            "88     \t [ 7.11878012 -1.20910938]. \t  -72.63365938492225 \t -0.041547044413976206\n",
            "89     \t [-8.17943794 -0.93216136]. \t  -280.967265407088 \t -0.041547044413976206\n",
            "90     \t [1.16800005 0.93498462]. \t  -0.7019347426668506 \t -0.041547044413976206\n",
            "91     \t [3.1056678 0.86494  ]. \t  -9.614337058883027 \t -0.041547044413976206\n",
            "92     \t [3.84302816 0.29679479]. \t  -34.97444363718047 \t -0.041547044413976206\n",
            "93     \t [1.75095315 0.73899235]. \t  -1.431790987516346 \t -0.041547044413976206\n",
            "94     \t [ 2.1995583  -9.43301258]. \t  -61787.33215146634 \t -0.041547044413976206\n",
            "95     \t [-1.41958965 -0.15751026]. \t  -10.171562020910802 \t -0.041547044413976206\n",
            "96     \t [-3.95788396  5.24743984]. \t  -6993.457889775188 \t -0.041547044413976206\n",
            "97     \t [ 0.72697305 -0.35989497]. \t  -0.5124499558965039 \t -0.041547044413976206\n",
            "98     \t [-3.18917536 -1.71868298]. \t  -183.05701438345855 \t -0.041547044413976206\n",
            "99     \t [ 9.63310453 -6.07846482]. \t  -8333.833502275556 \t -0.041547044413976206\n",
            "100    \t [9.84274302 3.50032441]. \t  -508.1308030874569 \t -0.041547044413976206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "28f86452-9f7e-46e0-dc27-af8598e0acbc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.62071325 6.42495699]. \t  -10714.638013792866 \t -9.740000060755142\n",
            "init   \t [ 2.91787228 -1.52623973]. \t  -9.740000060755142 \t -9.740000060755142\n",
            "init   \t [-5.95378042 -0.16186459]. \t  -120.5034756901465 \t -9.740000060755142\n",
            "init   \t [-7.22883337 -0.94957929]. \t  -230.87623811313406 \t -9.740000060755142\n",
            "init   \t [-7.72530745 -9.94151023]. \t  -84448.33746856163 \t -9.740000060755142\n",
            "1      \t [ 9.71879818 -6.86089495]. \t  -14331.165445255861 \t -9.740000060755142\n",
            "2      \t [-0.41525252  9.69422331]. \t  -70969.40865155176 \t -9.740000060755142\n",
            "3      \t [-9.5194484  9.1420568]. \t  -62537.95985021731 \t -9.740000060755142\n",
            "4      \t [ 0.94955298 -9.31626642]. \t  -59606.43954696483 \t -9.740000060755142\n",
            "5      \t [-0.73492853  2.66899284]. \t  -451.92907289794545 \t -9.740000060755142\n",
            "6      \t [ 8.16818502 -0.05450252]. \t  -184.62732969317273 \t -9.740000060755142\n",
            "7      \t [-1.84830907 -3.03508237]. \t  -830.0012970066132 \t -9.740000060755142\n",
            "8      \t [4.0982604  4.61962131]. \t  -2986.978599578109 \t -9.740000060755142\n",
            "9      \t [-10.          -4.60249827]. \t  -5605.391861048994 \t -9.740000060755142\n",
            "10     \t [-10.           3.02695087]. \t  -1725.595739967938 \t -9.740000060755142\n",
            "11     \t [ 4.64514964 -4.90395585]. \t  -3789.512070403482 \t -9.740000060755142\n",
            "12     \t [-5.17604036  3.47843611]. \t  -1763.9336490594949 \t -9.740000060755142\n",
            "13     \t [6.87610821 9.95490249]. \t  -73244.31967606739 \t -9.740000060755142\n",
            "14     \t [-5.51727271 -4.84045387]. \t  -5529.229591473661 \t -9.740000060755142\n",
            "15     \t [3.81024431 0.42372301]. \t  -31.71851035759085 \t -9.740000060755142\n",
            "16     \t [ 1.21552145 -1.06258319]. \t  \u001b[92m-2.2206651167663978\u001b[0m \t -2.2206651167663978\n",
            "17     \t [ 5.44945443 -9.50195117]. \t  -61357.11700936288 \t -2.2206651167663978\n",
            "18     \t [ 0.93472814 -0.45454527]. \t  \u001b[92m-0.5481960450786874\u001b[0m \t -0.5481960450786874\n",
            "19     \t [ 0.50727398 -0.8185992 ]. \t  -1.6303413971309233 \t -0.5481960450786874\n",
            "20     \t [-4.73142325  7.1388052 ]. \t  -22784.068262805846 \t -0.5481960450786874\n",
            "21     \t [1.69220352 0.61311038]. \t  -2.247830625816612 \t -0.5481960450786874\n",
            "22     \t [-3.5787386   0.06074801]. \t  -46.685349674750675 \t -0.5481960450786874\n",
            "23     \t [ 9.22226494 -2.27702436]. \t  -70.23876294356214 \t -0.5481960450786874\n",
            "24     \t [7.5178201 2.2895434]. \t  -60.078638751211855 \t -0.5481960450786874\n",
            "25     \t [ 1.64189516 -1.89916961]. \t  -62.501834483538666 \t -0.5481960450786874\n",
            "26     \t [1.87079499 1.19411038]. \t  -2.683022385928261 \t -0.5481960450786874\n",
            "27     \t [ 4.42012724 -1.45599882]. \t  -11.762259154948016 \t -0.5481960450786874\n",
            "28     \t [ 0.51753171 -4.65136484]. \t  -3655.846522957253 \t -0.5481960450786874\n",
            "29     \t [-3.47922698 -9.62118359]. \t  -71170.22443463434 \t -0.5481960450786874\n",
            "30     \t [ 5.45908371 -0.24999227]. \t  -76.78849073851329 \t -0.5481960450786874\n",
            "31     \t [-9.94522097 -0.65158699]. \t  -352.83394027924084 \t -0.5481960450786874\n",
            "32     \t [ 3.33727079 -1.36661083]. \t  -5.779610152409278 \t -0.5481960450786874\n",
            "33     \t [2.33199181 1.48496739]. \t  -10.412568833389955 \t -0.5481960450786874\n",
            "34     \t [9.88847254 2.36546456]. \t  -82.3972933037139 \t -0.5481960450786874\n",
            "35     \t [ 2.47617007 -0.74497698]. \t  -5.91202108079125 \t -0.5481960450786874\n",
            "36     \t [ 2.15944885 -0.0955775 ]. \t  -10.513614384293689 \t -0.5481960450786874\n",
            "37     \t [-0.34840984 -0.39820313]. \t  -2.7040995724777024 \t -0.5481960450786874\n",
            "38     \t [-0.69583359  5.43398648]. \t  -7143.539024985 \t -0.5481960450786874\n",
            "39     \t [-2.51726044  0.43586609]. \t  -29.15887607981675 \t -0.5481960450786874\n",
            "40     \t [1.67155688 1.70801227]. \t  -35.11304152241728 \t -0.5481960450786874\n",
            "41     \t [ 5.99506769 -7.48021104]. \t  -22459.674007112142 \t -0.5481960450786874\n",
            "42     \t [ 7.43668814 -2.52668697]. \t  -98.282997945517 \t -0.5481960450786874\n",
            "43     \t [0.26201761 0.56613448]. \t  -0.8318983274947083 \t -0.5481960450786874\n",
            "44     \t [ 0.509985   -0.39420586]. \t  \u001b[92m-0.3194668018231156\u001b[0m \t -0.3194668018231156\n",
            "45     \t [ 2.03874845 -1.00216889]. \t  -1.0808059614557575 \t -0.3194668018231156\n",
            "46     \t [-0.4014546   0.11202664]. \t  -2.327972559775687 \t -0.3194668018231156\n",
            "47     \t [0.39770016 0.1837852 ]. \t  -0.5807580700921706 \t -0.3194668018231156\n",
            "48     \t [-2.82695935 -5.75623563]. \t  -9563.009858686777 \t -0.3194668018231156\n",
            "49     \t [2.29444523 0.77259326]. \t  -4.098425280046522 \t -0.3194668018231156\n",
            "50     \t [-3.37658531  6.1059088 ]. \t  -12168.700444776267 \t -0.3194668018231156\n",
            "51     \t [3.01731532 8.09774599]. \t  -32838.49164715321 \t -0.3194668018231156\n",
            "52     \t [ 1.53432015 -1.01035666]. \t  -0.8002472121981175 \t -0.3194668018231156\n",
            "53     \t [ 0.15951655 -0.2440901 ]. \t  -0.7096697314989001 \t -0.3194668018231156\n",
            "54     \t [ 4.7535342  -6.15824446]. \t  -10122.920851352947 \t -0.3194668018231156\n",
            "55     \t [ 2.07234682 -0.72619705]. \t  -3.221038821325996 \t -0.3194668018231156\n",
            "56     \t [-0.23000497 -8.42508835]. \t  -40439.91226306904 \t -0.3194668018231156\n",
            "57     \t [-1.17872656 -0.17323914]. \t  -7.815853382713165 \t -0.3194668018231156\n",
            "58     \t [ 2.34349292 -6.85063238]. \t  -16753.199375838183 \t -0.3194668018231156\n",
            "59     \t [0.67519118 0.69616347]. \t  \u001b[92m-0.27848565286067795\u001b[0m \t -0.27848565286067795\n",
            "60     \t [0.65690509 0.51212397]. \t  \u001b[92m-0.15275413168537472\u001b[0m \t -0.15275413168537472\n",
            "61     \t [5.2508714  1.38939697]. \t  -21.934238530201043 \t -0.15275413168537472\n",
            "62     \t [ 3.69760981 -0.78029562]. \t  -19.576780824055373 \t -0.15275413168537472\n",
            "63     \t [ 3.85044779 -2.08285186]. \t  -54.707456621095346 \t -0.15275413168537472\n",
            "64     \t [4.27940725 6.92234959]. \t  -16776.65056137282 \t -0.15275413168537472\n",
            "65     \t [-0.17762626  6.75297554]. \t  -16703.08617415138 \t -0.15275413168537472\n",
            "66     \t [1.97432265 2.53632665]. \t  -238.20247058643878 \t -0.15275413168537472\n",
            "67     \t [3.19488932 0.7691128 ]. \t  -12.912381049471273 \t -0.15275413168537472\n",
            "68     \t [-5.97887901 -6.60691642]. \t  -17451.60666157324 \t -0.15275413168537472\n",
            "69     \t [-0.2352657   0.93375476]. \t  -9.35925101168596 \t -0.15275413168537472\n",
            "70     \t [1.09998269 1.00285959]. \t  -1.6715589986491275 \t -0.15275413168537472\n",
            "71     \t [-4.48846541 -1.73401253]. \t  -250.70996212841862 \t -0.15275413168537472\n",
            "72     \t [ 1.33865878 -0.06172848]. \t  -3.6580138959846766 \t -0.15275413168537472\n",
            "73     \t [ 2.32782963 -5.9831748 ]. \t  -9598.132944240877 \t -0.15275413168537472\n",
            "74     \t [4.63518173 0.35127548]. \t  -51.730529059530966 \t -0.15275413168537472\n",
            "75     \t [3.50274631 1.23262595]. \t  -6.694354945898021 \t -0.15275413168537472\n",
            "76     \t [-1.34023631  0.30420445]. \t  -10.12988993321029 \t -0.15275413168537472\n",
            "77     \t [ 1.07356487 -0.56995648]. \t  -0.36473333549387726 \t -0.15275413168537472\n",
            "78     \t [ 1.05483998 -0.73713101]. \t  \u001b[92m-0.005040638158741799\u001b[0m \t -0.005040638158741799\n",
            "79     \t [-0.69810267 -7.36598808]. \t  -23858.13215841738 \t -0.005040638158741799\n",
            "80     \t [7.64324379 4.56371502]. \t  -2357.7304008387296 \t -0.005040638158741799\n",
            "81     \t [7.43126588 1.47728842]. \t  -60.168070942265146 \t -0.005040638158741799\n",
            "82     \t [2.74498008 6.90529164]. \t  -17160.390579779105 \t -0.005040638158741799\n",
            "83     \t [6.55671111 2.04414989]. \t  -37.35982124725123 \t -0.005040638158741799\n",
            "84     \t [ 8.32326456 -5.33167961]. \t  -4764.020014189273 \t -0.005040638158741799\n",
            "85     \t [-6.34646301  6.46072723]. \t  -16192.27346302961 \t -0.005040638158741799\n",
            "86     \t [-7.98137333  0.2164104 ]. \t  -211.07761670808316 \t -0.005040638158741799\n",
            "87     \t [-9.81419467  4.16506142]. \t  -4079.167887251405 \t -0.005040638158741799\n",
            "88     \t [-4.3145491  -3.64523851]. \t  -1936.638108318239 \t -0.005040638158741799\n",
            "89     \t [-8.72172539  6.95274767]. \t  -22314.149777184477 \t -0.005040638158741799\n",
            "90     \t [ 5.28543151 -1.42081587]. \t  -21.479911249318455 \t -0.005040638158741799\n",
            "91     \t [-1.13533016 -4.2962465 ]. \t  -2900.286107739129 \t -0.005040638158741799\n",
            "92     \t [-1.36834628  4.22089697]. \t  -2743.651788682231 \t -0.005040638158741799\n",
            "93     \t [ 3.08294269 -8.08402201]. \t  -32578.006703472063 \t -0.005040638158741799\n",
            "94     \t [7.61503009 3.48124151]. \t  -596.4105271155338 \t -0.005040638158741799\n",
            "95     \t [5.77193761 2.4238855 ]. \t  -94.25641518721703 \t -0.005040638158741799\n",
            "96     \t [0.54794103 0.86379893]. \t  -1.9879743804767294 \t -0.005040638158741799\n",
            "97     \t [ 7.47631853 -0.76898239]. \t  -121.16277987722734 \t -0.005040638158741799\n",
            "98     \t [-0.21372332  5.24303817]. \t  -6093.924112011358 \t -0.005040638158741799\n",
            "99     \t [5.80831222 1.33051613]. \t  -33.405390483965064 \t -0.005040638158741799\n",
            "100    \t [2.39783797 1.0887014 ]. \t  -1.95544118582579 \t -0.005040638158741799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98Nt7Tguvna",
        "outputId": "516c6746-888a-4729-83d0-db3e0db8b8b7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.16961099  2.44217542]. \t  -706.482335023616 \t -706.482335023616\n",
            "init   \t [-1.24544522  5.70717167]. \t  -8820.05719953671 \t -706.482335023616\n",
            "init   \t [ 5.59951616 -4.54814789]. \t  -2580.3963775443385 \t -706.482335023616\n",
            "init   \t [-4.4707149   6.03744355]. \t  -12002.832292900235 \t -706.482335023616\n",
            "init   \t [9.16278707 7.51865269]. \t  -21656.00779050222 \t -706.482335023616\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -706.482335023616\n",
            "2      \t [-2.84346081 -5.74360125]. \t  -9487.534509082592 \t -706.482335023616\n",
            "3      \t [-10.          -2.67137482]. \t  -1299.3072318772613 \t -706.482335023616\n",
            "4      \t [4.40623431 2.31889613]. \t  \u001b[92m-92.20487377891885\u001b[0m \t -92.20487377891885\n",
            "5      \t [ 2.10496664 -9.86915982]. \t  -74264.45923358954 \t -92.20487377891885\n",
            "6      \t [9.13459748 0.31787954]. \t  -225.75088397007121 \t -92.20487377891885\n",
            "7      \t [-10.           8.29867251]. \t  -43772.81043974757 \t -92.20487377891885\n",
            "8      \t [ 9.78203716 -8.58451552]. \t  -37947.82343329771 \t -92.20487377891885\n",
            "9      \t [-0.24101415 -0.287723  ]. \t  \u001b[92m-1.8707358981018463\u001b[0m \t -1.8707358981018463\n",
            "10     \t [3.55786817 8.72800576]. \t  -44288.411413292735 \t -1.8707358981018463\n",
            "11     \t [-3.27582941 -0.8649867 ]. \t  -63.831140304418625 \t -1.8707358981018463\n",
            "12     \t [ 1.36046477 -3.35681792]. \t  -896.9749029925879 \t -1.8707358981018463\n",
            "13     \t [-0.39822671  0.91960706]. \t  -10.687735327844711 \t -1.8707358981018463\n",
            "14     \t [-7.23324312 -5.86377986]. \t  -11620.112045432445 \t -1.8707358981018463\n",
            "15     \t [-9.79604695  1.53789784]. \t  -538.5817881502703 \t -1.8707358981018463\n",
            "16     \t [ 9.60496963 -3.43585864]. \t  -466.3412103610556 \t -1.8707358981018463\n",
            "17     \t [1.85390154 0.37141398]. \t  -5.709346441111024 \t -1.8707358981018463\n",
            "18     \t [ 5.38101972 -0.4949701 ]. \t  -67.03766165012968 \t -1.8707358981018463\n",
            "19     \t [-6.87823681 -0.86352514]. \t  -202.16662728979242 \t -1.8707358981018463\n",
            "20     \t [-1.79784127  9.57157263]. \t  -68478.43588762768 \t -1.8707358981018463\n",
            "21     \t [ -4.6108991 -10.       ]. \t  -83762.7222486637 \t -1.8707358981018463\n",
            "22     \t [1.37215578 2.36816258]. \t  -193.9563157798652 \t -1.8707358981018463\n",
            "23     \t [7.729956   2.90476783]. \t  -212.56885482821832 \t -1.8707358981018463\n",
            "24     \t [-2.69767621  1.80196508]. \t  -182.6523776319083 \t -1.8707358981018463\n",
            "25     \t [ 3.23046965 -1.13503509]. \t  -5.830061609159467 \t -1.8707358981018463\n",
            "26     \t [ 1.59073033 -0.48953534]. \t  -2.8195628667886927 \t -1.8707358981018463\n",
            "27     \t [-0.28074501  0.13228121]. \t  \u001b[92m-1.8396933725659042\u001b[0m \t -1.8396933725659042\n",
            "28     \t [-0.62399236 -0.8233358 ]. \t  -10.476219127828683 \t -1.8396933725659042\n",
            "29     \t [5.36712493 4.64100569]. \t  -2863.2720202879955 \t -1.8396933725659042\n",
            "30     \t [ 2.47762841 -0.11326471]. \t  -14.207705631963465 \t -1.8396933725659042\n",
            "31     \t [ 0.85062771 -0.37427065]. \t  \u001b[92m-0.6731856539483387\u001b[0m \t -0.6731856539483387\n",
            "32     \t [0.47081769 0.28791741]. \t  \u001b[92m-0.4661142043144742\u001b[0m \t -0.4661142043144742\n",
            "33     \t [4.22547576 1.86197023]. \t  -25.074452447533126 \t -0.4661142043144742\n",
            "34     \t [-8.70420626  3.95213572]. \t  -3285.051678100838 \t -0.4661142043144742\n",
            "35     \t [-1.26468288 -1.66789007]. \t  -98.38281184101038 \t -0.4661142043144742\n",
            "36     \t [ 1.21825047 -0.19031865]. \t  -2.6733856423278466 \t -0.4661142043144742\n",
            "37     \t [9.9794062 3.1204156]. \t  -260.9238683374435 \t -0.4661142043144742\n",
            "38     \t [-0.88814345  0.22858237]. \t  -5.535766883494606 \t -0.4661142043144742\n",
            "39     \t [ 3.46573351 -1.73971576]. \t  -19.47003344424344 \t -0.4661142043144742\n",
            "40     \t [-8.18910393 -0.74667754]. \t  -257.5743670756286 \t -0.4661142043144742\n",
            "41     \t [ 0.51146573 -0.86219677]. \t  -2.1410890847320045 \t -0.4661142043144742\n",
            "42     \t [-4.72672964 -2.68320391]. \t  -764.3952363598074 \t -0.4661142043144742\n",
            "43     \t [ 7.5371475  -1.34334555]. \t  -73.59255520099123 \t -0.4661142043144742\n",
            "44     \t [5.62194607 0.89026162]. \t  -53.95412916553596 \t -0.4661142043144742\n",
            "45     \t [2.50068712 0.85822615]. \t  -4.363914987022905 \t -0.4661142043144742\n",
            "46     \t [ 0.16106039 -0.32524712]. \t  -0.7089223925176974 \t -0.4661142043144742\n",
            "47     \t [ 0.19685098 -0.70427219]. \t  -1.9095679080063404 \t -0.4661142043144742\n",
            "48     \t [ 1.23966921 -0.6307238 ]. \t  \u001b[92m-0.45179180458815715\u001b[0m \t -0.45179180458815715\n",
            "49     \t [-1.29103693 -0.49484572]. \t  -11.591215740150982 \t -0.45179180458815715\n",
            "50     \t [ 0.42410574 -0.16121444]. \t  -0.608609047374506 \t -0.45179180458815715\n",
            "51     \t [ 1.00972224 -9.34210833]. \t  -60232.43939583875 \t -0.45179180458815715\n",
            "52     \t [3.73691354 0.93872109]. \t  -15.28814616853369 \t -0.45179180458815715\n",
            "53     \t [3.02570292 1.13479744]. \t  -4.508782795609359 \t -0.45179180458815715\n",
            "54     \t [-5.95860536  9.73469198]. \t  -76478.80440664064 \t -0.45179180458815715\n",
            "55     \t [-4.24107999  0.33662509]. \t  -67.38983657548968 \t -0.45179180458815715\n",
            "56     \t [ 8.87043805 -6.82608291]. \t  -14281.795986853876 \t -0.45179180458815715\n",
            "57     \t [ 3.64369254 -1.17534646]. \t  -8.540776664729774 \t -0.45179180458815715\n",
            "58     \t [-3.26689353  3.06923042]. \t  -995.6668768159794 \t -0.45179180458815715\n",
            "59     \t [-2.68213897  0.45994372]. \t  -32.84312140239082 \t -0.45179180458815715\n",
            "60     \t [-2.37598484  0.96203763]. \t  -47.13263010098504 \t -0.45179180458815715\n",
            "61     \t [-3.1389655   2.30059096]. \t  -393.84951075171017 \t -0.45179180458815715\n",
            "62     \t [-3.8133098   1.07529751]. \t  -98.21976460867705 \t -0.45179180458815715\n",
            "63     \t [ 6.77588793 -4.7398027 ]. \t  -2945.056143172623 \t -0.45179180458815715\n",
            "64     \t [ 6.85791138 -6.5587892 ]. \t  -12572.4745327583 \t -0.45179180458815715\n",
            "65     \t [ 6.8431608  -0.20728075]. \t  -125.46284750646188 \t -0.45179180458815715\n",
            "66     \t [ 0.65681684 -0.64620393]. \t  \u001b[92m-0.1813865615295248\u001b[0m \t -0.1813865615295248\n",
            "67     \t [-2.41680753 -0.1507834 ]. \t  -23.80020743061185 \t -0.1813865615295248\n",
            "68     \t [-3.81265908 -4.99503952]. \t  -5793.441585001492 \t -0.1813865615295248\n",
            "69     \t [-5.86891714 -9.7963886 ]. \t  -78302.69821855053 \t -0.1813865615295248\n",
            "70     \t [3.28012838 3.6359738 ]. \t  -1078.0148941175532 \t -0.1813865615295248\n",
            "71     \t [2.83272497 0.84117893]. \t  -7.377839066231358 \t -0.1813865615295248\n",
            "72     \t [-1.85989975  0.1682356 ]. \t  -15.525018434221353 \t -0.1813865615295248\n",
            "73     \t [5.48138469 1.6134936 ]. \t  -20.233686594525135 \t -0.1813865615295248\n",
            "74     \t [1.78784924 0.71933266]. \t  -1.7546349376619523 \t -0.1813865615295248\n",
            "75     \t [3.43072363 6.91320283]. \t  -16990.636376821025 \t -0.1813865615295248\n",
            "76     \t [6.63283614 3.72989125]. \t  -929.8747398348743 \t -0.1813865615295248\n",
            "77     \t [ 2.01271661 -6.01062813]. \t  -9869.066925274641 \t -0.1813865615295248\n",
            "78     \t [ 4.62301327 -5.15861912]. \t  -4736.985788552286 \t -0.1813865615295248\n",
            "79     \t [ 1.05958563 -0.64936423]. \t  \u001b[92m-0.09706805460595494\u001b[0m \t -0.09706805460595494\n",
            "80     \t [6.26398562 3.76637446]. \t  -1005.1632549001689 \t -0.09706805460595494\n",
            "81     \t [-0.28452549 -7.4087819 ]. \t  -24230.092896333244 \t -0.09706805460595494\n",
            "82     \t [8.0865315  1.66853601]. \t  -62.904680315937874 \t -0.09706805460595494\n",
            "83     \t [7.43594702 1.60305864]. \t  -51.967888692075356 \t -0.09706805460595494\n",
            "84     \t [5.28926316 2.49920395]. \t  -122.15779008534562 \t -0.09706805460595494\n",
            "85     \t [ 5.24751685 -1.89578006]. \t  -25.572070066455044 \t -0.09706805460595494\n",
            "86     \t [8.27328039 2.32826816]. \t  -66.09380941527593 \t -0.09706805460595494\n",
            "87     \t [3.9638764  6.95957822]. \t  -17272.422004480584 \t -0.09706805460595494\n",
            "88     \t [4.09259652 0.15054964]. \t  -42.32487952645902 \t -0.09706805460595494\n",
            "89     \t [ 6.00106142 -6.84294708]. \t  -15390.331879948177 \t -0.09706805460595494\n",
            "90     \t [-7.64447184  4.58158647]. \t  -5000.274323967172 \t -0.09706805460595494\n",
            "91     \t [-3.17673625 -2.51350093]. \t  -517.4907760011075 \t -0.09706805460595494\n",
            "92     \t [0.81705881 0.31402923]. \t  -0.8018461592972518 \t -0.09706805460595494\n",
            "93     \t [-2.84232082 -7.60664852]. \t  -28129.85622966966 \t -0.09706805460595494\n",
            "94     \t [-9.46905503  2.44046659]. \t  -1023.8791326775374 \t -0.09706805460595494\n",
            "95     \t [-6.84857657  9.85863994]. \t  -81051.95808320583 \t -0.09706805460595494\n",
            "96     \t [ 1.04779365 -7.94495041]. \t  -31348.41990580627 \t -0.09706805460595494\n",
            "97     \t [4.90338108 9.29475615]. \t  -56383.70145988153 \t -0.09706805460595494\n",
            "98     \t [-6.1461776   5.99112053]. \t  -12198.24388708779 \t -0.09706805460595494\n",
            "99     \t [-4.9205641  -9.43264269]. \t  -66917.97104986853 \t -0.09706805460595494\n",
            "100    \t [-2.10756868 -2.72354809]. \t  -583.7872836272707 \t -0.09706805460595494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bpn-kmNuvqC",
        "outputId": "1ad2a504-ea2a-427d-b967-d848906a3f54"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.21288618 -1.72798696]. \t  -20.120935450808645 \t -20.120935450808645\n",
            "init   \t [ 3.36205116 -5.40113156]. \t  -6051.706374022101 \t -20.120935450808645\n",
            "init   \t [6.15356672 2.73636929]. \t  -182.2099268583363 \t -20.120935450808645\n",
            "init   \t [-6.55612296 -4.79228264]. \t  -5567.089701718418 \t -20.120935450808645\n",
            "init   \t [ 8.30639981 -0.74368981]. \t  -157.07069920616067 \t -20.120935450808645\n",
            "1      \t [-7.5961809  7.6177101]. \t  -30655.10899619148 \t -20.120935450808645\n",
            "2      \t [9.21200838 9.90573549]. \t  -70031.75643065354 \t -20.120935450808645\n",
            "3      \t [-0.35482891  6.84457911]. \t  -17693.150381619864 \t -20.120935450808645\n",
            "4      \t [-10.           0.97621088]. \t  -404.5044994223035 \t -20.120935450808645\n",
            "5      \t [ 8.38757304 -9.87720684]. \t  -69791.40203706689 \t -20.120935450808645\n",
            "6      \t [-1.81138042 -9.77546462]. \t  -74452.48916498462 \t -20.120935450808645\n",
            "7      \t [-2.1943543   0.23088913]. \t  -20.79286098486515 \t -20.120935450808645\n",
            "8      \t [-10. -10.]. \t  -88321.0 \t -20.120935450808645\n",
            "9      \t [-5.44567888  2.43911643]. \t  -643.193261116385 \t -20.120935450808645\n",
            "10     \t [-1.5030724 -3.6746114]. \t  -1631.7470885572116 \t -20.120935450808645\n",
            "11     \t [9.81264863 4.60115038]. \t  -2193.872126615322 \t -20.120935450808645\n",
            "12     \t [3.62436881 9.40766209]. \t  -60130.80141957688 \t -20.120935450808645\n",
            "13     \t [1.50006223 1.79084293]. \t  -48.548285025938384 \t -20.120935450808645\n",
            "14     \t [ 3.20518159 -9.61981179]. \t  -66162.89528687004 \t -20.120935450808645\n",
            "15     \t [ 7.27524997 -4.17007838]. \t  -1552.3031100253113 \t -20.120935450808645\n",
            "16     \t [3.42094237 4.11825421]. \t  -1866.2503312069557 \t -20.120935450808645\n",
            "17     \t [ 4.1820351  -0.02427814]. \t  -45.08446521226741 \t -20.120935450808645\n",
            "18     \t [-4.56922826 -1.07899737]. \t  -126.17280328458344 \t -20.120935450808645\n",
            "19     \t [-0.59448736  0.1948696 ]. \t  \u001b[92m-3.4413579792785853\u001b[0m \t -3.4413579792785853\n",
            "20     \t [ 0.74064892 -0.53895176]. \t  \u001b[92m-0.11827813797979712\u001b[0m \t -0.11827813797979712\n",
            "21     \t [ 1.30441999 -0.26027865]. \t  -2.825466429485022 \t -0.11827813797979712\n",
            "22     \t [-1.00565304  2.16383391]. \t  -219.09675275708685 \t -0.11827813797979712\n",
            "23     \t [ 1.31057711 -1.73330398]. \t  -44.240900478619544 \t -0.11827813797979712\n",
            "24     \t [ 0.37506243 -0.3977598 ]. \t  -0.39742349400352756 \t -0.11827813797979712\n",
            "25     \t [ 0.6709913  -0.35229995]. \t  -0.4657001039980887 \t -0.11827813797979712\n",
            "26     \t [-7.97046565 -1.24878144]. \t  -326.41776516646826 \t -0.11827813797979712\n",
            "27     \t [-9.8842948   4.47078389]. \t  -5090.529408185037 \t -0.11827813797979712\n",
            "28     \t [-3.86327434  9.189004  ]. \t  -59701.08116625261 \t -0.11827813797979712\n",
            "29     \t [ 0.40458748 -0.46305945]. \t  -0.35569323032869476 \t -0.11827813797979712\n",
            "30     \t [-2.82296718  3.65511888]. \t  -1760.163848412105 \t -0.11827813797979712\n",
            "31     \t [ 5.02449524 -2.03527978]. \t  -37.454791889002266 \t -0.11827813797979712\n",
            "32     \t [ 5.69244536 -0.1629766 ]. \t  -85.62296261428516 \t -0.11827813797979712\n",
            "33     \t [-9.78148192 -3.11630421]. \t  -1822.0092858813484 \t -0.11827813797979712\n",
            "34     \t [6.47945747 6.3528011 ]. \t  -11052.201520222883 \t -0.11827813797979712\n",
            "35     \t [-1.39576485 -0.00544225]. \t  -9.636339017556331 \t -0.11827813797979712\n",
            "36     \t [-6.02429845 -9.17071284]. \t  -60760.304410798686 \t -0.11827813797979712\n",
            "37     \t [-7.54127851 -2.72185118]. \t  -1072.7339937604422 \t -0.11827813797979712\n",
            "38     \t [ 2.68264592 -0.98901779]. \t  -3.886418079514091 \t -0.11827813797979712\n",
            "39     \t [3.03388896 0.23834055]. \t  -21.192734300871763 \t -0.11827813797979712\n",
            "40     \t [ 5.82415001 -9.03224803]. \t  -49534.30542867251 \t -0.11827813797979712\n",
            "41     \t [ 1.76252312 -0.47910843]. \t  -3.9793185044899944 \t -0.11827813797979712\n",
            "42     \t [ 2.33466359 -8.52038101]. \t  -40819.23845881398 \t -0.11827813797979712\n",
            "43     \t [-0.4270306  -0.17010151]. \t  -2.5066716842336056 \t -0.11827813797979712\n",
            "44     \t [9.4631004  1.62675135]. \t  -106.40955030065507 \t -0.11827813797979712\n",
            "45     \t [0.26088445 3.78095731]. \t  -1605.7685070364544 \t -0.11827813797979712\n",
            "46     \t [0.13255652 1.0102979 ]. \t  -8.039853190905582 \t -0.11827813797979712\n",
            "47     \t [1.59659001 0.80236401]. \t  -0.5468989288571424 \t -0.11827813797979712\n",
            "48     \t [-0.90797514  0.89588663]. \t  -16.272726213340462 \t -0.11827813797979712\n",
            "49     \t [1.64706937 1.09706868]. \t  -1.5740507536587631 \t -0.11827813797979712\n",
            "50     \t [1.36237147 0.68901181]. \t  -0.47228081410991873 \t -0.11827813797979712\n",
            "51     \t [ 1.98610175 -0.25119819]. \t  -7.890857885451686 \t -0.11827813797979712\n",
            "52     \t [-2.21527563 -0.51625863]. \t  -25.444540249624076 \t -0.11827813797979712\n",
            "53     \t [-0.19256959  0.6314155 ]. \t  -3.3821872186628372 \t -0.11827813797979712\n",
            "54     \t [-5.64174875 -3.92160871]. \t  -2694.0005780827005 \t -0.11827813797979712\n",
            "55     \t [1.43987715 0.84271774]. \t  -0.19425480704825995 \t -0.11827813797979712\n",
            "56     \t [4.06194435 6.61264068]. \t  -13917.85656268981 \t -0.11827813797979712\n",
            "57     \t [-2.41865885 -4.25492326]. \t  -2995.8397045771317 \t -0.11827813797979712\n",
            "58     \t [3.64003973 6.97317961]. \t  -17532.794323167178 \t -0.11827813797979712\n",
            "59     \t [ 0.62636285 -0.52987832]. \t  -0.14800819098751414 \t -0.11827813797979712\n",
            "60     \t [7.40763866 5.51959415]. \t  -5770.737914270966 \t -0.11827813797979712\n",
            "61     \t [ 2.3350368  -1.11960563]. \t  -1.8414890070958787 \t -0.11827813797979712\n",
            "62     \t [0.63733264 0.65791808]. \t  -0.23584225442787377 \t -0.11827813797979712\n",
            "63     \t [ 9.80067197 -5.81223533]. \t  -6750.69273000951 \t -0.11827813797979712\n",
            "64     \t [-4.09931234  1.97253809]. \t  -308.3252665364161 \t -0.11827813797979712\n",
            "65     \t [-2.18532263 -9.6048243 ]. \t  -69716.92313625086 \t -0.11827813797979712\n",
            "66     \t [0.40466002 0.28197741]. \t  -0.4751052495294833 \t -0.11827813797979712\n",
            "67     \t [4.08981309 0.7279362 ]. \t  -27.909119138463204 \t -0.11827813797979712\n",
            "68     \t [4.80742649 1.22440502]. \t  -21.042118328385396 \t -0.11827813797979712\n",
            "69     \t [ 3.57649453 -8.66059476]. \t  -42893.23753008888 \t -0.11827813797979712\n",
            "70     \t [-7.74233475  5.74188437]. \t  -10934.15107277863 \t -0.11827813797979712\n",
            "71     \t [1.03697539 0.82026617]. \t  -0.19195585381044677 \t -0.11827813797979712\n",
            "72     \t [-3.98859714  0.66553368]. \t  -72.40696445989835 \t -0.11827813797979712\n",
            "73     \t [7.28700357 7.52424658]. \t  -22486.75882202814 \t -0.11827813797979712\n",
            "74     \t [0.94257086 0.73041907]. \t  \u001b[92m-0.034275292030153844\u001b[0m \t -0.034275292030153844\n",
            "75     \t [ 9.91286349 -2.22542941]. \t  -79.43925695773675 \t -0.034275292030153844\n",
            "76     \t [ 5.40287929 -3.75820845]. \t  -1063.208321874586 \t -0.034275292030153844\n",
            "77     \t [ 5.39885301 -1.01843705]. \t  -41.453510262228875 \t -0.034275292030153844\n",
            "78     \t [ 4.38080036 -1.14466931]. \t  -17.62687475297985 \t -0.034275292030153844\n",
            "79     \t [ 1.46228444 -6.62887934]. \t  -14937.668100017143 \t -0.034275292030153844\n",
            "80     \t [2.01928411 9.20045794]. \t  -55964.60885655887 \t -0.034275292030153844\n",
            "81     \t [ 5.67102364 -3.82147844]. \t  -1129.740002070439 \t -0.034275292030153844\n",
            "82     \t [2.139813  4.7225701]. \t  -3607.940565359596 \t -0.034275292030153844\n",
            "83     \t [ 0.83746726 -0.59958682]. \t  -0.0544817487459128 \t -0.034275292030153844\n",
            "84     \t [-3.2540645  -8.91447672]. \t  -52629.19515382502 \t -0.034275292030153844\n",
            "85     \t [ 4.86723784 -1.70504543]. \t  -16.749608540448087 \t -0.034275292030153844\n",
            "86     \t [-8.69033094  3.93211189]. \t  -3232.3357421883297 \t -0.034275292030153844\n",
            "87     \t [ 6.07419361 -1.5161135 ]. \t  -30.110459256989284 \t -0.034275292030153844\n",
            "88     \t [-1.84270439  9.92074067]. \t  -78959.45602882205 \t -0.034275292030153844\n",
            "89     \t [ 5.35086292 -4.73006518]. \t  -3123.046485571771 \t -0.034275292030153844\n",
            "90     \t [-0.56609623 -1.23158242]. \t  -28.368146674778032 \t -0.034275292030153844\n",
            "91     \t [0.82574087 0.64960229]. \t  \u001b[92m-0.03103057252125801\u001b[0m \t -0.03103057252125801\n",
            "92     \t [ 8.76963296 -9.60728067]. \t  -61892.78078871723 \t -0.03103057252125801\n",
            "93     \t [-2.97216071  7.09633929]. \t  -21518.27341684187 \t -0.03103057252125801\n",
            "94     \t [-1.85301802 -9.63884415]. \t  -70446.43417333013 \t -0.03103057252125801\n",
            "95     \t [-0.17249246  1.00137998]. \t  -10.862247659516049 \t -0.03103057252125801\n",
            "96     \t [-7.06018259  3.18491234]. \t  -1560.7397815242407 \t -0.03103057252125801\n",
            "97     \t [-4.1757566   7.11125473]. \t  -22209.55836836178 \t -0.03103057252125801\n",
            "98     \t [-5.39054826  7.86566842]. \t  -33388.93420504962 \t -0.03103057252125801\n",
            "99     \t [ 5.99666688 -0.69597377]. \t  -75.526395702937 \t -0.03103057252125801\n",
            "100    \t [9.84428029 6.27909718]. \t  -9602.938005260863 \t -0.03103057252125801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NdFRXtPuvsP",
        "outputId": "d7f59463-b46d-4c21-e2e9-673b6b8804e2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.95102099 0.14299376]. \t  -20.744156010505872 \t -20.744156010505872\n",
            "init   \t [0.5668276  7.92570408]. \t  -31283.56791285067 \t -20.744156010505872\n",
            "init   \t [3.99982381 4.28594204]. \t  -2152.65365220131 \t -20.744156010505872\n",
            "init   \t [ 4.34676764 -5.54361076]. \t  -6535.783782469756 \t -20.744156010505872\n",
            "init   \t [-6.49690953 -0.86317023]. \t  -183.78911629570374 \t -20.744156010505872\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -20.744156010505872\n",
            "2      \t [-9.41010567  7.02228954]. \t  -23451.5819466236 \t -20.744156010505872\n",
            "3      \t [8.38403624 9.89223058]. \t  -70238.39460245872 \t -20.744156010505872\n",
            "4      \t [-2.12485184 -9.78521923]. \t  -74991.72959757554 \t -20.744156010505872\n",
            "5      \t [ 8.58953583 -1.38728645]. \t  -102.54399844056245 \t -20.744156010505872\n",
            "6      \t [ 9.69005186 -9.66164402]. \t  -62736.82929961719 \t -20.744156010505872\n",
            "7      \t [-1.39159844 -2.85854778]. \t  -634.7226835311569 \t -20.744156010505872\n",
            "8      \t [-2.75635572  2.76322823]. \t  -664.0712557435485 \t -20.744156010505872\n",
            "9      \t [9.62647615 3.68327915]. \t  -687.3791864613818 \t -20.744156010505872\n",
            "10     \t [-9.57779667 -4.25469865]. \t  -4304.004280902335 \t -20.744156010505872\n",
            "11     \t [-4.56391037  9.5532816 ]. \t  -70039.50318929649 \t -20.744156010505872\n",
            "12     \t [-10.           0.93752682]. \t  -397.4970392520219 \t -20.744156010505872\n",
            "13     \t [-6.58718286  2.83002229]. \t  -1079.5586537249903 \t -20.744156010505872\n",
            "14     \t [-5.40845893 -5.80999201]. \t  -10675.859199939272 \t -20.744156010505872\n",
            "15     \t [  3.26653645 -10.        ]. \t  -77413.24854657562 \t -20.744156010505872\n",
            "16     \t [ 0.88295929 -0.3224716 ]. \t  \u001b[92m-0.9249037526400484\u001b[0m \t -0.9249037526400484\n",
            "17     \t [0.01558116 0.34011125]. \t  -1.0621939903267643 \t -0.9249037526400484\n",
            "18     \t [ 8.83895874 -5.1022578 ]. \t  -3798.615463378062 \t -0.9249037526400484\n",
            "19     \t [ 4.93462122 -0.58925448]. \t  -51.43948921692635 \t -0.9249037526400484\n",
            "20     \t [ 0.49198435 -0.04437596]. \t  \u001b[92m-0.7344574965617643\u001b[0m \t -0.7344574965617643\n",
            "21     \t [ 2.10769036 -1.01173501]. \t  -1.2342923708095817 \t -0.7344574965617643\n",
            "22     \t [ 3.01680593 -1.69600171]. \t  -19.039310083837826 \t -0.7344574965617643\n",
            "23     \t [0.62190808 2.42414121]. \t  -247.94185923582103 \t -0.7344574965617643\n",
            "24     \t [6.60366256 2.26119887]. \t  -57.64428061895695 \t -0.7344574965617643\n",
            "25     \t [ 2.01070195 -2.17575694]. \t  -112.23923083842712 \t -0.7344574965617643\n",
            "26     \t [ 1.24469202 -0.6080799 ]. \t  \u001b[92m-0.5702670145254232\u001b[0m \t -0.5702670145254232\n",
            "27     \t [ 0.13144591 -5.3712972 ]. \t  -6629.415173571401 \t -0.5702670145254232\n",
            "28     \t [ 2.57154965 -0.86980677]. \t  -4.71028266341928 \t -0.5702670145254232\n",
            "29     \t [ 0.52220376 -0.00749302]. \t  -0.7734482553707618 \t -0.5702670145254232\n",
            "30     \t [-0.99402435 -0.18049733]. \t  -6.219870043688403 \t -0.5702670145254232\n",
            "31     \t [-3.68335349  0.01356765]. \t  -49.07341028788259 \t -0.5702670145254232\n",
            "32     \t [2.39081742 0.00810567]. \t  -13.365132340896892 \t -0.5702670145254232\n",
            "33     \t [ 5.71998605 -2.02471248]. \t  -34.56850782683551 \t -0.5702670145254232\n",
            "34     \t [6.22136964 0.75932875]. \t  -78.63619268900285 \t -0.5702670145254232\n",
            "35     \t [-3.48308968  5.36038837]. \t  -7450.053435480318 \t -0.5702670145254232\n",
            "36     \t [9.83193125 0.51441463]. \t  -251.08295897814943 \t -0.5702670145254232\n",
            "37     \t [ 1.19424952 -0.31428534]. \t  -2.024550545279648 \t -0.5702670145254232\n",
            "38     \t [-0.32274732  0.22902344]. \t  -2.1154308879030443 \t -0.5702670145254232\n",
            "39     \t [-4.54876149 -1.83362547]. \t  -284.95550334261225 \t -0.5702670145254232\n",
            "40     \t [ 1.69668058 -0.46466337]. \t  -3.685087754591003 \t -0.5702670145254232\n",
            "41     \t [3.08081134 1.13301819]. \t  -4.8568341590400665 \t -0.5702670145254232\n",
            "42     \t [7.3332658  5.78681798]. \t  -7154.269766736508 \t -0.5702670145254232\n",
            "43     \t [4.45226787 8.08434014]. \t  -31895.519246889573 \t -0.5702670145254232\n",
            "44     \t [-2.2873635  -0.38945279]. \t  -24.230320055542 \t -0.5702670145254232\n",
            "45     \t [ 1.58759931 -6.28185093]. \t  -11961.96712752897 \t -0.5702670145254232\n",
            "46     \t [2.58749808 0.69977818]. \t  -7.692244074984503 \t -0.5702670145254232\n",
            "47     \t [0.25299138 0.04802039]. \t  -0.6814065959290689 \t -0.5702670145254232\n",
            "48     \t [-1.25980935  0.1364566 ]. \t  -8.471416520231536 \t -0.5702670145254232\n",
            "49     \t [4.43027072 0.97124894]. \t  -24.706780058562657 \t -0.5702670145254232\n",
            "50     \t [-0.2510875   9.96919407]. \t  -79220.08167800932 \t -0.5702670145254232\n",
            "51     \t [-9.78985189  3.04528552]. \t  -1722.435073549983 \t -0.5702670145254232\n",
            "52     \t [ 0.89472625 -0.61788589]. \t  \u001b[92m-0.045488607899963125\u001b[0m \t -0.045488607899963125\n",
            "53     \t [-8.06691577 -0.48367722]. \t  -227.89468865631062 \t -0.045488607899963125\n",
            "54     \t [0.25171495 1.82199399]. \t  -82.16303464478517 \t -0.045488607899963125\n",
            "55     \t [-7.84198973 -5.30413911]. \t  -8298.306829346673 \t -0.045488607899963125\n",
            "56     \t [ 1.73803168 -0.87260757]. \t  -0.6372644408586302 \t -0.045488607899963125\n",
            "57     \t [ 1.14127837 -0.86980207]. \t  -0.29647898937252903 \t -0.045488607899963125\n",
            "58     \t [-0.39223762  1.12044925]. \t  -18.79373182668272 \t -0.045488607899963125\n",
            "59     \t [ 1.34604961 -0.82054928]. \t  -0.11975094439969902 \t -0.045488607899963125\n",
            "60     \t [-6.44961741  0.89599703]. \t  -185.27054250434546 \t -0.045488607899963125\n",
            "61     \t [0.34618561 0.85052139]. \t  -2.8500596953444686 \t -0.045488607899963125\n",
            "62     \t [-4.07034207 -1.94439828]. \t  -296.3017896312497 \t -0.045488607899963125\n",
            "63     \t [-8.14075251  2.79073766]. \t  -1208.563170401845 \t -0.045488607899963125\n",
            "64     \t [ 7.00355706 -3.51006426]. \t  -658.2086951066982 \t -0.045488607899963125\n",
            "65     \t [8.5032172  2.33748767]. \t  -68.05447515116728 \t -0.045488607899963125\n",
            "66     \t [ 5.34257373 -1.66391754]. \t  -18.93373899740341 \t -0.045488607899963125\n",
            "67     \t [ 6.52116839 -9.49089424]. \t  -60327.305906501824 \t -0.045488607899963125\n",
            "68     \t [8.08369547 1.50633416]. \t  -75.32144567329101 \t -0.045488607899963125\n",
            "69     \t [-8.34277372 -3.1991881 ]. \t  -1747.5942162311192 \t -0.045488607899963125\n",
            "70     \t [ 8.1898527  -1.10274243]. \t  -117.99783535303871 \t -0.045488607899963125\n",
            "71     \t [-2.73199795  4.57772709]. \t  -3999.952819559216 \t -0.045488607899963125\n",
            "72     \t [-5.82070485 -9.54768884]. \t  -70837.90670347554 \t -0.045488607899963125\n",
            "73     \t [-9.8685891   8.29621224]. \t  -43644.11708412386 \t -0.045488607899963125\n",
            "74     \t [4.12031716 0.18517141]. \t  -42.56957734983289 \t -0.045488607899963125\n",
            "75     \t [-3.28487492 -7.62804324]. \t  -28654.8945934056 \t -0.045488607899963125\n",
            "76     \t [ 8.24492868 -8.27259918]. \t  -33142.230176738776 \t -0.045488607899963125\n",
            "77     \t [ 3.1867796 -9.2083516]. \t  -55383.167089094495 \t -0.045488607899963125\n",
            "78     \t [-8.28717958 -9.40295244]. \t  -68623.7541473987 \t -0.045488607899963125\n",
            "79     \t [ 6.74504647 -8.46425389]. \t  -37320.518206723325 \t -0.045488607899963125\n",
            "80     \t [-5.78481188  3.27956315]. \t  -1536.1638856835025 \t -0.045488607899963125\n",
            "81     \t [-7.29670082  2.55242434]. \t  -895.163642157233 \t -0.045488607899963125\n",
            "82     \t [ 6.02491934 -1.07433006]. \t  -52.87529013403088 \t -0.045488607899963125\n",
            "83     \t [-2.35585868 -5.29245568]. \t  -6826.784218325494 \t -0.045488607899963125\n",
            "84     \t [ 6.79908003 -6.61180915]. \t  -13036.977935772078 \t -0.045488607899963125\n",
            "85     \t [3.47261069 0.87774443]. \t  -13.577043412262853 \t -0.045488607899963125\n",
            "86     \t [4.96304012 2.96040326]. \t  -331.4608594387051 \t -0.045488607899963125\n",
            "87     \t [5.15071078 2.07083554]. \t  -40.703473293895655 \t -0.045488607899963125\n",
            "88     \t [8.16694511 2.96724987]. \t  -229.67532676975765 \t -0.045488607899963125\n",
            "89     \t [-0.52126876  7.08258155]. \t  -20342.626382327842 \t -0.045488607899963125\n",
            "90     \t [ 8.15236323 -4.41999009]. \t  -1963.2814553179464 \t -0.045488607899963125\n",
            "91     \t [-6.10918752  7.5731273 ]. \t  -29242.441528680607 \t -0.045488607899963125\n",
            "92     \t [-1.61079141  2.53916057]. \t  -427.6332137066193 \t -0.045488607899963125\n",
            "93     \t [ 4.88716679 -3.65327652]. \t  -966.0864564035858 \t -0.045488607899963125\n",
            "94     \t [-1.54270808  1.49471755]. \t  -78.73127016339454 \t -0.045488607899963125\n",
            "95     \t [2.50158433 4.22111784]. \t  -2197.9902345163655 \t -0.045488607899963125\n",
            "96     \t [ 0.28359708 -0.42166557]. \t  -0.523603048363225 \t -0.045488607899963125\n",
            "97     \t [-1.66571496  0.68011934]. \t  -20.530935912231644 \t -0.045488607899963125\n",
            "98     \t [ 9.7014008  -1.18928938]. \t  -170.17915159284428 \t -0.045488607899963125\n",
            "99     \t [ 8.99303271 -1.29363549]. \t  -127.64426897721775 \t -0.045488607899963125\n",
            "100    \t [-3.29970925 -9.55423711]. \t  -69111.27565440805 \t -0.045488607899963125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86panpOuvum",
        "outputId": "82a5b138-9195-428a-bd66-4811f625008f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.55763717 -4.88997339]. \t  -4281.404311682752 \t -3267.0472724202245\n",
            "init   \t [-4.94946261  4.19808709]. \t  -3267.0472724202245 \t -3267.0472724202245\n",
            "init   \t [-1.04895274 -5.46114084]. \t  -7372.467127638806 \t -3267.0472724202245\n",
            "init   \t [-1.96580869  7.6451547 ]. \t  -28265.42637457811 \t -3267.0472724202245\n",
            "init   \t [-1.25573066  7.56850362]. \t  -26833.737231776184 \t -3267.0472724202245\n",
            "1      \t [-9.76533982 -4.68487047]. \t  -5874.976174155925 \t -3267.0472724202245\n",
            "2      \t [8.28335803 6.88744078]. \t  -15048.8154067911 \t -3267.0472724202245\n",
            "3      \t [ 9.8362843  -6.65190316]. \t  -12452.665647642647 \t -3267.0472724202245\n",
            "4      \t [-8.7683881   9.94621396]. \t  -85481.32833161899 \t -3267.0472724202245\n",
            "5      \t [3.6439568  1.63346221]. \t  \u001b[92m-12.719219233371792\u001b[0m \t -12.719219233371792\n",
            "6      \t [ -5.90931827 -10.        ]. \t  -84845.03338014381 \t -12.719219233371792\n",
            "7      \t [8.93552199 0.20048667]. \t  -219.79924073753492 \t -12.719219233371792\n",
            "8      \t [  2.81431851 -10.        ]. \t  -77767.67772264044 \t -12.719219233371792\n",
            "9      \t [-9.95019527  1.16479823]. \t  -440.64563062249374 \t -12.719219233371792\n",
            "10     \t [-5.38183544 -1.1908937 ]. \t  -175.8084390449327 \t -12.719219233371792\n",
            "11     \t [-0.7079951   0.54701321]. \t  \u001b[92m-6.330828714826574\u001b[0m \t -6.330828714826574\n",
            "12     \t [ 5.90128271 -3.02530477]. \t  -331.72389537591124 \t -6.330828714826574\n",
            "13     \t [3.72040235 9.59245682]. \t  -65030.831284512555 \t -6.330828714826574\n",
            "14     \t [0.25704099 2.82957029]. \t  -497.0484459568449 \t -6.330828714826574\n",
            "15     \t [-1.64474106  0.24852499]. \t  -13.248215686279082 \t -6.330828714826574\n",
            "16     \t [-0.29624282 -0.13664057]. \t  \u001b[92m-1.9028021518375346\u001b[0m \t -1.9028021518375346\n",
            "17     \t [5.70677519 2.91174148]. \t  -275.26531081965936 \t -1.9028021518375346\n",
            "18     \t [-0.62853012  0.17242353]. \t  -3.5987704904522926 \t -1.9028021518375346\n",
            "19     \t [ 2.34460879 -0.39384406]. \t  -10.085397190580132 \t -1.9028021518375346\n",
            "20     \t [ 4.54176809 -0.13648215]. \t  -53.12540198671977 \t -1.9028021518375346\n",
            "21     \t [ 0.70044559 -0.85632892]. \t  \u001b[92m-1.2637132327559573\u001b[0m \t -1.2637132327559573\n",
            "22     \t [-8.99212837  4.07774151]. \t  -3669.643163979375 \t -1.2637132327559573\n",
            "23     \t [ 1.18493869 -0.64547482]. \t  \u001b[92m-0.2815363351764409\u001b[0m \t -0.2815363351764409\n",
            "24     \t [3.24250352 4.49238034]. \t  -2760.885721187458 \t -0.2815363351764409\n",
            "25     \t [-5.48072925 -4.59305006]. \t  -4587.419612368779 \t -0.2815363351764409\n",
            "26     \t [2.63865905 1.24998859]. \t  -3.158147566735609 \t -0.2815363351764409\n",
            "27     \t [ 9.34139514 -2.88519841]. \t  -176.37344287785447 \t -0.2815363351764409\n",
            "28     \t [3.09906289 1.17150039]. \t  -4.6570321287409415 \t -0.2815363351764409\n",
            "29     \t [ 0.31754563 -0.33152858]. \t  -0.48484363007750664 \t -0.2815363351764409\n",
            "30     \t [ 5.62709174 -6.36322755]. \t  -11377.932328955048 \t -0.2815363351764409\n",
            "31     \t [ 1.77231288 -0.66430961]. \t  -2.1795935255634977 \t -0.2815363351764409\n",
            "32     \t [9.94034579 3.01432936]. \t  -215.46199536446963 \t -0.2815363351764409\n",
            "33     \t [-2.46627227  1.2703317 ]. \t  -76.85279283651194 \t -0.2815363351764409\n",
            "34     \t [-0.61682276 -1.21939376]. \t  -28.399866533359162 \t -0.2815363351764409\n",
            "35     \t [-9.9948344  -8.27133975]. \t  -43236.022323985 \t -0.2815363351764409\n",
            "36     \t [ 0.15773305 -0.2597102 ]. \t  -0.7104564240007516 \t -0.2815363351764409\n",
            "37     \t [ 0.07548293 -0.56950331]. \t  -1.5118141254390878 \t -0.2815363351764409\n",
            "38     \t [-2.38932745 -1.54437988]. \t  -114.00573559907444 \t -0.2815363351764409\n",
            "39     \t [2.09879845 0.44825969]. \t  -6.9664666537539865 \t -0.2815363351764409\n",
            "40     \t [ 2.38815579 -1.26345171]. \t  -3.22130319108295 \t -0.2815363351764409\n",
            "41     \t [-8.01213182 -1.42232433]. \t  -372.01623404127486 \t -0.2815363351764409\n",
            "42     \t [ 8.52218378 -9.84249941]. \t  -68674.9668152529 \t -0.2815363351764409\n",
            "43     \t [ 0.91848763 -0.65606032]. \t  \u001b[92m-0.013293005193297873\u001b[0m \t -0.013293005193297873\n",
            "44     \t [ 5.95424587 -0.3960496 ]. \t  -88.17582882496595 \t -0.013293005193297873\n",
            "45     \t [-4.59687493  0.73365113]. \t  -95.69910200973725 \t -0.013293005193297873\n",
            "46     \t [ 2.97032613 -1.44687801]. \t  -6.842347041782533 \t -0.013293005193297873\n",
            "47     \t [ 0.3794039  -0.42480457]. \t  -0.3858229843734759 \t -0.013293005193297873\n",
            "48     \t [6.62649932 9.19341493]. \t  -52786.490281699094 \t -0.013293005193297873\n",
            "49     \t [ 1.09969936 -0.9512198 ]. \t  -1.0179663780971386 \t -0.013293005193297873\n",
            "50     \t [-6.5107799  0.8593455]. \t  -184.01945269076842 \t -0.013293005193297873\n",
            "51     \t [ 0.91774604 -0.55424122]. \t  -0.1908438035179484 \t -0.013293005193297873\n",
            "52     \t [-4.73836531 -2.7767814 ]. \t  -845.7312696634915 \t -0.013293005193297873\n",
            "53     \t [1.26374272 0.38186474]. \t  -1.959522322902396 \t -0.013293005193297873\n",
            "54     \t [-1.5166449  -9.88752666]. \t  -77658.23039318589 \t -0.013293005193297873\n",
            "55     \t [3.97051309 1.06646228]. \t  -14.575623335779952 \t -0.013293005193297873\n",
            "56     \t [2.43116934 5.18720372]. \t  -5282.4727117377015 \t -0.013293005193297873\n",
            "57     \t [-8.41074348 -8.70603717]. \t  -51289.14438183774 \t -0.013293005193297873\n",
            "58     \t [-2.07398088  5.0103825 ]. \t  -5476.232284473293 \t -0.013293005193297873\n",
            "59     \t [-4.38452908 -0.14070639]. \t  -68.13892913132634 \t -0.013293005193297873\n",
            "60     \t [ 9.8845375  -3.76667601]. \t  -762.7809199151517 \t -0.013293005193297873\n",
            "61     \t [ 9.7594397  -1.34951802]. \t  -151.56418774222593 \t -0.013293005193297873\n",
            "62     \t [ 3.64445271 -4.63530097]. \t  -3100.30982698243 \t -0.013293005193297873\n",
            "63     \t [-9.07829232  3.54027274]. \t  -2433.382440195059 \t -0.013293005193297873\n",
            "64     \t [ 2.89586453 -3.25500391]. \t  -672.952472255595 \t -0.013293005193297873\n",
            "65     \t [-5.25129345  2.12510715]. \t  -447.11280209366765 \t -0.013293005193297873\n",
            "66     \t [ 8.35331433 -2.73762228]. \t  -142.13990155243454 \t -0.013293005193297873\n",
            "67     \t [0.29177252 3.76797388]. \t  -1580.112963263459 \t -0.013293005193297873\n",
            "68     \t [0.41906616 1.55405888]. \t  -39.25365212820006 \t -0.013293005193297873\n",
            "69     \t [-1.00101073  1.07403191]. \t  -25.89109270084497 \t -0.013293005193297873\n",
            "70     \t [-3.18739938 -0.01749346]. \t  -37.861147313249504 \t -0.013293005193297873\n",
            "71     \t [-0.09713035 -4.53205644]. \t  -3392.1628502213453 \t -0.013293005193297873\n",
            "72     \t [-8.52083373  2.0108288 ]. \t  -642.2775854060376 \t -0.013293005193297873\n",
            "73     \t [ 2.26559052 -1.72662489]. \t  -28.935511292074477 \t -0.013293005193297873\n",
            "74     \t [-9.32629639  3.54705748]. \t  -2485.687738481979 \t -0.013293005193297873\n",
            "75     \t [ 2.81695214 -2.03831764]. \t  -63.63698645942854 \t -0.013293005193297873\n",
            "76     \t [ 4.01917162 -7.50134124]. \t  -23562.76031559984 \t -0.013293005193297873\n",
            "77     \t [-2.44688676  3.52765592]. \t  -1506.3510075462314 \t -0.013293005193297873\n",
            "78     \t [1.76989233 0.99245847]. \t  -0.6727784482380204 \t -0.013293005193297873\n",
            "79     \t [1.67761574 0.55581344]. \t  -2.7053396518528046 \t -0.013293005193297873\n",
            "80     \t [ 2.74935569 -0.8011729 ]. \t  -7.356210035284359 \t -0.013293005193297873\n",
            "81     \t [ 3.39898761 -1.40720989]. \t  -6.3856875357102965 \t -0.013293005193297873\n",
            "82     \t [2.37740544 1.45046469]. \t  -8.597170244720756 \t -0.013293005193297873\n",
            "83     \t [-0.32753979 -7.9482235 ]. \t  -32095.407427567097 \t -0.013293005193297873\n",
            "84     \t [3.34558955 0.86833311]. \t  -12.255225877529249 \t -0.013293005193297873\n",
            "85     \t [1.92851199 9.24974436]. \t  -57249.36396355148 \t -0.013293005193297873\n",
            "86     \t [-4.91063715  3.2103253 ]. \t  -1337.7841449282078 \t -0.013293005193297873\n",
            "87     \t [-8.26180241 -0.01564831]. \t  -222.31192709882077 \t -0.013293005193297873\n",
            "88     \t [-0.072469   -3.07692734]. \t  -723.7151885814683 \t -0.013293005193297873\n",
            "89     \t [-1.97101032 -4.06030242]. \t  -2450.870425270952 \t -0.013293005193297873\n",
            "90     \t [-2.89804816 -1.01231984]. \t  -64.15288597645016 \t -0.013293005193297873\n",
            "91     \t [6.47232451 0.25390567]. \t  -110.42348666866121 \t -0.013293005193297873\n",
            "92     \t [-4.06431762  0.37047618]. \t  -63.298079846082146 \t -0.013293005193297873\n",
            "93     \t [-9.03410237  6.33762444]. \t  -16072.914496692341 \t -0.013293005193297873\n",
            "94     \t [7.47848886 4.32620845]. \t  -1836.4155354062034 \t -0.013293005193297873\n",
            "95     \t [-2.5427784   0.00949146]. \t  -25.484555399307027 \t -0.013293005193297873\n",
            "96     \t [ 7.43827626 -8.5017821 ]. \t  -37646.51348997539 \t -0.013293005193297873\n",
            "97     \t [5.4040928  1.08001512]. \t  -38.26090964952843 \t -0.013293005193297873\n",
            "98     \t [ 0.9744419  -0.64215307]. \t  -0.045485839241017643 \t -0.013293005193297873\n",
            "99     \t [-4.26426686 -2.52050317]. \t  -603.6837600478647 \t -0.013293005193297873\n",
            "100    \t [-4.95102259 -9.69287804]. \t  -74421.35628980433 \t -0.013293005193297873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "any0xrgYuvxA",
        "outputId": "8a4863d8-371e-4899-8e67-958ecbc8e144"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.27896664 -3.67267997]. \t  -815.4627708918207 \t -28.992408538517264\n",
            "init   \t [3.44301557 0.15823335]. \t  -28.992408538517264 \t -28.992408538517264\n",
            "init   \t [ 5.63320853 -4.32636643]. \t  -2044.16089755496 \t -28.992408538517264\n",
            "init   \t [-5.32582432  1.27599373]. \t  -187.32245577722023 \t -28.992408538517264\n",
            "init   \t [7.50048729 4.37885841]. \t  -1945.4933626881252 \t -28.992408538517264\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -28.992408538517264\n",
            "2      \t [-0.94408028  9.32615359]. \t  -61182.65342174163 \t -28.992408538517264\n",
            "3      \t [-9.58291173  9.55394704]. \t  -73946.57346618871 \t -28.992408538517264\n",
            "4      \t [-0.59203298 -8.9817774 ]. \t  -52449.51345826705 \t -28.992408538517264\n",
            "5      \t [-5.79185865 -4.56009413]. \t  -4536.005152110485 \t -28.992408538517264\n",
            "6      \t [  7.96507791 -10.        ]. \t  -73803.33491760446 \t -28.992408538517264\n",
            "7      \t [-0.24521386 -2.8607653 ]. \t  -553.5453995096099 \t -28.992408538517264\n",
            "8      \t [-10.          -0.63562548]. \t  -354.6274359667864 \t -28.992408538517264\n",
            "9      \t [1.54287264 4.00172595]. \t  -1858.934501939139 \t -28.992408538517264\n",
            "10     \t [9.93199294 8.89866837]. \t  -44149.00661955901 \t -28.992408538517264\n",
            "11     \t [-9.0765181   3.89167944]. \t  -3201.034902033773 \t -28.992408538517264\n",
            "12     \t [4.41793994 8.49712815]. \t  -39202.96560552072 \t -28.992408538517264\n",
            "13     \t [-3.13014937  4.48362184]. \t  -3773.054951639461 \t -28.992408538517264\n",
            "14     \t [ 6.970631   -0.05954088]. \t  -132.63023454599448 \t -28.992408538517264\n",
            "15     \t [-1.3637802   0.53897056]. \t  \u001b[92m-13.15162985248233\u001b[0m \t -13.15162985248233\n",
            "16     \t [-9.84340217 -4.82425605]. \t  -6477.310099691744 \t -13.15162985248233\n",
            "17     \t [-5.28048152 -9.39587845]. \t  -66175.04900218583 \t -13.15162985248233\n",
            "18     \t [-3.23878263 -1.19946724]. \t  -92.78371751059332 \t -13.15162985248233\n",
            "19     \t [ 9.80009726 -0.4530983 ]. \t  -253.767174459005 \t -13.15162985248233\n",
            "20     \t [ 3.82037368 -8.10677306]. \t  -32581.25716192727 \t -13.15162985248233\n",
            "21     \t [-5.58596321  7.51015695]. \t  -28076.17497776049 \t -13.15162985248233\n",
            "22     \t [ 1.63660318 -0.31345176]. \t  \u001b[92m-4.553034831889238\u001b[0m \t -4.553034831889238\n",
            "23     \t [ 2.63606259 -1.17735944]. \t  \u001b[92m-2.713849590250892\u001b[0m \t -2.713849590250892\n",
            "24     \t [0.19883586 0.56412988]. \t  \u001b[92m-1.0249375764513344\u001b[0m \t -1.0249375764513344\n",
            "25     \t [ 9.59677179 -5.40648713]. \t  -4849.174932793484 \t -1.0249375764513344\n",
            "26     \t [ 0.03095551 -0.10316497]. \t  \u001b[92m-0.9392342300294892\u001b[0m \t -0.9392342300294892\n",
            "27     \t [-7.4518512  -1.44068434]. \t  -340.6927253870056 \t -0.9392342300294892\n",
            "28     \t [ 2.20764331 -2.7287567 ]. \t  -323.25569280354216 \t -0.9392342300294892\n",
            "29     \t [ 2.2824476  -0.43435585]. \t  -8.90361793855035 \t -0.9392342300294892\n",
            "30     \t [ 3.9183673  -1.31825667]. \t  -8.908951134774943 \t -0.9392342300294892\n",
            "31     \t [ 3.39262659 -1.17470379]. \t  -6.525454196200927 \t -0.9392342300294892\n",
            "32     \t [4.92712691 2.03708852]. \t  -38.16757663631995 \t -0.9392342300294892\n",
            "33     \t [ 0.44995025 -0.10957639]. \t  \u001b[92m-0.6653981582294979\u001b[0m \t -0.6653981582294979\n",
            "34     \t [4.66692272 0.30688142]. \t  -53.56150622486405 \t -0.6653981582294979\n",
            "35     \t [ 2.12311761 -0.83002032]. \t  -2.372188761384333 \t -0.6653981582294979\n",
            "36     \t [0.81989756 0.39346895]. \t  \u001b[92m-0.5531713791374688\u001b[0m \t -0.5531713791374688\n",
            "37     \t [9.98331123 3.0518692 ]. \t  -230.15464001583865 \t -0.5531713791374688\n",
            "38     \t [-0.80034209  1.4197909 ]. \t  -49.936800268480376 \t -0.5531713791374688\n",
            "39     \t [0.68323943 0.44123089]. \t  \u001b[92m-0.27305644178590766\u001b[0m \t -0.27305644178590766\n",
            "40     \t [-2.01832129 -4.82538742]. \t  -4730.512899196404 \t -0.27305644178590766\n",
            "41     \t [-1.13474372 -0.43160206]. \t  -9.10106384862732 \t -0.27305644178590766\n",
            "42     \t [-1.40079456 -0.10387474]. \t  -9.810112813031612 \t -0.27305644178590766\n",
            "43     \t [0.01240183 0.1051229 ]. \t  -0.9755383178912602 \t -0.27305644178590766\n",
            "44     \t [3.40918956 1.18415265]. \t  -6.535650500633373 \t -0.27305644178590766\n",
            "45     \t [-2.38542659  1.20209129]. \t  -67.12235528828012 \t -0.27305644178590766\n",
            "46     \t [0.82950982 0.01096577]. \t  -1.4044421387685726 \t -0.27305644178590766\n",
            "47     \t [0.75253288 0.67373354]. \t  \u001b[92m-0.10947670301413454\u001b[0m \t -0.10947670301413454\n",
            "48     \t [1.48412277 0.62122935]. \t  -1.2490346760472286 \t -0.10947670301413454\n",
            "49     \t [4.67432013 1.10826181]. \t  -23.338183015419787 \t -0.10947670301413454\n",
            "50     \t [0.54062145 0.69794402]. \t  -0.5870990673779629 \t -0.10947670301413454\n",
            "51     \t [-8.03251586  4.12578886]. \t  -3622.4977333168395 \t -0.10947670301413454\n",
            "52     \t [3.79072506 1.0628854 ]. \t  -12.477748356831995 \t -0.10947670301413454\n",
            "53     \t [-6.21234888 -9.10408776]. \t  -59207.06821301126 \t -0.10947670301413454\n",
            "54     \t [ 4.02919038 -6.52720778]. \t  -13189.460951353916 \t -0.10947670301413454\n",
            "55     \t [ 0.71733069 -0.46317606]. \t  -0.24609716901289924 \t -0.10947670301413454\n",
            "56     \t [ 7.04365201 -4.0924781 ]. \t  -1436.0589353566659 \t -0.10947670301413454\n",
            "57     \t [-5.87599396  3.20777726]. \t  -1447.0835971801562 \t -0.10947670301413454\n",
            "58     \t [ 8.52377976 -9.26055523]. \t  -53189.38260389485 \t -0.10947670301413454\n",
            "59     \t [ 6.69722723 -1.72533586]. \t  -33.56445716067143 \t -0.10947670301413454\n",
            "60     \t [ 8.15388496 -2.02679179]. \t  -51.18572945723824 \t -0.10947670301413454\n",
            "61     \t [ 7.88889842 -2.66404981]. \t  -126.9736733355154 \t -0.10947670301413454\n",
            "62     \t [ 4.49545361 -1.45649364]. \t  -12.345916787527846 \t -0.10947670301413454\n",
            "63     \t [-0.94734154 -2.76240333]. \t  -529.2610645527533 \t -0.10947670301413454\n",
            "64     \t [2.81283766 0.79729967]. \t  -8.038603677751825 \t -0.10947670301413454\n",
            "65     \t [ 8.47139825 -1.67110709]. \t  -72.48209795439618 \t -0.10947670301413454\n",
            "66     \t [ 7.59911051 -1.92789779]. \t  -43.60306036955924 \t -0.10947670301413454\n",
            "67     \t [-2.53598717 -7.62877066]. \t  -28302.273204681238 \t -0.10947670301413454\n",
            "68     \t [ 7.17931782 -0.57421152]. \t  -123.20164106679526 \t -0.10947670301413454\n",
            "69     \t [6.33797836 5.1826123 ]. \t  -4518.405148097174 \t -0.10947670301413454\n",
            "70     \t [ 2.41619727 -0.83707023]. \t  -4.065350701685278 \t -0.10947670301413454\n",
            "71     \t [-0.0710583   7.88508369]. \t  -30961.893643346364 \t -0.10947670301413454\n",
            "72     \t [-1.00200229 -1.00765412]. \t  -22.402987782058112 \t -0.10947670301413454\n",
            "73     \t [ 4.10949713 -1.83279979]. \t  -23.28078275232637 \t -0.10947670301413454\n",
            "74     \t [3.73819659 1.41966354]. \t  -7.66905843336126 \t -0.10947670301413454\n",
            "75     \t [-6.52094594  0.82226348]. \t  -180.5385669132019 \t -0.10947670301413454\n",
            "76     \t [-5.53701779  5.0656976 ]. \t  -6508.76130733897 \t -0.10947670301413454\n",
            "77     \t [5.7634928  1.40118201]. \t  -29.43905215242559 \t -0.10947670301413454\n",
            "78     \t [-0.10241599  0.11671667]. \t  -1.2489452582909175 \t -0.10947670301413454\n",
            "79     \t [ 5.17783276 -1.11985997]. \t  -31.708456144429796 \t -0.10947670301413454\n",
            "80     \t [9.33855769 4.80413045]. \t  -2781.0713822845873 \t -0.10947670301413454\n",
            "81     \t [9.89781808 2.02557904]. \t  -84.89606366968972 \t -0.10947670301413454\n",
            "82     \t [ 6.88349109 -5.5330375 ]. \t  -5941.485893656064 \t -0.10947670301413454\n",
            "83     \t [ 8.16371685 -9.0944609 ]. \t  -49509.41347498307 \t -0.10947670301413454\n",
            "84     \t [ 2.52627823 -3.2849107 ]. \t  -728.5154544669196 \t -0.10947670301413454\n",
            "85     \t [-3.52935148 -6.33099623]. \t  -14029.343310965036 \t -0.10947670301413454\n",
            "86     \t [ 9.41192064 -2.15264295]. \t  -70.8019830075267 \t -0.10947670301413454\n",
            "87     \t [ 1.52087332 -1.74486749]. \t  -42.00915806357894 \t -0.10947670301413454\n",
            "88     \t [-8.10679451 -3.97384878]. \t  -3233.485078639014 \t -0.10947670301413454\n",
            "89     \t [0.61124987 4.40812793]. \t  -2926.5720044905806 \t -0.10947670301413454\n",
            "90     \t [ 1.33338474 -6.7834835 ]. \t  -16452.33364704236 \t -0.10947670301413454\n",
            "91     \t [5.10551579 7.71280412]. \t  -25949.235124425155 \t -0.10947670301413454\n",
            "92     \t [-0.34321087 -7.74449847]. \t  -28944.888935127223 \t -0.10947670301413454\n",
            "93     \t [-7.69801172 -6.19002446]. \t  -14299.028906402948 \t -0.10947670301413454\n",
            "94     \t [-4.58912948  6.6451055 ]. \t  -17293.53875567509 \t -0.10947670301413454\n",
            "95     \t [2.81436951 1.12710952]. \t  -3.4416700738560446 \t -0.10947670301413454\n",
            "96     \t [ 3.00188445 -1.29398595]. \t  -4.248241124859602 \t -0.10947670301413454\n",
            "97     \t [1.96532329 0.94996542]. \t  -0.9833404761362226 \t -0.10947670301413454\n",
            "98     \t [5.05108621 5.33718012]. \t  -5407.76273916 \t -0.10947670301413454\n",
            "99     \t [-7.08194145  8.05431634]. \t  -37508.00702857726 \t -0.10947670301413454\n",
            "100    \t [-6.40214562  9.5152672 ]. \t  -70354.36649607628 \t -0.10947670301413454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLyKt6Quvzx",
        "outputId": "2e8134bf-af1d-45e5-b01a-9d27b529bd6b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.03648056 4.61039249]. \t  -3120.691750998894 \t -133.8839693070206\n",
            "init   \t [-6.13265275  0.39354476]. \t  -133.8839693070206 \t -133.8839693070206\n",
            "init   \t [3.52766533 6.08889746]. \t  -9981.232217710998 \t -133.8839693070206\n",
            "init   \t [0.67025729 8.16068359]. \t  -35124.935571397844 \t -133.8839693070206\n",
            "init   \t [-1.13037742 -4.8805558 ]. \t  -4761.569630397088 \t -133.8839693070206\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -133.8839693070206\n",
            "2      \t [ 8.81602768 -5.83496092]. \t  -7088.736716131159 \t -133.8839693070206\n",
            "3      \t [-9.20794452  7.56905046]. \t  -30751.632331903027 \t -133.8839693070206\n",
            "4      \t [9.38422178 1.56611414]. \t  \u001b[92m-110.41438012079544\u001b[0m \t -110.41438012079544\n",
            "5      \t [9.65812984 9.94541442]. \t  -70886.65075415972 \t -110.41438012079544\n",
            "6      \t [  3.47186906 -10.        ]. \t  -77252.72263625677 \t -110.41438012079544\n",
            "7      \t [-10.          -3.40609209]. \t  -2325.868671116018 \t -110.41438012079544\n",
            "8      \t [ 4.40164635 -1.09432882]. \t  \u001b[92m-19.623565232231844\u001b[0m \t -19.623565232231844\n",
            "9      \t [ -3.66266664 -10.        ]. \t  -82978.7040300754 \t -19.623565232231844\n",
            "10     \t [-0.48952655  0.82299161]. \t  \u001b[92m-9.02051887654278\u001b[0m \t -9.02051887654278\n",
            "11     \t [ 4.11034628 -4.77421313]. \t  -3450.170805605411 \t -9.02051887654278\n",
            "12     \t [-3.60023019  4.03284354]. \t  -2631.6101602627455 \t -9.02051887654278\n",
            "13     \t [-10.           2.02536711]. \t  -783.7875332522349 \t -9.02051887654278\n",
            "14     \t [-5.84426959 -4.76713795]. \t  -5309.296162041112 \t -9.02051887654278\n",
            "15     \t [5.87325794 1.33501367]. \t  -34.409156782756526 \t -9.02051887654278\n",
            "16     \t [2.1096475  0.26706145]. \t  \u001b[92m-8.969526004451877\u001b[0m \t -8.969526004451877\n",
            "17     \t [8.94935273 4.74793873]. \t  -2674.8842545175467 \t -8.969526004451877\n",
            "18     \t [-2.7803316  -0.61035297]. \t  -39.14769986366584 \t -8.969526004451877\n",
            "19     \t [ 6.97143126 -1.50528318]. \t  -47.56203283739297 \t -8.969526004451877\n",
            "20     \t [-4.31603947  9.03343692]. \t  -56155.50437230632 \t -8.969526004451877\n",
            "21     \t [0.07184013 2.71879997]. \t  -433.74186423537907 \t -8.969526004451877\n",
            "22     \t [-1.08936955  0.30659647]. \t  \u001b[92m-7.628825319510929\u001b[0m \t -7.628825319510929\n",
            "23     \t [-0.66912713  0.49402902]. \t  \u001b[92m-5.4644705173106995\u001b[0m \t -5.4644705173106995\n",
            "24     \t [-0.99388893  0.67332361]. \t  -11.200292970644584 \t -5.4644705173106995\n",
            "25     \t [ 1.21904659 -0.62562383]. \t  \u001b[92m-0.4285855229412167\u001b[0m \t -0.4285855229412167\n",
            "26     \t [0.23241781 0.49599077]. \t  -0.7239624577660823 \t -0.4285855229412167\n",
            "27     \t [-1.22179221 -0.91680432]. \t  -21.789466423536144 \t -0.4285855229412167\n",
            "28     \t [ 1.60553338 -1.29162761]. \t  -6.359880092582057 \t -0.4285855229412167\n",
            "29     \t [  8.47261654 -10.        ]. \t  -73421.31722952986 \t -0.4285855229412167\n",
            "30     \t [ 0.78085351 -1.11597829]. \t  -5.89596239273167 \t -0.4285855229412167\n",
            "31     \t [-7.2405433  3.6542144]. \t  -2372.7189854361277 \t -0.4285855229412167\n",
            "32     \t [ 9.14025457 -1.48686197]. \t  -110.79671194174874 \t -0.4285855229412167\n",
            "33     \t [4.4425982  0.21561349]. \t  -49.68986783908259 \t -0.4285855229412167\n",
            "34     \t [4.53817702 9.84643969]. \t  -71731.91582202556 \t -0.4285855229412167\n",
            "35     \t [3.37773423 1.82837463]. \t  -27.5416416202328 \t -0.4285855229412167\n",
            "36     \t [ 1.92822454 -0.13695529]. \t  -8.011177273607737 \t -0.4285855229412167\n",
            "37     \t [ 3.19269616 -1.17287301]. \t  -5.1976443590780255 \t -0.4285855229412167\n",
            "38     \t [0.87398899 0.43288011]. \t  -0.5143172272325615 \t -0.4285855229412167\n",
            "39     \t [-4.32521197 -1.4076729 ]. \t  -165.74964857457095 \t -0.4285855229412167\n",
            "40     \t [ 1.13825091 -0.011535  ]. \t  -2.609132106809849 \t -0.4285855229412167\n",
            "41     \t [-2.13032677  0.0429478 ]. \t  -18.906992501480126 \t -0.4285855229412167\n",
            "42     \t [7.52748488 0.64187579]. \t  -132.4812346999585 \t -0.4285855229412167\n",
            "43     \t [ 2.82522772 -1.0140684 ]. \t  -4.512819869158035 \t -0.4285855229412167\n",
            "44     \t [2.37867083 1.27196326]. \t  -3.3700091519790423 \t -0.4285855229412167\n",
            "45     \t [ 1.76595693 -3.04971269]. \t  -567.4573711313951 \t -0.4285855229412167\n",
            "46     \t [1.76785012 0.67465846]. \t  -2.0602819143244298 \t -0.4285855229412167\n",
            "47     \t [5.48982849 2.01291751]. \t  -33.82293489953708 \t -0.4285855229412167\n",
            "48     \t [0.97258236 0.39414949]. \t  -0.876908024914944 \t -0.4285855229412167\n",
            "49     \t [ 1.84277394 -0.99182988]. \t  -0.7413576560665873 \t -0.4285855229412167\n",
            "50     \t [ 1.6603376  -1.14892722]. \t  -2.3557872097884056 \t -0.4285855229412167\n",
            "51     \t [8.46779831 7.12961414]. \t  -17426.383009816836 \t -0.4285855229412167\n",
            "52     \t [-8.87122825 -6.11949031]. \t  -14131.446577500716 \t -0.4285855229412167\n",
            "53     \t [-7.01007846 -1.14467003]. \t  -249.65894016951592 \t -0.4285855229412167\n",
            "54     \t [1.14493633 0.64073547]. \t  \u001b[92m-0.23076734911195318\u001b[0m \t -0.23076734911195318\n",
            "55     \t [6.33522202 2.62135662]. \t  -138.2155666086074 \t -0.23076734911195318\n",
            "56     \t [ 1.92185207 -6.41993558]. \t  -12964.342570814926 \t -0.23076734911195318\n",
            "57     \t [ 2.06816624 -1.19286077]. \t  -2.3505122608645435 \t -0.23076734911195318\n",
            "58     \t [5.21559652 1.3983291 ]. \t  -21.177032512686978 \t -0.23076734911195318\n",
            "59     \t [ 0.03828497 -4.77979984]. \t  -4169.625990517897 \t -0.23076734911195318\n",
            "60     \t [-5.50634066  4.88851737]. \t  -5724.441837423282 \t -0.23076734911195318\n",
            "61     \t [8.76679784 4.29391193]. \t  -1640.5056011511847 \t -0.23076734911195318\n",
            "62     \t [-8.98890867  0.60653298]. \t  -288.9168339602197 \t -0.23076734911195318\n",
            "63     \t [1.92514088 1.03845944]. \t  -0.9632138562469709 \t -0.23076734911195318\n",
            "64     \t [ 2.45485905 -8.43743202]. \t  -39160.49674376253 \t -0.23076734911195318\n",
            "65     \t [-3.55615229  0.65539769]. \t  -59.747292713954124 \t -0.23076734911195318\n",
            "66     \t [-1.85054602 -0.57104   ]. \t  -20.65282131045403 \t -0.23076734911195318\n",
            "67     \t [0.31687549 0.60476593]. \t  -0.8104589458355942 \t -0.23076734911195318\n",
            "68     \t [-9.0776245  -0.18304074]. \t  -268.8071160142377 \t -0.23076734911195318\n",
            "69     \t [ 0.86268324 -0.20039651]. \t  -1.2430481436937413 \t -0.23076734911195318\n",
            "70     \t [-0.182838    5.53966783]. \t  -7580.3405946686125 \t -0.23076734911195318\n",
            "71     \t [1.25853039 7.04421599]. \t  -19201.570925244465 \t -0.23076734911195318\n",
            "72     \t [8.06193148 8.60590626]. \t  -39284.21404271839 \t -0.23076734911195318\n",
            "73     \t [8.18071397 2.55809059]. \t  -99.71879318353649 \t -0.23076734911195318\n",
            "74     \t [-3.02927854 -1.38291775]. \t  -110.19524358375693 \t -0.23076734911195318\n",
            "75     \t [7.71939719 1.3140332 ]. \t  -81.54833457870807 \t -0.23076734911195318\n",
            "76     \t [-9.21247784  5.10503346]. \t  -7628.3109577788855 \t -0.23076734911195318\n",
            "77     \t [-5.41577237  1.00350708]. \t  -151.56674301338865 \t -0.23076734911195318\n",
            "78     \t [-4.97692702  8.28742049]. \t  -40556.84917917206 \t -0.23076734911195318\n",
            "79     \t [9.37697898 0.9479332 ]. \t  -185.08124840416218 \t -0.23076734911195318\n",
            "80     \t [4.92905531 1.29978043]. \t  -20.243697047984963 \t -0.23076734911195318\n",
            "81     \t [1.55605664 5.36534893]. \t  -6276.315307365103 \t -0.23076734911195318\n",
            "82     \t [ 2.15702298 -1.01548418]. \t  -1.3566030334491161 \t -0.23076734911195318\n",
            "83     \t [9.63573364 2.38494157]. \t  -80.63220154824702 \t -0.23076734911195318\n",
            "84     \t [4.83509331 2.71011485]. \t  -208.92443335302386 \t -0.23076734911195318\n",
            "85     \t [ 0.41241383 -0.75033658]. \t  -1.3636963983763384 \t -0.23076734911195318\n",
            "86     \t [3.65612439 0.84169355]. \t  -17.08328371258108 \t -0.23076734911195318\n",
            "87     \t [2.37416828 2.43785355]. \t  -182.84811143666028 \t -0.23076734911195318\n",
            "88     \t [ 0.07728894 -0.57940525]. \t  -1.557381264160874 \t -0.23076734911195318\n",
            "89     \t [2.94132707 0.99654688]. \t  -5.593242756039755 \t -0.23076734911195318\n",
            "90     \t [2.2903605  1.00965938]. \t  -1.7915713285785562 \t -0.23076734911195318\n",
            "91     \t [ 6.96562817 -2.40903003]. \t  -78.6706257254646 \t -0.23076734911195318\n",
            "92     \t [-4.91703696 -6.17035295]. \t  -13177.608925696788 \t -0.23076734911195318\n",
            "93     \t [0.78199885 1.18467404]. \t  -8.24801580874012 \t -0.23076734911195318\n",
            "94     \t [3.32960703 1.51822677]. \t  -8.706009480528564 \t -0.23076734911195318\n",
            "95     \t [ 3.71376938 -3.98894139]. \t  -1587.6567265140147 \t -0.23076734911195318\n",
            "96     \t [0.92084033 0.80901701]. \t  -0.3076285578771469 \t -0.23076734911195318\n",
            "97     \t [ 3.50179272 -1.61930937]. \t  -12.331809060118754 \t -0.23076734911195318\n",
            "98     \t [ 3.63808948 -2.08005183]. \t  -57.26280967866541 \t -0.23076734911195318\n",
            "99     \t [0.57308471 0.50838026]. \t  \u001b[92m-0.18856988701464567\u001b[0m \t -0.18856988701464567\n",
            "100    \t [ 5.6383431  -1.17743875]. \t  -37.93777214261284 \t -0.18856988701464567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3FtfYSuv2u",
        "outputId": "cda5722d-1322-4f0e-9d17-034c4fea3686"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [7.46858806 9.37081326]. \t  -56594.51755971677 \t -0.3114572247523004\n",
            "init   \t [7.3838908  0.61711383]. \t  -128.46197097057788 \t -0.3114572247523004\n",
            "init   \t [-5.34543344 -9.77202391]. \t  -77131.46635795479 \t -0.3114572247523004\n",
            "init   \t [-1.39062363 -1.9529728 ]. \t  -168.39363959555394 \t -0.3114572247523004\n",
            "init   \t [ 0.45349343 -0.43216408]. \t  -0.3114572247523004 \t -0.3114572247523004\n",
            "1      \t [1.13550865 0.60170966]. \t  -0.3568618976266401 \t -0.3114572247523004\n",
            "2      \t [ 1.67634189 -0.11922249]. \t  -5.888678715306406 \t -0.3114572247523004\n",
            "3      \t [0.12526212 2.11923098]. \t  -157.65869365560948 \t -0.3114572247523004\n",
            "4      \t [-9.29644966  8.17908998]. \t  -41056.3576701472 \t -0.3114572247523004\n",
            "5      \t [ 0.64284568 -0.23268903]. \t  -0.6990622504736652 \t -0.3114572247523004\n",
            "6      \t [1.57583895 1.11784167]. \t  -2.0365601424077386 \t -0.3114572247523004\n",
            "7      \t [1.1541479  0.14140931]. \t  -2.5064430341229795 \t -0.3114572247523004\n",
            "8      \t [ 2.8066687  -0.25783649]. \t  -17.561495661537435 \t -0.3114572247523004\n",
            "9      \t [ 2.30910669 -2.39677701]. \t  -170.25758449036078 \t -0.3114572247523004\n",
            "10     \t [1.80849446 0.14477759]. \t  -6.895226115446554 \t -0.3114572247523004\n",
            "11     \t [ 9.78631135 -7.75205722]. \t  -24454.610388035806 \t -0.3114572247523004\n",
            "12     \t [-9.26484186 -0.28708101]. \t  -283.20443930592813 \t -0.3114572247523004\n",
            "13     \t [ 2.26123231 -9.99183678]. \t  -77944.88004584343 \t -0.3114572247523004\n",
            "14     \t [0.55227379 8.76082007]. \t  -46788.62272324322 \t -0.3114572247523004\n",
            "15     \t [-10.          -5.57681886]. \t  -10547.20481040476 \t -0.3114572247523004\n",
            "16     \t [-5.01404127  2.64994262]. \t  -762.6164357121266 \t -0.3114572247523004\n",
            "17     \t [4.09233487 4.4038803 ]. \t  -2417.185777374596 \t -0.3114572247523004\n",
            "18     \t [ 5.76811032 -4.17509703]. \t  -1715.7450119072448 \t -0.3114572247523004\n",
            "19     \t [-5.5667289  -2.78948455]. \t  -936.0061718898531 \t -0.3114572247523004\n",
            "20     \t [9.58184694 3.94545659]. \t  -1002.574515868559 \t -0.3114572247523004\n",
            "21     \t [-3.73963118  6.35731762]. \t  -14326.838862426035 \t -0.3114572247523004\n",
            "22     \t [-1.21521995 -6.20358235]. \t  -12230.410368156558 \t -0.3114572247523004\n",
            "23     \t [-9.37099714  3.22636709]. \t  -1930.4160634038535 \t -0.3114572247523004\n",
            "24     \t [4.12617506 0.42047136]. \t  -38.23773138618567 \t -0.3114572247523004\n",
            "25     \t [ 0.59682223 -0.51509253]. \t  \u001b[92m-0.17131232405838073\u001b[0m \t -0.17131232405838073\n",
            "26     \t [ 9.26088527 -2.01734757]. \t  -70.75776274135734 \t -0.17131232405838073\n",
            "27     \t [-2.30664115  0.91232419]. \t  -42.47651390292444 \t -0.17131232405838073\n",
            "28     \t [ 5.46217487 -7.50830982]. \t  -23041.02232975199 \t -0.17131232405838073\n",
            "29     \t [-10. -10.]. \t  -88321.0 \t -0.17131232405838073\n",
            "30     \t [-5.37682421e-01 -3.09277148e-04]. \t  -2.9426724109848945 \t -0.17131232405838073\n",
            "31     \t [-5.66396942  9.89011051]. \t  -81081.7827527508 \t -0.17131232405838073\n",
            "32     \t [-6.65633714  0.30124334]. \t  -152.1313979342619 \t -0.17131232405838073\n",
            "33     \t [0.3868365  0.13631452]. \t  -0.6205121769831037 \t -0.17131232405838073\n",
            "34     \t [ 0.01735151 -0.3165693 ]. \t  -1.0326351704235495 \t -0.17131232405838073\n",
            "35     \t [9.7801873  0.77394804]. \t  -224.39987157760925 \t -0.17131232405838073\n",
            "36     \t [-3.93256088 -0.27586929]. \t  -57.70082597217292 \t -0.17131232405838073\n",
            "37     \t [0.15103147 4.41328257]. \t  -3012.081037622414 \t -0.17131232405838073\n",
            "38     \t [ 5.7145614  -0.45703053]. \t  -78.33943039495534 \t -0.17131232405838073\n",
            "39     \t [-1.04480107  1.34514129]. \t  -47.67975135147729 \t -0.17131232405838073\n",
            "40     \t [-0.12444779  0.79872309]. \t  -5.186426702021285 \t -0.17131232405838073\n",
            "41     \t [-0.04832628 -0.05751505]. \t  -1.1050252793242605 \t -0.17131232405838073\n",
            "42     \t [ 9.65278295 -3.62480762]. \t  -627.6969666536363 \t -0.17131232405838073\n",
            "43     \t [2.51330555 4.04324265]. \t  -1824.2345687737002 \t -0.17131232405838073\n",
            "44     \t [0.46214813 1.17307877]. \t  -10.778212561264654 \t -0.17131232405838073\n",
            "45     \t [6.94956323 2.62129441]. \t  -127.68171603815364 \t -0.17131232405838073\n",
            "46     \t [ 2.31939042 -4.61754847]. \t  -3253.809682553002 \t -0.17131232405838073\n",
            "47     \t [ 8.42564254 -1.32963664]. \t  -102.95997333157376 \t -0.17131232405838073\n",
            "48     \t [1.52673045 0.48621561]. \t  -2.4989363639196758 \t -0.17131232405838073\n",
            "49     \t [ 0.66043971 -1.40834511]. \t  -21.98028855234697 \t -0.17131232405838073\n",
            "50     \t [ 3.92571954 -1.22337911]. \t  -10.2985990501231 \t -0.17131232405838073\n",
            "51     \t [ 2.65437388 -1.03915466]. \t  -3.226387490781242 \t -0.17131232405838073\n",
            "52     \t [ 1.53800134 -1.12122053]. \t  -2.195650186198717 \t -0.17131232405838073\n",
            "53     \t [1.91938405 0.72577657]. \t  -2.344766115088218 \t -0.17131232405838073\n",
            "54     \t [ 9.92406052 -1.21631959]. \t  -176.66670598454985 \t -0.17131232405838073\n",
            "55     \t [2.50544651 0.48761377]. \t  -10.507455765258028 \t -0.17131232405838073\n",
            "56     \t [-4.74828467 -2.63371785]. \t  -726.542753551181 \t -0.17131232405838073\n",
            "57     \t [ 2.82235723 -1.76965861]. \t  -27.00230515863079 \t -0.17131232405838073\n",
            "58     \t [1.09198943 9.34927193]. \t  -60361.30298145525 \t -0.17131232405838073\n",
            "59     \t [-5.12175789 -5.55287161]. \t  -8959.424154188324 \t -0.17131232405838073\n",
            "60     \t [ 2.13566733 -1.3353243 ]. \t  -5.3824846806931514 \t -0.17131232405838073\n",
            "61     \t [ 9.37102078 -4.5953657 ]. \t  -2230.126257753908 \t -0.17131232405838073\n",
            "62     \t [ 3.00439812 -0.20606656]. \t  -21.06423677740036 \t -0.17131232405838073\n",
            "63     \t [ 9.98206191 -2.50070497]. \t  -93.42857266138743 \t -0.17131232405838073\n",
            "64     \t [ 2.91741373 -6.11095378]. \t  -10305.570540281722 \t -0.17131232405838073\n",
            "65     \t [ 3.90927432 -1.68601516]. \t  -14.772370702381068 \t -0.17131232405838073\n",
            "66     \t [ 5.80697974 -3.99152321]. \t  -1381.0973566289983 \t -0.17131232405838073\n",
            "67     \t [ 7.1112541  -3.07034068]. \t  -313.1308269798512 \t -0.17131232405838073\n",
            "68     \t [ 5.66563522 -1.74182199]. \t  -22.091766105963597 \t -0.17131232405838073\n",
            "69     \t [-6.18178689  6.26894534]. \t  -14427.260880981674 \t -0.17131232405838073\n",
            "70     \t [ 1.75732497 -0.87280213]. \t  -0.6828265860784071 \t -0.17131232405838073\n",
            "71     \t [0.35895092 0.57094438]. \t  -0.5826466577255895 \t -0.17131232405838073\n",
            "72     \t [-4.11978008  9.49501985]. \t  -68055.49271965375 \t -0.17131232405838073\n",
            "73     \t [ 5.6947737  -2.98164198]. \t  -314.1645512054774 \t -0.17131232405838073\n",
            "74     \t [ 5.30261968 -1.49998356]. \t  -19.801249461820515 \t -0.17131232405838073\n",
            "75     \t [ 3.30625134 -1.49496363]. \t  -8.02663741410172 \t -0.17131232405838073\n",
            "76     \t [8.15566945 8.31233999]. \t  -33869.05380235113 \t -0.17131232405838073\n",
            "77     \t [2.75282387 1.19886267]. \t  -3.1020228009022337 \t -0.17131232405838073\n",
            "78     \t [-4.30895206  9.99717508]. \t  -83420.17417554371 \t -0.17131232405838073\n",
            "79     \t [ 5.87100194 -1.06125949]. \t  -49.913144342822406 \t -0.17131232405838073\n",
            "80     \t [4.92638197 0.89556046]. \t  -37.49216083441669 \t -0.17131232405838073\n",
            "81     \t [-0.58991652  7.26217391]. \t  -22503.477832359345 \t -0.17131232405838073\n",
            "82     \t [ 6.20097963 -8.15946825]. \t  -32261.109856807794 \t -0.17131232405838073\n",
            "83     \t [0.18833227 9.67605813]. \t  -69986.44152893897 \t -0.17131232405838073\n",
            "84     \t [5.89672057 3.89720334]. \t  -1222.4860890315838 \t -0.17131232405838073\n",
            "85     \t [ 1.86863242 -6.05169292]. \t  -10190.204384489207 \t -0.17131232405838073\n",
            "86     \t [6.78639434 1.59289557]. \t  -39.34261612961099 \t -0.17131232405838073\n",
            "87     \t [-0.5543762   4.56040096]. \t  -3555.4739534323253 \t -0.17131232405838073\n",
            "88     \t [-4.5253429  -5.57373991]. \t  -8917.239622642455 \t -0.17131232405838073\n",
            "89     \t [2.14992986 1.02753381]. \t  -1.3252691653884492 \t -0.17131232405838073\n",
            "90     \t [ 0.94865285 -0.82380483]. \t  -0.3366358863896891 \t -0.17131232405838073\n",
            "91     \t [-1.41444092 -3.81254297]. \t  -1864.549814117909 \t -0.17131232405838073\n",
            "92     \t [7.46899251 2.00440936]. \t  -42.48930372505129 \t -0.17131232405838073\n",
            "93     \t [-2.096222   -0.61984963]. \t  -25.999019961128703 \t -0.17131232405838073\n",
            "94     \t [7.72771931 8.68302332]. \t  -40978.77317930908 \t -0.17131232405838073\n",
            "95     \t [-1.46501658 -2.1326753 ]. \t  -229.1721286101595 \t -0.17131232405838073\n",
            "96     \t [ 7.96616065 -5.40487512]. \t  -5140.787007532487 \t -0.17131232405838073\n",
            "97     \t [-2.85345816 -9.83264323]. \t  -77015.66153958983 \t -0.17131232405838073\n",
            "98     \t [4.97090132 6.92727254]. \t  -16578.97228770677 \t -0.17131232405838073\n",
            "99     \t [ 9.77428042 -6.17916373]. \t  -8945.403788429308 \t -0.17131232405838073\n",
            "100    \t [-0.98210894  5.46957316]. \t  -7400.7526049408825 \t -0.17131232405838073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YT-CgKvuv4q",
        "outputId": "472f555f-e4fc-4963-f92b-3216290e04d1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.18891153 5.22983981]. \t  -5727.433065247983 \t -79.36780179787098\n",
            "init   \t [-5.10936891  5.43378939]. \t  -8270.721382692287 \t -79.36780179787098\n",
            "init   \t [-2.91055837  5.93394799]. \t  -10771.054116716396 \t -79.36780179787098\n",
            "init   \t [-3.07999955 -1.12251129]. \t  -79.36780179787098 \t -79.36780179787098\n",
            "init   \t [-3.93392821 -1.1577216 ]. \t  -111.8486364354089 \t -79.36780179787098\n",
            "1      \t [ 1.46418724 -8.68671967]. \t  -44673.21769976243 \t -79.36780179787098\n",
            "2      \t [ 9.77192101 -1.83559751]. \t  -95.34580222090166 \t -79.36780179787098\n",
            "3      \t [-10.          -9.90826103]. \t  -85279.39447828391 \t -79.36780179787098\n",
            "4      \t [9.60900479 6.31645299]. \t  -9926.306792942905 \t -79.36780179787098\n",
            "5      \t [ 3.64268513 -1.21478073]. \t  \u001b[92m-7.939578013586245\u001b[0m \t -7.939578013586245\n",
            "6      \t [-9.8876954  -2.46578762]. \t  -1090.7628096346386 \t -7.939578013586245\n",
            "7      \t [ 8.31994416 -7.65544864]. \t  -23768.448806122535 \t -7.939578013586245\n",
            "8      \t [-4.83750344 -6.21023539]. \t  -13472.746706735039 \t -7.939578013586245\n",
            "9      \t [6.43501344 1.67197012]. \t  -30.964195961948846 \t -7.939578013586245\n",
            "10     \t [-10.           2.79292422]. \t  -1431.8071805892557 \t -7.939578013586245\n",
            "11     \t [-9.75067889  9.53644709]. \t  -73566.07228821774 \t -7.939578013586245\n",
            "12     \t [ 0.30965499 -3.12056982]. \t  -735.1673883943184 \t -7.939578013586245\n",
            "13     \t [4.78533373 8.98851618]. \t  -49187.75733270863 \t -7.939578013586245\n",
            "14     \t [ 5.75202169 -3.49672209]. \t  -722.1194772732323 \t -7.939578013586245\n",
            "15     \t [1.74133887 0.7363923 ]. \t  \u001b[92m-1.4123338159656569\u001b[0m \t -1.4123338159656569\n",
            "16     \t [2.5723173  0.92539981]. \t  -3.9499637311384097 \t -1.4123338159656569\n",
            "17     \t [9.70250737 1.09497334]. \t  -182.44724067822602 \t -1.4123338159656569\n",
            "18     \t [3.28533039e-03 9.54204293e+00]. \t  -66320.270226604 \t -1.4123338159656569\n",
            "19     \t [-0.36062402  0.92125256]. \t  -10.322326835202269 \t -1.4123338159656569\n",
            "20     \t [ 2.18621939 -0.00238434]. \t  -10.966127485590931 \t -1.4123338159656569\n",
            "21     \t [-6.68328867 -0.34704082]. \t  -154.9210004185833 \t -1.4123338159656569\n",
            "22     \t [-2.63397908  1.56411541]. \t  -126.51404406282077 \t -1.4123338159656569\n",
            "23     \t [3.66532046 1.80275621]. \t  -23.173161137757575 \t -1.4123338159656569\n",
            "24     \t [4.77021047 4.16010428]. \t  -1795.3909309623725 \t -1.4123338159656569\n",
            "25     \t [ 6.75011944 -0.63882227]. \t  -103.4869635145468 \t -1.4123338159656569\n",
            "26     \t [-5.15925848  9.75131885]. \t  -76350.00555886983 \t -1.4123338159656569\n",
            "27     \t [ 4.62212744 -0.25358159]. \t  -53.503254721084026 \t -1.4123338159656569\n",
            "28     \t [3.21416638 1.39740126]. \t  -5.858308093983411 \t -1.4123338159656569\n",
            "29     \t [0.71187799 1.6379167 ]. \t  -43.39619645864269 \t -1.4123338159656569\n",
            "30     \t [-1.02357626  0.17889897]. \t  -6.460547273450576 \t -1.4123338159656569\n",
            "31     \t [-6.44811119  1.82180537]. \t  -397.9643338459968 \t -1.4123338159656569\n",
            "32     \t [ -3.42690912 -10.        ]. \t  -82784.61222948559 \t -1.4123338159656569\n",
            "33     \t [ 3.03908229 -2.31977431]. \t  -123.46657395998766 \t -1.4123338159656569\n",
            "34     \t [-9.11374135 -5.41180735]. \t  -9265.905169818247 \t -1.4123338159656569\n",
            "35     \t [ 2.99903421 -0.31207157]. \t  -19.723850948687268 \t -1.4123338159656569\n",
            "36     \t [2.8662484  1.52275885]. \t  -9.75817822447908 \t -1.4123338159656569\n",
            "37     \t [ 3.28435675 -4.63882065]. \t  -3165.813498679299 \t -1.4123338159656569\n",
            "38     \t [2.06983724 1.25795395]. \t  -3.542860264543693 \t -1.4123338159656569\n",
            "39     \t [7.87131506 2.52055306]. \t  -93.97058957488775 \t -1.4123338159656569\n",
            "40     \t [-0.18777685 -0.14018029]. \t  -1.5139425615306104 \t -1.4123338159656569\n",
            "41     \t [-6.1345957  -2.52099597]. \t  -761.2034500023925 \t -1.4123338159656569\n",
            "42     \t [ 9.16444022 -3.57509708]. \t  -604.4598798975373 \t -1.4123338159656569\n",
            "43     \t [-0.55451056 -0.09011807]. \t  -3.0680212516129677 \t -1.4123338159656569\n",
            "44     \t [ 1.43761199 -0.50218138]. \t  -1.9333770041252007 \t -1.4123338159656569\n",
            "45     \t [1.3957547 0.2511028]. \t  -3.3806413455333684 \t -1.4123338159656569\n",
            "46     \t [ 2.78241515 -1.57106518]. \t  -12.45709468003966 \t -1.4123338159656569\n",
            "47     \t [ 3.33956827 -1.29796586]. \t  -5.475363199432503 \t -1.4123338159656569\n",
            "48     \t [ 0.63479053 -0.22095425]. \t  \u001b[92m-0.710435995546919\u001b[0m \t -0.710435995546919\n",
            "49     \t [-1.18488837 -3.71778966]. \t  -1666.9741100188212 \t -0.710435995546919\n",
            "50     \t [1.92865344 0.65127372]. \t  -3.1966598574705167 \t -0.710435995546919\n",
            "51     \t [-0.1300489   0.09908314]. \t  -1.3218210282368792 \t -0.710435995546919\n",
            "52     \t [-4.84351285 -1.7428499 ]. \t  -272.57673817873376 \t -0.710435995546919\n",
            "53     \t [ 3.50942493 -1.5853331 ]. \t  -10.90062386814027 \t -0.710435995546919\n",
            "54     \t [7.33480785 0.61386146]. \t  -126.75302120090174 \t -0.710435995546919\n",
            "55     \t [-0.90828624 -8.44749568]. \t  -41262.02132907984 \t -0.710435995546919\n",
            "56     \t [-9.88305268  3.57345874]. \t  -2627.9148983338264 \t -0.710435995546919\n",
            "57     \t [-8.40821609  0.69948889]. \t  -264.7380144356216 \t -0.710435995546919\n",
            "58     \t [-1.53378448  4.44504863]. \t  -3376.7398542969686 \t -0.710435995546919\n",
            "59     \t [9.53325184 9.96976773]. \t  -71710.9705251735 \t -0.710435995546919\n",
            "60     \t [-0.71799791  0.25619226]. \t  -4.394025244878004 \t -0.710435995546919\n",
            "61     \t [0.95205653 0.65289855]. \t  \u001b[92m-0.02210046413617886\u001b[0m \t -0.02210046413617886\n",
            "62     \t [3.78089228 1.02055329]. \t  -13.498644006284838 \t -0.02210046413617886\n",
            "63     \t [-5.26775709  4.094441  ]. \t  -3049.6450899565607 \t -0.02210046413617886\n",
            "64     \t [1.17571297 0.88564968]. \t  -0.3398323941865823 \t -0.02210046413617886\n",
            "65     \t [-8.51581064 -6.35642805]. \t  -16048.163840611456 \t -0.02210046413617886\n",
            "66     \t [-0.12631791 -8.28057783]. \t  -37683.12253195882 \t -0.02210046413617886\n",
            "67     \t [ 8.99129795 -2.75625089]. \t  -140.80384847932044 \t -0.02210046413617886\n",
            "68     \t [1.419442   4.07963932]. \t  -2031.2474747312667 \t -0.02210046413617886\n",
            "69     \t [ 7.94287408 -1.37985042]. \t  -82.39829188515836 \t -0.02210046413617886\n",
            "70     \t [1.12299172 2.78227148]. \t  -412.3813389976626 \t -0.02210046413617886\n",
            "71     \t [5.65992908 0.81195635]. \t  -59.41014923660819 \t -0.02210046413617886\n",
            "72     \t [0.42442235 0.76532345]. \t  -1.4473602534846945 \t -0.02210046413617886\n",
            "73     \t [-2.05073054 -8.33771244]. \t  -39819.615905485785 \t -0.02210046413617886\n",
            "74     \t [ 8.4179015  -2.46283236]. \t  -82.60074836574749 \t -0.02210046413617886\n",
            "75     \t [0.24378285 1.27811245]. \t  -18.85327585862136 \t -0.02210046413617886\n",
            "76     \t [5.95833852 5.32796835]. \t  -5189.148320446915 \t -0.02210046413617886\n",
            "77     \t [2.07476496 1.02182503]. \t  -1.155483559635286 \t -0.02210046413617886\n",
            "78     \t [-1.24901293 -9.33659734]. \t  -61670.94055074521 \t -0.02210046413617886\n",
            "79     \t [-1.26261038 -0.69624354]. \t  -15.084129560292531 \t -0.02210046413617886\n",
            "80     \t [-0.23602593  2.10356679]. \t  -166.63900281264824 \t -0.02210046413617886\n",
            "81     \t [-0.95850902  0.87228282]. \t  -16.13917317971884 \t -0.02210046413617886\n",
            "82     \t [ 7.79848131 -4.21075841]. \t  -1576.6462240394658 \t -0.02210046413617886\n",
            "83     \t [-4.80892289  3.33160919]. \t  -1492.625271016946 \t -0.02210046413617886\n",
            "84     \t [-8.88001853  5.7413491 ]. \t  -11289.549130960664 \t -0.02210046413617886\n",
            "85     \t [ 0.41227223 -0.62033464]. \t  -0.6008332574778612 \t -0.02210046413617886\n",
            "86     \t [-5.96788264  6.81807644]. \t  -19626.883857233865 \t -0.02210046413617886\n",
            "87     \t [-4.4992704  -8.03524031]. \t  -35743.89855337572 \t -0.02210046413617886\n",
            "88     \t [ 1.8961735  -1.32638718]. \t  -6.067700625202037 \t -0.02210046413617886\n",
            "89     \t [-2.79604495  4.4167307 ]. \t  -3510.74028683285 \t -0.02210046413617886\n",
            "90     \t [-0.07760973 -0.71279362]. \t  -3.5538611791910335 \t -0.02210046413617886\n",
            "91     \t [ 1.43867457 -1.01774934]. \t  -0.9936940752091266 \t -0.02210046413617886\n",
            "92     \t [0.53510524 0.30376131]. \t  -0.46191649685508945 \t -0.02210046413617886\n",
            "93     \t [-1.76357922 -0.07994578]. \t  -13.948293185401571 \t -0.02210046413617886\n",
            "94     \t [4.33586629 0.94515392]. \t  -24.125196097896414 \t -0.02210046413617886\n",
            "95     \t [-4.56624156 -3.7311091 ]. \t  -2131.6166846930196 \t -0.02210046413617886\n",
            "96     \t [-4.2956356  -9.65005052]. \t  -72640.99768931672 \t -0.02210046413617886\n",
            "97     \t [5.96686349 3.20976704]. \t  -453.2320428473832 \t -0.02210046413617886\n",
            "98     \t [5.88144339 2.11271107]. \t  -42.380490978042545 \t -0.02210046413617886\n",
            "99     \t [-9.88634809  7.60304415]. \t  -31618.46472055067 \t -0.02210046413617886\n",
            "100    \t [1.05089398 0.78511245]. \t  -0.06877206463151544 \t -0.02210046413617886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHz_Jg2_uv7E",
        "outputId": "a42ec86f-e6ea-4026-fa98-f807f89af241"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.41034571 1.2905751 ]. \t  -7.547501684041513 \t -7.547501684041513\n",
            "init   \t [-0.23116335 -3.27044493]. \t  -936.6052904493877 \t -7.547501684041513\n",
            "init   \t [-2.48263644  0.64071738]. \t  -33.95727983513621 \t -7.547501684041513\n",
            "init   \t [-8.63787423  1.6905812 ]. \t  -504.96347244129606 \t -7.547501684041513\n",
            "init   \t [-5.24204473 -6.78486837]. \t  -18977.794322697024 \t -7.547501684041513\n",
            "1      \t [ 9.84240059 -0.83299926]. \t  -221.1494177066687 \t -7.547501684041513\n",
            "2      \t [6.73867561 8.36273465]. \t  -35481.183803304404 \t -7.547501684041513\n",
            "3      \t [ 8.38641873 -9.74808405]. \t  -66058.08004040587 \t -7.547501684041513\n",
            "4      \t [-3.83701174  9.3377774 ]. \t  -63551.82998472117 \t -7.547501684041513\n",
            "5      \t [ 1.28249957 -9.77166709]. \t  -71963.50047559853 \t -7.547501684041513\n",
            "6      \t [1.04222689 5.72992205]. \t  -8351.952392862633 \t -7.547501684041513\n",
            "7      \t [-10.           7.25151661]. \t  -26648.79086636708 \t -7.547501684041513\n",
            "8      \t [ 5.43692712 -4.06202851]. \t  -1539.148970020672 \t -7.547501684041513\n",
            "9      \t [-10.          -3.42356568]. \t  -2357.68174254125 \t -7.547501684041513\n",
            "10     \t [-10. -10.]. \t  -88321.0 \t -7.547501684041513\n",
            "11     \t [5.81364025 2.25027199]. \t  -60.3890076381077 \t -7.547501684041513\n",
            "12     \t [-4.97488385 -1.90965407]. \t  -336.7285346315774 \t -7.547501684041513\n",
            "13     \t [-3.79684754  3.53072432]. \t  -1673.7063819819016 \t -7.547501684041513\n",
            "14     \t [9.25668229 3.88152593]. \t  -939.7712605560133 \t -7.547501684041513\n",
            "15     \t [ 9.57479367 -4.59830407]. \t  -2213.9396015619786 \t -7.547501684041513\n",
            "16     \t [ 3.00925451 -0.04743686]. \t  -22.094196894143497 \t -7.547501684041513\n",
            "17     \t [-0.71846719  0.09535146]. \t  \u001b[92m-4.038438853432253\u001b[0m \t -4.038438853432253\n",
            "18     \t [ 5.51440894 -0.4957413 ]. \t  -70.83873744507414 \t -4.038438853432253\n",
            "19     \t [-0.40228672  0.89522009]. \t  -10.007458502233797 \t -4.038438853432253\n",
            "20     \t [2.24537612 1.87345321]. \t  -47.13841828913171 \t -4.038438853432253\n",
            "21     \t [0.2881316  1.64564463]. \t  -53.10282494530721 \t -4.038438853432253\n",
            "22     \t [-0.95610003  0.39799096]. \t  -7.066843761047282 \t -4.038438853432253\n",
            "23     \t [0.94992995 0.43555192]. \t  \u001b[92m-0.6534908744516502\u001b[0m \t -0.6534908744516502\n",
            "24     \t [2.04126339 0.2563074 ]. \t  -8.379485400996032 \t -0.6534908744516502\n",
            "25     \t [2.89502442 9.87660257]. \t  -73884.91156089552 \t -0.6534908744516502\n",
            "26     \t [1.52179163 0.60755282]. \t  -1.5001701443017033 \t -0.6534908744516502\n",
            "27     \t [ 2.35187261 -0.88310616]. \t  -3.08246657976607 \t -0.6534908744516502\n",
            "28     \t [ 2.8953706  -1.06566316]. \t  -4.3714179900303245 \t -0.6534908744516502\n",
            "29     \t [7.17614776 0.80553273]. \t  -107.25554680980332 \t -0.6534908744516502\n",
            "30     \t [ 3.08937579 -1.91176107]. \t  -39.987102000861455 \t -0.6534908744516502\n",
            "31     \t [ 0.93865871 -0.22913929]. \t  -1.3937043458940253 \t -0.6534908744516502\n",
            "32     \t [-7.06736362 -1.18718653]. \t  -260.5557558799544 \t -0.6534908744516502\n",
            "33     \t [4.84821643 4.00314247]. \t  -1494.715398322395 \t -0.6534908744516502\n",
            "34     \t [-0.46424221  0.38160145]. \t  -3.2855099476439045 \t -0.6534908744516502\n",
            "35     \t [-1.68516898 -0.50785562]. \t  -16.89896653218432 \t -0.6534908744516502\n",
            "36     \t [5.47732709 1.66876394]. \t  -20.063466592208517 \t -0.6534908744516502\n",
            "37     \t [-0.31083107  0.17096781]. \t  -1.9910298573398262 \t -0.6534908744516502\n",
            "38     \t [1.2794607  0.49242837]. \t  -1.3405248153936646 \t -0.6534908744516502\n",
            "39     \t [-5.65446154  0.59766657]. \t  -125.40692418036254 \t -0.6534908744516502\n",
            "40     \t [6.38753464 1.59092958]. \t  -32.539010006532635 \t -0.6534908744516502\n",
            "41     \t [ 2.61151067 -5.43927486]. \t  -6400.644086143425 \t -0.6534908744516502\n",
            "42     \t [-2.64568232 -9.90080355]. \t  -78974.68745282889 \t -0.6534908744516502\n",
            "43     \t [ 0.26024204 -0.36613138]. \t  \u001b[92m-0.54736546440461\u001b[0m \t -0.54736546440461\n",
            "44     \t [-7.36547092  3.99795355]. \t  -3164.1093525856995 \t -0.54736546440461\n",
            "45     \t [2.34202676 6.18017505]. \t  -10967.747204003006 \t -0.54736546440461\n",
            "46     \t [-7.37019544 -5.39694407]. \t  -8683.13303435588 \t -0.54736546440461\n",
            "47     \t [0.29316005 0.11982036]. \t  -0.6394863133246302 \t -0.54736546440461\n",
            "48     \t [7.41942349 4.26504765]. \t  -1718.785273908734 \t -0.54736546440461\n",
            "49     \t [0.94526687 0.5560444 ]. \t  \u001b[92m-0.21671786321625783\u001b[0m \t -0.21671786321625783\n",
            "50     \t [-4.42871958 -2.82461813]. \t  -860.6208323518431 \t -0.21671786321625783\n",
            "51     \t [ 7.60864111 -1.21331383]. \t  -87.1870222845973 \t -0.21671786321625783\n",
            "52     \t [9.55322663 6.75971883]. \t  -13466.88613990843 \t -0.21671786321625783\n",
            "53     \t [-3.75530813 -0.08039253]. \t  -51.01213105957952 \t -0.21671786321625783\n",
            "54     \t [2.28949089 7.88077999]. \t  -29732.535394841874 \t -0.21671786321625783\n",
            "55     \t [ 5.62561432 -5.31808828]. \t  -5210.860236817998 \t -0.21671786321625783\n",
            "56     \t [ 5.02624    -1.52125245]. \t  -16.527133174138513 \t -0.21671786321625783\n",
            "57     \t [-1.70044178  7.3703767 ]. \t  -24359.483718936048 \t -0.21671786321625783\n",
            "58     \t [-8.13851557  3.27121202]. \t  -1828.7559536916694 \t -0.21671786321625783\n",
            "59     \t [1.13001729 0.86621904]. \t  -0.2916726196116057 \t -0.21671786321625783\n",
            "60     \t [ 5.77250497 -3.05899245]. \t  -357.7864087892431 \t -0.21671786321625783\n",
            "61     \t [ 9.30911183 -3.67573839]. \t  -696.5416206577552 \t -0.21671786321625783\n",
            "62     \t [-1.27487039 -5.93674144]. \t  -10305.51024121098 \t -0.21671786321625783\n",
            "63     \t [ 3.99422947 -1.33483875]. \t  -9.336312598995555 \t -0.21671786321625783\n",
            "64     \t [ 8.23985224 -1.70229546]. \t  -64.36400634172232 \t -0.21671786321625783\n",
            "65     \t [ 8.67495404 -2.69019428]. \t  -126.16952713016198 \t -0.21671786321625783\n",
            "66     \t [ 9.92636565 -2.32880448]. \t  -81.37388917743273 \t -0.21671786321625783\n",
            "67     \t [0.40128712 2.84430967]. \t  -498.30632354137344 \t -0.21671786321625783\n",
            "68     \t [ 4.48980304 -1.20368817]. \t  -17.248115546286638 \t -0.21671786321625783\n",
            "69     \t [4.45118899 5.23504676]. \t  -5084.218305375657 \t -0.21671786321625783\n",
            "70     \t [0.02052282 1.1664541 ]. \t  -15.547017694471727 \t -0.21671786321625783\n",
            "71     \t [ 0.78850177 -0.69759944]. \t  \u001b[92m-0.11302485229660898\u001b[0m \t -0.11302485229660898\n",
            "72     \t [5.24613594 2.43914684]. \t  -106.54753502182602 \t -0.11302485229660898\n",
            "73     \t [-3.07061282 -5.73791022]. \t  -9515.907317149677 \t -0.11302485229660898\n",
            "74     \t [7.42857409 3.56574496]. \t  -689.3625783898757 \t -0.11302485229660898\n",
            "75     \t [8.63842226 1.92918294]. \t  -61.201203457836506 \t -0.11302485229660898\n",
            "76     \t [7.58215234 2.14148405]. \t  -48.37937476815305 \t -0.11302485229660898\n",
            "77     \t [ 5.63854004 -4.65830064]. \t  -2873.3007456023993 \t -0.11302485229660898\n",
            "78     \t [5.67495951 0.39752895]. \t  -79.29088590752625 \t -0.11302485229660898\n",
            "79     \t [ 7.04624991 -3.80493904]. \t  -996.5545127629913 \t -0.11302485229660898\n",
            "80     \t [4.88899487 1.09802047]. \t  -27.402245424328697 \t -0.11302485229660898\n",
            "81     \t [1.04852511 0.67514145]. \t  \u001b[92m-0.03983415712446472\u001b[0m \t -0.03983415712446472\n",
            "82     \t [ 2.03856719 -1.30084846]. \t  -4.701226034498959 \t -0.03983415712446472\n",
            "83     \t [7.26627644 2.53258506]. \t  -101.13118362203005 \t -0.03983415712446472\n",
            "84     \t [ 5.49389575 -1.61967815]. \t  -20.317296060779206 \t -0.03983415712446472\n",
            "85     \t [-5.14931039  8.85200777]. \t  -52438.625272453966 \t -0.03983415712446472\n",
            "86     \t [ 5.72285315 -2.07760474]. \t  -39.24188789035354 \t -0.03983415712446472\n",
            "87     \t [-6.03804648 -2.8417385 ]. \t  -1034.2377038127527 \t -0.03983415712446472\n",
            "88     \t [-6.98147209  5.98337434]. \t  -12414.282514773455 \t -0.03983415712446472\n",
            "89     \t [-9.19495539  8.93306858]. \t  -57087.02526509337 \t -0.03983415712446472\n",
            "90     \t [-7.15846338  8.10771801]. \t  -38502.36355553295 \t -0.03983415712446472\n",
            "91     \t [-0.32014405  3.15326937]. \t  -818.3367131916744 \t -0.03983415712446472\n",
            "92     \t [-4.74184044 -1.00192525]. \t  -124.08155142362112 \t -0.03983415712446472\n",
            "93     \t [-0.9094418   3.29480777]. \t  -1027.0614777163194 \t -0.03983415712446472\n",
            "94     \t [ 0.40407557 -0.33970468]. \t  -0.41517578720733866 \t -0.03983415712446472\n",
            "95     \t [ 1.47552646 -1.18912417]. \t  -3.8846711523373267 \t -0.03983415712446472\n",
            "96     \t [7.6995205  9.17775011]. \t  -51734.15813973444 \t -0.03983415712446472\n",
            "97     \t [2.94291848 8.91224474]. \t  -48621.6854745948 \t -0.03983415712446472\n",
            "98     \t [ 1.2874976  -0.72904497]. \t  -0.18344140352880586 \t -0.03983415712446472\n",
            "99     \t [ 7.44174365 -0.70432951]. \t  -124.69031671498486 \t -0.03983415712446472\n",
            "100    \t [5.70373818 1.61882879]. \t  -22.55301138244736 \t -0.03983415712446472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUnhsKpCuv9o",
        "outputId": "e226b728-8101-4230-d166-2cc0cbfd1aff"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2238.116898059845"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJG0SLpwuwAL",
        "outputId": "cc504933-24a5-47b9-fb05-7abe5af61557"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.0859967871325082, -3.0859967871325082)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lA9eZf0uwCx",
        "outputId": "61d92c39-b35c-4c4b-fa22-6558dfd61aa5"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.6533479564675457, -3.937459128319854)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glrTGcpAuwFa",
        "outputId": "73a10abf-5f5a-4b61-dede-e5fd90df72b8"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.9963239726714779, -2.6543869839446814)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaRwfbxeuwIR",
        "outputId": "52902591-015f-440f-d7af-f0fe33b845ae"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.9062182196624358, -2.3198200247352103)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nb5NkfyuwKp",
        "outputId": "da5ac4da-fad5-4358-ca3b-eaeb4d244964"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.5455607208371007, -3.3776590880962476)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Q-WfXbuwNg",
        "outputId": "91d7e4e6-591d-4a9b-e4a6-3a36f9f970f3"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.950268075483134, -1.948138928947513)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqS7VLcuwPy",
        "outputId": "762a3f96-14b4-409d-e6ad-124c9b1a6e67"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.195765238071279, -4.869238516356011)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOi5iX8guwSS",
        "outputId": "96fa83e5-1997-4627-b684-9ae6cebbc629"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.901063041781975, -2.8019183744032277)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVApeazuwU5",
        "outputId": "0d8b071c-272b-4f03-eebe-2c7425b0537b"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.0900862275954375, -4.744360500024828)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g92jk9WJuwXb",
        "outputId": "04073e30-4efc-498b-dc16-73c44a96d37c"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.724081906157097, -3.1809288934484052)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmcF1x-NuwZz",
        "outputId": "749eef19-80a1-4009-f786-4052b4daa116"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.290222586116367, -5.290222586116367)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8axVhb6uwcc",
        "outputId": "408affda-71e1-4dd1-879c-31bd700ce606"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.8996216443716853, -2.332342952604858)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzlrL8XFuwfB",
        "outputId": "e3083a04-ba62-4eb0-e7f7-e7be1104d22e"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.163440944630179, -3.4727823501197728)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZlLJ1quwh6",
        "outputId": "cbe97d96-e257-4bce-9c73-872baee5a8c0"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.090293360199478, -3.090293360199478)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVBcHRiMuwjx",
        "outputId": "64f2d744-4814-4949-a5fe-104da9002686"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.810464417306107, -4.320517307417607)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8nZ_DrKuwnA",
        "outputId": "4173cedd-82c6-4ef4-bd22-41f75496c3ed"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.81624406813844, -2.2120435102235314)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg6qzzQFuwpU",
        "outputId": "415cbe03-f58a-454f-e5dd-d7030adfd4f9"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.4558938357040816, -1.6682865874127824)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrRtDLMFuwsB",
        "outputId": "0e9a1a4f-4faa-408a-925a-eb9b4debe562"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.7242391280535263, -1.7642669319411515)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkmSu4CUuwuh",
        "outputId": "856ba62e-bbca-4358-d70b-be1e287a2d12"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.676957653830942, -3.812156669041909)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eymecjwkuwxb",
        "outputId": "19afbc89-41b0-4f00-ed19-b124e23bedff"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.9545091109477264, -3.2230305155436243)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "84XIzmWD2oba",
        "outputId": "206fc75a-3c1e-41e2-fe40-cc3d4e3e8a05"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Red')\r\n",
        "plt.plot(median_winner, color = 'Yellow')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Yellow', alpha=0.4, label='GP EI Regret IQR: L-BFGS-B')\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP EI Regret IQR: Newton-CG with GP d$^{2}$EI')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show() "
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5dkR2Id2oiu",
        "outputId": "a6960370-c503-4667-b1a2-5e1d459ff778"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3465.702248573303, 2238.116898059845)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    }
  ]
}