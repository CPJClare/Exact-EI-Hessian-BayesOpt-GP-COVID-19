{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3c__Rastrigin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Rastrigin synthetic function:\n",
        "\n",
        "GP EI: Newton-CG (exact GP EI gradients + exact GP EI Hessian) vs. L-BFGS-B (exact GP EI gradients, without Hessian)\n",
        "\n",
        "https://www.sfu.ca/~ssurjano/rastr.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "f55dbcd8-0639-4bda-e574-200461903a83"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=4c4f224e7c5fca03c388dfe75d72ab34ee3388c7575b3ea2900d7e12bf402b58\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from matplotlib.pyplot import rc\n",
        "\n",
        "rc('text', usetex=False)\n",
        "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\n",
        "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\n",
        "\n",
        "n_start_AcqFunc = 100\n",
        "\n",
        "obj_func = 'Rastrigin'\n",
        "n_test = n_test = n_start_AcqFunc # test points\n",
        "\n",
        "util_loser = 'dEI_GP'\n",
        "util_winner = 'dEI_GP'\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "if obj_func == 'Rastrigin':\n",
        "            \n",
        "    # True y bounds:\n",
        "    y_lb = 0\n",
        "    operator = -1 # targets global minimum \n",
        "    y_global_orig = y_lb * operator # targets global minimum\n",
        "            \n",
        "# Constraints:\n",
        "    lb = -5.12\n",
        "    ub = +5.12\n",
        "    \n",
        "# Input array dimension(s):\n",
        "    dim = 2\n",
        "\n",
        "# 2-D inputs' parameter bounds:\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\n",
        "             'x2_training': ('cont', [lb, ub])}\n",
        "    \n",
        "    max_iter = 100  # iterations of Bayesian optimisation\n",
        "    \n",
        "# Test data:\n",
        "    x1_test = np.linspace(lb, ub, n_test)\n",
        "    x2_test = np.linspace(lb, ub, n_test)\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test))\n",
        "    \n",
        "    def f_syn_polarity(x1_training, x2_training):\n",
        "        return operator * (10 * dim + x1_training** 2 - 10 * np.cos(2 * np.pi * x1_training)\n",
        "                                    + x2_training** 2 - 10 * np.cos(2 * np.pi * x2_training)\n",
        "                          )"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 111\n",
        "run_num_2 = 113\n",
        "run_num_3 = 3333\n",
        "run_num_4 = 444\n",
        "run_num_5 = 5555\n",
        "run_num_6 = 6\n",
        "run_num_7 = 777\n",
        "run_num_8 = 887\n",
        "run_num_9 = 99\n",
        "run_num_10 = 1000\n",
        "run_num_11 = 1113\n",
        "run_num_12 = 1234\n",
        "run_num_13 = 2345\n",
        "run_num_14 = 88\n",
        "run_num_15 = 1556\n",
        "run_num_16 = 1666\n",
        "run_num_17 = 717\n",
        "run_num_18 = 8\n",
        "run_num_19 = 1998\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\n",
        "\n",
        "def l2norm_(X, Xstar):\n",
        "    \n",
        "    return cdist(X, Xstar)\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "class squaredExponentialDeriv(squaredExponential):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "\n",
        "    def K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\n",
        "        return K\n",
        "    \n",
        "    def dK(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\n",
        "        return dK\n",
        "    \n",
        "    def d2K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r **2 - 1)\n",
        "        return d2K\n",
        "    \n",
        "cov_func = squaredExponentialDeriv()\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "### Acquisition function derivatives:\n",
        "\n",
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=1e-08, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\n",
        "            \n",
        "        return f, df, d2f\n",
        "    \n",
        "    def _eval(self, tau, mean, std):\n",
        "    \n",
        "        return self.f(tau, mean, std, **self.params)\n",
        "    \n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "from scipy.linalg import cholesky, solve\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "\n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(K).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(self.L, Kstar.T)\n",
        "        dv = solve(self.L, dKstar.T)\n",
        "        d2v = solve(self.L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \n",
        "    n_start = n_start_AcqFunc\n",
        "    eps = 1e-08\n",
        "        \n",
        "    def func(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: Exact Hessian\n",
        "\n",
        "class d2GPGO(GPGO):  \n",
        "    n_start = n_start_AcqFunc\n",
        "    p = np.full((n_start,1),1)*0 + 1\n",
        "    eps = 1e-08\n",
        "    \n",
        "    def func(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "    \n",
        "    def hessp_nonzero(self, xnew, p):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        df2 = np.empty((self.n_start,))\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[2]\n",
        "        H2 = np.empty((self.n_start,))\n",
        "        df2 = np.asarray(df2)\n",
        "        p = np.asarray(p)\n",
        "        H2 = np.multiply(df2,p)\n",
        "        return H2\n",
        "\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        d2f = (z * norm.cdf(z) + norm.pdf(z)[0]) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx\n",
        "\n",
        "        return d2f\n",
        "\n",
        "    def hessp_nonzero1(self, xnew, p, *args):\n",
        "      new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "      new_std = np.sqrt(new_var + 1e-6)\n",
        "      ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "      df2 = np.empty((self.n_start,))\n",
        "      df2 = -self.dEI_GP(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0] * p\n",
        "      return df2\n",
        "\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,                  \n",
        "                                                                 hessp = self.hessp_nonzero1,                      \n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        \n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):    \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "e976afb9-2968-4386-8c83-d495ed487d8b"
      },
      "source": [
        "start_lose = time.time()\n",
        "start_lose"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616919507.1205885"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "ea1ad002-aa14-4e84-a060-0b064bdd28a3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_1 = d2GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.1486226  -3.38872572]. \t  -34.50899842538312 \t -32.79886533908709\n",
            "init   \t [-0.65475564  2.75724772]. \t  -33.20941543961683 \t -32.79886533908709\n",
            "init   \t [-2.09586888 -3.59257132]. \t  -37.414851075595465 \t -32.79886533908709\n",
            "init   \t [-4.88982196 -0.8169012 ]. \t  -32.79886533908709 \t -32.79886533908709\n",
            "init   \t [-2.67589487 -1.6624006 ]. \t  -39.644034416971316 \t -32.79886533908709\n",
            "1      \t [-11.62139992 -10.82542809]. \t  -274.9121675244618 \t -32.79886533908709\n",
            "2      \t [-20.4440414 -18.244832 ]. \t  -779.8963004310697 \t -32.79886533908709\n",
            "3      \t [-24.8505609  -23.25547082]. \t  -1172.8046469563496 \t -32.79886533908709\n",
            "4      \t [-50.48841542 -52.12927258]. \t  -5289.635955715127 \t -32.79886533908709\n",
            "5      \t [-4.39699561 -9.27710554]. \t  -135.07090613916318 \t -32.79886533908709\n",
            "6      \t [-62.56895407 -65.87408202]. \t  -8276.314586276883 \t -32.79886533908709\n",
            "7      \t [-68.25252148 -70.03426857]. \t  -9573.594795234305 \t -32.79886533908709\n",
            "8      \t [-101.39535263  -97.81081565]. \t  -19872.159455598216 \t -32.79886533908709\n",
            "9      \t [-88.50638482 -89.32366722]. \t  -15846.554766443072 \t -32.79886533908709\n",
            "10     \t [-9.79121216 -4.29750436]. \t  -134.71644876421843 \t -32.79886533908709\n",
            "11     \t [4.95177983 1.8014662 ]. \t  -35.043223893579615 \t -32.79886533908709\n",
            "12     \t [-32.02993594 -33.31433509]. \t  -2149.871196451165 \t -32.79886533908709\n",
            "13     \t [-18.53674415 -24.01990753]. \t  -940.3796441614396 \t -32.79886533908709\n",
            "14     \t [-17.11369985 -13.14025848]. \t  -471.6285148314183 \t -32.79886533908709\n",
            "15     \t [-5.25106004  3.96711382]. \t  -53.590948552811206 \t -32.79886533908709\n",
            "16     \t [-30.87320089 -25.8946319 ]. \t  -1628.8084555952719 \t -32.79886533908709\n",
            "17     \t [-57.7837327  -54.92769315]. \t  -6364.921976765318 \t -32.79886533908709\n",
            "18     \t [-12.9069146 -15.586765 ]. \t  -429.74830489183006 \t -32.79886533908709\n",
            "19     \t [-25.60688072 -29.87564512]. \t  -1568.9954135149012 \t -32.79886533908709\n",
            "20     \t [-0.51233124 -7.2962142 ]. \t  -86.33031529447251 \t -32.79886533908709\n",
            "21     \t [-41.6513762  -40.47451932]. \t  -3408.703646348299 \t -32.79886533908709\n",
            "22     \t [-15.55319232 -19.79960969]. \t  -660.306172030252 \t -32.79886533908709\n",
            "23     \t [-1911.03480653 -1907.37513761]. \t  -7290151.262702682 \t -32.79886533908709\n",
            "24     \t [-119.85509824 -115.07076944]. \t  -27611.365025561074 \t -32.79886533908709\n",
            "25     \t [-91.81025847 -95.43687521]. \t  -17562.84807001429 \t -32.79886533908709\n",
            "26     \t [ 4.81956902 -4.89298203]. \t  -55.11303879193123 \t -32.79886533908709\n",
            "27     \t [-43.55937263 -47.09985672]. \t  -4137.032192849808 \t -32.79886533908709\n",
            "28     \t [-170.23185154 -173.68094581]. \t  -59167.02032987537 \t -32.79886533908709\n",
            "29     \t [-5.37700809 -5.13729924]. \t  -75.9597122051628 \t -32.79886533908709\n",
            "30     \t [-161.58670905 -158.27461629]. \t  -51191.21149260643 \t -32.79886533908709\n",
            "31     \t [-539.67579789 -537.67196628]. \t  -580370.3142385611 \t -32.79886533908709\n",
            "32     \t [-8.83885125  0.27285172]. \t  -94.33344037524857 \t -32.79886533908709\n",
            "33     \t [-15.27858604  -6.42246204]. \t  -305.3062220785762 \t -32.79886533908709\n",
            "34     \t [-69.0869837  -64.30311896]. \t  -8922.635061450315 \t -32.79886533908709\n",
            "35     \t [-76.22112436 -73.18416308]. \t  -11179.75746698644 \t -32.79886533908709\n",
            "36     \t [ -7.57115442 -12.30074399]. \t  -240.78242533166423 \t -32.79886533908709\n",
            "37     \t [2.89840307 5.09559439]. \t  -38.08527973565818 \t -32.79886533908709\n",
            "38     \t [-37.80028996 -29.81746539]. \t  -2330.722509935707 \t -32.79886533908709\n",
            "39     \t [-166.3106575  -163.71135415]. \t  -54486.766299249066 \t -32.79886533908709\n",
            "40     \t [1.58658105 0.14452779]. \t  \u001b[92m-24.942068548791855\u001b[0m \t -24.942068548791855\n",
            "41     \t [-8.89837275 -8.00617461]. \t  -145.25774318112042 \t -24.942068548791855\n",
            "42     \t [ 5.0113476  -1.60568733]. \t  -45.59225100861243 \t -24.942068548791855\n",
            "43     \t [ -8.90345266 -16.32614055]. \t  -362.20220603511166 \t -24.942068548791855\n",
            "44     \t [-2.02155457  5.04032875]. \t  -29.902490966218185 \t -24.942068548791855\n",
            "45     \t [-36.1925584  -37.83089909]. \t  -2752.679930658048 \t -24.942068548791855\n",
            "46     \t [-4.06383607  1.52442804]. \t  -39.51474097920561 \t -24.942068548791855\n",
            "47     \t [ 2.28846963 -5.76227951]. \t  -60.063829620767386 \t -24.942068548791855\n",
            "48     \t [2.01363726 2.47358366]. \t  -30.072609845496277 \t -24.942068548791855\n",
            "49     \t [-7.16363128 -2.487754  ]. \t  -82.31269754203556 \t -24.942068548791855\n",
            "50     \t [-1.43942895  0.15453021]. \t  -25.73510918007334 \t -24.942068548791855\n",
            "51     \t [-3.19151408 -6.06280959]. \t  -54.11945979111887 \t -24.942068548791855\n",
            "52     \t [4.97854317 4.43661617]. \t  -63.77759765305103 \t -24.942068548791855\n",
            "53     \t [0.71776608 4.76930135]. \t  -44.063153925053555 \t -24.942068548791855\n",
            "54     \t [ 2.56715948 -1.6867049 ]. \t  -42.43103906655784 \t -24.942068548791855\n",
            "55     \t [-0.32416491 -1.4884221 ]. \t  -36.78712645281301 \t -24.942068548791855\n",
            "56     \t [-22.25912338 -14.67869187]. \t  -735.8375114383726 \t -24.942068548791855\n",
            "57     \t [-0.69430116 -4.76396432]. \t  -45.72979095328075 \t -24.942068548791855\n",
            "58     \t [-2.82082822  3.29596441]. \t  -37.36368164462901 \t -24.942068548791855\n",
            "59     \t [-4.68914375 -2.99317225]. \t  -44.68756350572393 \t -24.942068548791855\n",
            "60     \t [-0.13284548  1.00978103]. \t  \u001b[92m-4.342129730197785\u001b[0m \t -4.342129730197785\n",
            "61     \t [0.52998851 1.40926225]. \t  -40.50826662692708 \t -4.342129730197785\n",
            "62     \t [3.50379669 0.74869897]. \t  -42.916041405203444 \t -4.342129730197785\n",
            "63     \t [-0.40593711  0.66074665]. \t  -34.223727627399434 \t -4.342129730197785\n",
            "64     \t [-0.37896986  1.33275146]. \t  -34.13337538645564 \t -4.342129730197785\n",
            "65     \t [-2.43825779 -0.44306079]. \t  -44.765152557287806 \t -4.342129730197785\n",
            "66     \t [3.27739582 3.0627894 ]. \t  -32.603023358690756 \t -4.342129730197785\n",
            "67     \t [-1.34994365  3.60792217]. \t  -48.50212995802169 \t -4.342129730197785\n",
            "68     \t [0.03718032 1.06860976]. \t  \u001b[92m-2.329823304933317\u001b[0m \t -2.329823304933317\n",
            "69     \t [-0.89411853 -2.56397447]. \t  -28.70887537152951 \t -2.329823304933317\n",
            "70     \t [-0.03412285  3.7138012 ]. \t  -26.277320564753943 \t -2.329823304933317\n",
            "71     \t [-3.16320009  4.83565057]. \t  -43.07636603460466 \t -2.329823304933317\n",
            "72     \t [-2.10084511  0.15232706]. \t  -10.61896990640059 \t -2.329823304933317\n",
            "73     \t [ 3.4121046  -3.79126098]. \t  -51.96595135502769 \t -2.329823304933317\n",
            "74     \t [-5.11733913  0.45943786]. \t  -48.672120700267506 \t -2.329823304933317\n",
            "75     \t [-1.88644136 -0.46270235]. \t  -25.938721029403375 \t -2.329823304933317\n",
            "76     \t [ 2.33177896 -0.28895575]. \t  -32.859166337003224 \t -2.329823304933317\n",
            "77     \t [ 0.19712136 -2.15157851]. \t  -15.6091482821729 \t -2.329823304933317\n",
            "78     \t [ 1.10975704 -1.58849014]. \t  -24.533738500488486 \t -2.329823304933317\n",
            "79     \t [-3.28408718 -2.81829203]. \t  -36.692974070165015 \t -2.329823304933317\n",
            "80     \t [5.03720881 0.87501319]. \t  -29.339512878044104 \t -2.329823304933317\n",
            "81     \t [-3.85171102  2.6696095 ]. \t  -40.83702991582032 \t -2.329823304933317\n",
            "82     \t [-0.00828305  1.02459881]. \t  \u001b[92m-1.1826157576774623\u001b[0m \t -1.1826157576774623\n",
            "83     \t [2.05915316 1.32644305]. \t  -21.30284713077938 \t -1.1826157576774623\n",
            "84     \t [-1.86488842  0.52839359]. \t  -26.990435392529747 \t -1.1826157576774623\n",
            "85     \t [-3.61810421 -1.74225463]. \t  -43.983300713456956 \t -1.1826157576774623\n",
            "86     \t [ 5.08458424 -0.17319007]. \t  -32.62136745168524 \t -1.1826157576774623\n",
            "87     \t [0.57366009 0.87858597]. \t  -22.820389328492443 \t -1.1826157576774623\n",
            "88     \t [0.86734304 2.68016429]. \t  -25.461187605945984 \t -1.1826157576774623\n",
            "89     \t [ 4.95417858 -3.26858229]. \t  -46.80401428946973 \t -1.1826157576774623\n",
            "90     \t [ 0.73698261 -5.09597657]. \t  -39.09295431023707 \t -1.1826157576774623\n",
            "91     \t [-5.08300769  5.09868663]. \t  -55.02470735529392 \t -1.1826157576774623\n",
            "92     \t [ 3.6701657 -1.708481 ]. \t  -43.776661885321204 \t -1.1826157576774623\n",
            "93     \t [ 2.69518524 -3.26069108]. \t  -41.94379144631107 \t -1.1826157576774623\n",
            "94     \t [0.10697642 0.81083554]. \t  -9.114060990525934 \t -1.1826157576774623\n",
            "95     \t [0.67200117 3.15880314]. \t  -29.714987011846247 \t -1.1826157576774623\n",
            "96     \t [-0.96702362  5.11878514]. \t  -30.009255229344724 \t -1.1826157576774623\n",
            "97     \t [-2.37957082  2.28592782]. \t  -40.39731134842742 \t -1.1826157576774623\n",
            "98     \t [2.01544291 0.38979862]. \t  -21.95805167599152 \t -1.1826157576774623\n",
            "99     \t [3.97114244 3.45792958]. \t  -47.54384076316298 \t -1.1826157576774623\n",
            "100    \t [3.39523965 2.02795933]. \t  -33.70495094241642 \t -1.1826157576774623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j6UBhkdtdVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454f4c34-7598-4b2d-b2f8-01d8d80cc7cb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_2 = d2GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.60433145 -4.36322715]. \t  -66.48533678053353 \t -9.873801938301748\n",
            "init   \t [ 4.04410125 -0.65030607]. \t  -33.021397945058965 \t -9.873801938301748\n",
            "init   \t [-3.81258     0.77678463]. \t  -29.63268746352715 \t -9.873801938301748\n",
            "init   \t [ 3.48642222 -0.66436556]. \t  -47.68483871526274 \t -9.873801938301748\n",
            "init   \t [2.00612414 1.89069411]. \t  -9.873801938301748 \t -9.873801938301748\n",
            "1      \t [-18.08893204 -17.27950205]. \t  -639.1546805040547 \t -9.873801938301748\n",
            "2      \t [-6.60072093 -7.99004733]. \t  -125.4933815646179 \t -9.873801938301748\n",
            "3      \t [-9.20965725 -2.34158699]. \t  -113.23525569551234 \t -9.873801938301748\n",
            "4      \t [-13.50346276  -6.95448454]. \t  -251.11214427136377 \t -9.873801938301748\n",
            "5      \t [-2.0601632 -4.4976771]. \t  -45.17832134936412 \t -9.873801938301748\n",
            "6      \t [-30.53648478 -29.94327167]. \t  -1849.4433445655495 \t -9.873801938301748\n",
            "7      \t [4.68887695 5.0881977 ]. \t  -63.11866987595836 \t -9.873801938301748\n",
            "8      \t [-0.7277237   5.00150281]. \t  -36.94015366405149 \t -9.873801938301748\n",
            "9      \t [ -8.62061988 -13.96906571]. \t  -286.90114968852066 \t -9.873801938301748\n",
            "10     \t [-2.12781396 -8.67622398]. \t  -97.33075559036838 \t -9.873801938301748\n",
            "11     \t [-15.0794472  -12.31537396]. \t  -394.27144848793233 \t -9.873801938301748\n",
            "12     \t [-0.42216619 -0.96357608]. \t  -20.195264949615115 \t -9.873801938301748\n",
            "13     \t [-5.02164417  4.52607733]. \t  -65.66068413707116 \t -9.873801938301748\n",
            "14     \t [-22.19742362 -22.49506151]. \t  -1025.5048795611929 \t -9.873801938301748\n",
            "15     \t [-10.36546157 -10.06896794]. \t  -226.38606366065565 \t -9.873801938301748\n",
            "16     \t [-5.32487999 -3.0839218 ]. \t  -53.7563979697311 \t -9.873801938301748\n",
            "17     \t [ 0.73457858 -5.6807176 ]. \t  -57.994551840653344 \t -9.873801938301748\n",
            "18     \t [-6.84935278  0.22357206]. \t  -59.465817337202004 \t -9.873801938301748\n",
            "19     \t [5.11332453 1.7151704 ]. \t  -43.68856740477409 \t -9.873801938301748\n",
            "20     \t [-1.10847854  2.03150793]. \t  \u001b[92m-7.785356448177819\u001b[0m \t -7.785356448177819\n",
            "21     \t [1.74125468 4.13055959]. \t  -33.82290026985897 \t -7.785356448177819\n",
            "22     \t [ 1.10501872 -2.87316206]. \t  -14.586356764423003 \t -7.785356448177819\n",
            "23     \t [ 5.09710361 -1.10978395]. \t  -31.302514758099576 \t -7.785356448177819\n",
            "24     \t [-3.18432811 -1.90864071]. \t  -21.37545876528513 \t -7.785356448177819\n",
            "25     \t [-2.73386699  3.04001647]. \t  -28.042086762183807 \t -7.785356448177819\n",
            "26     \t [0.09896706 1.57403849]. \t  -23.29657371348053 \t -7.785356448177819\n",
            "27     \t [-1.239857    1.16041472]. \t  -16.910687352572413 \t -7.785356448177819\n",
            "28     \t [-1.66640916  2.31725647]. \t  -37.26179525334384 \t -7.785356448177819\n",
            "29     \t [-0.46119891  2.43912564]. \t  -45.143729308076104 \t -7.785356448177819\n",
            "30     \t [-0.76780181  1.41836279]. \t  -30.198129353346662 \t -7.785356448177819\n",
            "31     \t [-2.99544189  4.13489314]. \t  -29.4559606886824 \t -7.785356448177819\n",
            "32     \t [1.10813059 1.43253   ]. \t  -24.615433735823082 \t -7.785356448177819\n",
            "33     \t [-2.12634493  0.72118968]. \t  -19.83073079337573 \t -7.785356448177819\n",
            "34     \t [-3.90772561  2.70771436]. \t  -36.861925380701756 \t -7.785356448177819\n",
            "35     \t [-0.36436744 -2.10441904]. \t  -23.22072636393222 \t -7.785356448177819\n",
            "36     \t [2.96929088 2.06841597]. \t  -14.19040839422697 \t -7.785356448177819\n",
            "37     \t [-1.74991346  1.14163934]. \t  -18.076436821602115 \t -7.785356448177819\n",
            "38     \t [-1.91904945 -0.41833992]. \t  -23.835962236162864 \t -7.785356448177819\n",
            "39     \t [2.03786725 2.96087576]. \t  -13.502032346495294 \t -7.785356448177819\n",
            "40     \t [1.34069584 2.16116268]. \t  -26.566776094331104 \t -7.785356448177819\n",
            "41     \t [-1.81205577  0.29802584]. \t  -22.543299329036458 \t -7.785356448177819\n",
            "42     \t [-3.03151893 -0.49204206]. \t  -29.61517203353401 \t -7.785356448177819\n",
            "43     \t [-1.65661784 -2.02132396]. \t  -22.456226086351123 \t -7.785356448177819\n",
            "44     \t [ 0.65433733 -1.62388431]. \t  -35.840841777160264 \t -7.785356448177819\n",
            "45     \t [-4.24993348 -4.88624531]. \t  -54.38052327132978 \t -7.785356448177819\n",
            "46     \t [2.46890791 2.51299706]. \t  -52.187118400722156 \t -7.785356448177819\n",
            "47     \t [3.08133677 1.06095687]. \t  -12.622506194528977 \t -7.785356448177819\n",
            "48     \t [2.65120981 1.41010152]. \t  -43.280181318704244 \t -7.785356448177819\n",
            "49     \t [4.10195416 1.51645425]. \t  -41.05487493905035 \t -7.785356448177819\n",
            "50     \t [3.54152754 0.64837041]. \t  -48.584697068749364 \t -7.785356448177819\n",
            "51     \t [3.60863709 1.91227853]. \t  -35.91938259642911 \t -7.785356448177819\n",
            "52     \t [ 4.001021   -1.63019756]. \t  -35.50233599288938 \t -7.785356448177819\n",
            "53     \t [-1.2169068   2.44819589]. \t  -34.88509520699192 \t -7.785356448177819\n",
            "54     \t [1.58856472 2.77361835]. \t  -37.22920951627607 \t -7.785356448177819\n",
            "55     \t [2.95216882 1.74399306]. \t  -22.58235913638199 \t -7.785356448177819\n",
            "56     \t [1.85615181 3.28665042]. \t  -30.343791943980804 \t -7.785356448177819\n",
            "57     \t [4.82482853 0.80455825]. \t  -36.034730300337316 \t -7.785356448177819\n",
            "58     \t [ 2.68289704 -0.04905469]. \t  -21.763985367273186 \t -7.785356448177819\n",
            "59     \t [1.84442072 4.82660497]. \t  -36.47778563971854 \t -7.785356448177819\n",
            "60     \t [5.01427621 1.2884178 ]. \t  -39.23367802300178 \t -7.785356448177819\n",
            "61     \t [ 4.12745891 -1.21654435]. \t  -29.468281707319576 \t -7.785356448177819\n",
            "62     \t [ 3.00746975 -2.27474386]. \t  -25.778793033601737 \t -7.785356448177819\n",
            "63     \t [1.71564174 2.50871648]. \t  -41.36415558083381 \t -7.785356448177819\n",
            "64     \t [0.55069752 4.59707589]. \t  -59.13009598475136 \t -7.785356448177819\n",
            "65     \t [1.92291224 1.26443253]. \t  -17.352214451009836 \t -7.785356448177819\n",
            "66     \t [ 3.97025056 -2.77287004]. \t  -32.19386085814881 \t -7.785356448177819\n",
            "67     \t [2.71217189 3.99064855]. \t  -35.65290397926249 \t -7.785356448177819\n",
            "68     \t [3.11660147 2.91804938]. \t  -22.090662371971064 \t -7.785356448177819\n",
            "69     \t [1.64481012 1.54451582]. \t  -40.84080228634899 \t -7.785356448177819\n",
            "70     \t [0.5186403  0.36868873]. \t  -37.121591695319026 \t -7.785356448177819\n",
            "71     \t [0.06557954 2.99731619]. \t  -9.826602840651562 \t -7.785356448177819\n",
            "72     \t [1.56806418 0.31074482]. \t  -35.37949244737834 \t -7.785356448177819\n",
            "73     \t [0.46973228 2.31526237]. \t  -39.3873977639129 \t -7.785356448177819\n",
            "74     \t [-3.15261966  1.77368062]. \t  -25.858640453406874 \t -7.785356448177819\n",
            "75     \t [ 5.01900886 -0.51148848]. \t  -45.49726909525917 \t -7.785356448177819\n",
            "76     \t [2.72788096 0.26725254]. \t  -29.979956074832263 \t -7.785356448177819\n",
            "77     \t [ 2.49735757 -1.26363658]. \t  -38.687957376745565 \t -7.785356448177819\n",
            "78     \t [-0.4354701   3.28693669]. \t  -42.48284718835345 \t -7.785356448177819\n",
            "79     \t [0.35211879 0.86914635]. \t  -20.058160698491058 \t -7.785356448177819\n",
            "80     \t [-2.15428081  5.08808643]. \t  -36.36430774807513 \t -7.785356448177819\n",
            "81     \t [ 3.39993569 -1.24381809]. \t  -40.80611691499846 \t -7.785356448177819\n",
            "82     \t [6.14094538 3.94175958]. \t  -57.58242465447958 \t -7.785356448177819\n",
            "83     \t [3.51554409 1.02916283]. \t  -33.53797767222994 \t -7.785356448177819\n",
            "84     \t [-2.73490166  2.43372582]. \t  -43.495400060612475 \t -7.785356448177819\n",
            "85     \t [2.33690511 3.73563418]. \t  -45.510573107523555 \t -7.785356448177819\n",
            "86     \t [0.86458764 4.46474592]. \t  -43.84347913271905 \t -7.785356448177819\n",
            "87     \t [4.02458578 3.90939871]. \t  -33.176792249852696 \t -7.785356448177819\n",
            "88     \t [-4.92802715  2.80775265]. \t  -39.624534857129355 \t -7.785356448177819\n",
            "89     \t [3.85463309 4.18943603]. \t  -42.584562909240304 \t -7.785356448177819\n",
            "90     \t [ 5.08581447 -1.99477688]. \t  -31.268770586953092 \t -7.785356448177819\n",
            "91     \t [0.51205447 1.41867586]. \t  -40.96885461665741 \t -7.785356448177819\n",
            "92     \t [0.15465408 4.27961247]. \t  -34.55009353281763 \t -7.785356448177819\n",
            "93     \t [-0.98436333  0.01737561]. \t  \u001b[92m-1.0770335065906664\u001b[0m \t -1.0770335065906664\n",
            "94     \t [ 0.41209224 -2.05726764]. \t  -23.55556642679055 \t -1.0770335065906664\n",
            "95     \t [ 2.27882834 -3.88574691]. \t  -34.5614740291026 \t -1.0770335065906664\n",
            "96     \t [-1.84902516 -1.56645975]. \t  -29.185235543317944 \t -1.0770335065906664\n",
            "97     \t [-1.70652536 -3.31220194]. \t  -40.39018485598808 \t -1.0770335065906664\n",
            "98     \t [1.24394247 0.70982303]. \t  -24.16839489903169 \t -1.0770335065906664\n",
            "99     \t [-0.42248397  0.38230657]. \t  -36.549961995801674 \t -1.0770335065906664\n",
            "100    \t [-1.38308906 -1.19090568]. \t  -27.124084423856694 \t -1.0770335065906664\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWnL1rwztdYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2949df-01f5-4aaa-a989-2fbf1cac409d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_3 = d2GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.57613422 -4.00126694]. \t  -41.5244070968694 \t -3.7496552211609124\n",
            "init   \t [-0.1421181  -0.00172874]. \t  -3.7496552211609124 \t -3.7496552211609124\n",
            "init   \t [ 1.60880639 -2.70454056]. \t  -40.473138462528404 \t -3.7496552211609124\n",
            "init   \t [ 1.15501968 -3.89475939]. \t  -22.991135000503405 \t -3.7496552211609124\n",
            "init   \t [ 2.17295152 -1.30498005]. \t  -25.1566837825139 \t -3.7496552211609124\n",
            "1      \t [-1.34780332 -5.1439809 ]. \t  -47.863207941411204 \t -3.7496552211609124\n",
            "2      \t [-9.19308777 -9.63194155]. \t  -200.54301945763234 \t -3.7496552211609124\n",
            "3      \t [-4.51210917  5.02429865]. \t  -65.6900949646576 \t -3.7496552211609124\n",
            "4      \t [3.78754246 5.08137722]. \t  -49.107773225265674 \t -3.7496552211609124\n",
            "5      \t [-22.60391775 -21.38093495]. \t  -1003.3542957570972 \t -3.7496552211609124\n",
            "6      \t [-8.09551942 -0.12026844]. \t  -70.02135179065196 \t -3.7496552211609124\n",
            "7      \t [-5.80627849 -6.04314641]. \t  -77.13486103014304 \t -3.7496552211609124\n",
            "8      \t [-14.35376373 -13.36690561]. \t  -417.4746636977309 \t -3.7496552211609124\n",
            "9      \t [-3.76765402 -1.1844059 ]. \t  -30.48536282868995 \t -3.7496552211609124\n",
            "10     \t [-0.12743498  3.41849058]. \t  -33.45724036042844 \t -3.7496552211609124\n",
            "11     \t [4.98318854 1.48819798]. \t  -47.0751556889109 \t -3.7496552211609124\n",
            "12     \t [-4.23749127  1.80160049]. \t  -37.231307106112055 \t -3.7496552211609124\n",
            "13     \t [ 5.10936687 -3.93667648]. \t  -44.6537523790037 \t -3.7496552211609124\n",
            "14     \t [2.02468045 1.31889748]. \t  -20.153828167232334 \t -3.7496552211609124\n",
            "15     \t [-0.92621155 -1.65910754]. \t  -20.07166859520507 \t -3.7496552211609124\n",
            "16     \t [-1.23245004  1.14974889]. \t  -15.849783145558971 \t -3.7496552211609124\n",
            "17     \t [-4.75228369 -3.13187666]. \t  -45.49032677570709 \t -3.7496552211609124\n",
            "18     \t [0.32723659 0.3444879 ]. \t  -30.48461326368149 \t -3.7496552211609124\n",
            "19     \t [-1.69297795 -0.52719512]. \t  -36.50511847260713 \t -3.7496552211609124\n",
            "20     \t [-0.37637355 -0.83558479]. \t  -22.849681740875262 \t -3.7496552211609124\n",
            "21     \t [-1.4971596 -2.7908841]. \t  -37.48826352225518 \t -3.7496552211609124\n",
            "22     \t [ 4.31554725 -1.09675243]. \t  -35.62141063741682 \t -3.7496552211609124\n",
            "23     \t [-1.09443688  0.31501367]. \t  -16.978651491908778 \t -3.7496552211609124\n",
            "24     \t [-0.22947099 -0.09734754]. \t  -10.588829492791568 \t -3.7496552211609124\n",
            "25     \t [ 0.12051163 -2.105398  ]. \t  -9.293382664983032 \t -3.7496552211609124\n",
            "26     \t [2.50679897 1.54046013]. \t  -48.326536490424175 \t -3.7496552211609124\n",
            "27     \t [-0.13093121 -1.69807625]. \t  -19.302801405819018 \t -3.7496552211609124\n",
            "28     \t [-0.27878024 -3.64250659]. \t  -41.39615248202503 \t -3.7496552211609124\n",
            "29     \t [-0.70076697  5.09013651]. \t  -41.006186617766986 \t -3.7496552211609124\n",
            "30     \t [ 0.78025306 -2.16106178]. \t  -18.087732439436273 \t -3.7496552211609124\n",
            "31     \t [ 0.70651238 -2.94408536]. \t  -22.47614657411004 \t -3.7496552211609124\n",
            "32     \t [-0.11300223  0.47003826]. \t  -22.473500524258604 \t -3.7496552211609124\n",
            "33     \t [ 0.66281792 -0.21203122]. \t  -23.329144352884562 \t -3.7496552211609124\n",
            "34     \t [-2.20999166  0.44950151]. \t  -32.099548077329246 \t -3.7496552211609124\n",
            "35     \t [ 1.26692347 -5.20446891]. \t  -46.930981550412994 \t -3.7496552211609124\n",
            "36     \t [ 2.91216711 -1.34940954]. \t  -27.633966788024075 \t -3.7496552211609124\n",
            "37     \t [1.44073446 3.12107054]. \t  -33.887974672568426 \t -3.7496552211609124\n",
            "38     \t [-3.17094796 -5.10636671]. \t  -43.516007877576605 \t -3.7496552211609124\n",
            "39     \t [-2.86745346  3.65276767]. \t  -40.57332354020334 \t -3.7496552211609124\n",
            "40     \t [-5.05965481 -1.01871338]. \t  -27.401201734725248 \t -3.7496552211609124\n",
            "41     \t [ 0.15838247 -0.29793881]. \t  -17.636795756725014 \t -3.7496552211609124\n",
            "42     \t [1.96988577 0.23590696]. \t  -13.230241626514495 \t -3.7496552211609124\n",
            "43     \t [-2.66701263 -3.04363135]. \t  -31.731237933112595 \t -3.7496552211609124\n",
            "44     \t [-1.74709593  1.04878888]. \t  -14.800955341328548 \t -3.7496552211609124\n",
            "45     \t [ 0.29171731 -2.71522428]. \t  -32.216483337652704 \t -3.7496552211609124\n",
            "46     \t [-0.39295052 -4.66082248]. \t  -55.014211611440345 \t -3.7496552211609124\n",
            "47     \t [-1.28468915 -3.93102668]. \t  -30.19021614785626 \t -3.7496552211609124\n",
            "48     \t [-1.15694141 -1.13822457]. \t  -10.654744821986696 \t -3.7496552211609124\n",
            "49     \t [ 2.26897763 -0.71375786]. \t  -29.104817549470564 \t -3.7496552211609124\n",
            "50     \t [1.40906551 4.8910338 ]. \t  -46.57297572038062 \t -3.7496552211609124\n",
            "51     \t [-1.43362008  1.02907162]. \t  -22.423387156981107 \t -3.7496552211609124\n",
            "52     \t [-1.46491439  2.02127549]. \t  -26.07873742506594 \t -3.7496552211609124\n",
            "53     \t [-2.77017035  2.53496984]. \t  -42.59554623935067 \t -3.7496552211609124\n",
            "54     \t [-3.01666195  1.27136862]. \t  -22.109977359340085 \t -3.7496552211609124\n",
            "55     \t [-2.61954764  1.77326653]. \t  -35.85893981664573 \t -3.7496552211609124\n",
            "56     \t [-3.6559946   2.82986902]. \t  -42.133170108035394 \t -3.7496552211609124\n",
            "57     \t [-1.04206034 -0.80155038]. \t  -8.892878906541428 \t -3.7496552211609124\n",
            "58     \t [-0.48353655  1.8365729 ]. \t  -28.378123189920178 \t -3.7496552211609124\n",
            "59     \t [-4.80952503  0.83158653]. \t  -35.264935270933755 \t -3.7496552211609124\n",
            "60     \t [-3.62944214 -2.05901094]. \t  -34.96289462225681 \t -3.7496552211609124\n",
            "61     \t [ 3.39409386 -4.9329143 ]. \t  -54.59532904396704 \t -3.7496552211609124\n",
            "62     \t [-1.1348999   1.86764664]. \t  -11.421329717880875 \t -3.7496552211609124\n",
            "63     \t [-2.34191915  3.15835216]. \t  -35.47406351646635 \t -3.7496552211609124\n",
            "64     \t [ 4.6945397  -2.35898139]. \t  -57.34286858914169 \t -3.7496552211609124\n",
            "65     \t [ 5.04191545 -0.28212981]. \t  -37.85039868926511 \t -3.7496552211609124\n",
            "66     \t [ 0.29090209 -0.01407132]. \t  -12.665638717410989 \t -3.7496552211609124\n",
            "67     \t [0.88693362 2.55379849]. \t  -29.161749739587155 \t -3.7496552211609124\n",
            "68     \t [ 1.18402581 -3.41379072]. \t  -37.59679829728891 \t -3.7496552211609124\n",
            "69     \t [ 3.54553923 -3.02410336]. \t  -41.423939115030905 \t -3.7496552211609124\n",
            "70     \t [ 1.61495581 -0.10611969]. \t  -22.264054766930045 \t -3.7496552211609124\n",
            "71     \t [ 0.65350717 -4.03602475]. \t  -32.66981088199636 \t -3.7496552211609124\n",
            "72     \t [1.98198953 0.86812374]. \t  -7.986822473284435 \t -3.7496552211609124\n",
            "73     \t [ 1.46717524 -4.63406179]. \t  -60.072423791300245 \t -3.7496552211609124\n",
            "74     \t [ 2.53716425 -1.91128933]. \t  -31.332421976508826 \t -3.7496552211609124\n",
            "75     \t [ 3.36044115 -3.72823539]. \t  -52.951129181060175 \t -3.7496552211609124\n",
            "76     \t [2.65440644 2.84820574]. \t  -35.02351431289202 \t -3.7496552211609124\n",
            "77     \t [-0.72471653 -4.10113349]. \t  -30.878340547976723 \t -3.7496552211609124\n",
            "78     \t [-1.9400606   1.66274683]. \t  -22.441152152678733 \t -3.7496552211609124\n",
            "79     \t [3.58711701 0.0278851 ]. \t  -31.560232085298694 \t -3.7496552211609124\n",
            "80     \t [-5.03436384  1.56005441]. \t  -47.30728568293692 \t -3.7496552211609124\n",
            "81     \t [-3.32172957 -3.51263377]. \t  -57.69686253644126 \t -3.7496552211609124\n",
            "82     \t [-0.88924723 -3.69478373]. \t  -30.16745137267939 \t -3.7496552211609124\n",
            "83     \t [-2.58813221  0.96401628]. \t  -26.387836729164857 \t -3.7496552211609124\n",
            "84     \t [-2.61430923 -4.53504143]. \t  -64.68955555608015 \t -3.7496552211609124\n",
            "85     \t [-4.55225793 -1.38971822]. \t  -59.81397322965937 \t -3.7496552211609124\n",
            "86     \t [1.55129904 0.72912748]. \t  -33.73088474935756 \t -3.7496552211609124\n",
            "87     \t [2.86402542 0.68972848]. \t  -25.80840689578927 \t -3.7496552211609124\n",
            "88     \t [ 1.48651599 -0.65039847]. \t  -38.454458790803265 \t -3.7496552211609124\n",
            "89     \t [2.34942717 3.57111146]. \t  -53.13965718014056 \t -3.7496552211609124\n",
            "90     \t [-0.04798295  4.2000062 ]. \t  -25.00359142613922 \t -3.7496552211609124\n",
            "91     \t [-1.0038125   1.58432386]. \t  -22.14955889531342 \t -3.7496552211609124\n",
            "92     \t [-0.71133055  2.41529951]. \t  -37.36248582415527 \t -3.7496552211609124\n",
            "93     \t [0.97099974 4.25588533]. \t  -29.590653332565395 \t -3.7496552211609124\n",
            "94     \t [-5.43015774  0.63741267]. \t  -65.44404599000931 \t -3.7496552211609124\n",
            "95     \t [0.28323778 2.71291794]. \t  -31.8223025733262 \t -3.7496552211609124\n",
            "96     \t [0.30233743 1.84041372]. \t  -21.32784130473149 \t -3.7496552211609124\n",
            "97     \t [-3.39720461  2.62652153]. \t  -53.428449321575265 \t -3.7496552211609124\n",
            "98     \t [-3.8004947   1.37675938]. \t  -40.3683114708339 \t -3.7496552211609124\n",
            "99     \t [-5.1160143   3.52159475]. \t  -61.02450937588409 \t -3.7496552211609124\n",
            "100    \t [-3.78680884  3.47184936]. \t  -53.945436784697804 \t -3.7496552211609124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zciLjgNltda2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62f4bf2f-f996-44f7-de7c-b327da91e979"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_4 = d2GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.47505713 3.29006044]. \t  -55.26854109146732 \t -23.867844182996272\n",
            "init   \t [1.45620137 1.71235961]. \t  -37.01945021627456 \t -23.867844182996272\n",
            "init   \t [-4.71920737 -4.44751474]. \t  -73.43516933853265 \t -23.867844182996272\n",
            "init   \t [-2.28863345 -1.88046844]. \t  -23.867844182996272 \t -23.867844182996272\n",
            "init   \t [-3.1376137  -3.10176889]. \t  -24.952199788254394 \t -23.867844182996272\n",
            "1      \t [-10.07228624  -8.7125465 ]. \t  -190.70485298637533 \t -23.867844182996272\n",
            "2      \t [-337.08570719 -336.95307717]. \t  -227165.99696171694 \t -23.867844182996272\n",
            "3      \t [-17.67054828 -17.54280891]. \t  -654.4261753648173 \t -23.867844182996272\n",
            "4      \t [-4.6275527  4.4061656]. \t  -76.09703739304321 \t -23.867844182996272\n",
            "5      \t [-29.36686407 -28.14398832]. \t  -1675.0181295491743 \t -23.867844182996272\n",
            "6      \t [ 4.95514819 -5.03704078]. \t  -50.589348647285576 \t -23.867844182996272\n",
            "7      \t [-0.17906846 -7.34226467]. \t  -75.10810046361618 \t -23.867844182996272\n",
            "8      \t [-7.53504166 -0.13542384]. \t  -79.96066928140677 \t -23.867844182996272\n",
            "9      \t [-33.66926943 -33.37726037]. \t  -2279.6897016394555 \t -23.867844182996272\n",
            "10     \t [ 4.6571073 -0.9405952]. \t  -38.772734936076304 \t -23.867844182996272\n",
            "11     \t [-13.51816243 -13.38385175]. \t  -399.2563634685183 \t -23.867844182996272\n",
            "12     \t [ 1.14088881 -3.11790972]. \t  \u001b[92m-17.31291627506333\u001b[0m \t -17.31291627506333\n",
            "13     \t [ -4.75180974 -12.76080306]. \t  -204.62582820810414 \t -17.31291627506333\n",
            "14     \t [-38.50802357 -44.01118084]. \t  -3439.8638792689076 \t -17.31291627506333\n",
            "15     \t [-62.25048832 -67.6345041 ]. \t  -8476.216582004421 \t -17.31291627506333\n",
            "16     \t [-48.60595982 -47.78780691]. \t  -4671.7250626687355 \t -17.31291627506333\n",
            "17     \t [-10.9599058  -19.38829492]. \t  -513.9775591589846 \t -17.31291627506333\n",
            "18     \t [-3.91019785 -8.75737656]. \t  -103.06804083153277 \t -17.31291627506333\n",
            "19     \t [ -9.03787059 -14.02389199]. \t  -278.74687959808034 \t -17.31291627506333\n",
            "20     \t [-286.22674749 -286.15952952]. \t  -163826.18830361063 \t -17.31291627506333\n",
            "21     \t [-169.57033662 -172.69352772]. \t  -58609.667092480566 \t -17.31291627506333\n",
            "22     \t [-2.10440046  1.42953432]. \t  -27.583260720758823 \t -17.31291627506333\n",
            "23     \t [-0.26347004  4.78822881]. \t  -41.462933147433205 \t -17.31291627506333\n",
            "24     \t [-8.05765516 -5.58459445]. \t  -115.382875344811 \t -17.31291627506333\n",
            "25     \t [-28.0203584  -23.15116447]. \t  -1325.3800982822693 \t -17.31291627506333\n",
            "26     \t [-18.72765919 -12.04471311]. \t  -507.5914914092134 \t -17.31291627506333\n",
            "27     \t [-4.56248282  0.40844075]. \t  -58.61266135322592 \t -17.31291627506333\n",
            "28     \t [ 0.32355285 -0.85775798]. \t  -19.034068629744418 \t -17.31291627506333\n",
            "29     \t [-0.73235284 -4.27906604]. \t  -41.7694177246737 \t -17.31291627506333\n",
            "30     \t [ 2.27650302 -5.54534927]. \t  -67.18770408670187 \t -17.31291627506333\n",
            "31     \t [ 2.69201512 -2.59424808]. \t  -45.83756852587166 \t -17.31291627506333\n",
            "32     \t [5.06028941 4.66189857]. \t  -63.30594050757011 \t -17.31291627506333\n",
            "33     \t [4.93400108 1.6408038 ]. \t  -44.21940915818847 \t -17.31291627506333\n",
            "34     \t [-5.32944539 -2.25264137]. \t  -58.43030659716356 \t -17.31291627506333\n",
            "35     \t [-1.88761519  3.35686132]. \t  -33.443909009662434 \t -17.31291627506333\n",
            "36     \t [2.63718693 5.10558913]. \t  -51.652412370925234 \t -17.31291627506333\n",
            "37     \t [-0.70268561  0.42172994]. \t  -32.415790528476535 \t -17.31291627506333\n",
            "38     \t [ 2.33520028 -0.56268412]. \t  -40.10537759877993 \t -17.31291627506333\n",
            "39     \t [ 0.30127378 -1.85460394]. \t  -20.587120702782737 \t -17.31291627506333\n",
            "40     \t [ 5.04494804 -3.14861018]. \t  -39.81312491575146 \t -17.31291627506333\n",
            "41     \t [ 0.79905755 -2.94284193]. \t  \u001b[92m-16.90300023356707\u001b[0m \t -16.90300023356707\n",
            "42     \t [-6.79460152 -8.74026001]. \t  -140.4044951743432 \t -16.90300023356707\n",
            "43     \t [-2.98146342 -5.66601485]. \t  -56.096022753144794 \t -16.90300023356707\n",
            "44     \t [-2.11900285  4.90807631]. \t  -32.86905499717072 \t -16.90300023356707\n",
            "45     \t [ 0.87381349 -3.39224511]. \t  -33.0470010288545 \t -16.90300023356707\n",
            "46     \t [0.63493924 3.13146919]. \t  -30.047341293097805 \t -16.90300023356707\n",
            "47     \t [-3.52008955  2.18627509]. \t  -43.19344439684038 \t -16.90300023356707\n",
            "48     \t [ 0.55121805 -2.69488249]. \t  -40.44719493145177 \t -16.90300023356707\n",
            "49     \t [ 1.34074805 -2.15421163]. \t  -26.174392434394775 \t -16.90300023356707\n",
            "50     \t [-1.99933355 -0.66589498]. \t  -19.482769999200094 \t -16.90300023356707\n",
            "51     \t [-0.20030572 -1.50637813]. \t  -29.22937293875056 \t -16.90300023356707\n",
            "52     \t [ 1.06996242 -2.45481711]. \t  -27.72139545098083 \t -16.90300023356707\n",
            "53     \t [ 0.80646131 -1.22724038]. \t  -17.257714794690237 \t -16.90300023356707\n",
            "54     \t [ 2.50438543 -1.70835435]. \t  -41.77353974779082 \t -16.90300023356707\n",
            "55     \t [ 0.21429119 -1.25522753]. \t  -19.725039309710983 \t -16.90300023356707\n",
            "56     \t [0.79537222 0.48986506]. \t  -28.039954064633033 \t -16.90300023356707\n",
            "57     \t [-2.72655165 -0.99241907]. \t  -19.898300687064697 \t -16.90300023356707\n",
            "58     \t [-1.88791787 -4.19087047]. \t  -29.87620586375101 \t -16.90300023356707\n",
            "59     \t [-2.23606577  0.12370103]. \t  -17.01235328724786 \t -16.90300023356707\n",
            "60     \t [ 0.40127563 -0.12819773]. \t  -21.386898543493352 \t -16.90300023356707\n",
            "61     \t [2.24828491 0.49593875]. \t  -35.18972450124245 \t -16.90300023356707\n",
            "62     \t [-1.4573883  -5.16200007]. \t  -53.16220248398626 \t -16.90300023356707\n",
            "63     \t [-1.60897089  0.65354004]. \t  -36.458555601394764 \t -16.90300023356707\n",
            "64     \t [0.27802633 1.99720353]. \t  \u001b[92m-15.81952370553313\u001b[0m \t -15.81952370553313\n",
            "65     \t [ 2.03749181 -2.05723925]. \t  \u001b[92m-9.299567826602484\u001b[0m \t -9.299567826602484\n",
            "66     \t [-0.53159181  1.9425632 ]. \t  -24.503938820603114 \t -9.299567826602484\n",
            "67     \t [-2.49039527 -3.25203089]. \t  -46.887170447468066 \t -9.299567826602484\n",
            "68     \t [-3.59600293 -2.61931296]. \t  -55.34642768327024 \t -9.299567826602484\n",
            "69     \t [-3.35507378 -4.23189283]. \t  -54.162915778102345 \t -9.299567826602484\n",
            "70     \t [ 4.92625307 -0.2008433 ]. \t  -32.32304044910741 \t -9.299567826602484\n",
            "71     \t [ 1.91946791 -1.83300848]. \t  -13.315052565467337 \t -9.299567826602484\n",
            "72     \t [-2.90221022  0.0880574 ]. \t  -11.751513803443768 \t -9.299567826602484\n",
            "73     \t [-2.47079492 -0.3401667 ]. \t  -41.41975831753159 \t -9.299567826602484\n",
            "74     \t [-3.45114966  0.54326359]. \t  -51.37101138881297 \t -9.299567826602484\n",
            "75     \t [-4.10995396 -0.91494774]. \t  -21.416126553663947 \t -9.299567826602484\n",
            "76     \t [-1.51498957 -1.12432315]. \t  -26.413901984466037 \t -9.299567826602484\n",
            "77     \t [-3.2878463   3.03246717]. \t  -32.568749928727605 \t -9.299567826602484\n",
            "78     \t [-2.85082396  0.69286635]. \t  -26.20081038895211 \t -9.299567826602484\n",
            "79     \t [-2.11868522  2.50266465]. \t  -33.40476846209907 \t -9.299567826602484\n",
            "80     \t [-3.70527852 -0.41562699]. \t  -45.302343423517115 \t -9.299567826602484\n",
            "81     \t [-3.0055359 -1.938253 ]. \t  -13.539321973506176 \t -9.299567826602484\n",
            "82     \t [-4.97516586  2.09102649]. \t  -30.837613941230003 \t -9.299567826602484\n",
            "83     \t [-4.5738066  -1.34565306]. \t  -57.329069089628206 \t -9.299567826602484\n",
            "84     \t [-6.05676563 -3.32243419]. \t  -62.74800609170278 \t -9.299567826602484\n",
            "85     \t [-3.75799997  2.66654727]. \t  -45.737092505282135 \t -9.299567826602484\n",
            "86     \t [-4.01148324 -1.33279527]. \t  -32.865052096055386 \t -9.299567826602484\n",
            "87     \t [-2.61275722 -1.79172296]. \t  -35.038718438863505 \t -9.299567826602484\n",
            "88     \t [-1.16352448 -3.19479645]. \t  -22.991110227222862 \t -9.299567826602484\n",
            "89     \t [-5.51724833 -0.76830572]. \t  -59.82400719829411 \t -9.299567826602484\n",
            "90     \t [-1.07959691 -2.37977946]. \t  -25.33383343790431 \t -9.299567826602484\n",
            "91     \t [-1.61105442 -2.8117764 ]. \t  -34.37952595661954 \t -9.299567826602484\n",
            "92     \t [-1.7020431  -1.95384014]. \t  -20.0999217113688 \t -9.299567826602484\n",
            "93     \t [-3.08777466 -2.84193591]. \t  -23.633009400224907 \t -9.299567826602484\n",
            "94     \t [-1.16298531 -0.50634347]. \t  -26.402014685423516 \t -9.299567826602484\n",
            "95     \t [-5.01255014 -0.69972484]. \t  -38.752956277417375 \t -9.299567826602484\n",
            "96     \t [-4.71360985  1.61737973]. \t  -54.502020043005786 \t -9.299567826602484\n",
            "97     \t [-3.43528138 -1.47198576]. \t  -52.99794030735213 \t -9.299567826602484\n",
            "98     \t [-6.19686443 -2.6394824 ]. \t  -68.49033118667172 \t -9.299567826602484\n",
            "99     \t [-2.94723806 -3.83747799]. \t  -28.73313350302084 \t -9.299567826602484\n",
            "100    \t [-0.78286485  2.565479  ]. \t  -34.309796294478446 \t -9.299567826602484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH76hlE2tddm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fc58fc-f78b-474b-a247-77e98da2108d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_5 = d2GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.58729753 0.42762882]. \t  -38.044660353205785 \t -26.56648173948617\n",
            "init   \t [ 0.61873954 -1.20782752]. \t  -26.56648173948617 \t -26.56648173948617\n",
            "init   \t [-3.90047561  3.54303094]. \t  -49.29579717740555 \t -26.56648173948617\n",
            "init   \t [2.45495411 4.42734146]. \t  -64.2061730192077 \t -26.56648173948617\n",
            "init   \t [3.67951879 2.13099137]. \t  -35.56518260713154 \t -26.56648173948617\n",
            "1      \t [-9.85930898 -3.72259796]. \t  -126.43621071973061 \t -26.56648173948617\n",
            "2      \t [-1.44178692 -6.89087829]. \t  -71.16127164465547 \t -26.56648173948617\n",
            "3      \t [-10.36658105 -10.01192982]. \t  -224.4201285109921 \t -26.56648173948617\n",
            "4      \t [-16.72109982 -19.48823742]. \t  -691.1651678044432 \t -26.56648173948617\n",
            "5      \t [ 4.64513309 -5.0778421 ]. \t  -64.65662506880828 \t -26.56648173948617\n",
            "6      \t [-4.61102949 -2.06639377]. \t  -44.0529006709363 \t -26.56648173948617\n",
            "7      \t [-63.0089508  -65.69477947]. \t  -8299.348153530282 \t -26.56648173948617\n",
            "8      \t [-54.23923385 -57.31389034]. \t  -6250.007968245459 \t -26.56648173948617\n",
            "9      \t [ -9.99977525 -16.37920504]. \t  -385.52927585894156 \t -26.56648173948617\n",
            "10     \t [-8.38631313  1.28859512]. \t  -101.94744793185704 \t -26.56648173948617\n",
            "11     \t [-22.96176425 -21.73842806]. \t  -1010.81551426745 \t -26.56648173948617\n",
            "12     \t [-43.90927096 -45.83955724]. \t  -4035.535679329893 \t -26.56648173948617\n",
            "13     \t [-297.39992084 -299.78856972]. \t  -178345.5869378021 \t -26.56648173948617\n",
            "14     \t [-28.75593846 -24.77324043]. \t  -1458.7893449164528 \t -26.56648173948617\n",
            "15     \t [-6.08750516 -7.02224037]. \t  -87.94082464277457 \t -26.56648173948617\n",
            "16     \t [ 4.96100719 -1.76481716]. \t  -37.0951510227415 \t -26.56648173948617\n",
            "17     \t [-14.88854281 -13.86936594]. \t  -419.5651943027401 \t -26.56648173948617\n",
            "18     \t [-34.68372042 -40.31194638]. \t  -2855.853293598313 \t -26.56648173948617\n",
            "19     \t [ 1.1526028  -4.21338496]. \t  -31.05598875934919 \t -26.56648173948617\n",
            "20     \t [-92.34763642 -94.40590027]. \t  -17474.619488723536 \t -26.56648173948617\n",
            "21     \t [ -5.71590858 -11.83320052]. \t  -189.82915805146737 \t -26.56648173948617\n",
            "22     \t [-27.6556184  -34.05423464]. \t  -1940.6877929623874 \t -26.56648173948617\n",
            "23     \t [-23.46292222 -27.77252746]. \t  -1350.1411382579538 \t -26.56648173948617\n",
            "24     \t [-1029.48320655 -1028.44704384]. \t  -2117578.3904608195 \t -26.56648173948617\n",
            "25     \t [-66.25004149 -72.31032375]. \t  -9641.553675840825 \t -26.56648173948617\n",
            "26     \t [-19.5144449  -24.17457972]. \t  -990.6192878667918 \t -26.56648173948617\n",
            "27     \t [-130.78651601 -134.90896936]. \t  -35314.8600877 \t -26.56648173948617\n",
            "28     \t [-15.66360628  -8.10456188]. \t  -328.27968630403245 \t -26.56648173948617\n",
            "29     \t [-2.40991128 -9.98627024]. \t  -124.01074316141744 \t -26.56648173948617\n",
            "30     \t [-127.39292881 -126.39047069]. \t  -32239.054464341087 \t -26.56648173948617\n",
            "31     \t [-39.93976743 -42.10538724]. \t  -3370.869666785847 \t -26.56648173948617\n",
            "32     \t [-0.97433238  4.6866291 ]. \t  -36.92091326274955 \t -26.56648173948617\n",
            "33     \t [-27.17190487 -29.89834778]. \t  -1639.4825864547365 \t -26.56648173948617\n",
            "34     \t [-50.03078929 -48.91857883]. \t  -4897.57407297519 \t -26.56648173948617\n",
            "35     \t [-2.51702921  0.38676338]. \t  -44.00175389894249 \t -26.56648173948617\n",
            "36     \t [-252.79757533 -258.52242726]. \t  -130767.41544238017 \t -26.56648173948617\n",
            "37     \t [-1.78856272 -3.22541192]. \t  -29.66413354823511 \t -26.56648173948617\n",
            "38     \t [-21.29339343 -17.57941524]. \t  -793.9179863696961 \t -26.56648173948617\n",
            "39     \t [-74.16185372 -69.74721006]. \t  -10379.569581599419 \t -26.56648173948617\n",
            "40     \t [-32.74792918 -35.35532737]. \t  -2348.7014595216156 \t -26.56648173948617\n",
            "41     \t [-7.37211538 -1.66198205]. \t  -89.304728357757 \t -26.56648173948617\n",
            "42     \t [ 1.3357109 -6.7695032]. \t  -71.5167501094992 \t -26.56648173948617\n",
            "43     \t [-80.80388486 -83.21539402]. \t  -13468.59097328523 \t -26.56648173948617\n",
            "44     \t [-4.19727531 -4.79983072]. \t  -54.32291447288942 \t -26.56648173948617\n",
            "45     \t [ -9.25738987 -13.15989627]. \t  -273.98251978593754 \t -26.56648173948617\n",
            "46     \t [-10.10587169  -6.86779396]. \t  -154.68359874281896 \t -26.56648173948617\n",
            "47     \t [-73.81506158 -76.71097211]. \t  -11351.689220452909 \t -26.56648173948617\n",
            "48     \t [5.01899025 5.09014782]. \t  -52.732670062663566 \t -26.56648173948617\n",
            "49     \t [-5.00076594  0.86548319]. \t  -29.120975666225547 \t -26.56648173948617\n",
            "50     \t [ 2.47668826 -1.84195299]. \t  -33.95822774230571 \t -26.56648173948617\n",
            "51     \t [-6.26476567  3.59083177]. \t  -81.4829463157347 \t -26.56648173948617\n",
            "52     \t [-0.63398469  2.49736059]. \t  -43.29820757725485 \t -26.56648173948617\n",
            "53     \t [5.04543437 0.77188344]. \t  -35.08627657142859 \t -26.56648173948617\n",
            "54     \t [-0.7704305 -1.7168676]. \t  \u001b[92m-24.327800307752653\u001b[0m \t -24.327800307752653\n",
            "55     \t [2.61113901 0.83756446]. \t  -29.950440300288598 \t -24.327800307752653\n",
            "56     \t [-6.97581183 -9.47052024]. \t  -158.2969150597142 \t -24.327800307752653\n",
            "57     \t [-2.674217    4.97816938]. \t  -46.61121679266764 \t -24.327800307752653\n",
            "58     \t [-1.04049026 -4.27970353]. \t  -31.57586948404694 \t -24.327800307752653\n",
            "59     \t [ 2.94485546 -4.03991352]. \t  -25.900166041269966 \t -24.327800307752653\n",
            "60     \t [4.88178061 2.88735249]. \t  -37.20473427020248 \t -24.327800307752653\n",
            "61     \t [-2.72286182 -2.00273508]. \t  \u001b[92m-23.123292363639447\u001b[0m \t -23.123292363639447\n",
            "62     \t [ 0.61354956 -2.43746135]. \t  -43.11659317790125 \t -23.123292363639447\n",
            "63     \t [1.25967607 2.65639043]. \t  -34.79914843754799 \t -23.123292363639447\n",
            "64     \t [0.35239325 4.60886418]. \t  -55.115073326080065 \t -23.123292363639447\n",
            "65     \t [-1.11592942 -1.43860601]. \t  -25.117687301028344 \t -23.123292363639447\n",
            "66     \t [-2.54309397  2.47332112]. \t  -52.08013589637689 \t -23.123292363639447\n",
            "67     \t [ 3.49095084 -0.68360924]. \t  -46.6894229079177 \t -23.123292363639447\n",
            "68     \t [ 3.52730733 -3.21507997]. \t  -50.45527500518869 \t -23.123292363639447\n",
            "69     \t [ 2.15269638 -4.08892355]. \t  -27.13411798322354 \t -23.123292363639447\n",
            "70     \t [-4.1995019  -0.09108769]. \t  -26.11773052903097 \t -23.123292363639447\n",
            "71     \t [-2.8781917 -2.5565119]. \t  -36.9845055405383 \t -23.123292363639447\n",
            "72     \t [-2.4209071  -1.37887118]. \t  -43.79341001378403 \t -23.123292363639447\n",
            "73     \t [-1.97909666 -2.3537308 ]. \t  -25.60886311590454 \t -23.123292363639447\n",
            "74     \t [-0.44911581 -0.42819825]. \t  -38.87780028239696 \t -23.123292363639447\n",
            "75     \t [1.87378311 1.83359085]. \t  \u001b[92m-14.842315849406486\u001b[0m \t -14.842315849406486\n",
            "76     \t [ 2.76152838 -5.11534569]. \t  -45.582354807844865 \t -14.842315849406486\n",
            "77     \t [1.65995106 1.49229821]. \t  -40.33154901566153 \t -14.842315849406486\n",
            "78     \t [2.35429333 2.66911609]. \t  -43.62693417269064 \t -14.842315849406486\n",
            "79     \t [3.13512318 0.99169722]. \t  \u001b[92m-14.218754022114465\u001b[0m \t -14.218754022114465\n",
            "80     \t [ 1.43695965 -1.24185601]. \t  -32.32133051660905 \t -14.218754022114465\n",
            "81     \t [2.74147796 1.5477781 ]. \t  -39.99929560639761 \t -14.218754022114465\n",
            "82     \t [4.27933591 1.66336643]. \t  -48.09080496977872 \t -14.218754022114465\n",
            "83     \t [3.09007433 0.83847332]. \t  -16.533789668364548 \t -14.218754022114465\n",
            "84     \t [-3.49374378 -1.76224345]. \t  -44.5355026331155 \t -14.218754022114465\n",
            "85     \t [3.39472164 0.76416999]. \t  -39.109758163250625 \t -14.218754022114465\n",
            "86     \t [1.75837808 2.17017059]. \t  -22.467225318830835 \t -14.218754022114465\n",
            "87     \t [3.59331735 3.68702187]. \t  -58.69039971958301 \t -14.218754022114465\n",
            "88     \t [0.92757566 3.12944921]. \t  -14.800825326804915 \t -14.218754022114465\n",
            "89     \t [ 3.07360595 -1.48877784]. \t  -32.689175996388606 \t -14.218754022114465\n",
            "90     \t [-0.26244804  0.47074995]. \t  -30.90341403099391 \t -14.218754022114465\n",
            "91     \t [ 5.10128431 -0.18273574]. \t  -33.91237928159696 \t -14.218754022114465\n",
            "92     \t [0.82411286 3.37668947]. \t  -34.73675361403076 \t -14.218754022114465\n",
            "93     \t [ 0.21960506 -0.31844695]. \t  -22.420749999314573 \t -14.218754022114465\n",
            "94     \t [0.09802735 3.17674459]. \t  -17.49694984802118 \t -14.218754022114465\n",
            "95     \t [0.3288883  2.86033199]. \t  -26.655577261114516 \t -14.218754022114465\n",
            "96     \t [ 1.88233467 -2.73727102]. \t  -24.445453798487772 \t -14.218754022114465\n",
            "97     \t [-0.71911747  5.06591403]. \t  -38.9542808990315 \t -14.218754022114465\n",
            "98     \t [-1.20298417  2.90043458]. \t  -18.842189156412637 \t -14.218754022114465\n",
            "99     \t [-1.6237951   3.36455339]. \t  -47.67337032131829 \t -14.218754022114465\n",
            "100    \t [0.28494635 5.0857035 ]. \t  -39.53888141023608 \t -14.218754022114465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh_Amb8TtdgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7064f4-1768-4f58-e5c2-f0d9bdac1f64"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_6 = d2GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 4.02288795 -1.72052679]. \t  -31.08835710146886 \t -17.28954482757088\n",
            "init   \t [ 3.28938622 -4.69302655]. \t  -58.797867722203385 \t -17.28954482757088\n",
            "init   \t [-4.0175956   0.97333314]. \t  -17.28954482757088 \t -17.28954482757088\n",
            "init   \t [ 0.30532979 -0.83141193]. \t  -19.296253155889353 \t -17.28954482757088\n",
            "init   \t [-1.68542362  1.25459899]. \t  -28.650630936276173 \t -17.28954482757088\n",
            "1      \t [-70.84734758 -64.77047507]. \t  -9227.53594444011 \t -17.28954482757088\n",
            "2      \t [-6.33747    -8.61159755]. \t  -147.18727918833318 \t -17.28954482757088\n",
            "3      \t [-23.52843143 -24.41435867]. \t  -1188.0756956917219 \t -17.28954482757088\n",
            "4      \t [-12.60558893 -13.30049496]. \t  -366.8025865366067 \t -17.28954482757088\n",
            "5      \t [-16.36827882 -21.71989254]. \t  -768.320966907823 \t -17.28954482757088\n",
            "6      \t [-40.60670299 -36.30135236]. \t  -2997.698897494876 \t -17.28954482757088\n",
            "7      \t [4.07453055 4.6942    ]. \t  -53.148525310167656 \t -17.28954482757088\n",
            "8      \t [-52.53335455 -47.31525256]. \t  -5032.2537020819855 \t -17.28954482757088\n",
            "9      \t [-8.65418001 -3.2350663 ]. \t  -110.08692144066015 \t -17.28954482757088\n",
            "10     \t [-18.93267326 -12.4834438 ]. \t  -535.1099208349675 \t -17.28954482757088\n",
            "11     \t [-1.89939369 -5.49936396]. \t  -55.78290114965185 \t -17.28954482757088\n",
            "12     \t [-188.05903661 -188.55628045]. \t  -70939.73352058209 \t -17.28954482757088\n",
            "13     \t [-0.08191635  4.84903849]. \t  -28.986588341098606 \t -17.28954482757088\n",
            "14     \t [-4.2641635   4.45848777]. \t  -68.61171385011515 \t -17.28954482757088\n",
            "15     \t [-4.51267123 -2.70278911]. \t  -60.56063038997633 \t -17.28954482757088\n",
            "16     \t [-12.27812638  -7.48150177]. \t  -238.41583562908332 \t -17.28954482757088\n",
            "17     \t [-16.16050908 -16.76370989]. \t  -555.9924366959067 \t -17.28954482757088\n",
            "18     \t [-30.10744083 -25.11548223]. \t  -1541.957774035796 \t -17.28954482757088\n",
            "19     \t [2.25233969 1.85578155]. \t  -22.496163016547573 \t -17.28954482757088\n",
            "20     \t [-59.52575241 -56.44526758]. \t  -6768.667300160515 \t -17.28954482757088\n",
            "21     \t [-48.21718371 -53.83621608]. \t  -5236.031604273094 \t -17.28954482757088\n",
            "22     \t [-7.11717186  0.17834738]. \t  -58.924255792759965 \t -17.28954482757088\n",
            "23     \t [-29.68349527 -31.81880953]. \t  -1913.4146066863284 \t -17.28954482757088\n",
            "24     \t [ 0.44910874 -3.63216057]. \t  -49.63328438631142 \t -17.28954482757088\n",
            "25     \t [4.63551718 0.92029872]. \t  -40.151584413051076 \t -17.28954482757088\n",
            "26     \t [-45.78350083 -40.56428889]. \t  -3768.6962705858728 \t -17.28954482757088\n",
            "27     \t [-5.9271243  -5.42309207]. \t  -84.42586692347764 \t -17.28954482757088\n",
            "28     \t [-2.17482425 -1.48588694]. \t  -32.34868850801815 \t -17.28954482757088\n",
            "29     \t [0.44686777 2.67347651]. \t  -41.42006521807813 \t -17.28954482757088\n",
            "30     \t [ 2.13335285 -0.31695323]. \t  -22.045067098999894 \t -17.28954482757088\n",
            "31     \t [-5.59542727  1.9481497 ]. \t  -53.885782178425444 \t -17.28954482757088\n",
            "32     \t [-2.11643277  5.07933743]. \t  -34.05463612222143 \t -17.28954482757088\n",
            "33     \t [1.50541739 4.64124176]. \t  -60.11554332794529 \t -17.28954482757088\n",
            "34     \t [-12.1016494  -18.13032143]. \t  -480.29891703166027 \t -17.28954482757088\n",
            "35     \t [-2.14977406  2.73723259]. \t  -27.025978800793823 \t -17.28954482757088\n",
            "36     \t [-4.07096935  0.25013211]. \t  -27.62148790999602 \t -17.28954482757088\n",
            "37     \t [-3.7500302   2.01369527]. \t  -28.152797769576342 \t -17.28954482757088\n",
            "38     \t [ 4.97406444 -2.70404831]. \t  -45.03295994726038 \t -17.28954482757088\n",
            "39     \t [ 2.53683957 -2.39325125]. \t  -49.73023890998492 \t -17.28954482757088\n",
            "40     \t [0.84842309 0.34730123]. \t  -20.78286236476924 \t -17.28954482757088\n",
            "41     \t [3.6184673  2.33661826]. \t  -51.08602933070778 \t -17.28954482757088\n",
            "42     \t [ 5.10529117 -4.94348932]. \t  -53.23552899570069 \t -17.28954482757088\n",
            "43     \t [-1.88947832 -3.06208975]. \t  \u001b[92m-16.01369545784623\u001b[0m \t -16.01369545784623\n",
            "44     \t [-2.17392879 -3.43962689]. \t  -41.24633708146416 \t -16.01369545784623\n",
            "45     \t [-1.02069297 -2.16361782]. \t  \u001b[92m-10.642487947611883\u001b[0m \t -10.642487947611883\n",
            "46     \t [-2.95653203  0.69103484]. \t  -23.20997659706156 \t -10.642487947611883\n",
            "47     \t [-1.98710137 -2.60758747]. \t  -28.581773951068545 \t -10.642487947611883\n",
            "48     \t [-0.46272967 -2.42637625]. \t  -44.77746751085098 \t -10.642487947611883\n",
            "49     \t [-1.02504291 -2.97843127]. \t  \u001b[92m-10.136992874043985\u001b[0m \t -10.136992874043985\n",
            "50     \t [-0.96620817 -0.84088675]. \t  \u001b[92m-6.45997565402993\u001b[0m \t -6.45997565402993\n",
            "51     \t [ 4.92265533 -0.76069452]. \t  -35.29752167066455 \t -6.45997565402993\n",
            "52     \t [-1.62497336 -3.14367447]. \t  -33.400813683165794 \t -6.45997565402993\n",
            "53     \t [-2.38245139 -0.41943407]. \t  -41.99213559095263 \t -6.45997565402993\n",
            "54     \t [-2.99111588 -3.15280754]. \t  -23.168312824720758 \t -6.45997565402993\n",
            "55     \t [-1.18483249 -1.57314295]. \t  -28.85990063993591 \t -6.45997565402993\n",
            "56     \t [-0.40854711 -0.3327676 ]. \t  -33.64084321385854 \t -6.45997565402993\n",
            "57     \t [-0.95797733 -2.74665128]. \t  -19.018759606251773 \t -6.45997565402993\n",
            "58     \t [-0.19874076 -4.24868417]. \t  -34.8428196621467 \t -6.45997565402993\n",
            "59     \t [-3.68505605 -1.50697048]. \t  -49.80925560866356 \t -6.45997565402993\n",
            "60     \t [-0.54897892 -1.24464317]. \t  -31.044193205444262 \t -6.45997565402993\n",
            "61     \t [ 1.56497897 -0.44341014]. \t  -41.19834363117792 \t -6.45997565402993\n",
            "62     \t [-1.12684084  0.04700484]. \t  \u001b[92m-4.716137496260384\u001b[0m \t -4.716137496260384\n",
            "63     \t [-0.60669737 -3.42417263]. \t  -48.81512019875785 \t -4.716137496260384\n",
            "64     \t [ 1.02883294 -4.34587185]. \t  -35.774807524798696 \t -4.716137496260384\n",
            "65     \t [ 1.39387446 -2.75329153]. \t  -37.17471003404922 \t -4.716137496260384\n",
            "66     \t [-5.41044372 -2.92466711]. \t  -57.38421407091497 \t -4.716137496260384\n",
            "67     \t [-0.97384247 -6.35697003]. \t  -57.72059661691059 \t -4.716137496260384\n",
            "68     \t [-1.27821558 -0.42868744]. \t  -32.59402205523959 \t -4.716137496260384\n",
            "69     \t [ 2.88505059 -0.00363649]. \t  -10.822927982560586 \t -4.716137496260384\n",
            "70     \t [-3.14203527 -0.77653218]. \t  -22.5408443245136 \t -4.716137496260384\n",
            "71     \t [ 1.17592003 -1.17489858]. \t  -13.729269731847406 \t -4.716137496260384\n",
            "72     \t [-5.10763197 -1.60665612]. \t  -48.707477270160794 \t -4.716137496260384\n",
            "73     \t [-2.8210885  -2.11878903]. \t  -20.78667081185245 \t -4.716137496260384\n",
            "74     \t [1.66859329 0.76867133]. \t  -27.099394802384698 \t -4.716137496260384\n",
            "75     \t [-1.20996287  1.2139003 ]. \t  -18.199593602148415 \t -4.716137496260384\n",
            "76     \t [-2.66647152 -2.99542289]. \t  -31.097378666993745 \t -4.716137496260384\n",
            "77     \t [-3.59006483 -4.23403677]. \t  -58.25541043932609 \t -4.716137496260384\n",
            "78     \t [-5.17301365  0.32325528]. \t  -46.65577147134606 \t -4.716137496260384\n",
            "79     \t [ 0.25082242 -4.7513495 ]. \t  -42.60511773606977 \t -4.716137496260384\n",
            "80     \t [ 0.81122281 -0.88384693]. \t  -10.23369287183744 \t -4.716137496260384\n",
            "81     \t [ 0.44698333 -1.97090311]. \t  -23.70119442655837 \t -4.716137496260384\n",
            "82     \t [-3.17321521 -4.02292231]. \t  -31.71719940815668 \t -4.716137496260384\n",
            "83     \t [ 1.91505029 -1.73719266]. \t  -18.880068144836024 \t -4.716137496260384\n",
            "84     \t [-2.81938725 -1.63497369]. \t  -33.01351671805503 \t -4.716137496260384\n",
            "85     \t [-6.12436963 -3.97030512]. \t  -56.34576045038003 \t -4.716137496260384\n",
            "86     \t [-3.84847414 -2.64563049]. \t  -42.10781360578727 \t -4.716137496260384\n",
            "87     \t [-4.35725566 -3.63423097]. \t  -65.08303542474297 \t -4.716137496260384\n",
            "88     \t [-2.08452266 -6.47464459]. \t  -67.5169739579529 \t -4.716137496260384\n",
            "89     \t [-2.66593695 -0.03976687]. \t  -22.458991288229832 \t -4.716137496260384\n",
            "90     \t [-3.37101724  1.44203597]. \t  -49.679242667117435 \t -4.716137496260384\n",
            "91     \t [-1.95229159  3.63525767]. \t  -34.073434812587266 \t -4.716137496260384\n",
            "92     \t [ 1.3021656  -3.59233418]. \t  -46.1835784614689 \t -4.716137496260384\n",
            "93     \t [-3.81284755  2.87597242]. \t  -31.84788481506781 \t -4.716137496260384\n",
            "94     \t [2.03131943 2.7131703 ]. \t  -23.974031277950704 \t -4.716137496260384\n",
            "95     \t [-4.57865092  1.5345266 ]. \t  -61.88801648909687 \t -4.716137496260384\n",
            "96     \t [-3.74595437 -3.70076282]. \t  -51.026537864991326 \t -4.716137496260384\n",
            "97     \t [ 3.52097461 -2.99519756]. \t  -41.28630890622915 \t -4.716137496260384\n",
            "98     \t [-4.77946424 -5.11856078]. \t  -59.8509162320331 \t -4.716137496260384\n",
            "99     \t [-3.27957051 -0.02979421]. \t  -22.778481867618517 \t -4.716137496260384\n",
            "100    \t [0.92884282 1.52038581]. \t  -24.075327424316647 \t -4.716137496260384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNmoDTuUtdi3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2855fdab-7fe4-48a2-e6b0-d451260edfca"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_7 = d2GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-3.55672335 -2.02386832]. \t  -36.23014472109487 \t -36.23014472109487\n",
            "init   \t [-4.48474711 -0.4110301 ]. \t  -58.713796772117476 \t -36.23014472109487\n",
            "init   \t [3.43299466 4.37244977]. \t  -66.98740910066293 \t -36.23014472109487\n",
            "init   \t [2.3243672  2.74940131]. \t  -37.50394977069271 \t -36.23014472109487\n",
            "init   \t [-2.36334012  1.47485995]. \t  -44.17062022214734 \t -36.23014472109487\n",
            "1      \t [-28.8882353  -27.28935323]. \t  -1594.0524728232526 \t -36.23014472109487\n",
            "2      \t [-8.18893459 -8.77032358]. \t  -158.96032159270067 \t -36.23014472109487\n",
            "3      \t [ 5.03700714 -4.90050661]. \t  -51.54668520076324 \t -36.23014472109487\n",
            "4      \t [-0.27578152 -8.04902916]. \t  -76.94651330350143 \t -36.23014472109487\n",
            "5      \t [-130.70238329 -127.29555838]. \t  -33313.043177843116 \t -36.23014472109487\n",
            "6      \t [-61.59281025 -57.50544777]. \t  -7138.892295717944 \t -36.23014472109487\n",
            "7      \t [-46.85410863 -46.19272655]. \t  -4339.46933203087 \t -36.23014472109487\n",
            "8      \t [-16.69288415 -15.73900826]. \t  -550.5710044987939 \t -36.23014472109487\n",
            "9      \t [-22.5286493  -22.27907003]. \t  -1035.551807064976 \t -36.23014472109487\n",
            "10     \t [-40.03051666 -39.19528445]. \t  -3145.5252919926806 \t -36.23014472109487\n",
            "11     \t [-14.51853608  -9.37370997]. \t  -335.6001058476647 \t -36.23014472109487\n",
            "12     \t [-9.78509351 -2.9739959 ]. \t  -112.53872300804707 \t -36.23014472109487\n",
            "13     \t [-122.7407894  -121.85005298]. \t  -29927.43464507342 \t -36.23014472109487\n",
            "14     \t [ 1.18032741 -1.98992326]. \t  \u001b[92m-11.13383414415017\u001b[0m \t -11.13383414415017\n",
            "15     \t [ -8.19570712 -15.66374365]. \t  -334.33513819208497 \t -11.13383414415017\n",
            "16     \t [-16.8647157  -21.43080535]. \t  -766.1680442160565 \t -11.13383414415017\n",
            "17     \t [ -3.91326235 -11.53186522]. \t  -169.54644468687044 \t -11.13383414415017\n",
            "18     \t [-4.30540655 -6.18745711]. \t  -76.40322944509985 \t -11.13383414415017\n",
            "19     \t [-34.11104564 -36.92880525]. \t  -2530.620950786649 \t -11.13383414415017\n",
            "20     \t [ 4.83742105 -0.58654678]. \t  -47.08146609390417 \t -11.13383414415017\n",
            "21     \t [-100.75353155 -103.29136856]. \t  -20842.72915602248 \t -11.13383414415017\n",
            "22     \t [-4.76468347  4.8203735 ]. \t  -60.7379125407445 \t -11.13383414415017\n",
            "23     \t [-432.47373394 -434.753197  ]. \t  -376073.5361182896 \t -11.13383414415017\n",
            "24     \t [-12.46571309 -16.42808638]. \t  -464.04129187571033 \t -11.13383414415017\n",
            "25     \t [-0.58818288  4.7327147 ]. \t  -52.33238537891024 \t -11.13383414415017\n",
            "26     \t [-0.66765659 -4.28770218]. \t  -46.12299736506337 \t -11.13383414415017\n",
            "27     \t [-10.86673498 -13.03976159]. \t  -291.73727018278436 \t -11.13383414415017\n",
            "28     \t [0.68584166 0.19433328]. \t  -21.004270057675484 \t -11.13383414415017\n",
            "29     \t [ 2.22351932 -5.5148329 ]. \t  -63.65785815686964 \t -11.13383414415017\n",
            "30     \t [-7.06845039 -5.13464134]. \t  -80.60822996366186 \t -11.13383414415017\n",
            "31     \t [-7.66292815 -0.15623993]. \t  -78.3906798733562 \t -11.13383414415017\n",
            "32     \t [ 2.89289723 -2.84626275]. \t  -22.963903450918757 \t -11.13383414415017\n",
            "33     \t [5.08546697 1.55234367]. \t  -49.14332845208429 \t -11.13383414415017\n",
            "34     \t [-1.30777688 -1.44842385]. \t  -36.83872549896717 \t -11.13383414415017\n",
            "35     \t [ 2.21642414 -0.3694787 ]. \t  -29.7765882358515 \t -11.13383414415017\n",
            "36     \t [-4.88873236  2.08590668]. \t  -32.01831041400348 \t -11.13383414415017\n",
            "37     \t [-4.23898602 -3.82863578]. \t  -47.1937310837777 \t -11.13383414415017\n",
            "38     \t [-0.38017525  2.33916975]. \t  -38.22761797598265 \t -11.13383414415017\n",
            "39     \t [ 0.98274355 -1.9909281 ]. \t  \u001b[92m-5.004543278060384\u001b[0m \t -5.004543278060384\n",
            "40     \t [ 0.39000215 -2.6011013 ]. \t  -42.672352115265795 \t -5.004543278060384\n",
            "41     \t [ 0.72455668 -1.1970029 ]. \t  -20.28094006135977 \t -5.004543278060384\n",
            "42     \t [ 0.55330461 -1.7574658 ]. \t  -32.37027133654827 \t -5.004543278060384\n",
            "43     \t [ 3.85423293 -2.78073788]. \t  -34.57736348598369 \t -5.004543278060384\n",
            "44     \t [ 2.736334   -3.58724627]. \t  -49.74818530676181 \t -5.004543278060384\n",
            "45     \t [-2.3917525  -1.64802606]. \t  -42.18905981814011 \t -5.004543278060384\n",
            "46     \t [ 0.97625873 -2.2259547 ]. \t  -14.51393906579252 \t -5.004543278060384\n",
            "47     \t [-1.3000543  -0.38410734]. \t  -32.39499590914154 \t -5.004543278060384\n",
            "48     \t [ 0.98362841 -1.36733947]. \t  -19.61266379469587 \t -5.004543278060384\n",
            "49     \t [2.66943517 0.55505086]. \t  -41.690296394544866 \t -5.004543278060384\n",
            "50     \t [-0.40421083 -0.16822277]. \t  -23.51942989171537 \t -5.004543278060384\n",
            "51     \t [ 0.25035964 -0.5780334 ]. \t  -29.24132392086203 \t -5.004543278060384\n",
            "52     \t [-0.92767463 -3.15525595]. \t  -16.2235974442743 \t -5.004543278060384\n",
            "53     \t [-0.72443325  0.7618095 ]. \t  -21.963331669494156 \t -5.004543278060384\n",
            "54     \t [ 3.57945995 -0.66902557]. \t  -46.91059021477743 \t -5.004543278060384\n",
            "55     \t [-0.42088003 -3.44877442]. \t  -50.347243054755225 \t -5.004543278060384\n",
            "56     \t [-1.77216101 -5.19468893]. \t  -45.331663514258615 \t -5.004543278060384\n",
            "57     \t [-1.91033074 -2.82551257]. \t  -18.609911936023412 \t -5.004543278060384\n",
            "58     \t [-1.13272665 -2.90091055]. \t  -14.855101041659363 \t -5.004543278060384\n",
            "59     \t [-2.08482555 -3.62312438]. \t  -36.0144394702847 \t -5.004543278060384\n",
            "60     \t [-2.19421159  3.40565575]. \t  -41.273003922638495 \t -5.004543278060384\n",
            "61     \t [1.29106651 4.8736727 ]. \t  -40.95943380032315 \t -5.004543278060384\n",
            "62     \t [5.07784096 3.27282518]. \t  -49.09747885018923 \t -5.004543278060384\n",
            "63     \t [1.40389578 0.72407388]. \t  -32.348607280398014 \t -5.004543278060384\n",
            "64     \t [-4.87484426 -2.73378825]. \t  -45.19041341354919 \t -5.004543278060384\n",
            "65     \t [-3.7526511   3.22470677]. \t  -42.73201881452542 \t -5.004543278060384\n",
            "66     \t [0.88824604 3.11347429]. \t  -15.284123453703561 \t -5.004543278060384\n",
            "67     \t [-2.2552448   4.59558857]. \t  -54.78500673138909 \t -5.004543278060384\n",
            "68     \t [-3.43039523  1.64417911]. \t  -49.699496131883976 \t -5.004543278060384\n",
            "69     \t [-0.20507972  3.38395949]. \t  -36.16585473724644 \t -5.004543278060384\n",
            "70     \t [-1.29952261 -3.4321183 ]. \t  -45.63396142456931 \t -5.004543278060384\n",
            "71     \t [-3.66870035 -3.06764059]. \t  -38.648320957966654 \t -5.004543278060384\n",
            "72     \t [-2.91072863 -2.96850036]. \t  -19.011833304822908 \t -5.004543278060384\n",
            "73     \t [1.70644197 0.22314884]. \t  -23.98541690152051 \t -5.004543278060384\n",
            "74     \t [-0.03093561  1.08732461]. \t  \u001b[92m-2.839389809843013\u001b[0m \t -2.839389809843013\n",
            "75     \t [0.58985164 1.65222573]. \t  -37.2901884487137 \t -2.839389809843013\n",
            "76     \t [-2.77410685 -4.1173191 ]. \t  -35.73513648133359 \t -2.839389809843013\n",
            "77     \t [ 2.53572634 -0.13662462]. \t  -29.661479413554485 \t -2.839389809843013\n",
            "78     \t [-0.83232647 -5.06298131]. \t  -32.15425411517034 \t -2.839389809843013\n",
            "79     \t [ 3.22314429 -1.92124488]. \t  -23.5999628409256 \t -2.839389809843013\n",
            "80     \t [3.44259309 1.95616827]. \t  -35.41139052130532 \t -2.839389809843013\n",
            "81     \t [4.9870321  4.70964566]. \t  -59.59288487482519 \t -2.839389809843013\n",
            "82     \t [1.24185629 3.82026753]. \t  -31.352193259723364 \t -2.839389809843013\n",
            "83     \t [0.10255505 1.07200923]. \t  -4.171153118417248 \t -2.839389809843013\n",
            "84     \t [-0.16967442  0.88424296]. \t  -8.50565194319811 \t -2.839389809843013\n",
            "85     \t [-0.72526924 -2.87985175]. \t  -23.083888799921723 \t -2.839389809843013\n",
            "86     \t [ 3.48606785 -3.95352364]. \t  -48.16808370985883 \t -2.839389809843013\n",
            "87     \t [ 4.98287117 -2.54356491]. \t  -50.984288500842425 \t -2.839389809843013\n",
            "88     \t [-0.84848557  0.6012423 ]. \t  -23.32485778191307 \t -2.839389809843013\n",
            "89     \t [3.39841933 2.04715641]. \t  -34.2072426625908 \t -2.839389809843013\n",
            "90     \t [-5.07394898 -4.56632116]. \t  -66.80065899229945 \t -2.839389809843013\n",
            "91     \t [-5.10238691  3.28477999]. \t  -50.99096025906376 \t -2.839389809843013\n",
            "92     \t [0.5486957  3.52805835]. \t  -52.12883368481721 \t -2.839389809843013\n",
            "93     \t [-0.06126281  1.47356278]. \t  -22.769230825305073 \t -2.839389809843013\n",
            "94     \t [-0.72994614  3.25066432]. \t  -32.39807016300092 \t -2.839389809843013\n",
            "95     \t [1.64655804 2.16545867]. \t  -28.38620359342633 \t -2.839389809843013\n",
            "96     \t [2.46663977 3.9350303 ]. \t  -42.171597561880574 \t -2.839389809843013\n",
            "97     \t [-2.69217071  2.41152207]. \t  -45.111452698792476 \t -2.839389809843013\n",
            "98     \t [ 0.90658599 -0.26552609]. \t  -13.539982430682912 \t -2.839389809843013\n",
            "99     \t [-2.58615608 -3.41910019]. \t  -55.68435677880935 \t -2.839389809843013\n",
            "100    \t [-2.76368477 -2.52481741]. \t  -43.03254759050539 \t -2.839389809843013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_21yVprtdlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2883c7-36ec-49c9-9229-8365bc020055"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_8 = d2GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-3.99872128 -4.68771825]. \t  -51.77895852895087 \t -5.803967830456797\n",
            "init   \t [ 1.02085207 -2.04932398]. \t  -5.803967830456797 \t -5.803967830456797\n",
            "init   \t [ 3.03730465 -1.37015168]. \t  -28.228393086698507 \t -5.803967830456797\n",
            "init   \t [ 3.89742944 -2.07010168]. \t  -22.435544673579564 \t -5.803967830456797\n",
            "init   \t [-1.74998258  0.70366122]. \t  -26.429262404618022 \t -5.803967830456797\n",
            "1      \t [-13.86370928 -11.42618768]. \t  -345.15182658708045 \t -5.803967830456797\n",
            "2      \t [-15.35798181 -17.70850968]. \t  -578.3124305365898 \t -5.803967830456797\n",
            "3      \t [-7.25091821 -9.87415373]. \t  -163.09904991876647 \t -5.803967830456797\n",
            "4      \t [-10.45306943  -3.13430017]. \t  -142.01284874706565 \t -5.803967830456797\n",
            "5      \t [3.88824209 4.9977422 ]. \t  -42.462601215995015 \t -5.803967830456797\n",
            "6      \t [ 2.3509348  -7.70129562]. \t  -93.77476394097789 \t -5.803967830456797\n",
            "7      \t [-6.61758187  0.93461236]. \t  -62.89089210176904 \t -5.803967830456797\n",
            "8      \t [-3.61660843  5.09013516]. \t  -57.98450305279159 \t -5.803967830456797\n",
            "9      \t [-2.21484496 -9.68293529]. \t  -120.56403697569564 \t -5.803967830456797\n",
            "10     \t [0.08739638 4.36135837]. \t  -36.939052702185045 \t -5.803967830456797\n",
            "11     \t [-0.35161448 -4.08432085]. \t  -34.13586178104753 \t -5.803967830456797\n",
            "12     \t [4.21415061 1.67189005]. \t  -43.03393028594548 \t -5.803967830456797\n",
            "13     \t [-6.67610566 -2.36938982]. \t  -81.4797650688992 \t -5.803967830456797\n",
            "14     \t [ 3.76116111 -4.93630271]. \t  -48.60297280663562 \t -5.803967830456797\n",
            "15     \t [-2.50684645 -1.93000391]. \t  -30.951568026161986 \t -5.803967830456797\n",
            "16     \t [1.35022597 1.05326339]. \t  -19.376599109406133 \t -5.803967830456797\n",
            "17     \t [-3.98023325  2.12772195]. \t  -23.497377488767256 \t -5.803967830456797\n",
            "18     \t [-0.47664736 -1.03799714]. \t  -21.48081703676975 \t -5.803967830456797\n",
            "19     \t [ 1.36705411 -2.44609312]. \t  -43.99343642800085 \t -5.803967830456797\n",
            "20     \t [ 0.699918   -1.87209537]. \t  -20.148847890999978 \t -5.803967830456797\n",
            "21     \t [ 1.72946215 -1.12648108]. \t  -18.541891552158834 \t -5.803967830456797\n",
            "22     \t [ 4.61074903 -0.48092751]. \t  -59.09362859828131 \t -5.803967830456797\n",
            "23     \t [-1.73421003 -0.43522862]. \t  -33.37063618040469 \t -5.803967830456797\n",
            "24     \t [-0.67188149 -0.42948839]. \t  -34.384029583643404 \t -5.803967830456797\n",
            "25     \t [-1.57224375  2.08143872]. \t  -27.072524886056975 \t -5.803967830456797\n",
            "26     \t [-1.26501536 -1.4132998 ]. \t  -33.09227518449937 \t -5.803967830456797\n",
            "27     \t [-3.65740357 -1.28772278]. \t  -42.878184024954656 \t -5.803967830456797\n",
            "28     \t [ 1.53601923 -1.5204705 ]. \t  -44.3335818051989 \t -5.803967830456797\n",
            "29     \t [ 2.52918584 -0.85261616]. \t  -30.94602782739059 \t -5.803967830456797\n",
            "30     \t [ 1.49697563 -0.08224476]. \t  -23.551644914966037 \t -5.803967830456797\n",
            "31     \t [ 0.68591443 -2.85182537]. \t  -26.551819016908865 \t -5.803967830456797\n",
            "32     \t [1.43318925 2.55167572]. \t  -47.17435156452514 \t -5.803967830456797\n",
            "33     \t [-1.76030098 -4.19873581]. \t  -36.91564821459826 \t -5.803967830456797\n",
            "34     \t [-0.82488954 -5.090394  ]. \t  -33.6288449712315 \t -5.803967830456797\n",
            "35     \t [-3.15432586  0.78174553]. \t  -22.92365042036957 \t -5.803967830456797\n",
            "36     \t [ 0.65231    -2.46012444]. \t  -41.92530934601708 \t -5.803967830456797\n",
            "37     \t [ 0.90532859 -4.23754981]. \t  -29.712584296261884 \t -5.803967830456797\n",
            "38     \t [ 1.25663328 -0.98704554]. \t  -13.00315418678868 \t -5.803967830456797\n",
            "39     \t [ 0.78406008 -3.68711522]. \t  -35.934967943275886 \t -5.803967830456797\n",
            "40     \t [ 1.17865647 -5.43609894]. \t  -55.81112973639383 \t -5.803967830456797\n",
            "41     \t [ 3.38777639 -1.50760259]. \t  -51.35381418138931 \t -5.803967830456797\n",
            "42     \t [ 4.87502527 -3.54332741]. \t  -58.880575696487966 \t -5.803967830456797\n",
            "43     \t [ 4.44182968 -2.8373319 ]. \t  -51.90379602765969 \t -5.803967830456797\n",
            "44     \t [ 0.26739291 -4.83098421]. \t  -39.628926877364414 \t -5.803967830456797\n",
            "45     \t [4.99408775 0.34767698]. \t  -40.827837393697 \t -5.803967830456797\n",
            "46     \t [ 3.17818034 -2.07176106]. \t  -21.031456097036767 \t -5.803967830456797\n",
            "47     \t [ 5.11443347 -4.53868331]. \t  -68.93854662371805 \t -5.803967830456797\n",
            "48     \t [ 1.59410601 -4.55406359]. \t  -61.01149182624632 \t -5.803967830456797\n",
            "49     \t [ 0.65357693 -1.43613144]. \t  -37.38974434927142 \t -5.803967830456797\n",
            "50     \t [-3.45673644 -2.6474599 ]. \t  -54.5970914396671 \t -5.803967830456797\n",
            "51     \t [ 0.97570253 -2.66382487]. \t  -23.31809425532573 \t -5.803967830456797\n",
            "52     \t [ 6.2788406  -1.27072332]. \t  -64.13919194272397 \t -5.803967830456797\n",
            "53     \t [-1.06189193 -0.9048427 ]. \t  \u001b[92m-4.4277596811474815\u001b[0m \t -4.4277596811474815\n",
            "54     \t [ 4.97008437 -1.88787168]. \t  -30.82272791011561 \t -4.4277596811474815\n",
            "55     \t [-0.14487904 -3.54506074]. \t  -36.05524307053035 \t -4.4277596811474815\n",
            "56     \t [0.79283731 1.52053224]. \t  -30.198342136714295 \t -4.4277596811474815\n",
            "57     \t [-2.58867878 -0.33425295]. \t  -40.350408642335154 \t -4.4277596811474815\n",
            "58     \t [1.71029655 1.69421146]. \t  -31.698267829798496 \t -4.4277596811474815\n",
            "59     \t [-1.92875613  3.24986654]. \t  -25.258629088796294 \t -4.4277596811474815\n",
            "60     \t [ 0.35850867 -3.52485597]. \t  -48.73319483231661 \t -4.4277596811474815\n",
            "61     \t [-1.28174414 -3.9572187 ]. \t  -29.642897529134764 \t -4.4277596811474815\n",
            "62     \t [-2.85784773 -4.59832092]. \t  -51.194115834612354 \t -4.4277596811474815\n",
            "63     \t [2.04508195 4.46507558]. \t  -44.2779635795241 \t -4.4277596811474815\n",
            "64     \t [-1.33510943 -4.6205491 ]. \t  -55.49434742537997 \t -4.4277596811474815\n",
            "65     \t [-2.32026055 -4.16919187]. \t  -42.17639940130603 \t -4.4277596811474815\n",
            "66     \t [4.8794712  2.65431856]. \t  -49.24393928634934 \t -4.4277596811474815\n",
            "67     \t [-0.8812053  -0.94374475]. \t  -4.94405920507781 \t -4.4277596811474815\n",
            "68     \t [-2.66721925  1.16657767]. \t  -28.440021522772817 \t -4.4277596811474815\n",
            "69     \t [-4.29469213  0.169393  ]. \t  -36.39348126700958 \t -4.4277596811474815\n",
            "70     \t [-1.97582059 -1.24482783]. \t  -15.243725683449467 \t -4.4277596811474815\n",
            "71     \t [-3.55544159 -6.23697186]. \t  -80.1226873170178 \t -4.4277596811474815\n",
            "72     \t [ 1.91754307 -0.54384324]. \t  -24.908041876456824 \t -4.4277596811474815\n",
            "73     \t [1.08408569 0.18942655]. \t  -8.859880643519485 \t -4.4277596811474815\n",
            "74     \t [-4.5394363   2.73913023]. \t  -58.48633214327133 \t -4.4277596811474815\n",
            "75     \t [ 3.35422077 -2.84176703]. \t  -39.96506957632515 \t -4.4277596811474815\n",
            "76     \t [ 1.62260873 -0.53064706]. \t  -39.90612548052417 \t -4.4277596811474815\n",
            "77     \t [-0.21517591  2.24291298]. \t  -22.461171153112236 \t -4.4277596811474815\n",
            "78     \t [2.75618131 1.05480012]. \t  -18.907798648323087 \t -4.4277596811474815\n",
            "79     \t [1.76660359 0.53541218]. \t  -32.119696941914995 \t -4.4277596811474815\n",
            "80     \t [ 2.14102245 -3.6477426 ]. \t  -37.55739970528001 \t -4.4277596811474815\n",
            "81     \t [3.35956454 2.22929848]. \t  -41.31252968515506 \t -4.4277596811474815\n",
            "82     \t [ 1.59234513 -3.05325512]. \t  -30.775963039517265 \t -4.4277596811474815\n",
            "83     \t [ 3.07912165 -3.53541477]. \t  -42.94407697452969 \t -4.4277596811474815\n",
            "84     \t [3.81147004 1.83279151]. \t  -29.14897400246947 \t -4.4277596811474815\n",
            "85     \t [0.95443462 1.93921374]. \t  -5.799064652407711 \t -4.4277596811474815\n",
            "86     \t [3.84869083 1.12144883]. \t  -23.03189326742444 \t -4.4277596811474815\n",
            "87     \t [ 4.98620577 -0.90172386]. \t  -27.55953130450274 \t -4.4277596811474815\n",
            "88     \t [2.90807699 1.5881754 ]. \t  -31.10544220032941 \t -4.4277596811474815\n",
            "89     \t [-0.78395299  2.40594887]. \t  -32.59015716561196 \t -4.4277596811474815\n",
            "90     \t [-3.53969489 -0.08251072]. \t  -33.540847873830295 \t -4.4277596811474815\n",
            "91     \t [-1.53318072  1.6658524 ]. \t  -39.95341423501588 \t -4.4277596811474815\n",
            "92     \t [3.38072518 1.00083829]. \t  -29.75192003348674 \t -4.4277596811474815\n",
            "93     \t [3.60835114 3.60338316]. \t  -61.73872947109679 \t -4.4277596811474815\n",
            "94     \t [-5.11202579  1.04547627]. \t  -30.00790697156429 \t -4.4277596811474815\n",
            "95     \t [-2.317161    2.41411526]. \t  -43.871898791724064 \t -4.4277596811474815\n",
            "96     \t [3.89253833 2.49377059]. \t  -43.557277889611875 \t -4.4277596811474815\n",
            "97     \t [ 2.53860263 -3.24134478]. \t  -46.11455680305646 \t -4.4277596811474815\n",
            "98     \t [-0.94643861 -4.34829962]. \t  -36.155498733136355 \t -4.4277596811474815\n",
            "99     \t [-2.45453357 -5.35765004]. \t  -70.58364953227873 \t -4.4277596811474815\n",
            "100    \t [-5.21554725 -0.47468397]. \t  -55.153152766688926 \t -4.4277596811474815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkq32q1utdoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a36718-3268-4be6-b33a-e4dd3a7caea3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_9 = d2GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.76413244 -0.12207719]. \t  -15.040540034920513 \t -15.040540034920513\n",
            "init   \t [ 3.33307058 -4.79798899]. \t  -56.14599841787976 \t -15.040540034920513\n",
            "init   \t [3.15443162 0.67192238]. \t  -29.462925809392473 \t -15.040540034920513\n",
            "init   \t [-2.07234561 -4.64183582]. \t  -43.141657603020725 \t -15.040540034920513\n",
            "init   \t [ 5.02402457 -5.05010449]. \t  -51.34955862002178 \t -15.040540034920513\n",
            "1      \t [-4.74552942  3.02987162]. \t  -42.15664864154001 \t -15.040540034920513\n",
            "2      \t [-12.24593749 -13.25001341]. \t  -345.271455767369 \t -15.040540034920513\n",
            "3      \t [-10.17223088  -4.2780447 ]. \t  -138.83470837350558 \t -15.040540034920513\n",
            "4      \t [0.34208905 5.05265953]. \t  -41.657419727100475 \t -15.040540034920513\n",
            "5      \t [-5.65487245 -1.69248178]. \t  -64.00535662139534 \t -15.040540034920513\n",
            "6      \t [ -4.98334091 -11.13277793]. \t  -152.10996358072896 \t -15.040540034920513\n",
            "7      \t [-6.25843184 -6.29987559]. \t  -102.46867637378391 \t -15.040540034920513\n",
            "8      \t [-1.66255226  0.21213527]. \t  -25.674535670052002 \t -15.040540034920513\n",
            "9      \t [ 0.77094503 -2.86561557]. \t  -20.851807944332734 \t -15.040540034920513\n",
            "10     \t [-11.62604066  -9.07443925]. \t  -235.60894528053893 \t -15.040540034920513\n",
            "11     \t [-17.01347846 -17.76661365]. \t  -614.1048768306454 \t -15.040540034920513\n",
            "12     \t [5.05606844 5.07122908]. \t  -52.8802166894539 \t -15.040540034920513\n",
            "13     \t [-2.99949204 -7.62120855]. \t  -84.31731847707908 \t -15.040540034920513\n",
            "14     \t [-2.30892068  4.7633238 ]. \t  -50.80229458524945 \t -15.040540034920513\n",
            "15     \t [ 4.71485157 -1.60202785]. \t  -55.001474931674515 \t -15.040540034920513\n",
            "16     \t [ 0.2136035  -6.25960309]. \t  -57.56429218948965 \t -15.040540034920513\n",
            "17     \t [0.11591386 2.09927755]. \t  \u001b[92m-8.840619513481949\u001b[0m \t -8.840619513481949\n",
            "18     \t [2.85331125 2.99179136]. \t  -21.060612474343518 \t -8.840619513481949\n",
            "19     \t [-2.82311036 -1.93018525]. \t  -18.2085482607164 \t -8.840619513481949\n",
            "20     \t [4.7376284  2.25124825]. \t  -48.36821843775994 \t -8.840619513481949\n",
            "21     \t [-17.24511125 -12.69706415]. \t  -481.56724977579796 \t -8.840619513481949\n",
            "22     \t [-3.96240925  0.53549425]. \t  -36.01742083040561 \t -8.840619513481949\n",
            "23     \t [-1.23363492  2.48602953]. \t  -36.637259804591395 \t -8.840619513481949\n",
            "24     \t [0.48189387 2.02959203]. \t  -24.459180383420147 \t -8.840619513481949\n",
            "25     \t [ 1.30725363e-04 -7.60630208e-01]. \t  -9.911142535316346 \t -8.840619513481949\n",
            "26     \t [-0.36450806  1.75227957]. \t  -29.65002650934305 \t -8.840619513481949\n",
            "27     \t [0.18751639 3.18507386]. \t  -22.38675589471304 \t -8.840619513481949\n",
            "28     \t [-0.17142067  2.49045533]. \t  -31.47465193980425 \t -8.840619513481949\n",
            "29     \t [-2.23006002  1.90111909]. \t  -19.206531489837083 \t -8.840619513481949\n",
            "30     \t [-2.51353688  2.55382954]. \t  -52.23722777697097 \t -8.840619513481949\n",
            "31     \t [1.10415004 3.86947743]. \t  -21.436334301298448 \t -8.840619513481949\n",
            "32     \t [ 2.11743521 -1.31329424]. \t  -22.6821150203195 \t -8.840619513481949\n",
            "33     \t [-4.72641917 -4.20397906]. \t  -58.63723692104413 \t -8.840619513481949\n",
            "34     \t [-0.9751915  -1.47433951]. \t  -23.116223237263668 \t -8.840619513481949\n",
            "35     \t [ 0.10006027 -0.97541019]. \t  \u001b[92m-2.9926113106673906\u001b[0m \t -2.9926113106673906\n",
            "36     \t [ 0.94653248 -1.2674353 ]. \t  -14.154630519867085 \t -2.9926113106673906\n",
            "37     \t [ 0.12181516 -1.46246518]. \t  -24.665690045547663 \t -2.9926113106673906\n",
            "38     \t [ 1.09577081 -3.7869628 ]. \t  -24.99669460396481 \t -2.9926113106673906\n",
            "39     \t [ 1.03937395 -0.92567925]. \t  -3.3122816029008533 \t -2.9926113106673906\n",
            "40     \t [-2.29491242 -0.39519281]. \t  -36.11639582299093 \t -2.9926113106673906\n",
            "41     \t [2.99047836 4.70994288]. \t  -43.63478826023272 \t -2.9926113106673906\n",
            "42     \t [ 2.35428995 -2.68504679]. \t  -42.81469614832395 \t -2.9926113106673906\n",
            "43     \t [-4.74375951  4.97084212]. \t  -57.77187686581027 \t -2.9926113106673906\n",
            "44     \t [ 0.76815338 -0.93936429]. \t  -11.051339276556178 \t -2.9926113106673906\n",
            "45     \t [-0.84815829 -0.90011883]. \t  -7.651186825794216 \t -2.9926113106673906\n",
            "46     \t [ 1.72913687 -1.26148661]. \t  -26.6094795778098 \t -2.9926113106673906\n",
            "47     \t [ 3.17167636 -1.36636716]. \t  -33.878828396671686 \t -2.9926113106673906\n",
            "48     \t [ 2.18629056 -0.45382029]. \t  -30.67088275438064 \t -2.9926113106673906\n",
            "49     \t [-2.21548303  0.71498146]. \t  -25.450329303442714 \t -2.9926113106673906\n",
            "50     \t [ 2.87920055 -1.4682361 ]. \t  -32.991806277059645 \t -2.9926113106673906\n",
            "51     \t [ 4.6642559  -0.10010501]. \t  -38.8096147865191 \t -2.9926113106673906\n",
            "52     \t [ 1.57172843 -3.18919485]. \t  -37.91455735229492 \t -2.9926113106673906\n",
            "53     \t [ 0.0368933  -4.01602947]. \t  -16.448002101064017 \t -2.9926113106673906\n",
            "54     \t [ 3.04155308 -0.95869164]. \t  -10.843971111834286 \t -2.9926113106673906\n",
            "55     \t [-1.87563774 -2.53681115]. \t  -32.58779459297692 \t -2.9926113106673906\n",
            "56     \t [3.09906963 1.72800455]. \t  -25.843460465477175 \t -2.9926113106673906\n",
            "57     \t [ 4.17952153 -0.51730747]. \t  -43.39196132684618 \t -2.9926113106673906\n",
            "58     \t [4.26802704 0.85153799]. \t  -34.11566774227063 \t -2.9926113106673906\n",
            "59     \t [ 1.38055036 -0.26306612]. \t  -30.10848387447272 \t -2.9926113106673906\n",
            "60     \t [2.00498719 0.46850331]. \t  -24.04919526136946 \t -2.9926113106673906\n",
            "61     \t [-3.56732048 -0.81471765]. \t  -38.55301773847936 \t -2.9926113106673906\n",
            "62     \t [3.87776193 3.74831266]. \t  -42.00019602058648 \t -2.9926113106673906\n",
            "63     \t [-0.58431119 -0.69067686]. \t  -33.08949524517216 \t -2.9926113106673906\n",
            "64     \t [-2.77730385 -3.48456752]. \t  -48.10150413172056 \t -2.9926113106673906\n",
            "65     \t [-4.72392466  0.00793416]. \t  -33.95899275901128 \t -2.9926113106673906\n",
            "66     \t [-4.05404374 -1.60914308]. \t  -37.334962377768946 \t -2.9926113106673906\n",
            "67     \t [-1.94006167  1.52923699]. \t  -26.634957295768828 \t -2.9926113106673906\n",
            "68     \t [-1.81033584 -0.97529507]. \t  -10.647892860326976 \t -2.9926113106673906\n",
            "69     \t [6.21461547 0.44018351]. \t  -65.91220529629476 \t -2.9926113106673906\n",
            "70     \t [ 2.28296119 -3.41345454]. \t  -47.477408789507855 \t -2.9926113106673906\n",
            "71     \t [-0.4299349  -1.03393324]. \t  -20.526820582257983 \t -2.9926113106673906\n",
            "72     \t [-3.29306857 -2.545993  ]. \t  -49.58489622957774 \t -2.9926113106673906\n",
            "73     \t [-4.97601069  1.11798683]. \t  -28.748268712594957 \t -2.9926113106673906\n",
            "74     \t [-1.75247448 -0.40238912]. \t  -31.255103396497116 \t -2.9926113106673906\n",
            "75     \t [-1.51425529 -1.1354947 ]. \t  -26.95246038818587 \t -2.9926113106673906\n",
            "76     \t [-1.31892349 -3.37769225]. \t  -44.53452257671324 \t -2.9926113106673906\n",
            "77     \t [ 0.36534614 -4.92373292]. \t  -42.13240091331257 \t -2.9926113106673906\n",
            "78     \t [-2.92236779 -1.09665098]. \t  -12.69705611627629 \t -2.9926113106673906\n",
            "79     \t [-0.45257495 -2.38303683]. \t  -42.86197182864153 \t -2.9926113106673906\n",
            "80     \t [-4.56148514 -0.80768185]. \t  -47.17707157595101 \t -2.9926113106673906\n",
            "81     \t [-1.6539601   1.02058254]. \t  -19.535433610568585 \t -2.9926113106673906\n",
            "82     \t [2.74840867 0.97490009]. \t  -18.72826552190456 \t -2.9926113106673906\n",
            "83     \t [-2.71859303 -2.59966367]. \t  -44.21214775243103 \t -2.9926113106673906\n",
            "84     \t [-3.90906095  0.07373969]. \t  -17.928941616638532 \t -2.9926113106673906\n",
            "85     \t [-2.69143001 -5.05156577]. \t  -46.87996541398654 \t -2.9926113106673906\n",
            "86     \t [ 1.90089666 -0.55954657]. \t  -25.111558916864823 \t -2.9926113106673906\n",
            "87     \t [4.5546436  0.98148553]. \t  -41.19204857325594 \t -2.9926113106673906\n",
            "88     \t [ 3.90051884 -1.19120517]. \t  -24.912993423947324 \t -2.9926113106673906\n",
            "89     \t [-1.34440936 -2.4434126 ]. \t  -42.74235798807992 \t -2.9926113106673906\n",
            "90     \t [-1.92983673 -3.72758683]. \t  -29.97888646415524 \t -2.9926113106673906\n",
            "91     \t [-3.82681154 -4.1348221 ]. \t  -40.478715992983 \t -2.9926113106673906\n",
            "92     \t [-4.48898209 -2.96697364]. \t  -49.14447210809303 \t -2.9926113106673906\n",
            "93     \t [2.64395945 2.46495145]. \t  -49.005608301612305 \t -2.9926113106673906\n",
            "94     \t [ 4.85040911 -3.14308659]. \t  -41.283212471171765 \t -2.9926113106673906\n",
            "95     \t [-5.74723628 -4.44973231]. \t  -82.50983759049714 \t -2.9926113106673906\n",
            "96     \t [ 4.32164834 -1.91569096]. \t  -38.06835188896687 \t -2.9926113106673906\n",
            "97     \t [2.29930445 0.41635166]. \t  -37.159067383639794 \t -2.9926113106673906\n",
            "98     \t [2.38307857 3.96406213]. \t  -39.06743641621314 \t -2.9926113106673906\n",
            "99     \t [-1.77960794 -3.26094431]. \t  -32.63826129961949 \t -2.9926113106673906\n",
            "100    \t [0.26183366 4.05531341]. \t  -27.85485052586378 \t -2.9926113106673906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5Aur-aRtdq-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3765b92-7b5f-4715-d16c-e73e2a4619b2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_10 = d2GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.57275736 -3.9423289 ]. \t  -37.63803811714605 \t -29.87920356309123\n",
            "init   \t [ 4.61089653 -0.18236005]. \t  -44.839569549068806 \t -29.87920356309123\n",
            "init   \t [ 3.81413924 -2.94571335]. \t  -29.87920356309123 \t -29.87920356309123\n",
            "init   \t [-4.70313344 -1.05272872]. \t  -36.67385216521674 \t -29.87920356309123\n",
            "init   \t [-2.7327263   3.49942502]. \t  -50.7969122914016 \t -29.87920356309123\n",
            "1      \t [-2.46509489 -9.65474922]. \t  -134.68520072832305 \t -29.87920356309123\n",
            "2      \t [-12.47061337  -8.05868484]. \t  -240.9607536564799 \t -29.87920356309123\n",
            "3      \t [-33.9130404  -37.36463995]. \t  -2564.26266618855 \t -29.87920356309123\n",
            "4      \t [-12.36906989 -18.26055087]. \t  -513.9068063388513 \t -29.87920356309123\n",
            "5      \t [-75.58926452 -79.12338344]. \t  -11995.572143780848 \t -29.87920356309123\n",
            "6      \t [-9.83128525 -2.80914576]. \t  -116.02603618006083 \t -29.87920356309123\n",
            "7      \t [-16.55381823 -23.41373236]. \t  -860.2320651051624 \t -29.87920356309123\n",
            "8      \t [-37.88666124 -44.55307336]. \t  -3442.2548414664016 \t -29.87920356309123\n",
            "9      \t [-26.81081539 -20.39288521]. \t  -1158.780171751353 \t -29.87920356309123\n",
            "10     \t [2.80673356 4.89629442]. \t  -40.410656314185914 \t -29.87920356309123\n",
            "11     \t [ -7.12652373 -13.84515498]. \t  -249.84372436379678 \t -29.87920356309123\n",
            "12     \t [-116.94508258 -121.24077842]. \t  -28385.489043769358 \t -29.87920356309123\n",
            "13     \t [-42.85520907 -49.09280647]. \t  -4252.185660294655 \t -29.87920356309123\n",
            "14     \t [-99.18033357 -96.50886951]. \t  -19176.446116341365 \t -29.87920356309123\n",
            "15     \t [-58.58154198 -55.42255402]. \t  -6541.0117806780445 \t -29.87920356309123\n",
            "16     \t [-68.11683087 -72.29501212]. \t  -9881.837527682928 \t -29.87920356309123\n",
            "17     \t [-179.6837454  -179.00256701]. \t  -64342.21235948255 \t -29.87920356309123\n",
            "18     \t [-15.61403914 -13.24611653]. \t  -446.5547391909872 \t -29.87920356309123\n",
            "19     \t [-27.92319157 -35.91870537]. \t  -2072.2765279257346 \t -29.87920356309123\n",
            "20     \t [-5.26731411 -5.76193418]. \t  -81.2810723745206 \t -29.87920356309123\n",
            "21     \t [0.02665027 0.06476156]. \t  \u001b[92m-0.9612861458018482\u001b[0m \t -0.9612861458018482\n",
            "22     \t [-1.95139505 -3.4764699 ]. \t  -36.24740932470604 \t -0.9612861458018482\n",
            "23     \t [0.99333423 1.74727174]. \t  -14.219853951853654 \t -0.9612861458018482\n",
            "24     \t [-0.98197338 -0.17538125]. \t  -6.540557628779742 \t -0.9612861458018482\n",
            "25     \t [ 0.43848545 -0.87963979]. \t  -22.9541740086028 \t -0.9612861458018482\n",
            "26     \t [-0.16189956  0.16693725]. \t  -9.81169480340763 \t -0.9612861458018482\n",
            "27     \t [1.1340096  0.44297794]. \t  -24.187551360084026 \t -0.9612861458018482\n",
            "28     \t [-2.37492465 -0.61256327]. \t  -40.684697341126395 \t -0.9612861458018482\n",
            "29     \t [-0.62597989 -0.35330943]. \t  -33.5888717530049 \t -0.9612861458018482\n",
            "30     \t [-1.85747777  1.01453234]. \t  -8.26982101661687 \t -0.9612861458018482\n",
            "31     \t [-2.08396567 -0.04656049]. \t  -6.12963946788912 \t -0.9612861458018482\n",
            "32     \t [0.67180816 0.26882012]. \t  -26.42101031310533 \t -0.9612861458018482\n",
            "33     \t [2.36856037 0.84296043]. \t  -27.5855294475029 \t -0.9612861458018482\n",
            "34     \t [-1.52227881  0.32423309]. \t  -36.82156307900718 \t -0.9612861458018482\n",
            "35     \t [-2.74113268  1.13200361]. \t  -22.598941538215815 \t -0.9612861458018482\n",
            "36     \t [-1.91705383  1.56000364]. \t  -26.73399583898229 \t -0.9612861458018482\n",
            "37     \t [ 1.11896626 -0.75177916]. \t  -14.371477131406584 \t -0.9612861458018482\n",
            "38     \t [-1.8672912  -0.70853573]. \t  -19.844288016716675 \t -0.9612861458018482\n",
            "39     \t [1.65979244 0.82509387]. \t  -24.259805884678112 \t -0.9612861458018482\n",
            "40     \t [1.05752168 3.1557455 ]. \t  -16.141091885817755 \t -0.9612861458018482\n",
            "41     \t [2.09558864 0.24352178]. \t  -15.793907750198525 \t -0.9612861458018482\n",
            "42     \t [4.17974752 2.15940318]. \t  -32.47127740792191 \t -0.9612861458018482\n",
            "43     \t [-2.65710441  4.65875538]. \t  -59.69932270696922 \t -0.9612861458018482\n",
            "44     \t [ 0.19032599 -0.17266733]. \t  -11.733840756243222 \t -0.9612861458018482\n",
            "45     \t [ 0.46375209 -2.39689972]. \t  -43.67610154585639 \t -0.9612861458018482\n",
            "46     \t [ 0.52400143 -1.46104343]. \t  -41.99765692432404 \t -0.9612861458018482\n",
            "47     \t [ 0.05278877 -3.61334823]. \t  -31.17345023005415 \t -0.9612861458018482\n",
            "48     \t [3.28674758 0.66792569]. \t  -38.46862773300496 \t -0.9612861458018482\n",
            "49     \t [3.33009372 1.90425388]. \t  -31.294039016710705 \t -0.9612861458018482\n",
            "50     \t [-2.32048875  0.78011975]. \t  -28.397610664555508 \t -0.9612861458018482\n",
            "51     \t [-1.65537851 -0.27937691]. \t  -30.254820139114 \t -0.9612861458018482\n",
            "52     \t [-3.95368858  1.7650342 ]. \t  -28.22415422754791 \t -0.9612861458018482\n",
            "53     \t [-1.74311342 -2.09275178]. \t  -19.501237889159025 \t -0.9612861458018482\n",
            "54     \t [-3.30705061  0.03684517]. \t  -24.71304071806426 \t -0.9612861458018482\n",
            "55     \t [-5.43608388 -1.38869693]. \t  -68.33655306995928 \t -0.9612861458018482\n",
            "56     \t [-2.48566028 -4.46260303]. \t  -65.77797951348515 \t -0.9612861458018482\n",
            "57     \t [ 0.60767612 -3.05672892]. \t  -28.13878665689198 \t -0.9612861458018482\n",
            "58     \t [-0.41134075 -3.32132516]. \t  -44.02150226539826 \t -0.9612861458018482\n",
            "59     \t [ 0.95133448 -5.09823428]. \t  -29.206007341906663 \t -0.9612861458018482\n",
            "60     \t [-3.61726792  1.46450094]. \t  -52.38778672787236 \t -0.9612861458018482\n",
            "61     \t [-2.46137453 -0.1882053 ]. \t  -32.014878959466216 \t -0.9612861458018482\n",
            "62     \t [-4.34431201  0.01073032]. \t  -34.4809093941701 \t -0.9612861458018482\n",
            "63     \t [-4.1949192   2.65752064]. \t  -46.7567522710171 \t -0.9612861458018482\n",
            "64     \t [-2.08642285 -1.88224786]. \t  -11.948809204992422 \t -0.9612861458018482\n",
            "65     \t [-4.73424784  1.79831976]. \t  -43.64558509858691 \t -0.9612861458018482\n",
            "66     \t [-3.19596326  2.0378629 ]. \t  -21.318343326061875 \t -0.9612861458018482\n",
            "67     \t [-0.71603023  0.02619343]. \t  -12.766724647346269 \t -0.9612861458018482\n",
            "68     \t [0.80917511 0.98435899]. \t  -8.03897156928786 \t -0.9612861458018482\n",
            "69     \t [0.25455894 1.96785301]. \t  -14.426951193329295 \t -0.9612861458018482\n",
            "70     \t [-4.05075177  0.04323539]. \t  -17.28131774021779 \t -0.9612861458018482\n",
            "71     \t [-1.50623767 -3.91024547]. \t  -39.09955881149189 \t -0.9612861458018482\n",
            "72     \t [-5.02284248 -0.63479258]. \t  -42.35761535642757 \t -0.9612861458018482\n",
            "73     \t [-2.27825485 -1.38879737]. \t  -36.54194450682101 \t -0.9612861458018482\n",
            "74     \t [-4.7986804  -3.05162986]. \t  -39.85014734630392 \t -0.9612861458018482\n",
            "75     \t [-2.06252821 -1.15110234]. \t  -10.519264663203277 \t -0.9612861458018482\n",
            "76     \t [-0.98552481 -5.0106943 ]. \t  -26.142214677813662 \t -0.9612861458018482\n",
            "77     \t [0.62337122 2.18985922]. \t  -28.637667851758223 \t -0.9612861458018482\n",
            "78     \t [-2.82872605 -2.71899247]. \t  -32.58333046607183 \t -0.9612861458018482\n",
            "79     \t [1.86706008 4.18847158]. \t  -30.54918779042611 \t -0.9612861458018482\n",
            "80     \t [-5.13432205 -2.26189759]. \t  -45.57928425873814 \t -0.9612861458018482\n",
            "81     \t [-5.8360386   0.26115443]. \t  -49.68135341219706 \t -0.9612861458018482\n",
            "82     \t [-2.10371731 -2.57880183]. \t  -31.924242005099373 \t -0.9612861458018482\n",
            "83     \t [ 0.83686771 -0.91790567]. \t  -7.6529118328365815 \t -0.9612861458018482\n",
            "84     \t [3.37617131 1.60770738]. \t  -48.90232233681222 \t -0.9612861458018482\n",
            "85     \t [4.23471692 3.24318767]. \t  -47.064403851086354 \t -0.9612861458018482\n",
            "86     \t [-2.55464476  4.03471427]. \t  -42.45840591459787 \t -0.9612861458018482\n",
            "87     \t [-4.44772829  2.28844446]. \t  -56.876869975832655 \t -0.9612861458018482\n",
            "88     \t [2.74851348 3.05187311]. \t  -27.488116742993277 \t -0.9612861458018482\n",
            "89     \t [0.67851313 4.07276941]. \t  -32.41715221599897 \t -0.9612861458018482\n",
            "90     \t [-3.24786167  4.11815686]. \t  -40.004998393112814 \t -0.9612861458018482\n",
            "91     \t [1.50633301 0.04137627]. \t  -22.598870663189288 \t -0.9612861458018482\n",
            "92     \t [ 2.35836745 -1.30859965]. \t  -37.16849588249338 \t -0.9612861458018482\n",
            "93     \t [-3.40484972  0.52179168]. \t  -50.0371720301902 \t -0.9612861458018482\n",
            "94     \t [-5.51970747  0.94422056]. \t  -51.89005132065311 \t -0.9612861458018482\n",
            "95     \t [-3.65924356  3.17243277]. \t  -44.169699218232594 \t -0.9612861458018482\n",
            "96     \t [ 5.07932979 -1.11768337]. \t  -30.876979647190392 \t -0.9612861458018482\n",
            "97     \t [5.02918169 2.53666142]. \t  -51.630806845025845 \t -0.9612861458018482\n",
            "98     \t [1.8059765  3.54405747]. \t  -41.9961418594993 \t -0.9612861458018482\n",
            "99     \t [-3.82551018 -0.25631116]. \t  -30.528218106894283 \t -0.9612861458018482\n",
            "100    \t [-4.50317408 -1.95927925]. \t  -44.4408932104864 \t -0.9612861458018482\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMu7U9NNtdtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee54058-3a6d-4e1c-af03-fdb1bc7ec6ca"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_11 = d2GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [4.92580518 3.28957798]. \t  -48.613167836867184 \t -11.082229452321341\n",
            "init   \t [ 1.4939506  -0.78143474]. \t  -30.873019488911932 \t -11.082229452321341\n",
            "init   \t [-3.04833558 -0.08287467]. \t  -11.082229452321341 \t -11.082229452321341\n",
            "init   \t [-3.70116269 -0.4861846 ]. \t  -46.917938977325136 \t -11.082229452321341\n",
            "init   \t [-3.95535741 -5.09005324]. \t  -43.50282995110477 \t -11.082229452321341\n",
            "1      \t [-1.73925418  0.0961581 ]. \t  -15.4792313394182 \t -11.082229452321341\n",
            "2      \t [-3.00835518  1.29200695]. \t  -23.342095844445744 \t -11.082229452321341\n",
            "3      \t [-12.6729033 -11.5970816]. \t  -327.9482532588635 \t -11.082229452321341\n",
            "4      \t [-9.78570554 -3.16120249]. \t  -118.23423796241333 \t -11.082229452321341\n",
            "5      \t [-18.98810856 -12.66995332]. \t  -535.9239900425572 \t -11.082229452321341\n",
            "6      \t [-23.52083895 -17.37129568]. \t  -891.8107699509402 \t -11.082229452321341\n",
            "7      \t [-14.05609725 -18.1684856 ]. \t  -533.3818075343569 \t -11.082229452321341\n",
            "8      \t [-23.49246331 -26.35321476]. \t  -1282.4166059899837 \t -11.082229452321341\n",
            "9      \t [ 3.02789906 -6.55783279]. \t  -71.67362096227832 \t -11.082229452321341\n",
            "10     \t [-8.16678621 -8.30440905]. \t  -154.01853306762504 \t -11.082229452321341\n",
            "11     \t [0.31624705 4.1996748 ]. \t  -38.670950985073816 \t -11.082229452321341\n",
            "12     \t [ 5.05502135 -2.28956864]. \t  -43.84764998158669 \t -11.082229452321341\n",
            "13     \t [-4.64478275  4.99085047]. \t  -62.638965807826295 \t -11.082229452321341\n",
            "14     \t [-3.20877641 -9.99058314]. \t  -117.56420685436285 \t -11.082229452321341\n",
            "15     \t [-13.02629694  -6.61246941]. \t  -231.15066372178103 \t -11.082229452321341\n",
            "16     \t [-0.26475671 -4.24801153]. \t  -38.916626158240476 \t -11.082229452321341\n",
            "17     \t [-7.33590687  0.09705381]. \t  -70.76674035338904 \t -11.082229452321341\n",
            "18     \t [2.46811109 1.89113825]. \t  -31.717395815919783 \t -11.082229452321341\n",
            "19     \t [-6.54123719 -4.95504239]. \t  -87.40275673983513 \t -11.082229452321341\n",
            "20     \t [ 2.3666483  -3.22439161]. \t  -41.08608476293012 \t -11.082229452321341\n",
            "21     \t [4.50346009 0.21594287]. \t  -48.20184212985859 \t -11.082229452321341\n",
            "22     \t [-2.20636045 -0.33700853]. \t  -27.472507811557925 \t -11.082229452321341\n",
            "23     \t [-0.27719586  1.17002132]. \t  -18.3298869710689 \t -11.082229452321341\n",
            "24     \t [-2.82848582  0.14219344]. \t  -17.01915471020255 \t -11.082229452321341\n",
            "25     \t [-1.02127864 -0.06166804]. \t  \u001b[92m-1.8773820247058755\u001b[0m \t -1.8773820247058755\n",
            "26     \t [-4.67061669  2.25310943]. \t  -51.87006647165369 \t -1.8773820247058755\n",
            "27     \t [-1.2575081   1.27524636]. \t  -25.258784366937142 \t -1.8773820247058755\n",
            "28     \t [2.6025032  4.96852516]. \t  -49.65090408460532 \t -1.8773820247058755\n",
            "29     \t [-1.20294667  0.16473107]. \t  -13.45569853829979 \t -1.8773820247058755\n",
            "30     \t [0.77857399 1.3845109 ]. \t  -28.218073008289537 \t -1.8773820247058755\n",
            "31     \t [-0.56819455 -1.10339472]. \t  -22.673345900178234 \t -1.8773820247058755\n",
            "32     \t [-0.57679518  2.55764702]. \t  -45.083713865149605 \t -1.8773820247058755\n",
            "33     \t [-2.25500814  2.21082517]. \t  -27.85078076077736 \t -1.8773820247058755\n",
            "34     \t [ 0.10295022 -0.3052953 ]. \t  -15.528804597312295 \t -1.8773820247058755\n",
            "35     \t [-1.06098893 -0.3233238 ]. \t  -16.401339647060276 \t -1.8773820247058755\n",
            "36     \t [ 2.59062775 -0.63610204]. \t  -42.09907740192945 \t -1.8773820247058755\n",
            "37     \t [ 0.75695226 -1.96004371]. \t  -14.291549009931316 \t -1.8773820247058755\n",
            "38     \t [-0.3667365  -2.27499733]. \t  -33.56885404581242 \t -1.8773820247058755\n",
            "39     \t [ 4.68187617 -4.47919799]. \t  -76.04871992909067 \t -1.8773820247058755\n",
            "40     \t [-1.91386124 -4.96383105]. \t  -29.988822983479135 \t -1.8773820247058755\n",
            "41     \t [-2.22170957  1.44124047]. \t  -34.571147545812 \t -1.8773820247058755\n",
            "42     \t [-3.64215852  1.38646995]. \t  -49.01864292259427 \t -1.8773820247058755\n",
            "43     \t [-0.79197509  2.01405798]. \t  -12.11573485884928 \t -1.8773820247058755\n",
            "44     \t [-0.84638599 -0.3245908 ]. \t  -19.64606446521454 \t -1.8773820247058755\n",
            "45     \t [-0.2228227   0.80546948]. \t  -15.583998763653813 \t -1.8773820247058755\n",
            "46     \t [ 0.09046607 -1.13869879]. \t  -6.440245706401301 \t -1.8773820247058755\n",
            "47     \t [-0.64736796 -2.0150154 ]. \t  -20.534678588829337 \t -1.8773820247058755\n",
            "48     \t [0.23390185 3.06312415]. \t  -19.203969925173958 \t -1.8773820247058755\n",
            "49     \t [1.50063181 0.15262262]. \t  -26.531363423897247 \t -1.8773820247058755\n",
            "50     \t [-0.5580845   3.66114354]. \t  -48.354290491350554 \t -1.8773820247058755\n",
            "51     \t [1.11894634 2.0730627 ]. \t  -9.250119167116807 \t -1.8773820247058755\n",
            "52     \t [-0.14752588 -1.54671265]. \t  -25.983541527953562 \t -1.8773820247058755\n",
            "53     \t [ 0.24406083 -3.52085331]. \t  -41.99717566121063 \t -1.8773820247058755\n",
            "54     \t [ 2.24010279 -2.26666579]. \t  -30.579604271507257 \t -1.8773820247058755\n",
            "55     \t [ 1.10842683 -2.08432037]. \t  -9.1761608797004 \t -1.8773820247058755\n",
            "56     \t [ 3.51345331 -1.98197965]. \t  -36.300923738914165 \t -1.8773820247058755\n",
            "57     \t [-0.55708621 -3.42623129]. \t  -50.35792540793972 \t -1.8773820247058755\n",
            "58     \t [ 0.49302154 -2.72593626]. \t  -39.17040264647426 \t -1.8773820247058755\n",
            "59     \t [ 0.79228173 -1.12880864]. \t  -12.376573513704454 \t -1.8773820247058755\n",
            "60     \t [ 0.37416374 -5.15110798]. \t  -47.88633816760499 \t -1.8773820247058755\n",
            "61     \t [ 3.38352136 -2.8779291 ]. \t  -39.970041301788214 \t -1.8773820247058755\n",
            "62     \t [-1.39062416 -0.13609577]. \t  -23.121110909764585 \t -1.8773820247058755\n",
            "63     \t [-3.45670951  2.62138863]. \t  -55.682551004187964 \t -1.8773820247058755\n",
            "64     \t [-3.48376457 -0.80983298]. \t  -39.068973294562845 \t -1.8773820247058755\n",
            "65     \t [ 0.91754886 -0.03381313]. \t  -2.380039708825155 \t -1.8773820247058755\n",
            "66     \t [-5.66473323  2.13256214]. \t  -55.01462113134948 \t -1.8773820247058755\n",
            "67     \t [-2.73368721 -0.23767581]. \t  -27.779126462057768 \t -1.8773820247058755\n",
            "68     \t [-2.11721433 -1.83784485]. \t  -15.208443087245643 \t -1.8773820247058755\n",
            "69     \t [-1.44160061 -1.65013275]. \t  -40.00656885300507 \t -1.8773820247058755\n",
            "70     \t [-3.02335196 -2.42183282]. \t  -33.931341814549725 \t -1.8773820247058755\n",
            "71     \t [0.66210796 1.92729882]. \t  -20.424132850786762 \t -1.8773820247058755\n",
            "72     \t [-2.88240337 -1.68957329]. \t  -27.476844403889544 \t -1.8773820247058755\n",
            "73     \t [-2.10704512  3.28337886]. \t  -29.47998826669753 \t -1.8773820247058755\n",
            "74     \t [-2.45273185 -2.95604756]. \t  -34.69522213143506 \t -1.8773820247058755\n",
            "75     \t [1.80874441 1.64932044]. \t  -28.296375989048492 \t -1.8773820247058755\n",
            "76     \t [3.63089629 2.31370536]. \t  -49.23764489817308 \t -1.8773820247058755\n",
            "77     \t [ 0.54966522 -0.29181772]. \t  -32.50169046596367 \t -1.8773820247058755\n",
            "78     \t [ 2.68241514 -1.45687748]. \t  -43.073024838286656 \t -1.8773820247058755\n",
            "79     \t [-0.52392084 -5.50690695]. \t  -70.47836531383837 \t -1.8773820247058755\n",
            "80     \t [0.99475644 0.6558859 ]. \t  -16.99986264358705 \t -1.8773820247058755\n",
            "81     \t [1.97208476 2.25959668]. \t  -19.750932947452938 \t -1.8773820247058755\n",
            "82     \t [-2.72228269 -2.28696566]. \t  -36.67556964691995 \t -1.8773820247058755\n",
            "83     \t [-4.01957019 -3.36679173]. \t  -44.26487930491954 \t -1.8773820247058755\n",
            "84     \t [-3.11829929  2.45837879]. \t  -38.064995276138546 \t -1.8773820247058755\n",
            "85     \t [-5.07267826  2.78616273]. \t  -42.26675417092231 \t -1.8773820247058755\n",
            "86     \t [-3.48202669 -3.2332045 ]. \t  -51.46108850900512 \t -1.8773820247058755\n",
            "87     \t [-4.76214342 -2.63456368]. \t  -55.490339780644724 \t -1.8773820247058755\n",
            "88     \t [-3.83885938 -2.8072739 ]. \t  -33.798551828310586 \t -1.8773820247058755\n",
            "89     \t [-5.02049544 -4.10333391]. \t  -44.1602489080787 \t -1.8773820247058755\n",
            "90     \t [-0.25856436  0.31056299]. \t  -24.415271599683958 \t -1.8773820247058755\n",
            "91     \t [-1.75479427  2.5904999 ]. \t  -37.9152126312746 \t -1.8773820247058755\n",
            "92     \t [-2.78946892  3.83729637]. \t  -34.837343200980676 \t -1.8773820247058755\n",
            "93     \t [ 1.51818656 -1.58322364]. \t  -43.409968904026705 \t -1.8773820247058755\n",
            "94     \t [-1.86547139 -1.21426277]. \t  -16.092496203062023 \t -1.8773820247058755\n",
            "95     \t [ 2.72863186 -2.4949394 ]. \t  -45.00366937790466 \t -1.8773820247058755\n",
            "96     \t [ 4.19357648 -3.33873761]. \t  -50.552984973483305 \t -1.8773820247058755\n",
            "97     \t [4.56210452 2.28033808]. \t  -57.15569017810514 \t -1.8773820247058755\n",
            "98     \t [ 4.00102007 -1.16213332]. \t  -22.114304795243548 \t -1.8773820247058755\n",
            "99     \t [4.13515682 3.0988277 ]. \t  -31.963285274835947 \t -1.8773820247058755\n",
            "100    \t [-5.63588514 -2.91469634]. \t  -58.23226301885253 \t -1.8773820247058755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEjKllHqtdwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d51936f-01a6-4b5c-a569-f68fce5db3e6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_12 = d2GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-3.15884083  1.25039382]. \t  -26.146884045802807 \t -26.146884045802807\n",
            "init   \t [-0.63766795  2.9220719 ]. \t  -26.606539329930673 \t -26.146884045802807\n",
            "init   \t [ 2.86695228 -2.32865172]. \t  -31.680531551175278 \t -26.146884045802807\n",
            "init   \t [-2.28900603  3.0911711 ]. \t  -28.81762335775992 \t -26.146884045802807\n",
            "init   \t [4.69134698 3.84955018]. \t  -54.575233812225456 \t -26.146884045802807\n",
            "1      \t [-19.58408469 -14.51221895]. \t  -632.7479682788643 \t -26.146884045802807\n",
            "2      \t [-12.81865978  -7.21179641]. \t  -229.76918053269924 \t -26.146884045802807\n",
            "3      \t [-5.88794133 -9.75487457]. \t  -141.89716124932573 \t -26.146884045802807\n",
            "4      \t [-8.64357006 -1.47608567]. \t  -112.97727126757574 \t -26.146884045802807\n",
            "5      \t [-38.78738563 -43.81562928]. \t  -3437.935452961329 \t -26.146884045802807\n",
            "6      \t [ 0.09553084 -7.80895906]. \t  -69.116599597754 \t -26.146884045802807\n",
            "7      \t [-36.52104402 -30.5778809 ]. \t  -2307.5326004708218 \t -26.146884045802807\n",
            "8      \t [-76.53183576 -73.5255827 ]. \t  -11302.80489231799 \t -26.146884045802807\n",
            "9      \t [-3.79063035 -4.11000128]. \t  -41.030666850420225 \t -26.146884045802807\n",
            "10     \t [-16.56093416 -22.02480029]. \t  -778.7535271524339 \t -26.146884045802807\n",
            "11     \t [-10.07760888 -15.6578362 ]. \t  -363.36414252063713 \t -26.146884045802807\n",
            "12     \t [-30.62995602 -26.57883888]. \t  -1680.2743470172427 \t -26.146884045802807\n",
            "13     \t [-63.56806574 -69.44876565]. \t  -8902.615693518139 \t -26.146884045802807\n",
            "14     \t [-92.640267   -87.52165091]. \t  -16278.527365635344 \t -26.146884045802807\n",
            "15     \t [-47.75720253 -49.52585032]. \t  -4762.976234405446 \t -26.146884045802807\n",
            "16     \t [-14.96455706 -12.37332299]. \t  -394.2802000049069 \t -26.146884045802807\n",
            "17     \t [-6.64709206  2.60703867]. \t  -84.82755791909509 \t -26.146884045802807\n",
            "18     \t [ 3.96947583 -5.91267951]. \t  -52.36758007968809 \t -26.146884045802807\n",
            "19     \t [-22.7858913  -23.99621403]. \t  -1102.7819081214884 \t -26.146884045802807\n",
            "20     \t [-7.68350249 -5.9366398 ]. \t  -109.11958015421743 \t -26.146884045802807\n",
            "21     \t [-24.86150979 -30.30281439]. \t  -1553.1660402238915 \t -26.146884045802807\n",
            "22     \t [-40.90433737 -37.73319197]. \t  -3109.7653774892947 \t -26.146884045802807\n",
            "23     \t [-24.38067108 -18.11848463]. \t  -942.6605604293171 \t -26.146884045802807\n",
            "24     \t [-100.22543142  -96.53916504]. \t  -19393.108668470726 \t -26.146884045802807\n",
            "25     \t [-111.97619414 -108.63881296]. \t  -24357.60292340698 \t -26.146884045802807\n",
            "26     \t [-31.52896381 -37.89922468]. \t  -2452.2002139581728 \t -26.146884045802807\n",
            "27     \t [-0.59829133 -1.81777156]. \t  -27.684363593798047 \t -26.146884045802807\n",
            "28     \t [-1341.16215006 -1336.96642381]. \t  -3586200.109126394 \t -26.146884045802807\n",
            "29     \t [-13.11555645 -19.24788486]. \t  -554.8880549130595 \t -26.146884045802807\n",
            "30     \t [-36.05367804 -36.81156692]. \t  -2661.749935780503 \t -26.146884045802807\n",
            "31     \t [-32.6718207  -33.48452353]. \t  -2223.3309178901104 \t -26.146884045802807\n",
            "32     \t [2.25897845 1.09421614]. \t  \u001b[92m-18.565737193925045\u001b[0m \t -18.565737193925045\n",
            "33     \t [-173.72548758 -175.81721814]. \t  -61109.67431154282 \t -18.565737193925045\n",
            "34     \t [-11.08680997 -11.04052306]. \t  -246.58392355937255 \t -18.565737193925045\n",
            "35     \t [-18.4167886  -18.52016854]. \t  -720.7586412332467 \t -18.565737193925045\n",
            "36     \t [-92.38614673 -94.53299263]. \t  -17509.02127004321 \t -18.565737193925045\n",
            "37     \t [-4.9572804  -1.22080811]. \t  -34.59916370948778 \t -18.565737193925045\n",
            "38     \t [ 0.75246381 -4.62978622]. \t  -48.70157924485656 \t -18.565737193925045\n",
            "39     \t [ 5.09605158 -0.52711456]. \t  -47.869332630995835 \t -18.565737193925045\n",
            "40     \t [-3.51268315 -7.13230393]. \t  -86.43773407844606 \t -18.565737193925045\n",
            "41     \t [-4.03668037  4.79923766]. \t  -46.54729849460311 \t -18.565737193925045\n",
            "42     \t [1.16812757 4.1547123 ]. \t  -28.070083872143652 \t -18.565737193925045\n",
            "43     \t [-137.92075809 -139.25579339]. \t  -38425.88951999433 \t -18.565737193925045\n",
            "44     \t [-0.04707435  0.68453248]. \t  \u001b[92m-14.903461432158506\u001b[0m \t -14.903461432158506\n",
            "45     \t [-13.6501526  -15.80061085]. \t  -458.7294126990715 \t -14.903461432158506\n",
            "46     \t [-0.88379117  4.9136404 ]. \t  -28.910661459244654 \t -14.903461432158506\n",
            "47     \t [ 5.01525277 -4.12014568]. \t  -44.890830725590135 \t -14.903461432158506\n",
            "48     \t [-2.46869126 -0.79369089]. \t  -33.82068414886996 \t -14.903461432158506\n",
            "49     \t [-1.4438697  -4.91950252]. \t  -46.92281315610269 \t -14.903461432158506\n",
            "50     \t [4.02748822 1.64974577]. \t  -34.9818698498354 \t -14.903461432158506\n",
            "51     \t [0.82902435 1.71408312]. \t  -21.099252681583543 \t -14.903461432158506\n",
            "52     \t [ 1.59929531 -0.71764149]. \t  -33.208037515405174 \t -14.903461432158506\n",
            "53     \t [-5.00700463  0.79832025]. \t  -32.727470457073785 \t -14.903461432158506\n",
            "54     \t [2.93187799 4.99259521]. \t  -34.43485626059242 \t -14.903461432158506\n",
            "55     \t [-1.07190088  1.09893963]. \t  \u001b[92m-5.23071551764469\u001b[0m \t -5.23071551764469\n",
            "56     \t [-1.00438935  0.26983273]. \t  -12.328315423839754 \t -5.23071551764469\n",
            "57     \t [ 0.99185168 -2.6582381 ]. \t  -23.51451025022637 \t -5.23071551764469\n",
            "58     \t [-1.12741768  1.26879468]. \t  -17.09623085885638 \t -5.23071551764469\n",
            "59     \t [-1.0503863  0.7704743]. \t  -10.911012746735166 \t -5.23071551764469\n",
            "60     \t [-0.0901538   1.30872706]. \t  -16.889560856728583 \t -5.23071551764469\n",
            "61     \t [-0.77878675 -0.84433946]. \t  -13.934080044407946 \t -5.23071551764469\n",
            "62     \t [1.76117531 2.68743251]. \t  -33.453194731244054 \t -5.23071551764469\n",
            "63     \t [-0.67563426  0.86692152]. \t  -19.009197781170453 \t -5.23071551764469\n",
            "64     \t [-0.79815641 -2.88076446]. \t  -18.633576644139676 \t -5.23071551764469\n",
            "65     \t [-2.26693996  4.04358701]. \t  -32.92464626462149 \t -5.23071551764469\n",
            "66     \t [0.62213459 0.4684845 ]. \t  -37.60833539442982 \t -5.23071551764469\n",
            "67     \t [0.00835069 3.36827204]. \t  -28.12501228405145 \t -5.23071551764469\n",
            "68     \t [-1.25814788  3.64868217]. \t  -41.35217846695184 \t -5.23071551764469\n",
            "69     \t [-1.36121144 -0.71269503]. \t  -31.116071023527432 \t -5.23071551764469\n",
            "70     \t [-3.45592947  3.28802193]. \t  -54.73993150331037 \t -5.23071551764469\n",
            "71     \t [-0.23340267 -0.44246927]. \t  -28.56306178049138 \t -5.23071551764469\n",
            "72     \t [-3.3991475  -0.06939272]. \t  -30.553137774489215 \t -5.23071551764469\n",
            "73     \t [ 2.80383    -0.29592512]. \t  -27.476596281948865 \t -5.23071551764469\n",
            "74     \t [-0.29590594 -2.49906081]. \t  -39.177218618871365 \t -5.23071551764469\n",
            "75     \t [ 2.37203515 -1.68415653]. \t  -39.42111844023366 \t -5.23071551764469\n",
            "76     \t [-3.3012422  -1.78365883]. \t  -35.14482611866655 \t -5.23071551764469\n",
            "77     \t [1.28984697 4.98121115]. \t  -39.02335717506715 \t -5.23071551764469\n",
            "78     \t [-1.43256229 -2.9822186 ]. \t  -30.12385147279597 \t -5.23071551764469\n",
            "79     \t [ 1.66749481 -2.82454152]. \t  -31.19922449004845 \t -5.23071551764469\n",
            "80     \t [ 3.6564925  -2.92352888]. \t  -38.59226419233714 \t -5.23071551764469\n",
            "81     \t [2.52214862 1.60906083]. \t  -46.596245852569226 \t -5.23071551764469\n",
            "82     \t [-3.72526373  1.21508247]. \t  -34.725616065325475 \t -5.23071551764469\n",
            "83     \t [-0.92350586 -0.47664583]. \t  -22.10553354957451 \t -5.23071551764469\n",
            "84     \t [-1.11161506 -3.74767781]. \t  -27.78662698134008 \t -5.23071551764469\n",
            "85     \t [-2.24273682  0.38796438]. \t  -32.34716085201531 \t -5.23071551764469\n",
            "86     \t [ 0.00816074 -3.5224543 ]. \t  -32.32153461142356 \t -5.23071551764469\n",
            "87     \t [-2.91299002  2.98634803]. \t  -18.89810376079896 \t -5.23071551764469\n",
            "88     \t [ 1.40404479 -1.71114077]. \t  -35.553677186373044 \t -5.23071551764469\n",
            "89     \t [ 1.85156681 -5.52698575]. \t  -57.875257592953034 \t -5.23071551764469\n",
            "90     \t [-1.97588728 -0.04197166]. \t  \u001b[92m-4.366160426476043\u001b[0m \t -4.366160426476043\n",
            "91     \t [-0.74373273  1.51825442]. \t  -33.18621351025402 \t -4.366160426476043\n",
            "92     \t [1.99377489 4.05256706]. \t  -20.9465986587167 \t -4.366160426476043\n",
            "93     \t [-2.93833134  3.36148314]. \t  -37.12046623836307 \t -4.366160426476043\n",
            "94     \t [-0.42948629 -1.2672034 ]. \t  -31.90356124101692 \t -4.366160426476043\n",
            "95     \t [-1.64253973 -1.70550718]. \t  -34.61646513957841 \t -4.366160426476043\n",
            "96     \t [-4.5414889   2.67344279]. \t  -62.0614252625121 \t -4.366160426476043\n",
            "97     \t [-2.1257354  -3.75730037]. \t  -31.13920267402094 \t -4.366160426476043\n",
            "98     \t [-1.16911383 -1.44463866]. \t  -27.988646840572013 \t -4.366160426476043\n",
            "99     \t [-3.48069153  0.62845123]. \t  -49.352746792693516 \t -4.366160426476043\n",
            "100    \t [-2.46743666 -2.53771419]. \t  -52.040205889609865 \t -4.366160426476043\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoONg4VEtdy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338eac39-017e-4c1e-a3fb-dab754437cc4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_13 = d2GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.64499773 -0.88472932]. \t  -22.128094062039295 \t -22.128094062039295\n",
            "init   \t [ 1.72137019 -2.76537936]. \t  -31.434805818064365 \t -22.128094062039295\n",
            "init   \t [3.15062616 1.40102108]. \t  -34.17104049244217 \t -22.128094062039295\n",
            "init   \t [-3.35673495 -2.45364871]. \t  -53.08180488505975 \t -22.128094062039295\n",
            "init   \t [ 4.2528767  -0.38076919]. \t  -45.735368766041006 \t -22.128094062039295\n",
            "1      \t [-3.68998462  4.94754404]. \t  -52.31457314781095 \t -22.128094062039295\n",
            "2      \t [-6.1194918  -8.52955569]. \t  -132.71805826066293 \t -22.128094062039295\n",
            "3      \t [-9.88985773  0.02386177]. \t  -100.22260543228418 \t -22.128094062039295\n",
            "4      \t [-13.00915342 -14.79829331]. \t  -395.2560832003703 \t -22.128094062039295\n",
            "5      \t [ 0.57580751 -8.77575979]. \t  -104.62095274185045 \t -22.128094062039295\n",
            "6      \t [-7.47345794 -4.2380646 ]. \t  -102.92580753855495 \t -22.128094062039295\n",
            "7      \t [-11.81017702  -9.09377452]. \t  -230.17159146370184 \t -22.128094062039295\n",
            "8      \t [0.45360341 4.90259977]. \t  -45.634226292871936 \t -22.128094062039295\n",
            "9      \t [-5.75214986  1.19039826]. \t  -50.71123294942642 \t -22.128094062039295\n",
            "10     \t [5.11103192 4.99782633]. \t  -53.43820606126725 \t -22.128094062039295\n",
            "11     \t [-1.43255915  1.40969199]. \t  -41.587912363735356 \t -22.128094062039295\n",
            "12     \t [-2.57974675 -6.06852376]. \t  -63.16542156347637 \t -22.128094062039295\n",
            "13     \t [-29.55578415 -35.06293051]. \t  -2123.117067060654 \t -22.128094062039295\n",
            "14     \t [ -7.85629576 -13.2800061 ]. \t  -253.76093719072216 \t -22.128094062039295\n",
            "15     \t [-19.75655934 -25.60311227]. \t  -1073.402665136667 \t -22.128094062039295\n",
            "16     \t [ 4.80660191 -4.72324902]. \t  -63.603506964962946 \t -22.128094062039295\n",
            "17     \t [-27.1614857  -19.67039969]. \t  -1144.1872254239393 \t -22.128094062039295\n",
            "18     \t [-28.84816704 -26.06383587]. \t  -1516.5496404499834 \t -22.128094062039295\n",
            "19     \t [-24.40034548 -29.15164623]. \t  -1467.5043886695898 \t -22.128094062039295\n",
            "20     \t [-38.03609016 -36.93699077]. \t  -2812.1149412473997 \t -22.128094062039295\n",
            "21     \t [ 1.30180885 -5.62680677]. \t  -63.54406064265116 \t -22.128094062039295\n",
            "22     \t [-19.20422916 -16.44977762]. \t  -666.0674507970141 \t -22.128094062039295\n",
            "23     \t [-16.54313682 -12.14465019]. \t  -444.6564199476641 \t -22.128094062039295\n",
            "24     \t [-0.70560941 -1.83831662]. \t  \u001b[92m-21.361748164367143\u001b[0m \t -21.361748164367143\n",
            "25     \t [-5.96445518 -1.63732475]. \t  -55.00676436551498 \t -21.361748164367143\n",
            "26     \t [2.7620324  3.51991589]. \t  -49.18514109749937 \t -21.361748164367143\n",
            "27     \t [-4.81551837 -5.28705876]. \t  -69.44834546284444 \t -21.361748164367143\n",
            "28     \t [-0.70833394 -4.03439197]. \t  -29.598778959047706 \t -21.361748164367143\n",
            "29     \t [0.96390358 0.98789107]. \t  \u001b[92m-2.1900595776666645\u001b[0m \t -2.1900595776666645\n",
            "30     \t [0.45738207 1.76559233]. \t  -31.992002238507418 \t -2.1900595776666645\n",
            "31     \t [1.05509137 0.04077004]. \t  \u001b[92m-2.034332656065434\u001b[0m \t -2.034332656065434\n",
            "32     \t [1.3768304  0.50503272]. \t  -39.29764193006285 \t -2.034332656065434\n",
            "33     \t [ 0.3530969  -0.68416918]. \t  -30.64624951687327 \t -2.034332656065434\n",
            "34     \t [0.13137231 0.76640511]. \t  -12.793333753883635 \t -2.034332656065434\n",
            "35     \t [4.84884838 2.06459134]. \t  -32.76699449547059 \t -2.034332656065434\n",
            "36     \t [-2.92956843  1.58986468]. \t  -30.52118127151042 \t -2.034332656065434\n",
            "37     \t [-1.49378568  3.67718678]. \t  -50.162532929774606 \t -2.034332656065434\n",
            "38     \t [0.54360533 0.54671644]. \t  -39.79371202510151 \t -2.034332656065434\n",
            "39     \t [-0.85622531  0.87577252]. \t  -8.20507165942227 \t -2.034332656065434\n",
            "40     \t [-0.5415353   1.08260284]. \t  -22.4435690097715 \t -2.034332656065434\n",
            "41     \t [ 1.25469909 -0.75557718]. \t  -22.090023405930122 \t -2.034332656065434\n",
            "42     \t [1.6349445  1.56356218]. \t  -40.94655448132153 \t -2.034332656065434\n",
            "43     \t [-1.03871884 -0.6130308 ]. \t  -19.331564520675908 \t -2.034332656065434\n",
            "44     \t [ 1.87104523 -0.17671541]. \t  -12.195232744344608 \t -2.034332656065434\n",
            "45     \t [ 3.29910241 -1.92500948]. \t  -28.71588690584487 \t -2.034332656065434\n",
            "46     \t [3.63247185 1.84603409]. \t  -37.65967352048178 \t -2.034332656065434\n",
            "47     \t [0.9454594  1.51432745]. \t  -23.728042450629445 \t -2.034332656065434\n",
            "48     \t [-1.88869663  0.53238191]. \t  -25.99166147775643 \t -2.034332656065434\n",
            "49     \t [2.92704093 2.20927023]. \t  -21.9496096932107 \t -2.034332656065434\n",
            "50     \t [-0.95501215 -0.08851257]. \t  -2.8237456576375717 \t -2.034332656065434\n",
            "51     \t [0.29244654 1.4028169 ]. \t  -32.88184043035503 \t -2.034332656065434\n",
            "52     \t [0.09212456 3.28747566]. \t  -24.777947932118895 \t -2.034332656065434\n",
            "53     \t [0.10382073 2.57448225]. \t  -27.61677773511809 \t -2.034332656065434\n",
            "54     \t [ 2.20701683 -3.61921345]. \t  -42.62505604988243 \t -2.034332656065434\n",
            "55     \t [-2.18437449  1.56523491]. \t  -32.385608905607995 \t -2.034332656065434\n",
            "56     \t [-3.06730899  0.40815674]. \t  -28.836668689959417 \t -2.034332656065434\n",
            "57     \t [ 2.92973464 -0.91929284]. \t  -11.645710554830963 \t -2.034332656065434\n",
            "58     \t [-4.01752559  1.731279  ]. \t  -30.371970353675383 \t -2.034332656065434\n",
            "59     \t [-0.57844378 -0.26120052]. \t  -29.91574466656744 \t -2.034332656065434\n",
            "60     \t [-2.62166126 -0.40568021]. \t  -42.55027455693748 \t -2.034332656065434\n",
            "61     \t [0.57757682 3.19484929]. \t  -35.979766152727784 \t -2.034332656065434\n",
            "62     \t [-0.34343533  4.26863135]. \t  -45.04638421749976 \t -2.034332656065434\n",
            "63     \t [ 1.72717683 -0.52762308]. \t  -34.540399032733994 \t -2.034332656065434\n",
            "64     \t [2.45447368 0.18331731]. \t  -31.583418110654964 \t -2.034332656065434\n",
            "65     \t [ 1.37462394 -2.14721383]. \t  -27.535887328186817 \t -2.034332656065434\n",
            "66     \t [ 1.94565474 -1.87553393]. \t  -10.785786674365657 \t -2.034332656065434\n",
            "67     \t [ 1.02604445 -3.89585853]. \t  -18.42957900906917 \t -2.034332656065434\n",
            "68     \t [ 4.05129134 -2.00766295]. \t  -20.9700813922109 \t -2.034332656065434\n",
            "69     \t [4.99224157 0.49748171]. \t  -45.18059148318649 \t -2.034332656065434\n",
            "70     \t [ 1.33736252 -3.31461901]. \t  -41.94235453395298 \t -2.034332656065434\n",
            "71     \t [ 0.42737447 -4.81026172]. \t  -48.60153959334907 \t -2.034332656065434\n",
            "72     \t [-1.46272719 -2.31022087]. \t  -40.89785802368753 \t -2.034332656065434\n",
            "73     \t [2.84433927 4.24260988]. \t  -40.03938682932563 \t -2.034332656065434\n",
            "74     \t [ 2.34627116 -0.23622429]. \t  -30.383029309656145 \t -2.034332656065434\n",
            "75     \t [ 3.45560746 -1.30631162]. \t  -46.72599129654299 \t -2.034332656065434\n",
            "76     \t [ 4.24034669 -3.07225961]. \t  -37.826245295258175 \t -2.034332656065434\n",
            "77     \t [2.37858258 1.45510133]. \t  -44.608118940401894 \t -2.034332656065434\n",
            "78     \t [3.89397779 0.53551605]. \t  -37.339861709055725 \t -2.034332656065434\n",
            "79     \t [-1.82517985 -0.51413251]. \t  -29.0062440490113 \t -2.034332656065434\n",
            "80     \t [ 2.70381608 -1.37670872]. \t  -39.21379121290953 \t -2.034332656065434\n",
            "81     \t [-2.5704852   4.04171171]. \t  -42.31955070634976 \t -2.034332656065434\n",
            "82     \t [ 3.5740567 -2.9124441]. \t  -41.6684622323433 \t -2.034332656065434\n",
            "83     \t [ 4.84571044 -2.65007812]. \t  -50.720007526093944 \t -2.034332656065434\n",
            "84     \t [ 1.84677962 -3.54978143]. \t  -39.81338100813379 \t -2.034332656065434\n",
            "85     \t [ 1.39881859 -1.54255323]. \t  -42.027171079028 \t -2.034332656065434\n",
            "86     \t [ 1.81590646 -4.85049543]. \t  -36.89812986447846 \t -2.034332656065434\n",
            "87     \t [ 0.49056524 -4.18395204]. \t  -43.696721149319416 \t -2.034332656065434\n",
            "88     \t [-1.73742298 -4.10124876]. \t  -32.58449251938174 \t -2.034332656065434\n",
            "89     \t [-0.81881997 -3.0832927 ]. \t  -17.32504051069931 \t -2.034332656065434\n",
            "90     \t [ 3.63574011 -2.57692385]. \t  -55.291845741727585 \t -2.034332656065434\n",
            "91     \t [ 3.88215611 -4.3649445 ]. \t  -53.352626907281376 \t -2.034332656065434\n",
            "92     \t [ 4.89310629 -1.26616613]. \t  -38.731602558290426 \t -2.034332656065434\n",
            "93     \t [ 4.4468131  -4.13877132]. \t  -59.916827012973584 \t -2.034332656065434\n",
            "94     \t [ 3.15505336 -3.50948159]. \t  -46.635021792052 \t -2.034332656065434\n",
            "95     \t [0.29334079 3.82539113]. \t  -32.847527391195534 \t -2.034332656065434\n",
            "96     \t [ 1.12340398 -4.55164183]. \t  -44.31604131527531 \t -2.034332656065434\n",
            "97     \t [-0.55652726 -5.61587613]. \t  -68.68824956163112 \t -2.034332656065434\n",
            "98     \t [2.27281629 1.93681117]. \t  -21.12346938041539 \t -2.034332656065434\n",
            "99     \t [ 4.89815377 -0.06402187]. \t  -26.772785778568643 \t -2.034332656065434\n",
            "100    \t [6.52245514 2.9130503 ]. \t  -72.38449971961774 \t -2.034332656065434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoQHSe2Dtd1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9efd28c5-441b-4f5f-d5f2-91cf1553127d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_14 = d2GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.51092275 0.07321281]. \t  -23.304224567038325 \t -16.03632029095263\n",
            "init   \t [0.29021573 4.05796049]. \t  -29.707120177200387 \t -16.03632029095263\n",
            "init   \t [2.04790979 2.19440232]. \t  -16.03632029095263 \t -16.03632029095263\n",
            "init   \t [ 2.22554503 -2.83832871]. \t  -26.209337872186715 \t -16.03632029095263\n",
            "init   \t [-3.32641768 -0.44194316]. \t  -45.22147500850562 \t -16.03632029095263\n",
            "1      \t [-9.21096992 -6.93785577]. \t  -141.3006571309632 \t -16.03632029095263\n",
            "2      \t [-2.31562314 -7.25882591]. \t  -82.61430890106884 \t -16.03632029095263\n",
            "3      \t [-26.42931107 -29.4324506 ]. \t  -1602.9201424327987 \t -16.03632029095263\n",
            "4      \t [-15.2652671  -15.64667231]. \t  -504.8502326108299 \t -16.03632029095263\n",
            "5      \t [-221.81297912 -221.96358825]. \t  -98475.23815353459 \t -16.03632029095263\n",
            "6      \t [ -8.18081762 -13.09308776]. \t  -245.8057290069456 \t -16.03632029095263\n",
            "7      \t [-4.69066073  4.27567154]. \t  -65.53228468248732 \t -16.03632029095263\n",
            "8      \t [-18.6946157 -21.2376901]. \t  -823.1655421259858 \t -16.03632029095263\n",
            "9      \t [-8.14988471 -1.91632046]. \t  -75.55983625271433 \t -16.03632029095263\n",
            "10     \t [ 2.43187652 -7.26184062]. \t  -88.48947689438312 \t -16.03632029095263\n",
            "11     \t [-109.3876023  -109.39993794]. \t  -23969.69006757116 \t -16.03632029095263\n",
            "12     \t [-34.92398219 -34.45797252]. \t  -2427.8089179486124 \t -16.03632029095263\n",
            "13     \t [-82.63634795 -82.38093418]. \t  -13649.263412193079 \t -16.03632029095263\n",
            "14     \t [4.62484125 5.07188123]. \t  -65.19394112929302 \t -16.03632029095263\n",
            "15     \t [-55.17819198 -55.13532361]. \t  -6093.57862041997 \t -16.03632029095263\n",
            "16     \t [ -4.37767099 -10.88241909]. \t  -157.386895395194 \t -16.03632029095263\n",
            "17     \t [-13.07151457 -10.64434405]. \t  -301.3207949951801 \t -16.03632029095263\n",
            "18     \t [5.02749211 0.0807618 ]. \t  -26.691113594269645 \t -16.03632029095263\n",
            "19     \t [-4.79391671 -4.09285224]. \t  -48.66268999770053 \t -16.03632029095263\n",
            "20     \t [-13.512881   -20.13813829]. \t  -611.6459006414641 \t -16.03632029095263\n",
            "21     \t [-1.04024001 -3.52793385]. \t  -33.692716186824896 \t -16.03632029095263\n",
            "22     \t [-6.09061921  0.96345636]. \t  -39.863970820024655 \t -16.03632029095263\n",
            "23     \t [ 5.08794556 -3.87589202]. \t  -45.2874003618501 \t -16.03632029095263\n",
            "24     \t [-5.53116309 -7.60671086]. \t  -126.09994149570777 \t -16.03632029095263\n",
            "25     \t [-1.83185042  2.15999876]. \t  -17.743844044569006 \t -16.03632029095263\n",
            "26     \t [-32.73559451 -28.30129261]. \t  -1896.6535089271947 \t -16.03632029095263\n",
            "27     \t [-0.77671616 -0.65525275]. \t  -24.969582388056438 \t -16.03632029095263\n",
            "28     \t [4.1814955  2.30589664]. \t  -42.069811849933586 \t -16.03632029095263\n",
            "29     \t [-2.11542498  4.4008142 ]. \t  -44.478896763865905 \t -16.03632029095263\n",
            "30     \t [ 0.84086241 -4.67983488]. \t  -41.471141338629145 \t -16.03632029095263\n",
            "31     \t [ 3.37394464 -0.73473517]. \t  -39.905010262389524 \t -16.03632029095263\n",
            "32     \t [0.24103439 1.82921747]. \t  -18.066713420667305 \t -16.03632029095263\n",
            "33     \t [-5.49114405 -1.75129318]. \t  -63.12296146596179 \t -16.03632029095263\n",
            "34     \t [1.86977441 5.06354241]. \t  -33.086853691577254 \t -16.03632029095263\n",
            "35     \t [-3.56685812  2.08412614]. \t  -37.56137370682498 \t -16.03632029095263\n",
            "36     \t [-1.04748976  1.09216374]. \t  \u001b[92m-4.362282476422166\u001b[0m \t -4.362282476422166\n",
            "37     \t [-0.98580085  1.14911576]. \t  -6.409331894013433 \t -4.362282476422166\n",
            "38     \t [-2.02707514  1.39668151]. \t  -24.16996828917194 \t -4.362282476422166\n",
            "39     \t [-2.97917055 -4.88184773]. \t  -35.424747408079085 \t -4.362282476422166\n",
            "40     \t [-1.3426413   0.26996442]. \t  -28.624327736066355 \t -4.362282476422166\n",
            "41     \t [0.43315239 2.41933857]. \t  -43.91461453738306 \t -4.362282476422166\n",
            "42     \t [0.78645288 1.01372819]. \t  -9.412901840198474 \t -4.362282476422166\n",
            "43     \t [-0.03315272 -1.23696754]. \t  -10.929419587152926 \t -4.362282476422166\n",
            "44     \t [-2.14624015 -2.0688804 ]. \t  -13.741298918486816 \t -4.362282476422166\n",
            "45     \t [-1.47009645  1.21318335]. \t  -31.164321012767687 \t -4.362282476422166\n",
            "46     \t [-0.35317103  0.24732557]. \t  -26.05573310399278 \t -4.362282476422166\n",
            "47     \t [-0.79910273 -1.8100706 ]. \t  -17.19304796036736 \t -4.362282476422166\n",
            "48     \t [-2.35394569  2.66292548]. \t  -43.910998160818956 \t -4.362282476422166\n",
            "49     \t [1.12433365 1.84092706]. \t  -12.145169818595628 \t -4.362282476422166\n",
            "50     \t [-3.03879166  1.15373565]. \t  -15.174567273500273 \t -4.362282476422166\n",
            "51     \t [-2.64638939  1.67804596]. \t  -40.24763401107922 \t -4.362282476422166\n",
            "52     \t [-4.65749332  1.82477356]. \t  -45.98539839370977 \t -4.362282476422166\n",
            "53     \t [-0.1905334   0.86388571]. \t  -10.572098472262521 \t -4.362282476422166\n",
            "54     \t [-4.07665624  2.71919669]. \t  -37.074186064676375 \t -4.362282476422166\n",
            "55     \t [2.25482429 1.28052614]. \t  -28.933328296539464 \t -4.362282476422166\n",
            "56     \t [1.20595208 1.15127109]. \t  -14.234276123529979 \t -4.362282476422166\n",
            "57     \t [2.98087003 1.97735643]. \t  -12.968712903941343 \t -4.362282476422166\n",
            "58     \t [-0.90244588  0.95072071]. \t  \u001b[92m-4.0142758065585635\u001b[0m \t -4.0142758065585635\n",
            "59     \t [ 0.6891774  -0.42012123]. \t  -33.14745330833959 \t -4.0142758065585635\n",
            "60     \t [-1.31631713  2.90081155]. \t  -26.074655254530064 \t -4.0142758065585635\n",
            "61     \t [ 0.45809932 -0.01098702]. \t  -19.88923655069607 \t -4.0142758065585635\n",
            "62     \t [-2.59983105  0.48045815]. \t  -45.01108017358964 \t -4.0142758065585635\n",
            "63     \t [ 1.94867507 -1.22095957]. \t  -13.989009841283421 \t -4.0142758065585635\n",
            "64     \t [-4.04519081  1.69581928]. \t  -32.978687239479 \t -4.0142758065585635\n",
            "65     \t [-4.06224489 -1.01063426]. \t  -18.300611844578466 \t -4.0142758065585635\n",
            "66     \t [ 2.26489487 -0.37139788]. \t  -33.1114244995496 \t -4.0142758065585635\n",
            "67     \t [ 1.03165717 -2.07117998]. \t  -6.534818599736379 \t -4.0142758065585635\n",
            "68     \t [ 4.18252039 -1.25463199]. \t  -35.24459752130129 \t -4.0142758065585635\n",
            "69     \t [3.28307137 3.23510763]. \t  -42.373143166884134 \t -4.0142758065585635\n",
            "70     \t [-4.38110552  4.98512231]. \t  -61.42624995946038 \t -4.0142758065585635\n",
            "71     \t [-4.08092815  2.14483216]. \t  -26.382056361621764 \t -4.0142758065585635\n",
            "72     \t [-0.75460736 -2.89528711]. \t  -20.750076926419933 \t -4.0142758065585635\n",
            "73     \t [ 0.87611428 -1.24477082]. \t  -14.868130088792357 \t -4.0142758065585635\n",
            "74     \t [-5.29344937  1.01473441]. \t  -41.7893357234466 \t -4.0142758065585635\n",
            "75     \t [0.29375482 1.11775748]. \t  -16.664956406964087 \t -4.0142758065585635\n",
            "76     \t [ 1.09316843 -3.40411007]. \t  -32.68728820223707 \t -4.0142758065585635\n",
            "77     \t [0.89319985 4.85091956]. \t  -30.57300710565737 \t -4.0142758065585635\n",
            "78     \t [-1.73483374 -0.97666691]. \t  -15.022283745008545 \t -4.0142758065585635\n",
            "79     \t [-0.57772185  4.63361049]. \t  -57.31358817196055 \t -4.0142758065585635\n",
            "80     \t [ 0.52144192 -2.63880981]. \t  -43.5762855125435 \t -4.0142758065585635\n",
            "81     \t [-3.58252974  3.13728649]. \t  -44.85779954369681 \t -4.0142758065585635\n",
            "82     \t [ 1.49780399 -2.50407221]. \t  -48.509569369523646 \t -4.0142758065585635\n",
            "83     \t [ 2.28265477 -3.85741063]. \t  -35.87948024103331 \t -4.0142758065585635\n",
            "84     \t [ 3.55319355 -2.50398996]. \t  -58.33865625165685 \t -4.0142758065585635\n",
            "85     \t [-1.65813855 -2.74305595]. \t  -36.16659858220744 \t -4.0142758065585635\n",
            "86     \t [-3.46129243 -2.05773169]. \t  -36.57122749109447 \t -4.0142758065585635\n",
            "87     \t [ 1.54179738 -0.64751203]. \t  -38.45713814642614 \t -4.0142758065585635\n",
            "88     \t [ 3.35439964 -1.5481618 ]. \t  -49.29364391445099 \t -4.0142758065585635\n",
            "89     \t [ 5.06595094 -0.74754323]. \t  -37.22338395381143 \t -4.0142758065585635\n",
            "90     \t [ 2.67303247 -1.90268039]. \t  -27.22699048021316 \t -4.0142758065585635\n",
            "91     \t [-1.59955393  2.80523623]. \t  -35.133194636515306 \t -4.0142758065585635\n",
            "92     \t [-2.22269381  4.82647827]. \t  -41.90549955899665 \t -4.0142758065585635\n",
            "93     \t [-3.59740196  0.39547794]. \t  -49.202661464621485 \t -4.0142758065585635\n",
            "94     \t [-0.81587244  3.80989926]. \t  -27.483893224502093 \t -4.0142758065585635\n",
            "95     \t [-3.12955289 -1.48068532]. \t  -35.047059353035706 \t -4.0142758065585635\n",
            "96     \t [-0.90779721  2.84589062]. \t  -14.887848862174607 \t -4.0142758065585635\n",
            "97     \t [ 2.49739226 -3.36989734]. \t  -54.43260134616971 \t -4.0142758065585635\n",
            "98     \t [ 1.69783696 -4.59305536]. \t  -55.53680933199829 \t -4.0142758065585635\n",
            "99     \t [ 1.5997829  -4.10470303]. \t  -39.59309983935347 \t -4.0142758065585635\n",
            "100    \t [ 2.71753159 -5.31115016]. \t  -61.367563456622094 \t -4.0142758065585635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq4J_FTytd4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fceb1d94-2de8-4152-dd84-dea01e6cb64f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_15 = d2GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.79751023 -2.50366638]. \t  -33.96069680534386 \t -17.757681446799715\n",
            "init   \t [-2.53412486  2.14942059]. \t  -34.905546618784385 \t -17.757681446799715\n",
            "init   \t [-0.5370638  -2.79610411]. \t  -34.98023435189899 \t -17.757681446799715\n",
            "init   \t [-1.00649405  3.91431921]. \t  -17.757681446799715 \t -17.757681446799715\n",
            "init   \t [-0.6429341   3.87507385]. \t  -34.58633031923943 \t -17.757681446799715\n",
            "1      \t [-2.10831682  4.48384208]. \t  -46.72623633154846 \t -17.757681446799715\n",
            "2      \t [-3.3999092  -8.21527273]. \t  -104.97219783418127 \t -17.757681446799715\n",
            "3      \t [-9.23540724 -5.01218957]. \t  -119.5285003590765 \t -17.757681446799715\n",
            "4      \t [ -7.91543488 -12.4812412 ]. \t  -239.7448132898009 \t -17.757681446799715\n",
            "5      \t [-14.34502575 -12.26750461]. \t  -382.99123594192724 \t -17.757681446799715\n",
            "6      \t [-8.09770188  0.89402161]. \t  -70.33413019096285 \t -17.757681446799715\n",
            "7      \t [5.11979655 2.50844585]. \t  -55.19210934805591 \t -17.757681446799715\n",
            "8      \t [ 5.11237913 -5.01684246]. \t  -53.75208047948594 \t -17.757681446799715\n",
            "9      \t [-5.01015981 -2.74871842]. \t  -42.75804594833463 \t -17.757681446799715\n",
            "10     \t [ 0.87125488 -6.84162121]. \t  -55.22013675471511 \t -17.757681446799715\n",
            "11     \t [ 4.43213451 -1.2917404 ]. \t  -53.00963327740921 \t -17.757681446799715\n",
            "12     \t [-7.3334992 -8.3328875]. \t  -153.20196775563318 \t -17.757681446799715\n",
            "13     \t [-19.81538326 -27.63614074]. \t  -1178.9713061747823 \t -17.757681446799715\n",
            "14     \t [-11.59048634 -19.6033453 ]. \t  -555.0222313428177 \t -17.757681446799715\n",
            "15     \t [1.48869533 0.58187119]. \t  -41.23539519204936 \t -17.757681446799715\n",
            "16     \t [2.65729099 4.99708714]. \t  -47.53494992046141 \t -17.757681446799715\n",
            "17     \t [-6.02563628  3.8822765 ]. \t  -54.122960449454254 \t -17.757681446799715\n",
            "18     \t [-20.75658398 -13.25456949]. \t  -626.3928955968264 \t -17.757681446799715\n",
            "19     \t [-2.51734073 -4.92086625]. \t  -51.703473918009095 \t -17.757681446799715\n",
            "20     \t [-4.98018364  0.30784988]. \t  -38.52971868264045 \t -17.757681446799715\n",
            "21     \t [-20.18493467 -20.41823698]. \t  -849.0699083494328 \t -17.757681446799715\n",
            "22     \t [-2.09603067 -0.59587729]. \t  -24.753878618676392 \t -17.757681446799715\n",
            "23     \t [ 2.60315126 -3.67300607]. \t  -52.89075400676825 \t -17.757681446799715\n",
            "24     \t [-5.19401752 -5.810895  ]. \t  -73.56545034431247 \t -17.757681446799715\n",
            "25     \t [-1.28800238  2.70991077]. \t  -33.86003048227436 \t -17.757681446799715\n",
            "26     \t [-3.13296075 -2.45959134]. \t  -38.835761991659844 \t -17.757681446799715\n",
            "27     \t [-0.50997447 -0.38750829]. \t  -37.995002368744835 \t -17.757681446799715\n",
            "28     \t [4.95191215 4.90481579]. \t  -50.7673531069652 \t -17.757681446799715\n",
            "29     \t [2.58913908 2.36532213]. \t  -47.39881721148949 \t -17.757681446799715\n",
            "30     \t [ 0.43053373 -5.04165254]. \t  -45.00662978667103 \t -17.757681446799715\n",
            "31     \t [-4.55104548  5.08508556]. \t  -67.4554883965343 \t -17.757681446799715\n",
            "32     \t [-2.6124703   0.27669228]. \t  -36.17610560482981 \t -17.757681446799715\n",
            "33     \t [-4.42137341  2.46422319]. \t  -64.17365532376387 \t -17.757681446799715\n",
            "34     \t [-1.57666272  3.88281623]. \t  -39.01460910239565 \t -17.757681446799715\n",
            "35     \t [-0.67923979  1.27160389]. \t  -27.732554226317983 \t -17.757681446799715\n",
            "36     \t [-0.56643388  4.73016086]. \t  -53.07996753439533 \t -17.757681446799715\n",
            "37     \t [ 2.26100693 -0.56685399]. \t  -35.25517135038786 \t -17.757681446799715\n",
            "38     \t [5.11040982 0.71913938]. \t  -40.871658157172675 \t -17.757681446799715\n",
            "39     \t [-2.25599917 -1.52772325]. \t  -37.648991881695814 \t -17.757681446799715\n",
            "40     \t [-4.83945358 -1.30635495]. \t  -43.26498232474389 \t -17.757681446799715\n",
            "41     \t [ 4.21870377 -2.73320742]. \t  -44.367284495700986 \t -17.757681446799715\n",
            "42     \t [-1.6990149   1.58861522]. \t  -37.048907206632464 \t -17.757681446799715\n",
            "43     \t [ 1.90135235 -1.52979449]. \t  -27.640874069565143 \t -17.757681446799715\n",
            "44     \t [0.43537373 1.79009595]. \t  -30.08810288205545 \t -17.757681446799715\n",
            "45     \t [-1.10166222  4.12846193]. \t  -23.313918748411467 \t -17.757681446799715\n",
            "46     \t [3.39730872 0.36982577]. \t  -46.50558890073003 \t -17.757681446799715\n",
            "47     \t [ 2.95944052 -4.93339991]. \t  -34.28253806762521 \t -17.757681446799715\n",
            "48     \t [-3.95413496 -3.70839748]. \t  -42.38406076912866 \t -17.757681446799715\n",
            "49     \t [-0.92878761  3.00547876]. \t  \u001b[92m-10.885899560321189\u001b[0m \t -10.885899560321189\n",
            "50     \t [-2.58230244  5.00647593]. \t  -50.43382265843355 \t -10.885899560321189\n",
            "51     \t [-1.02185005 -4.48918304]. \t  -41.26794654614331 \t -10.885899560321189\n",
            "52     \t [ 5.06098649 -2.38545181]. \t  -49.54904972749267 \t -10.885899560321189\n",
            "53     \t [-1.24380669  0.48516928]. \t  -31.35002045310994 \t -10.885899560321189\n",
            "54     \t [ 0.75251772 -1.30033344]. \t  -25.209051618414247 \t -10.885899560321189\n",
            "55     \t [3.7974717  3.93922483]. \t  -37.71985467841564 \t -10.885899560321189\n",
            "56     \t [-1.80029257 -3.5035793 ]. \t  -42.405945034313845 \t -10.885899560321189\n",
            "57     \t [-2.88200936  1.24576171]. \t  -22.21610349869499 \t -10.885899560321189\n",
            "58     \t [0.03804475 0.87983816]. \t  \u001b[92m-3.7771892521888297\u001b[0m \t -3.7771892521888297\n",
            "59     \t [-2.24487889 -0.07204167]. \t  -15.730050225754772 \t -3.7771892521888297\n",
            "60     \t [0.34725125 0.17317447]. \t  -21.246027013135006 \t -3.7771892521888297\n",
            "61     \t [1.24190452 0.7663755 ]. \t  -20.59413485974628 \t -3.7771892521888297\n",
            "62     \t [0.10561413 1.12405611]. \t  -6.283936473619456 \t -3.7771892521888297\n",
            "63     \t [1.33621956 1.25483087]. \t  -28.819789367241356 \t -3.7771892521888297\n",
            "64     \t [0.75042621 0.72394408]. \t  -22.69029354844374 \t -3.7771892521888297\n",
            "65     \t [ 3.34188986 -0.70057125]. \t  -40.17318816371845 \t -3.7771892521888297\n",
            "66     \t [3.69559346 2.90506801]. \t  -37.17589646321909 \t -3.7771892521888297\n",
            "67     \t [-0.41974762  0.43584182]. \t  -38.31998686441917 \t -3.7771892521888297\n",
            "68     \t [-0.59391593 -1.50359233]. \t  -40.919878099609036 \t -3.7771892521888297\n",
            "69     \t [0.35601032 2.71864517]. \t  -35.65424355129469 \t -3.7771892521888297\n",
            "70     \t [ 0.40462474 -3.50237587]. \t  -50.68677367148375 \t -3.7771892521888297\n",
            "71     \t [-2.48479603  0.64449524]. \t  -42.69809048253774 \t -3.7771892521888297\n",
            "72     \t [-4.07176475 -0.14926759]. \t  -21.686025099860135 \t -3.7771892521888297\n",
            "73     \t [-0.13864968 -0.91940757]. \t  -5.680077739156177 \t -3.7771892521888297\n",
            "74     \t [-3.85464016  1.30086477]. \t  -33.581109779205946 \t -3.7771892521888297\n",
            "75     \t [3.85383004 1.2558556 ]. \t  -30.72619430629283 \t -3.7771892521888297\n",
            "76     \t [2.99041285 2.7743679 ]. \t  -25.13271866762891 \t -3.7771892521888297\n",
            "77     \t [ 0.11388115 -0.8078193 ]. \t  -9.564617747855781 \t -3.7771892521888297\n",
            "78     \t [ 2.30643313 -1.07386469]. \t  -21.00255943868203 \t -3.7771892521888297\n",
            "79     \t [-3.52936764 -1.58352194]. \t  -53.44854039823429 \t -3.7771892521888297\n",
            "80     \t [-3.48770064  1.63900854]. \t  -51.24267435261571 \t -3.7771892521888297\n",
            "81     \t [-4.88612932  3.08870497]. \t  -37.37991282000393 \t -3.7771892521888297\n",
            "82     \t [4.10189453 0.35710215]. \t  -35.166328697101164 \t -3.7771892521888297\n",
            "83     \t [-3.87540795  0.67836114]. \t  -32.74052918931277 \t -3.7771892521888297\n",
            "84     \t [-2.63541228 -2.41370413]. \t  -47.930719510078404 \t -3.7771892521888297\n",
            "85     \t [-0.13501879  2.35061951]. \t  -24.840707793643254 \t -3.7771892521888297\n",
            "86     \t [-3.00540979 -0.16589429]. \t  -14.023815984905813 \t -3.7771892521888297\n",
            "87     \t [-4.74832494  1.28775923]. \t  -46.66044747577178 \t -3.7771892521888297\n",
            "88     \t [-5.2000889  -0.83227437]. \t  -39.7064804352374 \t -3.7771892521888297\n",
            "89     \t [-0.98586723 -3.48747193]. \t  -33.14282975664884 \t -3.7771892521888297\n",
            "90     \t [-1.04147052  3.26365753]. \t  -22.9307554652179 \t -3.7771892521888297\n",
            "91     \t [1.41952016 3.35122009]. \t  -47.93391456960912 \t -3.7771892521888297\n",
            "92     \t [-3.12323772  4.10533778]. \t  -31.570958545101952 \t -3.7771892521888297\n",
            "93     \t [-4.62546041  3.24486371]. \t  -58.65194077657857 \t -3.7771892521888297\n",
            "94     \t [3.8289863  2.21501873]. \t  -32.625534813037504 \t -3.7771892521888297\n",
            "95     \t [-0.8285493   2.53536544]. \t  -32.13124263389663 \t -3.7771892521888297\n",
            "96     \t [2.416405   3.00771041]. \t  -33.54908981656563 \t -3.7771892521888297\n",
            "97     \t [ 1.57756735 -1.37797946]. \t  -40.425416602215705 \t -3.7771892521888297\n",
            "98     \t [1.71676611 4.95108503]. \t  -40.00212286491416 \t -3.7771892521888297\n",
            "99     \t [-2.59684716  4.19978118]. \t  -49.48354871618849 \t -3.7771892521888297\n",
            "100    \t [0.33488205 4.89449802]. \t  -41.27011081732528 \t -3.7771892521888297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tJ-ta9Gtd7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22462683-2960-4d65-ed09-af1ffa686621"
      },
      "source": [
        "\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_16 = d2GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.72683092 -1.88041214]. \t  -31.568449729559287 \t -13.57707882297342\n",
            "init   \t [1.76282397 0.08101548]. \t  -13.57707882297342 \t -13.57707882297342\n",
            "init   \t [ 2.88420277 -2.21509961]. \t  -23.5820775563595 \t -13.57707882297342\n",
            "init   \t [-2.72682205  0.65330879]. \t  -35.02194386260499 \t -13.57707882297342\n",
            "init   \t [3.84024949 2.2419755 ]. \t  -33.89849286588301 \t -13.57707882297342\n",
            "1      \t [-6.22536627 -8.05682945]. \t  -112.75682595020012 \t -13.57707882297342\n",
            "2      \t [-23.60427671 -24.41516907]. \t  -1189.8045049749355 \t -13.57707882297342\n",
            "3      \t [-20.13155145 -18.58131044]. \t  -772.493478957798 \t -13.57707882297342\n",
            "4      \t [-52.50959068 -53.17605433]. \t  -5610.45093526606 \t -13.57707882297342\n",
            "5      \t [ -9.89878514 -13.35717858]. \t  -294.59175147682885 \t -13.57707882297342\n",
            "6      \t [-43.13615288 -43.64596277]. \t  -3785.2203025589133 \t -13.57707882297342\n",
            "7      \t [ 0.66456981 -7.62559917]. \t  -90.74947432758226 \t -13.57707882297342\n",
            "8      \t [-8.49398605 -1.22334726]. \t  -101.970414485394 \t -13.57707882297342\n",
            "9      \t [-62.691048   -63.94721623]. \t  -8033.578842194121 \t -13.57707882297342\n",
            "10     \t [-16.53370585 -11.41503893]. \t  -442.05179110524983 \t -13.57707882297342\n",
            "11     \t [-0.01617115  4.91315725]. \t  -25.64304447621189 \t -13.57707882297342\n",
            "12     \t [-2.76069196 -4.10588006]. \t  -35.94084241286666 \t -13.57707882297342\n",
            "13     \t [-68.4857345  -69.15024877]. \t  -9496.147392445482 \t -13.57707882297342\n",
            "14     \t [-4.38628948  4.82362861]. \t  -65.598410042651 \t -13.57707882297342\n",
            "15     \t [-14.50686479 -17.00510475]. \t  -519.6185559186094 \t -13.57707882297342\n",
            "16     \t [ 3.69755028 -5.22282347]. \t  -62.48668237508404 \t -13.57707882297342\n",
            "17     \t [-10.49526715  -8.96135393]. \t  -210.74543864551717 \t -13.57707882297342\n",
            "18     \t [-6.75955986 -4.31105386]. \t  -87.41926296155458 \t -13.57707882297342\n",
            "19     \t [-29.35043    -27.86385391]. \t  -1657.1828534492083 \t -13.57707882297342\n",
            "20     \t [-110.59882207 -112.57445116]. \t  -24942.165616254482 \t -13.57707882297342\n",
            "21     \t [ 0.32749794 -2.98635558]. \t  -23.741484507662776 \t -13.57707882297342\n",
            "22     \t [-6.06665359  1.59884294]. \t  -58.35748377614332 \t -13.57707882297342\n",
            "23     \t [ -5.8170929  -11.80854068]. \t  -185.59254612583092 \t -13.57707882297342\n",
            "24     \t [-2.76211124 -8.17535642]. \t  -89.18553338647936 \t -13.57707882297342\n",
            "25     \t [3.11654508 4.9890874 ]. \t  -37.190785375066284 \t -13.57707882297342\n",
            "26     \t [-0.03199167  2.30062131]. \t  -18.622500421053928 \t -13.57707882297342\n",
            "27     \t [-4.78246236 -1.67659687]. \t  -48.10764598258501 \t -13.57707882297342\n",
            "28     \t [-0.45817762 -0.78885366]. \t  -28.0718673097315 \t -13.57707882297342\n",
            "29     \t [-2.11511124  3.44094858]. \t  -38.136876145399654 \t -13.57707882297342\n",
            "30     \t [ 0.11279215 -5.09309846]. \t  -30.022878572566334 \t -13.57707882297342\n",
            "31     \t [4.92506551 0.8064949 ]. \t  -32.5191682249875 \t -13.57707882297342\n",
            "32     \t [1.53697722 2.25819275]. \t  -37.70758655141729 \t -13.57707882297342\n",
            "33     \t [ 2.02919011 -3.23337167]. \t  -23.697135271982404 \t -13.57707882297342\n",
            "34     \t [-2.19625773 -1.61584633]. \t  -31.58742505443736 \t -13.57707882297342\n",
            "35     \t [ 4.97782851 -2.99805793]. \t  -33.864748714470984 \t -13.57707882297342\n",
            "36     \t [4.88295739 5.04367539]. \t  -52.24048877215135 \t -13.57707882297342\n",
            "37     \t [ 2.08183019 -0.55137001]. \t  -25.414549185524027 \t -13.57707882297342\n",
            "38     \t [0.24975774 0.77684118]. \t  -18.972142164739857 \t -13.57707882297342\n",
            "39     \t [2.13603219 0.6618207 ]. \t  -23.69763812177434 \t -13.57707882297342\n",
            "40     \t [-3.69540135 -5.11711152]. \t  -55.79174685298728 \t -13.57707882297342\n",
            "41     \t [1.00891977 0.30840258]. \t  -14.71647288284973 \t -13.57707882297342\n",
            "42     \t [-0.39521037  1.07059871]. \t  -20.179827907364242 \t -13.57707882297342\n",
            "43     \t [-3.71852968  2.59406078]. \t  -50.82492462206836 \t -13.57707882297342\n",
            "44     \t [1.4329138  4.49041145]. \t  -61.323595461040824 \t -13.57707882297342\n",
            "45     \t [-1.35045063 -3.15892211]. \t  -32.287913466583845 \t -13.57707882297342\n",
            "46     \t [ 1.50730712 -0.01618137]. \t  -22.313338910271415 \t -13.57707882297342\n",
            "47     \t [ 2.51278097 -0.49492123]. \t  -46.52169683679014 \t -13.57707882297342\n",
            "48     \t [1.20183049 0.37718845]. \t  -25.773710161148955 \t -13.57707882297342\n",
            "49     \t [ 0.42296735 -0.17887973]. \t  -24.740881735513174 \t -13.57707882297342\n",
            "50     \t [-1.25610048  2.06316782]. \t  -16.99500421369448 \t -13.57707882297342\n",
            "51     \t [-1.1877591  -1.22494832]. \t  -17.53192958130713 \t -13.57707882297342\n",
            "52     \t [1.66938854 2.95361698]. \t  -26.7835486282885 \t -13.57707882297342\n",
            "53     \t [-1.63063931  0.97064787]. \t  -20.586857072032533 \t -13.57707882297342\n",
            "54     \t [ 3.63854589 -2.59486092]. \t  -54.692404378613375 \t -13.57707882297342\n",
            "55     \t [2.68358697 0.57421587]. \t  -40.516489909108856 \t -13.57707882297342\n",
            "56     \t [ 5.00171531 -1.70340106]. \t  -40.80555568380379 \t -13.57707882297342\n",
            "57     \t [-0.93008847  2.34228616]. \t  -22.779726467995346 \t -13.57707882297342\n",
            "58     \t [-1.07444666  1.22633195]. \t  \u001b[92m-12.250898231838624\u001b[0m \t -12.250898231838624\n",
            "59     \t [14.51961253 15.21612986]. \t  -470.1618281225121 \t -12.250898231838624\n",
            "60     \t [5.06253987 2.29220033]. \t  -44.26622037937556 \t -12.250898231838624\n",
            "61     \t [0.98288395 0.18190441]. \t  \u001b[92m-6.907700958454681\u001b[0m \t -6.907700958454681\n",
            "62     \t [ 2.4595862  -1.97434122]. \t  -29.756596962819344 \t -6.907700958454681\n",
            "63     \t [-3.30864704 -1.37649941]. \t  -43.58134028979129 \t -6.907700958454681\n",
            "64     \t [ 1.94533172 -4.17833093]. \t  -27.47447072686893 \t -6.907700958454681\n",
            "65     \t [-2.76289025 -2.40542213]. \t  -40.8962701364764 \t -6.907700958454681\n",
            "66     \t [-0.8345366  -1.31022349]. \t  -21.042104362243542 \t -6.907700958454681\n",
            "67     \t [-0.96188337  3.42550163]. \t  -31.869029449274798 \t -6.907700958454681\n",
            "68     \t [-1.45843681 -0.56705524]. \t  -41.235025546873885 \t -6.907700958454681\n",
            "69     \t [-1.24617553 -4.01927995]. \t  -27.540573409662905 \t -6.907700958454681\n",
            "70     \t [ 0.38412478 -3.53642734]. \t  -49.8577128732518 \t -6.907700958454681\n",
            "71     \t [-3.79111673e+00 -3.12305775e-03]. \t  -21.819702140407223 \t -6.907700958454681\n",
            "72     \t [1.02054722 2.45521676]. \t  -26.75955463994568 \t -6.907700958454681\n",
            "73     \t [-1.20602576  1.56613317]. \t  -30.328346068036396 \t -6.907700958454681\n",
            "74     \t [-3.56988486  0.80608485]. \t  -38.99376596263791 \t -6.907700958454681\n",
            "75     \t [-2.03126752  2.90382785]. \t  -14.521440603343466 \t -6.907700958454681\n",
            "76     \t [-0.90764736  0.52490499]. \t  -22.61401100849489 \t -6.907700958454681\n",
            "77     \t [-1.68832815  5.02919278]. \t  -42.08969146228516 \t -6.907700958454681\n",
            "78     \t [2.19005309 2.75162543]. \t  -28.587504480705025 \t -6.907700958454681\n",
            "79     \t [-3.19783844 -0.20078746]. \t  -24.004361617236288 \t -6.907700958454681\n",
            "80     \t [-1.0524274   2.71797049]. \t  -21.031553459020287 \t -6.907700958454681\n",
            "81     \t [-0.85147239  4.06003433]. \t  -21.959469130634723 \t -6.907700958454681\n",
            "82     \t [0.407568   1.79100399]. \t  -29.186276031844535 \t -6.907700958454681\n",
            "83     \t [-0.06638681  4.12826073]. \t  -20.979627767191353 \t -6.907700958454681\n",
            "84     \t [-0.90996003 -1.07660926]. \t  \u001b[92m-4.681475015363116\u001b[0m \t -4.681475015363116\n",
            "85     \t [-1.84256185 -4.97113991]. \t  -32.77774927350987 \t -4.681475015363116\n",
            "86     \t [0.45214302 5.09920189]. \t  -47.63806039152645 \t -4.681475015363116\n",
            "87     \t [-1.62309577  0.62806457]. \t  -37.11767736671284 \t -4.681475015363116\n",
            "88     \t [1.99414633 0.14489252]. \t  -7.869971122768624 \t -4.681475015363116\n",
            "89     \t [-1.60383392  2.2604918 ]. \t  -36.28709150326225 \t -4.681475015363116\n",
            "90     \t [-2.18426503 -1.26833681]. \t  -23.515462665265957 \t -4.681475015363116\n",
            "91     \t [-1.64339808  2.9562279 ]. \t  -28.02415421759113 \t -4.681475015363116\n",
            "92     \t [-1.07092371  5.28048141]. \t  -41.91047177628871 \t -4.681475015363116\n",
            "93     \t [-2.55966982  3.12650613]. \t  -38.62849557225446 \t -4.681475015363116\n",
            "94     \t [-0.44605068  3.11317664]. \t  -31.74541486824379 \t -4.681475015363116\n",
            "95     \t [-4.36998246  2.00020763]. \t  -39.94225319943292 \t -4.681475015363116\n",
            "96     \t [-5.52584549  4.27649299]. \t  -80.34872094273399 \t -4.681475015363116\n",
            "97     \t [4.16426154 1.62777575]. \t  -41.80710637273145 \t -4.681475015363116\n",
            "98     \t [-0.49260777  3.82283882]. \t  -40.42747505421416 \t -4.681475015363116\n",
            "99     \t [0.75639356 3.79247128]. \t  -31.916368549056987 \t -4.681475015363116\n",
            "100    \t [0.11279996 2.58218952]. \t  -27.784584668689014 \t -4.681475015363116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqjDLo0otd9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a37798f-1d74-44e3-b02b-614f18d060c8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_17 = d2GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.55467805 2.36052095]. \t  -43.80415029879179 \t -20.520783967472433\n",
            "init   \t [-3.13991821  0.20149492]. \t  -20.520783967472433 \t -20.520783967472433\n",
            "init   \t [1.80616465 3.1175155 ]. \t  -22.129339239545118 \t -20.520783967472433\n",
            "init   \t [0.34317173 4.17827   ]. \t  -38.745269185045146 \t -20.520783967472433\n",
            "init   \t [-0.57875324 -2.49884457]. \t  -45.3794522559586 \t -20.520783967472433\n",
            "1      \t [3.48763249 4.56729682]. \t  -72.11288244713099 \t -20.520783967472433\n",
            "2      \t [-19.77509708 -16.01821163]. \t  -656.1325957703277 \t -20.520783967472433\n",
            "3      \t [-78.03027428 -74.56258695]. \t  -11667.720151349366 \t -20.520783967472433\n",
            "4      \t [-9.25757537 -5.58100091]. \t  -146.05866447857906 \t -20.520783967472433\n",
            "5      \t [-157.62751212 -154.76617486]. \t  -48824.9454909276 \t -20.520783967472433\n",
            "6      \t [-11.88274148 -12.89763774]. \t  -312.140050693259 \t -20.520783967472433\n",
            "7      \t [-2.24129565 -8.31229752]. \t  -97.38613594871501 \t -20.520783967472433\n",
            "8      \t [ 4.93712098 -1.29499149]. \t  -39.61190870147234 \t -20.520783967472433\n",
            "9      \t [ 2.9387719  -6.44024897]. \t  -70.1476168397158 \t -20.520783967472433\n",
            "10     \t [-23.99629736 -21.18652745]. \t  -1030.8107201659038 \t -20.520783967472433\n",
            "11     \t [-592.56542854 -589.09865554]. \t  -698192.040340903 \t -20.520783967472433\n",
            "12     \t [-31.80790892 -27.19112078]. \t  -1763.9256690031314 \t -20.520783967472433\n",
            "13     \t [-7.34330488 -0.95593401]. \t  -70.75121835303221 \t -20.520783967472433\n",
            "14     \t [-62.76080682 -59.48862098]. \t  -7507.110860768276 \t -20.520783967472433\n",
            "15     \t [-36.31584356 -32.10456273]. \t  -2365.6451487793292 \t -20.520783967472433\n",
            "16     \t [ -6.27365178 -12.09225901]. \t  -198.69567905988464 \t -20.520783967472433\n",
            "17     \t [-4.32324557  4.16008425]. \t  -55.084367266236804 \t -20.520783967472433\n",
            "18     \t [-4.56961486 -4.30244271]. \t  -71.6867091980846 \t -20.520783967472433\n",
            "19     \t [-118.6871718  -114.26611846]. \t  -27168.24747866106 \t -20.520783967472433\n",
            "20     \t [-58.01758807 -54.64295206]. \t  -6368.183968729518 \t -20.520783967472433\n",
            "21     \t [ 2.56689034 -3.13851565]. \t  -39.123112888004066 \t -20.520783967472433\n",
            "22     \t [-12.80823188  -8.4367526 ]. \t  -260.8725933609825 \t -20.520783967472433\n",
            "23     \t [-130.47542844 -127.83675385]. \t  -33390.76916380633 \t -20.520783967472433\n",
            "24     \t [-6.09767291 -8.0998195 ]. \t  -106.51661483034243 \t -20.520783967472433\n",
            "25     \t [-28.2506433  -30.26265075]. \t  -1734.7613291335215 \t -20.520783967472433\n",
            "26     \t [-52.39983558 -48.22392769]. \t  -5097.743210125842 \t -20.520783967472433\n",
            "27     \t [-27.92241912 -24.26530836]. \t  -1380.5917725483516 \t -20.520783967472433\n",
            "28     \t [-0.83675371 -5.50687382]. \t  -55.83154343284261 \t -20.520783967472433\n",
            "29     \t [4.50141889 1.83960707]. \t  -48.30912252314337 \t -20.520783967472433\n",
            "30     \t [-16.25299782 -18.49403328]. \t  -636.3705257962334 \t -20.520783967472433\n",
            "31     \t [-5.31483887  1.46570077]. \t  -64.12665458209577 \t -20.520783967472433\n",
            "32     \t [-1.87702528  2.46065909]. \t  -32.113640698562534 \t -20.520783967472433\n",
            "33     \t [-48.67876696 -45.63292289]. \t  -4483.024217855954 \t -20.520783967472433\n",
            "34     \t [ 1.6411078  -0.59103435]. \t  -37.771288614716326 \t -20.520783967472433\n",
            "35     \t [-16.24235687 -13.23577386]. \t  -457.62715365778854 \t -20.520783967472433\n",
            "36     \t [ 4.93658339 -4.03035236]. \t  -41.578293275289994 \t -20.520783967472433\n",
            "37     \t [-0.97640458  0.03697702]. \t  \u001b[92m-1.3331112226931907\u001b[0m \t -1.3331112226931907\n",
            "38     \t [-0.88315823 -0.08344723]. \t  -4.706176189751217 \t -1.3331112226931907\n",
            "39     \t [-1.07886758  0.49602651]. \t  -22.609758870035456 \t -1.3331112226931907\n",
            "40     \t [-1.44143932 -0.77089033]. \t  -30.693882631749922 \t -1.3331112226931907\n",
            "41     \t [-1.27148043 -0.15876385]. \t  -17.56374594930257 \t -1.3331112226931907\n",
            "42     \t [-2.51162793 -1.68637623]. \t  -43.017434708882135 \t -1.3331112226931907\n",
            "43     \t [-3.84180628 -0.03680538]. \t  -19.57328991720319 \t -1.3331112226931907\n",
            "44     \t [-0.41665514  0.27422333]. \t  -30.424819440987555 \t -1.3331112226931907\n",
            "45     \t [-2.28172636  3.20549904]. \t  -34.70196240002146 \t -1.3331112226931907\n",
            "46     \t [ 3.44138101 -0.20903745]. \t  -38.67072496035995 \t -1.3331112226931907\n",
            "47     \t [-3.30070608  2.14192175]. \t  -32.334081069119904 \t -1.3331112226931907\n",
            "48     \t [-2.6702055  -4.35324389]. \t  -56.92846168339686 \t -1.3331112226931907\n",
            "49     \t [-3.49020568  0.48701561]. \t  -52.366529535857666 \t -1.3331112226931907\n",
            "50     \t [-2.90458378 -0.96941637]. \t  -11.304362085530354 \t -1.3331112226931907\n",
            "51     \t [-4.73544208 -0.99959525]. \t  -34.33706069961261 \t -1.3331112226931907\n",
            "52     \t [-3.51240325 -0.43572298]. \t  -51.69197207362491 \t -1.3331112226931907\n",
            "53     \t [-4.62462021 -0.50803934]. \t  -58.72038246394257 \t -1.3331112226931907\n",
            "54     \t [-4.89378477 -2.33906771]. \t  -46.874563273925105 \t -1.3331112226931907\n",
            "55     \t [-2.2757455  -0.34238255]. \t  -32.39089384568675 \t -1.3331112226931907\n",
            "56     \t [-3.29865374  2.62068269]. \t  -48.01896254831594 \t -1.3331112226931907\n",
            "57     \t [-5.53347741 -1.64545677]. \t  -69.212858201842 \t -1.3331112226931907\n",
            "58     \t [-4.71378476 -1.95273105]. \t  -38.72662156947806 \t -1.3331112226931907\n",
            "59     \t [-1.61201884  4.30292457]. \t  -52.00182484338636 \t -1.3331112226931907\n",
            "60     \t [-2.35958015 -0.8378661 ]. \t  -27.378945306408234 \t -1.3331112226931907\n",
            "61     \t [-4.43446054  5.08197854]. \t  -65.9524761174612 \t -1.3331112226931907\n",
            "62     \t [-0.8344977  -1.60493381]. \t  -26.11307387128387 \t -1.3331112226931907\n",
            "63     \t [ 0.44811329 -3.60392506]. \t  -50.60511204239535 \t -1.3331112226931907\n",
            "64     \t [ 2.65398208 -1.61383487]. \t  -42.871022813529635 \t -1.3331112226931907\n",
            "65     \t [-5.05682434  3.52681025]. \t  -58.49895286509921 \t -1.3331112226931907\n",
            "66     \t [1.14679802 4.93142652]. \t  -30.508628749167073 \t -1.3331112226931907\n",
            "67     \t [5.0642675  4.86211506]. \t  -53.61517052296803 \t -1.3331112226931907\n",
            "68     \t [-2.94279832 -1.33464418]. \t  -26.15144571180936 \t -1.3331112226931907\n",
            "69     \t [-2.85208029 -0.33632552]. \t  -27.42630766369317 \t -1.3331112226931907\n",
            "70     \t [-1.96893297  1.13114771]. \t  -8.55337733217261 \t -1.3331112226931907\n",
            "71     \t [-1.81494999  0.38380596]. \t  -26.92404133199707 \t -1.3331112226931907\n",
            "72     \t [-2.56244     0.44632062]. \t  -45.442129130646805 \t -1.3331112226931907\n",
            "73     \t [-1.56511798 -2.14448463]. \t  -30.068383496020264 \t -1.3331112226931907\n",
            "74     \t [-1.96698629  2.02211639]. \t  -8.268754111925434 \t -1.3331112226931907\n",
            "75     \t [ 2.28203778 -0.16464629]. \t  -22.12469980804748 \t -1.3331112226931907\n",
            "76     \t [2.88469103 1.75253831]. \t  -23.745094441125115 \t -1.3331112226931907\n",
            "77     \t [-1.28412262  3.56402406]. \t  -45.680564981504496 \t -1.3331112226931907\n",
            "78     \t [ 2.33199047 -4.71177649]. \t  -54.944402332656566 \t -1.3331112226931907\n",
            "79     \t [5.07216501 0.08247151]. \t  -28.056942005407933 \t -1.3331112226931907\n",
            "80     \t [-2.31997667  2.30831313]. \t  -38.54956204928523 \t -1.3331112226931907\n",
            "81     \t [-2.40418173  3.89626705]. \t  -41.25269113064637 \t -1.3331112226931907\n",
            "82     \t [ 4.56583696 -2.88119898]. \t  -50.963660200495916 \t -1.3331112226931907\n",
            "83     \t [-0.73439757  0.04282693]. \t  -11.879800813398784 \t -1.3331112226931907\n",
            "84     \t [1.66266197 0.55244751]. \t  -37.74787019903171 \t -1.3331112226931907\n",
            "85     \t [1.10824083 0.73211842]. \t  -15.110249610074598 \t -1.3331112226931907\n",
            "86     \t [0.26716331 1.08204677]. \t  -13.61812573589112 \t -1.3331112226931907\n",
            "87     \t [-0.71410277 -3.41272579]. \t  -42.92686013914033 \t -1.3331112226931907\n",
            "88     \t [-3.82103488  2.39367604]. \t  -43.863746301604586 \t -1.3331112226931907\n",
            "89     \t [-1.84337336  3.3914956 ]. \t  -37.12894852435064 \t -1.3331112226931907\n",
            "90     \t [0.84514542 0.82875595]. \t  -11.023825584984506 \t -1.3331112226931907\n",
            "91     \t [2.75314372 2.59560823]. \t  -42.368734455930905 \t -1.3331112226931907\n",
            "92     \t [ 2.0782261  -0.54624399]. \t  -25.38202702763808 \t -1.3331112226931907\n",
            "93     \t [ 1.80265621 -0.10888366]. \t  -12.263314101857631 \t -1.3331112226931907\n",
            "94     \t [-0.85267427  4.84550929]. \t  -32.545787420137984 \t -1.3331112226931907\n",
            "95     \t [0.4426212  2.90715019]. \t  -29.658578790052992 \t -1.3331112226931907\n",
            "96     \t [-1.87060226  1.83938125]. \t  -14.684066984570146 \t -1.3331112226931907\n",
            "97     \t [ 0.75525634 -2.90086204]. \t  -20.53331950834022 \t -1.3331112226931907\n",
            "98     \t [-1.59318862 -3.04018096]. \t  -30.432193140378274 \t -1.3331112226931907\n",
            "99     \t [-0.13795074  3.46365998]. \t  -35.28350697817034 \t -1.3331112226931907\n",
            "100    \t [-2.09597837  1.60698145]. \t  -26.56405188608889 \t -1.3331112226931907\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOqNmgSIteAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af44494d-4725-4c01-d322-bdcac1603c11"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_18 = d2GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.82391708 4.79785639]. \t  -50.20079446939181 \t -13.871821018360485\n",
            "init   \t [3.78055209 0.31596228]. \t  -36.5114251593508 \t -13.871821018360485\n",
            "init   \t [-2.73686192 -5.00327624]. \t  -43.34985765011677 \t -13.871821018360485\n",
            "init   \t [-0.7119993  -0.99992207]. \t  -13.871821018360485 \t -13.871821018360485\n",
            "init   \t [ 0.23218863 -0.22126801]. \t  -17.190590355445654 \t -13.871821018360485\n",
            "1      \t [-20.87190776 -20.99448523]. \t  -879.47858908504 \t -13.871821018360485\n",
            "2      \t [-3.96968715  4.91411309]. \t  -41.508833543187585 \t -13.871821018360485\n",
            "3      \t [-8.81533111 -9.52825981]. \t  -194.35000730958697 \t -13.871821018360485\n",
            "4      \t [-7.25440529 -0.36787488]. \t  -79.78601297458643 \t -13.871821018360485\n",
            "5      \t [-13.8827358 -13.2645946]. \t  -382.18927029531534 \t -13.871821018360485\n",
            "6      \t [ 3.8288596  -5.81023842]. \t  -59.96924318057419 \t -13.871821018360485\n",
            "7      \t [-7.52930192 -4.92345936]. \t  -101.89613477824699 \t -13.871821018360485\n",
            "8      \t [-33.56242788 -35.13202414]. \t  -2383.1840040241595 \t -13.871821018360485\n",
            "9      \t [-55.14063471 -54.36016436]. \t  -6015.555797520894 \t -13.871821018360485\n",
            "10     \t [-0.09133387  3.59240686]. \t  -32.8769382227626 \t -13.871821018360485\n",
            "11     \t [-26.74810937 -27.7174712 ]. \t  -1505.867998437895 \t -13.871821018360485\n",
            "12     \t [ -3.20134365 -10.30906524]. \t  -137.14223276509802 \t -13.871821018360485\n",
            "13     \t [-3.44606824  0.95377692]. \t  -32.63519468498099 \t -13.871821018360485\n",
            "14     \t [-22.56075226 -16.04806812]. \t  -786.260920874359 \t -13.871821018360485\n",
            "15     \t [-14.02566519  -7.11455271]. \t  -249.946218491789 \t -13.871821018360485\n",
            "16     \t [ 0.19656187 -6.97602705]. \t  -55.521933193144726 \t -13.871821018360485\n",
            "17     \t [ 1.9518188  -2.96802097]. \t  \u001b[92m-13.274674189809595\u001b[0m \t -13.274674189809595\n",
            "18     \t [-133.44751664 -139.15756348]. \t  -37197.04146834162 \t -13.274674189809595\n",
            "19     \t [-43.98157537 -48.35480332]. \t  -4288.75221177815 \t -13.274674189809595\n",
            "20     \t [-19.25434076 -10.70398957]. \t  -508.42855566071506 \t -13.274674189809595\n",
            "21     \t [-4.32008935 -2.19872433]. \t  -44.59413343585345 \t -13.274674189809595\n",
            "22     \t [ -8.84988332 -14.76530944]. \t  -309.50244023271546 \t -13.274674189809595\n",
            "23     \t [-37.24163297 -42.95845105]. \t  -3242.1810987223553 \t -13.274674189809595\n",
            "24     \t [-22.33834264 -26.63726641]. \t  -1240.3212024528502 \t -13.274674189809595\n",
            "25     \t [-11.1939553   -4.31664783]. \t  -164.55530341902002 \t -13.274674189809595\n",
            "26     \t [ 4.65550534 -3.09737981]. \t  -48.676199107472165 \t -13.274674189809595\n",
            "27     \t [-14.48025274 -19.62097749]. \t  -631.8311078202676 \t -13.274674189809595\n",
            "28     \t [-0.51396156 -3.53356371]. \t  -52.49023322531478 \t -13.274674189809595\n",
            "29     \t [-5.02151875  2.40792202]. \t  -49.47760569785503 \t -13.274674189809595\n",
            "30     \t [1.95061625 2.13388972]. \t  \u001b[92m-12.170644244346224\u001b[0m \t -12.170644244346224\n",
            "31     \t [-1.37401092  1.22866951]. \t  -29.088297129095732 \t -12.170644244346224\n",
            "32     \t [-4.99135811 -5.65420421]. \t  -72.56053735175881 \t -12.170644244346224\n",
            "33     \t [1.44631418 5.06634461]. \t  -48.052463023466686 \t -12.170644244346224\n",
            "34     \t [4.91791791 2.14211095]. \t  -33.80377585988395 \t -12.170644244346224\n",
            "35     \t [ 1.95033771 -1.57399986]. \t  -25.70261012017353 \t -12.170644244346224\n",
            "36     \t [-31.07035597 -30.68465551]. \t  -1921.867713619002 \t -12.170644244346224\n",
            "37     \t [-17.77651783 -17.26021182]. \t  -632.9022198914694 \t -12.170644244346224\n",
            "38     \t [-1.69000908  4.77351609]. \t  -47.85111255498849 \t -12.170644244346224\n",
            "39     \t [ 1.68230295 -4.55414912]. \t  -57.123624931489594 \t -12.170644244346224\n",
            "40     \t [-2.55092999 -1.24326381]. \t  -37.12217380303139 \t -12.170644244346224\n",
            "41     \t [0.99547046 0.54471263]. \t  -20.899681594189744 \t -12.170644244346224\n",
            "42     \t [2.3061161  2.42112634]. \t  -43.43024122697779 \t -12.170644244346224\n",
            "43     \t [1.11190734 1.89523514]. \t  \u001b[92m-9.289466624399026\u001b[0m \t -9.289466624399026\n",
            "44     \t [2.01826368 1.22129521]. \t  -13.83690782344632 \t -9.289466624399026\n",
            "45     \t [1.31559717 1.90707294]. \t  -21.030298021984542 \t -9.289466624399026\n",
            "46     \t [0.13916927 1.46599385]. \t  -25.526734547601365 \t -9.289466624399026\n",
            "47     \t [0.12024206 2.81102618]. \t  -16.895942232152347 \t -9.289466624399026\n",
            "48     \t [2.52464574 1.54585847]. \t  -48.23160411180292 \t -9.289466624399026\n",
            "49     \t [1.89670904 0.26538312]. \t  -16.666070641504625 \t -9.289466624399026\n",
            "50     \t [0.46412022 2.24937359]. \t  -34.982690912656835 \t -9.289466624399026\n",
            "51     \t [0.63073862 1.16859233]. \t  -23.68015289342835 \t -9.289466624399026\n",
            "52     \t [-0.93098548  2.72179703]. \t  -20.96323738441006 \t -9.289466624399026\n",
            "53     \t [ 0.69618582 -0.61621813]. \t  -31.6318592202337 \t -9.289466624399026\n",
            "54     \t [ 4.78292464 -0.30203518]. \t  -44.125130441771994 \t -9.289466624399026\n",
            "55     \t [-0.34909637  3.0852748 ]. \t  -26.873995219491597 \t -9.289466624399026\n",
            "56     \t [0.97797563 3.85530718]. \t  -19.77111700475379 \t -9.289466624399026\n",
            "57     \t [-1.10747919  1.75874121]. \t  -15.965609205145396 \t -9.289466624399026\n",
            "58     \t [ 2.18453735 -2.49922056]. \t  -37.02004627377542 \t -9.289466624399026\n",
            "59     \t [-2.43194177  3.01588018]. \t  -34.159153932051346 \t -9.289466624399026\n",
            "60     \t [-1.15061001 -0.22725499]. \t  -14.104494976776365 \t -9.289466624399026\n",
            "61     \t [1.30714165 0.05629301]. \t  -15.844487974847432 \t -9.289466624399026\n",
            "62     \t [0.24895806 3.42336467]. \t  -40.57888730634562 \t -9.289466624399026\n",
            "63     \t [1.29686924 4.47042373]. \t  -54.396885224269816 \t -9.289466624399026\n",
            "64     \t [-0.45050803  4.47090681]. \t  -59.5457351696851 \t -9.289466624399026\n",
            "65     \t [-0.33303632  2.03096271]. \t  -19.40819412666454 \t -9.289466624399026\n",
            "66     \t [1.84638697 3.57445974]. \t  -39.418635927335885 \t -9.289466624399026\n",
            "67     \t [-0.80786231  0.17141184]. \t  -12.386376552523602 \t -9.289466624399026\n",
            "68     \t [ 1.39018727 -2.73224541]. \t  -38.22365421804627 \t -9.289466624399026\n",
            "69     \t [-1.79784023  2.41981145]. \t  -34.884241449965735 \t -9.289466624399026\n",
            "70     \t [-2.92214157  4.78431135]. \t  -40.462265411845465 \t -9.289466624399026\n",
            "71     \t [-2.2883406  -0.09978021]. \t  -19.533957632493486 \t -9.289466624399026\n",
            "72     \t [ 2.22792334 -3.84659644]. \t  -32.67376171321915 \t -9.289466624399026\n",
            "73     \t [-1.61020769 -1.15034897]. \t  -25.752779774153332 \t -9.289466624399026\n",
            "74     \t [5.10697919 4.04945356]. \t  -45.133469238954916 \t -9.289466624399026\n",
            "75     \t [-0.76945396  0.9599236 ]. \t  -10.60958907213544 \t -9.289466624399026\n",
            "76     \t [-2.84721903  3.71427905]. \t  -38.3925428946643 \t -9.289466624399026\n",
            "77     \t [0.88869537 1.57895135]. \t  -24.424862249012932 \t -9.289466624399026\n",
            "78     \t [-2.18046076  0.60494762]. \t  -28.792357846121583 \t -9.289466624399026\n",
            "79     \t [2.21650007 0.63674392]. \t  -29.75949179995408 \t -9.289466624399026\n",
            "80     \t [4.82673252 0.62328945]. \t  -46.195865393111255 \t -9.289466624399026\n",
            "81     \t [1.75682129 2.51988305]. \t  -38.92983357995386 \t -9.289466624399026\n",
            "82     \t [-4.41073832  0.35329433]. \t  -54.09151348746303 \t -9.289466624399026\n",
            "83     \t [-0.16578525  0.62856536]. \t  -22.28559693359969 \t -9.289466624399026\n",
            "84     \t [-1.66140398 -2.43331111]. \t  -43.09973126671388 \t -9.289466624399026\n",
            "85     \t [ 3.57739351 -0.96454665]. \t  -32.81596552990079 \t -9.289466624399026\n",
            "86     \t [4.30258967 0.31909262]. \t  -46.064739227479336 \t -9.289466624399026\n",
            "87     \t [-1.52681627  0.2686413 ]. \t  -33.43031565576946 \t -9.289466624399026\n",
            "88     \t [4.08200202 0.06507196]. \t  -18.789450177622992 \t -9.289466624399026\n",
            "89     \t [-0.71852346  2.28395255]. \t  -29.81473438038002 \t -9.289466624399026\n",
            "90     \t [-1.99703197  3.4629969 ]. \t  -35.713163226192066 \t -9.289466624399026\n",
            "91     \t [-4.01278961  3.40521822]. \t  -46.00876721719035 \t -9.289466624399026\n",
            "92     \t [-1.75820731  2.78443601]. \t  -28.182089483475938 \t -9.289466624399026\n",
            "93     \t [-3.24367848  2.20612899]. \t  -32.269645296190106 \t -9.289466624399026\n",
            "94     \t [1.4560064  5.58236851]. \t  -71.59358494186185 \t -9.289466624399026\n",
            "95     \t [-1.13540211  3.73591976]. \t  -29.53562268303505 \t -9.289466624399026\n",
            "96     \t [ 4.99721563 -1.17100153]. \t  -31.582641206131335 \t -9.289466624399026\n",
            "97     \t [-2.55501162  1.80045236]. \t  -36.06108416622129 \t -9.289466624399026\n",
            "98     \t [-4.90816297  4.97429736]. \t  -50.58294604030863 \t -9.289466624399026\n",
            "99     \t [-3.44424969  2.91661583]. \t  -41.10358139772206 \t -9.289466624399026\n",
            "100    \t [-5.0583657   3.66784898]. \t  -54.640632583412454 \t -9.289466624399026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4HnKuoqteDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b823e74-f48d-406a-806e-3bbda0f84668"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_19 = d2GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.60872271 2.67767798]. \t  -39.68588898862962 \t -22.797176954563277\n",
            "init   \t [-2.61599688  2.78210017]. \t  -40.03979647504547 \t -22.797176954563277\n",
            "init   \t [-1.49020588  3.03818137]. \t  -31.718714938133186 \t -22.797176954563277\n",
            "init   \t [-1.57695977 -0.57472578]. \t  -40.5884745864365 \t -22.797176954563277\n",
            "init   \t [-2.01417124 -0.59275346]. \t  -22.797176954563277 \t -22.797176954563277\n",
            "1      \t [-3.24838972 -0.72088615]. \t  -32.78962969886806 \t -22.797176954563277\n",
            "2      \t [-12.51029704  -8.31901463]. \t  -259.89430553118643 \t -22.797176954563277\n",
            "3      \t [ 4.75602657 -4.56582648]. \t  -72.24478919249417 \t -22.797176954563277\n",
            "4      \t [-5.21567495 -6.6786067 ]. \t  -94.00386711817383 \t -22.797176954563277\n",
            "5      \t [-47.06806732 -45.3930712 ]. \t  -4294.661262360631 \t -22.797176954563277\n",
            "6      \t [-9.46967885 -1.1241467 ]. \t  -113.6487175305451 \t -22.797176954563277\n",
            "7      \t [-16.82004956 -14.01338459]. \t  -485.06374614724035 \t -22.797176954563277\n",
            "8      \t [ 0.08377382 -6.74142207]. \t  -57.346113689504044 \t -22.797176954563277\n",
            "9      \t [-31.7394842  -30.25172805]. \t  -1943.330728231127 \t -22.797176954563277\n",
            "10     \t [5.11466912 4.72037238]. \t  -62.77773775646213 \t -22.797176954563277\n",
            "11     \t [ -7.60784924 -15.39087844]. \t  -330.2892716672986 \t -22.797176954563277\n",
            "12     \t [4.89608406 0.1157517 ]. \t  -28.57213067828593 \t -22.797176954563277\n",
            "13     \t [-6.5942968   2.19753451]. \t  -73.37234497554736 \t -22.797176954563277\n",
            "14     \t [-12.05867917 -12.95931332]. \t  -314.3525383247862 \t -22.797176954563277\n",
            "15     \t [-11.12986984  -4.65469937]. \t  -164.3252446638076 \t -22.797176954563277\n",
            "16     \t [ -3.93874509 -11.41456469]. \t  -165.13101499342588 \t -22.797176954563277\n",
            "17     \t [-25.19450562 -19.82180426]. \t  -1039.8903552209847 \t -22.797176954563277\n",
            "18     \t [-17.11482057 -20.54966537]. \t  -737.2143100416649 \t -22.797176954563277\n",
            "19     \t [-13.47947186 -17.6888474 ]. \t  -528.2569000879635 \t -22.797176954563277\n",
            "20     \t [-36.51642269 -35.09145872]. \t  -2586.412595685112 \t -22.797176954563277\n",
            "21     \t [ 1.30180697 -3.04060633]. \t  -24.461648408128326 \t -22.797176954563277\n",
            "22     \t [-22.05837433 -26.05645526]. \t  -1166.7984165353103 \t -22.797176954563277\n",
            "23     \t [-2.41542323 -4.1794637 ]. \t  -47.634854643006676 \t -22.797176954563277\n",
            "24     \t [-6.08331037 -2.84396369]. \t  -50.86695798656127 \t -22.797176954563277\n",
            "25     \t [-8.42578651 -8.22511645]. \t  -166.02169318741338 \t -22.797176954563277\n",
            "26     \t [-112.32014919 -115.99285484]. \t  -26084.434637375107 \t -22.797176954563277\n",
            "27     \t [-37.69227059 -41.81317242]. \t  -3188.731045045924 \t -22.797176954563277\n",
            "28     \t [-21.14640742 -19.69670617]. \t  -852.3581456137481 \t -22.797176954563277\n",
            "29     \t [-25.83394612 -30.26147528]. \t  -1598.8367397208874 \t -22.797176954563277\n",
            "30     \t [ -7.57332632 -11.64177174]. \t  -228.1315062816502 \t -22.797176954563277\n",
            "31     \t [ -98.09500466 -101.63667247]. \t  -19970.906418062325 \t -22.797176954563277\n",
            "32     \t [-120.51038665 -124.61504051]. \t  -30089.1397522799 \t -22.797176954563277\n",
            "33     \t [-4.39667926  5.10135878]. \t  -65.28073375213852 \t -22.797176954563277\n",
            "34     \t [ 2.12061753 -0.17274972]. \t  \u001b[92m-12.598364569603516\u001b[0m \t -12.598364569603516\n",
            "35     \t [3.23658022 2.09826788]. \t  -25.882328985296233 \t -12.598364569603516\n",
            "36     \t [-0.919381    4.77568689]. \t  -33.30122578095086 \t -12.598364569603516\n",
            "37     \t [2.17999903 4.40480034]. \t  -48.16056141745382 \t -12.598364569603516\n",
            "38     \t [ 3.11037777 -1.83463336]. \t  -20.279776643687864 \t -12.598364569603516\n",
            "39     \t [-2.33677144  0.58295732]. \t  -39.65828261399995 \t -12.598364569603516\n",
            "40     \t [ 2.12821968 -5.64841272]. \t  -55.46554477634436 \t -12.598364569603516\n",
            "41     \t [-2.29364157 -1.63282261]. \t  -37.349867007536645 \t -12.598364569603516\n",
            "42     \t [ 4.69219339 -1.8304497 ]. \t  -44.077709771715874 \t -12.598364569603516\n",
            "43     \t [ 1.30905782 -0.32815423]. \t  -30.163043701916664 \t -12.598364569603516\n",
            "44     \t [3.10526203 0.63485151]. \t  -28.774354617420197 \t -12.598364569603516\n",
            "45     \t [ 2.22065644 -0.27347384]. \t  -24.642383615675964 \t -12.598364569603516\n",
            "46     \t [1.06016619 0.44853174]. \t  -21.512870211478962 \t -12.598364569603516\n",
            "47     \t [1.84625267 1.48365168]. \t  -29.87140477512122 \t -12.598364569603516\n",
            "48     \t [ 2.73620941 -3.15638949]. \t  -32.76662835554894 \t -12.598364569603516\n",
            "49     \t [1.61521818 0.10972756]. \t  -22.39697546167622 \t -12.598364569603516\n",
            "50     \t [-0.02203561  0.44314661]. \t  -19.661282784893388 \t -12.598364569603516\n",
            "51     \t [0.18076029 1.42581245]. \t  -26.78421917039954 \t -12.598364569603516\n",
            "52     \t [ 3.16579256 -2.51678345]. \t  -41.253402932754504 \t -12.598364569603516\n",
            "53     \t [ 0.49887083 -1.34813301]. \t  -37.84863017985354 \t -12.598364569603516\n",
            "54     \t [2.68760761 0.92895096]. \t  -22.886761659691096 \t -12.598364569603516\n",
            "55     \t [ 2.13311557 -3.95270918]. \t  -23.91083755041962 \t -12.598364569603516\n",
            "56     \t [2.38230386 0.15690326]. \t  -27.56647421436878 \t -12.598364569603516\n",
            "57     \t [4.21215399 1.54609728]. \t  -47.36054422097281 \t -12.598364569603516\n",
            "58     \t [0.80636877 1.12323403]. \t  \u001b[92m-11.294623437845136\u001b[0m \t -11.294623437845136\n",
            "59     \t [2.1802141  2.53405519]. \t  -36.70109759349677 \t -11.294623437845136\n",
            "60     \t [0.35022167 0.45936926]. \t  -35.89869012271752 \t -11.294623437845136\n",
            "61     \t [-1.16329131  1.13281101]. \t  \u001b[92m-10.738320214328782\u001b[0m \t -10.738320214328782\n",
            "62     \t [ 1.42191931 -1.21873546]. \t  -30.375896116508684 \t -10.738320214328782\n",
            "63     \t [-0.86151279  0.13725141]. \t  \u001b[92m-7.80750364494227\u001b[0m \t -7.80750364494227\n",
            "64     \t [-3.1360954   0.39900623]. \t  -31.48627614318842 \t -7.80750364494227\n",
            "65     \t [-0.71843395  2.5515484 ]. \t  -38.47697385157388 \t -7.80750364494227\n",
            "66     \t [ 2.15801503 -2.85406221]. \t  -21.257141417380055 \t -7.80750364494227\n",
            "67     \t [-0.80870191  0.81108582]. \t  -13.96197664271252 \t -7.80750364494227\n",
            "68     \t [-1.36649364  1.86481151]. \t  -25.42381926512292 \t -7.80750364494227\n",
            "69     \t [-0.66342627 -0.12896394]. \t  -18.739260483114695 \t -7.80750364494227\n",
            "70     \t [-2.28837203  3.37534088]. \t  -46.10346508987131 \t -7.80750364494227\n",
            "71     \t [-4.30524739  0.53246381]. \t  -52.01336216878831 \t -7.80750364494227\n",
            "72     \t [ 3.52384834 -1.90977365]. \t  -37.51703676485348 \t -7.80750364494227\n",
            "73     \t [ 0.1727989  -2.99447934]. \t  -14.340089203583155 \t -7.80750364494227\n",
            "74     \t [-0.46106297  0.211188  ]. \t  -27.544873474423575 \t -7.80750364494227\n",
            "75     \t [-0.56667256  3.33237857]. \t  -45.50912672864234 \t -7.80750364494227\n",
            "76     \t [-2.89804973 -2.04645408]. \t  -14.992081860527595 \t -7.80750364494227\n",
            "77     \t [1.25648075 3.3650338 ]. \t  -39.923993332484514 \t -7.80750364494227\n",
            "78     \t [-0.43057271  1.06554553]. \t  -21.220434055102956 \t -7.80750364494227\n",
            "79     \t [-1.67001261  0.78747508]. \t  -25.89297025281585 \t -7.80750364494227\n",
            "80     \t [-2.58290355 -0.35232505]. \t  -41.46465610222518 \t -7.80750364494227\n",
            "81     \t [-3.67825853  1.961484  ]. \t  -32.02494851890972 \t -7.80750364494227\n",
            "82     \t [-2.73430376 -1.58506683]. \t  -39.578761496221745 \t -7.80750364494227\n",
            "83     \t [-4.37383137 -1.17320683]. \t  -42.88576925695572 \t -7.80750364494227\n",
            "84     \t [-3.3371515   1.46884441]. \t  -48.30939521454455 \t -7.80750364494227\n",
            "85     \t [-2.47642445 -2.80333056]. \t  -40.593327292372905 \t -7.80750364494227\n",
            "86     \t [-3.93764717 -0.79001167]. \t  -24.399244777212566 \t -7.80750364494227\n",
            "87     \t [-4.04416597 -2.68850434]. \t  -37.73436656290418 \t -7.80750364494227\n",
            "88     \t [-4.06206938  2.62825469]. \t  -41.08401222613259 \t -7.80750364494227\n",
            "89     \t [-3.137766   -4.99827469]. \t  -38.34715019363225 \t -7.80750364494227\n",
            "90     \t [-5.10501111  1.14636663]. \t  -33.413176188016415 \t -7.80750364494227\n",
            "91     \t [-3.23125998 -2.60029203]. \t  -44.10717953565708 \t -7.80750364494227\n",
            "92     \t [ 0.65414759 -3.72414563]. \t  -41.57956261499036 \t -7.80750364494227\n",
            "93     \t [-2.19148938 -5.04177221]. \t  -36.97047640732677 \t -7.80750364494227\n",
            "94     \t [-4.64915933 -2.01644619]. \t  -41.65458376784481 \t -7.80750364494227\n",
            "95     \t [1.40476874 0.87195217]. \t  -24.06193758630465 \t -7.80750364494227\n",
            "96     \t [ 5.07083689 -1.3140026 ]. \t  -42.32811997882636 \t -7.80750364494227\n",
            "97     \t [-2.24513891 -2.34633519]. \t  -35.93057445873107 \t -7.80750364494227\n",
            "98     \t [1.49307207 4.15848554]. \t  -44.074426209000464 \t -7.80750364494227\n",
            "99     \t [0.3760566  1.10621861]. \t  -20.62860303538518 \t -7.80750364494227\n",
            "100    \t [-5.19755894 -3.82494442]. \t  -53.872355067178994 \t -7.80750364494227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfiDmH_bteGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71733353-778a-43d4-e416-912d6e65afde"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\n",
        "\n",
        "loser_20 = d2GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.722097   0.66077445]. \t  -28.019395863326952 \t -20.02750192535725\n",
            "init   \t [-0.11835563 -1.6744678 ]. \t  -20.02750192535725 \t -20.02750192535725\n",
            "init   \t [-1.27110986  0.3280473 ]. \t  -27.755482301965866 \t -20.02750192535725\n",
            "init   \t [-4.42259161  0.86557758]. \t  -42.50859246850971 \t -20.02750192535725\n",
            "init   \t [-2.6839269  -3.47385261]. \t  -53.16973204675458 \t -20.02750192535725\n",
            "1      \t [-59.62097249 -62.09697239]. \t  -7429.741548870836 \t -20.02750192535725\n",
            "2      \t [-8.15687388 -6.51856168]. \t  -133.43523979717767 \t -20.02750192535725\n",
            "3      \t [-28.84009134 -29.64550912]. \t  -1731.3477297860352 \t -20.02750192535725\n",
            "4      \t [-12.47925542 -11.5973006 ]. \t  -328.3330670014494 \t -20.02750192535725\n",
            "5      \t [4.46191141 4.96382743]. \t  -64.52040441410881 \t -20.02750192535725\n",
            "6      \t [-2.88304711 -9.46404617]. \t  -120.20664694847021 \t -20.02750192535725\n",
            "7      \t [-21.0333381 -20.2456795]. \t  -862.2360051807475 \t -20.02750192535725\n",
            "8      \t [-38.01182849 -35.35978552]. \t  -2711.6049924559184 \t -20.02750192535725\n",
            "9      \t [-14.82523912 -17.60370672]. \t  -553.0760116255047 \t -20.02750192535725\n",
            "10     \t [ 4.63192212 -3.91118408]. \t  -55.026090092425875 \t -20.02750192535725\n",
            "11     \t [-0.21682804  5.04284839]. \t  -33.76836109723192 \t -20.02750192535725\n",
            "12     \t [-26.10620068 -23.83057786]. \t  -1256.725746039396 \t -20.02750192535725\n",
            "13     \t [ 1.05831697 -6.96973211]. \t  -50.541323210752836 \t -20.02750192535725\n",
            "14     \t [-32.62856892 -34.63606698]. \t  -2297.7540687580313 \t -20.02750192535725\n",
            "15     \t [-46.41710527 -48.71087414]. \t  -4558.4045849158165 \t -20.02750192535725\n",
            "16     \t [-66.19382962 -67.91456509]. \t  -9001.961276018075 \t -20.02750192535725\n",
            "17     \t [4.63430967 0.70651503]. \t  -51.319955243427955 \t -20.02750192535725\n",
            "18     \t [-4.79971035  4.71309363]. \t  -64.47578424268322 \t -20.02750192535725\n",
            "19     \t [ -6.87752135 -13.49239686]. \t  -252.1514677244895 \t -20.02750192535725\n",
            "20     \t [-19.02641374 -12.63448513]. \t  -538.4093864410844 \t -20.02750192535725\n",
            "21     \t [-54.60696337 -50.43445773]. \t  -5562.544277755352 \t -20.02750192535725\n",
            "22     \t [-8.79008678 -1.52397252]. \t  -106.98271458665795 \t -20.02750192535725\n",
            "23     \t [-13.9087463   -6.43212239]. \t  -255.52885620656872 \t -20.02750192535725\n",
            "24     \t [-5.88970523 -3.64101869]. \t  -66.57712329439593 \t -20.02750192535725\n",
            "25     \t [-6.95628151 -9.66887956]. \t  -157.13110286621628 \t -20.02750192535725\n",
            "26     \t [1.96056793 3.18956332]. \t  -20.61575411987543 \t -20.02750192535725\n",
            "27     \t [ 1.58160777 -3.57788266]. \t  -52.84306181828624 \t -20.02750192535725\n",
            "28     \t [-64.39595251 -63.91071776]. \t  -8250.889275484375 \t -20.02750192535725\n",
            "29     \t [-4.12920487 -6.42520452]. \t  -80.36767339350295 \t -20.02750192535725\n",
            "30     \t [-2.08204685  2.92716967]. \t  \u001b[92m-15.231729235061291\u001b[0m \t -15.231729235061291\n",
            "31     \t [ 3.04583054 -1.1089084 ]. \t  \u001b[92m-13.16984448455877\u001b[0m \t -13.16984448455877\n",
            "32     \t [-4.26737196 -1.25672128]. \t  -41.30134320287069 \t -13.16984448455877\n",
            "33     \t [-0.91780388 -5.64826122]. \t  -50.015345120848565 \t -13.16984448455877\n",
            "34     \t [ 5.09608432 -1.55127637]. \t  -49.62969180203231 \t -13.16984448455877\n",
            "35     \t [-2.43883109  4.80015607]. \t  -55.16037840820789 \t -13.16984448455877\n",
            "36     \t [-0.81827946  2.20975569]. \t  -18.891107439475817 \t -13.16984448455877\n",
            "37     \t [ 2.05899101 -0.86985783]. \t  \u001b[92m-8.836224124842378\u001b[0m \t -8.836224124842378\n",
            "38     \t [ 2.07822183 -1.13090864]. \t  -9.977875360085031 \t -8.836224124842378\n",
            "39     \t [-2.85703201  2.23396616]. \t  -25.918052456518836 \t -8.836224124842378\n",
            "40     \t [2.47561707 0.79752266]. \t  -33.705832341036555 \t -8.836224124842378\n",
            "41     \t [4.61469652 2.97026294]. \t  -47.80563806963282 \t -8.836224124842378\n",
            "42     \t [1.83436545 4.85052415]. \t  -35.93196087204692 \t -8.836224124842378\n",
            "43     \t [-2.14686574 -1.08949988]. \t  -11.299944079744108 \t -8.836224124842378\n",
            "44     \t [ 1.55060789 -0.89521871]. \t  -24.794528995074465 \t -8.836224124842378\n",
            "45     \t [-0.95603605 -2.14067184]. \t  -9.533928375931806 \t -8.836224124842378\n",
            "46     \t [ 2.28546758 -0.4618462 ]. \t  -37.36078576848119 \t -8.836224124842378\n",
            "47     \t [1.56133921 2.09182332]. \t  -27.698624343837622 \t -8.836224124842378\n",
            "48     \t [2.78939364 2.1598661 ]. \t  -24.630389660266665 \t -8.836224124842378\n",
            "49     \t [-1.31722078 -1.29647331]. \t  -30.393746390644583 \t -8.836224124842378\n",
            "50     \t [-0.70687143 -2.66299167]. \t  -35.46661638681103 \t -8.836224124842378\n",
            "51     \t [-2.63354579 -1.34812117]. \t  -41.216314348314796 \t -8.836224124842378\n",
            "52     \t [-2.82312833 -0.10472939]. \t  -15.634251477430823 \t -8.836224124842378\n",
            "53     \t [-1.92271776 -0.04116097]. \t  \u001b[92m-5.187054283410136\u001b[0m \t -5.187054283410136\n",
            "54     \t [-2.01192967 -0.26162343]. \t  -14.874059582373656 \t -5.187054283410136\n",
            "55     \t [-2.15254816 -0.05671836]. \t  -9.517416790846793 \t -5.187054283410136\n",
            "56     \t [-2.51990073  0.80968831]. \t  -33.26439189889037 \t -5.187054283410136\n",
            "57     \t [-1.73296373 -1.59790369]. \t  -34.79172303129937 \t -5.187054283410136\n",
            "58     \t [-3.50186868  2.81094971]. \t  -46.427171782397124 \t -5.187054283410136\n",
            "59     \t [-4.40409606  0.05223124]. \t  -38.1712284485733 \t -5.187054283410136\n",
            "60     \t [-1.80120226  0.4041607 ]. \t  -28.486800950830457 \t -5.187054283410136\n",
            "61     \t [-1.00385003  3.56097829]. \t  -32.96616849318955 \t -5.187054283410136\n",
            "62     \t [-0.25825601  2.43810847]. \t  -35.782935146342304 \t -5.187054283410136\n",
            "63     \t [-2.71938168 -4.52075942]. \t  -59.65931722436087 \t -5.187054283410136\n",
            "64     \t [-1.00782821  2.45477602]. \t  -26.652738017345744 \t -5.187054283410136\n",
            "65     \t [1.55233114 1.50486954]. \t  -44.13396892876737 \t -5.187054283410136\n",
            "66     \t [-3.41255999  0.53760717]. \t  -50.18508286338866 \t -5.187054283410136\n",
            "67     \t [-5.43871086  1.19219644]. \t  -56.71596642208074 \t -5.187054283410136\n",
            "68     \t [-0.60932653 -1.85863515]. \t  -25.249909294276 \t -5.187054283410136\n",
            "69     \t [ 0.38435992 -0.47482399]. \t  -37.72279158404638 \t -5.187054283410136\n",
            "70     \t [-1.18912361  0.02855769]. \t  -7.842993010465037 \t -5.187054283410136\n",
            "71     \t [3.18549963 1.39147754]. \t  -35.90494479113876 \t -5.187054283410136\n",
            "72     \t [-1.19766403 -2.79172833]. \t  -23.406791987364016 \t -5.187054283410136\n",
            "73     \t [0.35781876 1.49826032]. \t  -38.64026542636893 \t -5.187054283410136\n",
            "74     \t [1.93308344 2.31882873]. \t  -24.175810316623597 \t -5.187054283410136\n",
            "75     \t [ 2.28943316 -5.02666099]. \t  -43.10119130473908 \t -5.187054283410136\n",
            "76     \t [-2.62760711  0.425465  ]. \t  -42.96292450634995 \t -5.187054283410136\n",
            "77     \t [-3.55652792 -0.90444435]. \t  -34.5916339618388 \t -5.187054283410136\n",
            "78     \t [-5.41324114 -1.68148148]. \t  -64.85458440097585 \t -5.187054283410136\n",
            "79     \t [-3.01259887 -0.57053877]. \t  -28.466387608419737 \t -5.187054283410136\n",
            "80     \t [-4.39596762  3.06377955]. \t  -47.44221297833059 \t -5.187054283410136\n",
            "81     \t [-2.59284337  1.74737221]. \t  -38.287456921675805 \t -5.187054283410136\n",
            "82     \t [-2.07372568  3.68505855]. \t  -32.90196767546554 \t -5.187054283410136\n",
            "83     \t [-3.77735248 -0.34252439]. \t  -38.16706430145209 \t -5.187054283410136\n",
            "84     \t [-3.74328077 -2.60523361]. \t  -49.11399489823676 \t -5.187054283410136\n",
            "85     \t [-6.06214071 -0.6054089 ]. \t  -55.754436193990955 \t -5.187054283410136\n",
            "86     \t [ 0.30861348 -1.48033207]. \t  -35.810472986809586 \t -5.187054283410136\n",
            "87     \t [ 3.05678633 -1.92833112]. \t  -14.689080031062266 \t -5.187054283410136\n",
            "88     \t [-5.42236307  3.13509959]. \t  -61.45607441446717 \t -5.187054283410136\n",
            "89     \t [-3.97809158 -1.93480384]. \t  -20.49062856219725 \t -5.187054283410136\n",
            "90     \t [-3.64218642 -3.67211954]. \t  -57.71818584144664 \t -5.187054283410136\n",
            "91     \t [-3.54552664  1.52421258]. \t  -54.372137773680116 \t -5.187054283410136\n",
            "92     \t [-2.79087883  3.2888888 ]. \t  -38.48466358044867 \t -5.187054283410136\n",
            "93     \t [-1.48561475  3.518911  ]. \t  -54.47845672071189 \t -5.187054283410136\n",
            "94     \t [-3.01299099  1.58963401]. \t  -30.093924269099467 \t -5.187054283410136\n",
            "95     \t [-0.11251443  1.21844768]. \t  -11.924273242937588 \t -5.187054283410136\n",
            "96     \t [-3.7259115  -1.96196493]. \t  -29.5236801844211 \t -5.187054283410136\n",
            "97     \t [1.36863218 0.22353317]. \t  -27.050385022558 \t -5.187054283410136\n",
            "98     \t [-2.60893419  3.0244869 ]. \t  -33.81982868552737 \t -5.187054283410136\n",
            "99     \t [-3.27851315  4.85211398]. \t  -50.088835548191625 \t -5.187054283410136\n",
            "100    \t [-0.66082729  4.19338038]. \t  -39.85248965488171 \t -5.187054283410136\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kM4bcwSteJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467b215a-b5d5-4a97-a5b9-bee2bd3b0e20"
      },
      "source": [
        "end_lose = time.time()\n",
        "end_lose\n",
        "\n",
        "time_lose = end_lose - start_lose\n",
        "time_lose\n",
        "\n",
        "start_win = time.time()\n",
        "start_win"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616926931.4427652"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "2c21604e-10e6-4fc2-def9-9c92223d5e63"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.1486226  -3.38872572]. \t  -34.50899842538312 \t -32.79886533908709\n",
            "init   \t [-0.65475564  2.75724772]. \t  -33.20941543961683 \t -32.79886533908709\n",
            "init   \t [-2.09586888 -3.59257132]. \t  -37.414851075595465 \t -32.79886533908709\n",
            "init   \t [-4.88982196 -0.8169012 ]. \t  -32.79886533908709 \t -32.79886533908709\n",
            "init   \t [-2.67589487 -1.6624006 ]. \t  -39.644034416971316 \t -32.79886533908709\n",
            "1      \t [4.59908713 1.5941869 ]. \t  -60.11619838154264 \t -32.79886533908709\n",
            "2      \t [-5.12        4.98167653]. \t  -53.80801590311394 \t -32.79886533908709\n",
            "3      \t [ 5.11450137 -4.78533007]. \t  -59.334048263889535 \t -32.79886533908709\n",
            "4      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -32.79886533908709\n",
            "5      \t [1.97738418 4.90968925]. \t  \u001b[92m-29.683087077313246\u001b[0m \t -29.683087077313246\n",
            "6      \t [ 1.30622516 -0.02588094]. \t  \u001b[92m-15.298526959868948\u001b[0m \t -15.298526959868948\n",
            "7      \t [ 4.55755575 -1.66409367]. \t  -58.033071156082016 \t -15.298526959868948\n",
            "8      \t [-3.98624151  2.02757403]. \t  -20.188227587223516 \t -15.298526959868948\n",
            "9      \t [-0.88968294  0.08900352]. \t  \u001b[92m-4.630376638276061\u001b[0m \t -4.630376638276061\n",
            "10     \t [4.63253276 4.812696  ]. \t  -67.51279616578032 \t -4.630376638276061\n",
            "11     \t [-0.33003947 -5.12      ]. \t  -43.85334938704551 \t -4.630376638276061\n",
            "12     \t [-1.77064717  4.88658785]. \t  -38.153554166838774 \t -4.630376638276061\n",
            "13     \t [-0.35714447 -1.26982695]. \t  -29.217534210404203 \t -4.630376638276061\n",
            "14     \t [-1.27144924  0.53416864]. \t  -33.015968052143336 \t -4.630376638276061\n",
            "15     \t [-0.33200975 -0.52896681]. \t  -35.1526724230343 \t -4.630376638276061\n",
            "16     \t [-1.50824142 -0.69882696]. \t  -35.90992966090253 \t -4.630376638276061\n",
            "17     \t [-0.50528516  0.34587386]. \t  -36.035589207862856 \t -4.630376638276061\n",
            "18     \t [1.9277731  1.12913756]. \t  -9.118576903986702 \t -4.630376638276061\n",
            "19     \t [-1.09194583 -0.07918387]. \t  \u001b[92m-4.033820944267827\u001b[0m \t -4.033820944267827\n",
            "20     \t [ 1.91410593 -1.01678069]. \t  -6.174488341458012 \t -4.033820944267827\n",
            "21     \t [1.74272    0.03011218]. \t  -13.673686571400335 \t -4.033820944267827\n",
            "22     \t [2.02229394 2.02721131]. \t  -8.443010203328365 \t -4.033820944267827\n",
            "23     \t [-0.53863317 -2.420148  ]. \t  -44.62161681402098 \t -4.033820944267827\n",
            "24     \t [ 1.62350871 -1.91524477]. \t  -24.825717124964584 \t -4.033820944267827\n",
            "25     \t [-4.3561877  -2.95820974]. \t  -44.257995777435276 \t -4.033820944267827\n",
            "26     \t [ 1.94844847 -4.83024381]. \t  -32.81676234124274 \t -4.033820944267827\n",
            "27     \t [ 1.61365939 -1.0240869 ]. \t  -21.323503136619774 \t -4.033820944267827\n",
            "28     \t [ 2.60586072 -1.26235543]. \t  -37.027874916145805 \t -4.033820944267827\n",
            "29     \t [1.44387993 0.77895306]. \t  -30.267135956431012 \t -4.033820944267827\n",
            "30     \t [2.41085011 1.65665336]. \t  -42.563089750881275 \t -4.033820944267827\n",
            "31     \t [1.62631533 1.91500993]. \t  -24.716815218896432 \t -4.033820944267827\n",
            "32     \t [1.83917541 2.90896553]. \t  -18.1219217798563 \t -4.033820944267827\n",
            "33     \t [0.15161252 4.87394965]. \t  -30.95853887493251 \t -4.033820944267827\n",
            "34     \t [-1.83457175  3.12906003]. \t  -21.2010306435419 \t -4.033820944267827\n",
            "35     \t [-5.01274326  0.50771474]. \t  -45.40566084486785 \t -4.033820944267827\n",
            "36     \t [-4.59622844  3.03936316]. \t  -48.89451452163108 \t -4.033820944267827\n",
            "37     \t [-0.91500471 -0.15553047]. \t  -6.660617068759318 \t -4.033820944267827\n",
            "38     \t [-0.38477565  3.6586535 ]. \t  -46.455087158854866 \t -4.033820944267827\n",
            "39     \t [ 2.29013029 -2.96698529]. \t  -26.756906666567232 \t -4.033820944267827\n",
            "40     \t [-3.46086441  5.0227901 ]. \t  -57.00754673260547 \t -4.033820944267827\n",
            "41     \t [5.05561119 3.20403173]. \t  -43.58101206202803 \t -4.033820944267827\n",
            "42     \t [-2.75674185 -4.44181715]. \t  -56.2450776773982 \t -4.033820944267827\n",
            "43     \t [ 3.58757526 -4.36907704]. \t  -67.28654284836338 \t -4.033820944267827\n",
            "44     \t [-0.9796294  -0.06753522]. \t  \u001b[92m-1.9329113266396174\u001b[0m \t -1.9329113266396174\n",
            "45     \t [-1.85267738  1.8850234 ]. \t  -13.470532751507955 \t -1.9329113266396174\n",
            "46     \t [-2.02134044 -1.5895831 ]. \t  -25.15963810774884 \t -1.9329113266396174\n",
            "47     \t [ 0.36801143 -1.97696039]. \t  -20.902260249677795 \t -1.9329113266396174\n",
            "48     \t [2.08622833 0.34658087]. \t  -21.607297823572072 \t -1.9329113266396174\n",
            "49     \t [ 1.00657645 -0.28742815]. \t  -13.434410693646095 \t -1.9329113266396174\n",
            "50     \t [-1.09918066 -4.10360976]. \t  -21.972703063915482 \t -1.9329113266396174\n",
            "51     \t [ 0.1668415  -1.53669867]. \t  -27.134125045312054 \t -1.9329113266396174\n",
            "52     \t [ 1.37463189 -2.60925864]. \t  -43.48727813319312 \t -1.9329113266396174\n",
            "53     \t [ 0.47756985 -0.16708196]. \t  -25.1794579683283 \t -1.9329113266396174\n",
            "54     \t [ 0.25904962 -3.03085682]. \t  -20.008855638743665 \t -1.9329113266396174\n",
            "55     \t [-2.69470656  2.43750274]. \t  -45.84644513606298 \t -1.9329113266396174\n",
            "56     \t [0.06841548 1.22212451]. \t  -10.665529994960451 \t -1.9329113266396174\n",
            "57     \t [3.13104671 3.85690408]. \t  -31.65861950032813 \t -1.9329113266396174\n",
            "58     \t [ 5.00585628 -0.50630582]. \t  -45.31386371939543 \t -1.9329113266396174\n",
            "59     \t [ 1.95328851 -1.65668808]. \t  -22.520365671735533 \t -1.9329113266396174\n",
            "60     \t [ 0.54812533 -3.77466704]. \t  -42.55117779695009 \t -1.9329113266396174\n",
            "61     \t [ 1.73761301 -4.14468213]. \t  -34.830361563159926 \t -1.9329113266396174\n",
            "62     \t [ 2.3761028  -0.62406124]. \t  -40.26786299681315 \t -1.9329113266396174\n",
            "63     \t [ 2.79597087 -3.43598777]. \t  -45.97704472913224 \t -1.9329113266396174\n",
            "64     \t [-1.52106643  2.12275423]. \t  -29.562117538958553 \t -1.9329113266396174\n",
            "65     \t [-1.52076294 -4.92615332]. \t  -47.552001843117594 \t -1.9329113266396174\n",
            "66     \t [0.02757456 1.48714338]. \t  -22.32945934109235 \t -1.9329113266396174\n",
            "67     \t [2.05643193 2.63337923]. \t  -28.474810471925945 \t -1.9329113266396174\n",
            "68     \t [ 3.79595833 -5.0327126 ]. \t  -47.100302452259484 \t -1.9329113266396174\n",
            "69     \t [-4.6537185   1.84717857]. \t  -45.022893481265584 \t -1.9329113266396174\n",
            "70     \t [2.73584407 4.7255072 ]. \t  -52.23639292513139 \t -1.9329113266396174\n",
            "71     \t [1.50350493 3.70725784]. \t  -48.65526668759055 \t -1.9329113266396174\n",
            "72     \t [4.05163365 4.24260117]. \t  -44.4723395354371 \t -1.9329113266396174\n",
            "73     \t [-1.27656803  3.15280746]. \t  -27.497161044190484 \t -1.9329113266396174\n",
            "74     \t [-3.85106253 -1.0774818 ]. \t  -21.221726447116367 \t -1.9329113266396174\n",
            "75     \t [-3.3153011   1.75318078]. \t  -37.853857600094905 \t -1.9329113266396174\n",
            "76     \t [1.31573173 5.06056135]. \t  -42.069339490324374 \t -1.9329113266396174\n",
            "77     \t [-2.51041927  3.99142362]. \t  -42.226761497627635 \t -1.9329113266396174\n",
            "78     \t [ 5.01733384 -3.40852246]. \t  -55.2440863195689 \t -1.9329113266396174\n",
            "79     \t [-1.08309974 -0.48167674]. \t  -22.671334699176278 \t -1.9329113266396174\n",
            "80     \t [-2.29352385 -1.27740395]. \t  -31.306084079489505 \t -1.9329113266396174\n",
            "81     \t [-3.75658608 -2.05683505]. \t  -28.55968498585198 \t -1.9329113266396174\n",
            "82     \t [-3.35282173 -3.27749771]. \t  -49.72290938947798 \t -1.9329113266396174\n",
            "83     \t [ 1.86033854 -4.97116125]. \t  -31.946405776659578 \t -1.9329113266396174\n",
            "84     \t [2.9748321  4.13193639]. \t  -29.2910217950936 \t -1.9329113266396174\n",
            "85     \t [ 2.34412642 -5.03078265]. \t  -46.56551522913623 \t -1.9329113266396174\n",
            "86     \t [5.09448978 0.6475435 ]. \t  -44.08637274746762 \t -1.9329113266396174\n",
            "87     \t [-5.03610244 -3.81265309]. \t  -46.31910896982851 \t -1.9329113266396174\n",
            "88     \t [-2.70441357  0.35660938]. \t  -36.47495522202593 \t -1.9329113266396174\n",
            "89     \t [-4.41457825  0.11207418]. \t  -40.47355424260359 \t -1.9329113266396174\n",
            "90     \t [-0.87574263  5.11835921]. \t  -32.50067082351381 \t -1.9329113266396174\n",
            "91     \t [-3.64070877 -0.42773499]. \t  -48.76434247698346 \t -1.9329113266396174\n",
            "92     \t [-3.06206647  0.66138158]. \t  -25.84926454077644 \t -1.9329113266396174\n",
            "93     \t [ 2.69518524 -3.26069108]. \t  -41.94379144631107 \t -1.9329113266396174\n",
            "94     \t [-2.85315173  0.91786875]. \t  -14.24828801649739 \t -1.9329113266396174\n",
            "95     \t [-2.21515196  1.31475803]. \t  -28.420894989222386 \t -1.9329113266396174\n",
            "96     \t [-4.13430253 -3.63815392]. \t  -50.14587690771528 \t -1.9329113266396174\n",
            "97     \t [-2.12100009  1.99900609]. \t  -11.248334099406256 \t -1.9329113266396174\n",
            "98     \t [ 4.30285955 -2.1382469 ]. \t  -39.88851554321876 \t -1.9329113266396174\n",
            "99     \t [0.85672395 0.95567465]. \t  -5.818315137134565 \t -1.9329113266396174\n",
            "100    \t [-2.73872024  0.84717634]. \t  -23.19302869687955 \t -1.9329113266396174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBLcW6tlteS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b46d17-a035-4163-8491-e671e48186b2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.60433145 -4.36322715]. \t  -66.48533678053353 \t -9.873801938301748\n",
            "init   \t [ 4.04410125 -0.65030607]. \t  -33.021397945058965 \t -9.873801938301748\n",
            "init   \t [-3.81258     0.77678463]. \t  -29.63268746352715 \t -9.873801938301748\n",
            "init   \t [ 3.48642222 -0.66436556]. \t  -47.68483871526274 \t -9.873801938301748\n",
            "init   \t [2.00612414 1.89069411]. \t  -9.873801938301748 \t -9.873801938301748\n",
            "1      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -9.873801938301748\n",
            "2      \t [-1.47060593  5.06917331]. \t  -48.61887070266401 \t -9.873801938301748\n",
            "3      \t [5.0007729  4.76270861]. \t  -56.89358387176269 \t -9.873801938301748\n",
            "4      \t [-0.9330719  -3.00671512]. \t  -10.791097604445175 \t -9.873801938301748\n",
            "5      \t [-5.02224898  4.22263019]. \t  -51.439912041877854 \t -9.873801938301748\n",
            "6      \t [-0.44431309  0.13176058]. \t  -22.844460501837744 \t -9.873801938301748\n",
            "7      \t [5.10963258 1.20018625]. \t  -36.74992676197222 \t -9.873801938301748\n",
            "8      \t [-5.12       -1.89414269]. \t  -34.64406974098618 \t -9.873801938301748\n",
            "9      \t [ 0.22844583 -5.12      ]. \t  -37.62674885001399 \t -9.873801938301748\n",
            "10     \t [1.78807378 4.62726797]. \t  -49.20891252631002 \t -9.873801938301748\n",
            "11     \t [ 4.95851093 -2.00695199]. \t  -28.962085042496348 \t -9.873801938301748\n",
            "12     \t [-2.39083103 -4.97417357]. \t  -48.32815877113683 \t -9.873801938301748\n",
            "13     \t [-2.29586283 -1.87202055]. \t  -24.679914483457384 \t -9.873801938301748\n",
            "14     \t [-1.07136534  2.35486938]. \t  -23.80440511856798 \t -9.873801938301748\n",
            "15     \t [ 0.59738941 -2.44439523]. \t  -43.91330075136969 \t -9.873801938301748\n",
            "16     \t [1.06667996 1.62395479]. \t  -21.75727267355915 \t -9.873801938301748\n",
            "17     \t [3.03455069 1.96639877]. \t  -13.531972489814523 \t -9.873801938301748\n",
            "18     \t [2.26151573 1.90544411]. \t  -21.1816360894417 \t -9.873801938301748\n",
            "19     \t [3.9895026  2.17866008]. \t  -26.35061636755934 \t -9.873801938301748\n",
            "20     \t [1.31421423 2.44944244]. \t  -41.15272873400597 \t -9.873801938301748\n",
            "21     \t [3.65629477 1.15830282]. \t  -34.815521907001205 \t -9.873801938301748\n",
            "22     \t [-2.35360049  2.54290074]. \t  -47.70403194445671 \t -9.873801938301748\n",
            "23     \t [3.35888262 2.30350143]. \t  -46.2068379810009 \t -9.873801938301748\n",
            "24     \t [3.246429   1.59142349]. \t  -41.24259385338464 \t -9.873801938301748\n",
            "25     \t [4.30750286 1.4229912 ]. \t  -52.96645739963254 \t -9.873801938301748\n",
            "26     \t [4.91755598 2.50763125]. \t  -51.77102096161019 \t -9.873801938301748\n",
            "27     \t [4.04758242 0.22901787]. \t  -25.56443599541473 \t -9.873801938301748\n",
            "28     \t [1.55704731 1.92594195]. \t  -26.561326978415412 \t -9.873801938301748\n",
            "29     \t [1.01950357 0.37793362]. \t  -18.45740989115404 \t -9.873801938301748\n",
            "30     \t [0.96532968 3.96131183]. \t  -17.15418689915885 \t -9.873801938301748\n",
            "31     \t [-0.28351899  1.08929451]. \t  -14.890523232323437 \t -9.873801938301748\n",
            "32     \t [0.46813619 1.03418431]. \t  -21.318724981785273 \t -9.873801938301748\n",
            "33     \t [1.04771007 3.29385755]. \t  -25.1140612514117 \t -9.873801938301748\n",
            "34     \t [1.39447416 0.93162774]. \t  -21.602402700476418 \t -9.873801938301748\n",
            "35     \t [-0.36574671  2.05609194]. \t  -21.62417747459334 \t -9.873801938301748\n",
            "36     \t [ 0.88152451 -0.72065701]. \t  -15.774757376178549 \t -9.873801938301748\n",
            "37     \t [-1.62178473  0.88734413]. \t  -23.032335676511284 \t -9.873801938301748\n",
            "38     \t [1.81254661 2.47946272]. \t  -35.52038040318576 \t -9.873801938301748\n",
            "39     \t [0.6728194  5.05731694]. \t  -41.33219391711624 \t -9.873801938301748\n",
            "40     \t [-1.4634363   2.69394771]. \t  -42.58577765182104 \t -9.873801938301748\n",
            "41     \t [3.63303549 1.79976143]. \t  -40.067379360596604 \t -9.873801938301748\n",
            "42     \t [ 4.00913475 -1.79204673]. \t  -26.689810228227742 \t -9.873801938301748\n",
            "43     \t [-1.81414107  1.98331087]. \t  -13.35766661270995 \t -9.873801938301748\n",
            "44     \t [1.13859728 4.48368815]. \t  -44.90548684793309 \t -9.873801938301748\n",
            "45     \t [-0.4120216   4.81816139]. \t  -47.742115730613165 \t -9.873801938301748\n",
            "46     \t [ 4.41394653 -1.39428141]. \t  -57.874279173995966 \t -9.873801938301748\n",
            "47     \t [ 5.02345154 -2.76296772]. \t  -42.16353557538893 \t -9.873801938301748\n",
            "48     \t [2.77243059 4.95747919]. \t  -41.21304760842309 \t -9.873801938301748\n",
            "49     \t [0.11740104 1.54308442]. \t  -24.630217949879096 \t -9.873801938301748\n",
            "50     \t [-0.89715367  0.68670778]. \t  -17.16546451414776 \t -9.873801938301748\n",
            "51     \t [ 0.26530044 -0.510048  ]. \t  -31.270485877170678 \t -9.873801938301748\n",
            "52     \t [-2.41205339  4.85987948]. \t  -51.579732786004335 \t -9.873801938301748\n",
            "53     \t [-1.57820444  1.57172095]. \t  -42.779597391119026 \t -9.873801938301748\n",
            "54     \t [-2.87655276  0.64864616]. \t  -27.50203758067299 \t -9.873801938301748\n",
            "55     \t [ 0.65438186 -0.14125089]. \t  -19.787596770767138 \t -9.873801938301748\n",
            "56     \t [-0.77293062 -1.81599714]. \t  -18.43057344555402 \t -9.873801938301748\n",
            "57     \t [-4.85577859  0.92891385]. \t  -29.25481231847239 \t -9.873801938301748\n",
            "58     \t [-0.07565783  4.46310211]. \t  -40.766241400026956 \t -9.873801938301748\n",
            "59     \t [-0.11434098  5.07329637]. \t  -29.2648302706344 \t -9.873801938301748\n",
            "60     \t [-2.18822801  0.56401193]. \t  -30.523942184424172 \t -9.873801938301748\n",
            "61     \t [-2.25817023  1.17160451]. \t  -22.256166084510056 \t -9.873801938301748\n",
            "62     \t [-2.2990822  -3.14750741]. \t  -32.22402592780886 \t -9.873801938301748\n",
            "63     \t [ 3.11804752 -1.99338943]. \t  -16.331332018937857 \t -9.873801938301748\n",
            "64     \t [3.86855927 0.69978136]. \t  -31.779474277738924 \t -9.873801938301748\n",
            "65     \t [ 2.00118553 -1.12036045]. \t  \u001b[92m-7.9860641993901\u001b[0m \t -7.9860641993901\n",
            "66     \t [0.08416039 2.26589764]. \t  -17.50443876523197 \t -7.9860641993901\n",
            "67     \t [4.55830807 0.35167128]. \t  -56.20069950479944 \t -7.9860641993901\n",
            "68     \t [-3.39099336  5.04899065]. \t  -55.20593788777555 \t -7.9860641993901\n",
            "69     \t [4.61779535 1.71436889]. \t  -53.86701713500848 \t -7.9860641993901\n",
            "70     \t [4.3784398  3.29148681]. \t  -59.804124115906475 \t -7.9860641993901\n",
            "71     \t [5.12 5.12]. \t  -57.849427451571785 \t -7.9860641993901\n",
            "72     \t [ 4.09883096 -3.03069251]. \t  -28.037760265206593 \t -7.9860641993901\n",
            "73     \t [4.31618044 2.35382468]. \t  -54.279895261058066 \t -7.9860641993901\n",
            "74     \t [ 4.82796542 -0.67054978]. \t  -43.84100206513172 \t -7.9860641993901\n",
            "75     \t [5.06498737 3.68514571]. \t  -54.019609871864404 \t -7.9860641993901\n",
            "76     \t [4.4476495  4.24372192]. \t  -66.86029246546589 \t -7.9860641993901\n",
            "77     \t [2.52097868 5.04761329]. \t  -52.191151272228176 \t -7.9860641993901\n",
            "78     \t [ 2.70352064 -2.48180331]. \t  -46.28213161987091 \t -7.9860641993901\n",
            "79     \t [1.007124   1.12395949]. \t  \u001b[92m-5.170454761426291\u001b[0m \t -5.170454761426291\n",
            "80     \t [ 1.86091071 -0.63036053]. \t  -24.27105832824839 \t -5.170454761426291\n",
            "81     \t [-0.0151987  -1.55505406]. \t  -21.871643640203267 \t -5.170454761426291\n",
            "82     \t [ 4.12958345 -2.10098651]. \t  -26.549495858908244 \t -5.170454761426291\n",
            "83     \t [ 1.36539889 -2.19737525]. \t  -30.078081147743205 \t -5.170454761426291\n",
            "84     \t [ 4.78637367 -4.28519907]. \t  -61.20035953340241 \t -5.170454761426291\n",
            "85     \t [ 4.07314525 -3.69112807]. \t  -44.86782502738652 \t -5.170454761426291\n",
            "86     \t [ 2.55815314 -5.03495926]. \t  -51.47509224627508 \t -5.170454761426291\n",
            "87     \t [1.41388837 2.88894135]. \t  -31.25415570912078 \t -5.170454761426291\n",
            "88     \t [ 2.79848864 -3.60556473]. \t  -45.71167044765529 \t -5.170454761426291\n",
            "89     \t [3.59609007 0.35628265]. \t  -47.48346613814378 \t -5.170454761426291\n",
            "90     \t [ 0.12643688 -3.59375989]. \t  -34.23850564779744 \t -5.170454761426291\n",
            "91     \t [ 1.17408713 -0.405225  ]. \t  -25.230495970800597 \t -5.170454761426291\n",
            "92     \t [-1.46477384 -2.35604272]. \t  -43.63326395324212 \t -5.170454761426291\n",
            "93     \t [-3.60340465 -2.37942552]. \t  -53.87370956364633 \t -5.170454761426291\n",
            "94     \t [ 1.03164676 -1.81319182]. \t  -10.68204329717382 \t -5.170454761426291\n",
            "95     \t [-1.16203855 -3.65220655]. \t  -35.20438903062877 \t -5.170454761426291\n",
            "96     \t [-0.33369     0.62452769]. \t  -32.61279940895708 \t -5.170454761426291\n",
            "97     \t [-1.079541   -1.03440772]. \t  \u001b[92m-3.691266064236048\u001b[0m \t -3.691266064236048\n",
            "98     \t [-2.82923427 -2.15462361]. \t  -22.231271792492223 \t -3.691266064236048\n",
            "99     \t [-0.66192419 -0.77542043]. \t  -24.704789128550164 \t -3.691266064236048\n",
            "100    \t [ 1.36551317 -3.14496524]. \t  -32.26190659420569 \t -3.691266064236048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emgjXwfeuvRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99add281-6853-42e0-e0ca-571006b4263a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.57613422 -4.00126694]. \t  -41.5244070968694 \t -3.7496552211609124\n",
            "init   \t [-0.1421181  -0.00172874]. \t  -3.7496552211609124 \t -3.7496552211609124\n",
            "init   \t [ 1.60880639 -2.70454056]. \t  -40.473138462528404 \t -3.7496552211609124\n",
            "init   \t [ 1.15501968 -3.89475939]. \t  -22.991135000503405 \t -3.7496552211609124\n",
            "init   \t [ 2.17295152 -1.30498005]. \t  -25.1566837825139 \t -3.7496552211609124\n",
            "1      \t [-1.29658024 -5.12      ]. \t  -43.490952790351386 \t -3.7496552211609124\n",
            "2      \t [-4.60158607  3.93941673]. \t  -55.4405800500881 \t -3.7496552211609124\n",
            "3      \t [3.07292341 4.60801938]. \t  -49.49201941569163 \t -3.7496552211609124\n",
            "4      \t [-5.12       -1.45932783]. \t  -50.729593667972956 \t -3.7496552211609124\n",
            "5      \t [-0.74375259  3.10516972]. \t  -22.69267243710091 \t -3.7496552211609124\n",
            "6      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -3.7496552211609124\n",
            "7      \t [4.46372469 0.99552617]. \t  -40.661235793899664 \t -3.7496552211609124\n",
            "8      \t [-1.75396136 -1.65159112]. \t  -31.351940172242188 \t -3.7496552211609124\n",
            "9      \t [-3.63028952  1.12823393]. \t  -34.358174527335585 \t -3.7496552211609124\n",
            "10     \t [1.43244118 1.16365366]. \t  -27.35544501745627 \t -3.7496552211609124\n",
            "11     \t [ 4.95283041 -3.59721222]. \t  -56.09833180478792 \t -3.7496552211609124\n",
            "12     \t [-0.80886904  0.4907103 ]. \t  -27.262953140846346 \t -3.7496552211609124\n",
            "13     \t [ 0.27396531 -0.53349631]. \t  -31.63911800107208 \t -3.7496552211609124\n",
            "14     \t [0.22560531 0.25837577]. \t  -19.116909055710224 \t -3.7496552211609124\n",
            "15     \t [-0.27820002  4.77019742]. \t  -43.329143795063864 \t -3.7496552211609124\n",
            "16     \t [ 3.55430617 -1.19400725]. \t  -40.036229304722056 \t -3.7496552211609124\n",
            "17     \t [-0.9787911  -0.67789248]. \t  -15.883462016984023 \t -3.7496552211609124\n",
            "18     \t [-0.49745077 -0.33062573]. \t  -35.20743979435045 \t -3.7496552211609124\n",
            "19     \t [-1.91677138 -0.8271172 ]. \t  -11.036565376364884 \t -3.7496552211609124\n",
            "20     \t [2.19978901 2.43379683]. \t  -36.80692402968803 \t -3.7496552211609124\n",
            "21     \t [2.10879145 1.33593328]. \t  -23.61920637625763 \t -3.7496552211609124\n",
            "22     \t [-1.49825434 -1.01433907]. \t  -23.313606419374636 \t -3.7496552211609124\n",
            "23     \t [-2.00867291 -2.77892306]. \t  -19.96472094605075 \t -3.7496552211609124\n",
            "24     \t [-2.95430931 -0.10757273]. \t  -11.347330474567833 \t -3.7496552211609124\n",
            "25     \t [-0.62286975 -2.33226065]. \t  -37.93400058203216 \t -3.7496552211609124\n",
            "26     \t [-3.22189261 -2.00290336]. \t  -22.637004245230237 \t -3.7496552211609124\n",
            "27     \t [-1.9778777  -0.11322973]. \t  -6.447066172025886 \t -3.7496552211609124\n",
            "28     \t [1.55871856 1.79509031]. \t  -32.18369559920885 \t -3.7496552211609124\n",
            "29     \t [-3.08040017 -1.00828678]. \t  -11.768133399844608 \t -3.7496552211609124\n",
            "30     \t [ 1.246265   -1.37901004]. \t  -30.467155833360636 \t -3.7496552211609124\n",
            "31     \t [2.69262146 3.06252393]. \t  -30.91865164869435 \t -3.7496552211609124\n",
            "32     \t [-2.57469669 -2.1066159 ]. \t  -32.14667913586097 \t -3.7496552211609124\n",
            "33     \t [-1.46103328 -2.49191733]. \t  -48.03315051074774 \t -3.7496552211609124\n",
            "34     \t [-4.34654356 -2.77688374]. \t  -50.62317224968111 \t -3.7496552211609124\n",
            "35     \t [-0.15443169 -1.96534109]. \t  -8.472262992884652 \t -3.7496552211609124\n",
            "36     \t [-2.62570477 -4.38734889]. \t  -60.780731996204345 \t -3.7496552211609124\n",
            "37     \t [-2.58528705 -0.73194591]. \t  -36.949619573304155 \t -3.7496552211609124\n",
            "38     \t [-4.23352345 -1.13878613]. \t  -31.75333024401879 \t -3.7496552211609124\n",
            "39     \t [-1.59598044 -0.58679886]. \t  -39.67684641536538 \t -3.7496552211609124\n",
            "40     \t [-2.20739869  0.39557506]. \t  -30.307862889716887 \t -3.7496552211609124\n",
            "41     \t [-0.63022787  1.0069362 ]. \t  -18.25562971023893 \t -3.7496552211609124\n",
            "42     \t [-0.64997346 -1.14076442]. \t  -21.26585199453262 \t -3.7496552211609124\n",
            "43     \t [-3.90754585  0.25333349]. \t  -27.18287172025826 \t -3.7496552211609124\n",
            "44     \t [ 0.20783471 -3.12955966]. \t  -20.353286347872697 \t -3.7496552211609124\n",
            "45     \t [-1.56250888 -3.5391768 ]. \t  -53.904353227059055 \t -3.7496552211609124\n",
            "46     \t [ 1.42727216 -0.37602287]. \t  -38.26883420282185 \t -3.7496552211609124\n",
            "47     \t [ 0.1122817  -2.40712532]. \t  -26.539024510607355 \t -3.7496552211609124\n",
            "48     \t [-3.56624251 -1.56549436]. \t  -53.480286224384244 \t -3.7496552211609124\n",
            "49     \t [-3.79257524 -3.32553385]. \t  -47.369278022674415 \t -3.7496552211609124\n",
            "50     \t [-3.99661149  2.1082123 ]. \t  -22.64348588385567 \t -3.7496552211609124\n",
            "51     \t [ 3.09084983 -2.26886381]. \t  -27.469022778448533 \t -3.7496552211609124\n",
            "52     \t [-4.03979067 -0.71763654]. \t  -29.165288011397955 \t -3.7496552211609124\n",
            "53     \t [-2.42437496 -1.621529  ]. \t  -44.62270424328746 \t -3.7496552211609124\n",
            "54     \t [-3.3329942  -2.79053234]. \t  -41.358174085166155 \t -3.7496552211609124\n",
            "55     \t [-4.90381066 -3.98504293]. \t  -41.74348140791588 \t -3.7496552211609124\n",
            "56     \t [-0.24142116 -4.49160396]. \t  -49.680115191993444 \t -3.7496552211609124\n",
            "57     \t [-4.81716885  2.26895144]. \t  -45.44502723775114 \t -3.7496552211609124\n",
            "58     \t [-4.50199716 -3.6863829 ]. \t  -67.74819639994153 \t -3.7496552211609124\n",
            "59     \t [-3.91066225  0.79164109]. \t  -24.86783448809277 \t -3.7496552211609124\n",
            "60     \t [-3.38910779  0.25828415]. \t  -39.74231376973753 \t -3.7496552211609124\n",
            "61     \t [-3.42539582  2.01101829]. \t  -34.72281395484943 \t -3.7496552211609124\n",
            "62     \t [-0.37925501  0.56509182]. \t  -36.89598014107493 \t -3.7496552211609124\n",
            "63     \t [ 2.07359766 -2.11565045]. \t  -12.352042537747856 \t -3.7496552211609124\n",
            "64     \t [ 0.62899234 -0.13099465]. \t  -20.50449748271873 \t -3.7496552211609124\n",
            "65     \t [ 2.26048671 -2.69131169]. \t  -36.615874373300535 \t -3.7496552211609124\n",
            "66     \t [ 2.90623141 -4.79711916]. \t  -40.22696245151463 \t -3.7496552211609124\n",
            "67     \t [-1.26009289  1.14860771]. \t  -17.59246421129682 \t -3.7496552211609124\n",
            "68     \t [2.16633108 0.30945617]. \t  -23.419957884505745 \t -3.7496552211609124\n",
            "69     \t [-1.10073887  0.85609878]. \t  -7.698260725460013 \t -3.7496552211609124\n",
            "70     \t [-1.00974011  1.27739782]. \t  -14.383007304385227 \t -3.7496552211609124\n",
            "71     \t [ 4.46998424 -1.77725003]. \t  -51.258247042563134 \t -3.7496552211609124\n",
            "72     \t [-2.11423978  2.04036495]. \t  -11.420382089912792 \t -3.7496552211609124\n",
            "73     \t [-3.61530961 -0.49848662]. \t  -50.80673208159552 \t -3.7496552211609124\n",
            "74     \t [-4.63969117 -0.91979433]. \t  -40.00510103310002 \t -3.7496552211609124\n",
            "75     \t [-1.08228978  3.55860939]. \t  -34.47177456781044 \t -3.7496552211609124\n",
            "76     \t [-0.02023182  3.38972835]. \t  -29.26559831598165 \t -3.7496552211609124\n",
            "77     \t [-4.98514787 -3.19809382]. \t  -41.919161191817544 \t -3.7496552211609124\n",
            "78     \t [ 1.70972077 -0.99331388]. \t  -16.422528947368598 \t -3.7496552211609124\n",
            "79     \t [ 2.07204952 -0.43052926]. \t  -24.54843213010131 \t -3.7496552211609124\n",
            "80     \t [ 2.8835384  -1.64303464]. \t  -29.800467585121215 \t -3.7496552211609124\n",
            "81     \t [ 3.79946217 -3.50060451]. \t  -53.63205927848491 \t -3.7496552211609124\n",
            "82     \t [1.83187616 0.97244476]. \t  -9.530422369531099 \t -3.7496552211609124\n",
            "83     \t [0.22514733 2.00500019]. \t  -12.520450835263523 \t -3.7496552211609124\n",
            "84     \t [-5.12       -4.67930924]. \t  -65.11767235136644 \t -3.7496552211609124\n",
            "85     \t [2.93563254 2.35363614]. \t  -31.025418426714836 \t -3.7496552211609124\n",
            "86     \t [ 4.40198383 -0.86654522]. \t  -41.60553424347823 \t -3.7496552211609124\n",
            "87     \t [-1.91373833 -4.34408256]. \t  -39.539714721001275 \t -3.7496552211609124\n",
            "88     \t [-0.57877535  3.54494586]. \t  -51.3053840364146 \t -3.7496552211609124\n",
            "89     \t [1.678889   4.96111976]. \t  -42.049139924358336 \t -3.7496552211609124\n",
            "90     \t [-2.4044759  -0.05931519]. \t  -24.723755586823685 \t -3.7496552211609124\n",
            "91     \t [-0.43202616  1.90998973]. \t  -24.493514618654253 \t -3.7496552211609124\n",
            "92     \t [-5.07154364  0.54970365]. \t  -46.532482397413744 \t -3.7496552211609124\n",
            "93     \t [-3.21252733 -4.99289605]. \t  -42.92651904632036 \t -3.7496552211609124\n",
            "94     \t [-0.36505376 -3.91067302]. \t  -33.57641749956034 \t -3.7496552211609124\n",
            "95     \t [-1.69461384  2.03811375]. \t  -20.721197325881167 \t -3.7496552211609124\n",
            "96     \t [0.6225781  5.05097911]. \t  -43.58646594724203 \t -3.7496552211609124\n",
            "97     \t [-4.87754355  0.37972404]. \t  -44.029260272762976 \t -3.7496552211609124\n",
            "98     \t [-4.29841519  1.52293536]. \t  -53.687365733260535 \t -3.7496552211609124\n",
            "99     \t [-5.11858079 -2.0429406 ]. \t  -33.38480540790288 \t -3.7496552211609124\n",
            "100    \t [-1.89613244  0.31627278]. \t  -19.795131759540578 \t -3.7496552211609124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8riJpBBKuvT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "746ac0fe-f26c-4472-9f84-98f0d93acc39"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.47505713 3.29006044]. \t  -55.26854109146732 \t -23.867844182996272\n",
            "init   \t [1.45620137 1.71235961]. \t  -37.01945021627456 \t -23.867844182996272\n",
            "init   \t [-4.71920737 -4.44751474]. \t  -73.43516933853265 \t -23.867844182996272\n",
            "init   \t [-2.28863345 -1.88046844]. \t  -23.867844182996272 \t -23.867844182996272\n",
            "init   \t [-3.1376137  -3.10176889]. \t  -24.952199788254394 \t -23.867844182996272\n",
            "1      \t [-4.12361753  4.5902107 ]. \t  -59.378212489103646 \t -23.867844182996272\n",
            "2      \t [ 3.54522045 -4.88293993]. \t  -58.59586778762512 \t -23.867844182996272\n",
            "3      \t [ 4.42097765 -1.01567924]. \t  -39.417629242831794 \t -23.867844182996272\n",
            "4      \t [-5.12        0.59212945]. \t  -47.646164574243876 \t -23.867844182996272\n",
            "5      \t [-0.84767894 -5.12      ]. \t  -33.884025805414396 \t -23.867844182996272\n",
            "6      \t [ 0.9115617  -1.78628421]. \t  \u001b[92m-13.266199865399482\u001b[0m \t -13.266199865399482\n",
            "7      \t [-0.65031695  4.7190384 ]. \t  -50.48709236328925 \t -13.266199865399482\n",
            "8      \t [-1.3660993   0.71078281]. \t  -31.475439439526575 \t -13.266199865399482\n",
            "9      \t [ 1.00973084 -3.67038876]. \t  -29.306112531142254 \t -13.266199865399482\n",
            "10     \t [-2.9837456  2.2529915]. \t  -24.218765508724676 \t -13.266199865399482\n",
            "11     \t [-5.12       -2.00290321]. \t  -32.937998693146554 \t -13.266199865399482\n",
            "12     \t [4.97636287 1.60560303]. \t  -45.33050221229761 \t -13.266199865399482\n",
            "13     \t [4.89055198 4.85688287]. \t  -53.55749842224593 \t -13.266199865399482\n",
            "14     \t [ 0.7525904 -0.5832699]. \t  -29.406089869355533 \t -13.266199865399482\n",
            "15     \t [ 2.59961015 -2.40561761]. \t  -48.942067695421876 \t -13.266199865399482\n",
            "16     \t [-0.0611718  -2.25629894]. \t  -16.219888373079858 \t -13.266199865399482\n",
            "17     \t [-3.0613751  -0.41516049]. \t  -28.89131262597595 \t -13.266199865399482\n",
            "18     \t [2.05573661 4.68146024]. \t  -40.923690995060845 \t -13.266199865399482\n",
            "19     \t [-0.67458128  2.50831923]. \t  -41.296397693400905 \t -13.266199865399482\n",
            "20     \t [ 0.58502171 -1.80186325]. \t  -28.994390239741094 \t -13.266199865399482\n",
            "21     \t [-0.94798418 -2.70700017]. \t  -21.424869722056844 \t -13.266199865399482\n",
            "22     \t [ 1.71499414 -1.18832666]. \t  -22.756320527530473 \t -13.266199865399482\n",
            "23     \t [ 0.74818395 -2.55280816]. \t  -36.645275811170166 \t -13.266199865399482\n",
            "24     \t [ 0.95536898 -4.56950472]. \t  -41.245200939415014 \t -13.266199865399482\n",
            "25     \t [-0.45660167 -2.54030884]. \t  -45.97316829440564 \t -13.266199865399482\n",
            "26     \t [-1.26258542 -1.75657227]. \t  -25.056778819286365 \t -13.266199865399482\n",
            "27     \t [-2.24212591 -2.69709454]. \t  -35.07017196519335 \t -13.266199865399482\n",
            "28     \t [-1.7116958  -3.57871737]. \t  -46.92228358813905 \t -13.266199865399482\n",
            "29     \t [0.56123205 0.45242645]. \t  -39.34523068084263 \t -13.266199865399482\n",
            "30     \t [ 1.74051225 -3.57234376]. \t  -45.371392654119376 \t -13.266199865399482\n",
            "31     \t [-4.01813602 -2.68084576]. \t  -37.60685619212391 \t -13.266199865399482\n",
            "32     \t [-1.438675 -2.386428]. \t  -44.59176363804696 \t -13.266199865399482\n",
            "33     \t [-1.01283391 -0.81153321]. \t  \u001b[92m-7.946270791601196\u001b[0m \t -7.946270791601196\n",
            "34     \t [-3.22287329 -1.73071769]. \t  -32.894696501565896 \t -7.946270791601196\n",
            "35     \t [ 1.56683594 -1.96245729]. \t  -25.71426464866837 \t -7.946270791601196\n",
            "36     \t [ 1.13358747 -0.20415362]. \t  -11.806328422902707 \t -7.946270791601196\n",
            "37     \t [ 3.93019982 -2.6531543 ]. \t  -39.14845842475623 \t -7.946270791601196\n",
            "38     \t [-1.46847645 -3.13682606]. \t  -35.27396463362152 \t -7.946270791601196\n",
            "39     \t [-0.45533369 -2.12697346]. \t  -27.35726193607723 \t -7.946270791601196\n",
            "40     \t [-1.89605078 -0.18827884]. \t  -11.907036620515544 \t -7.946270791601196\n",
            "41     \t [-2.32197768 -4.9181665 ]. \t  -45.242851006991394 \t -7.946270791601196\n",
            "42     \t [-1.520818   -1.01084409]. \t  -23.272470677788462 \t -7.946270791601196\n",
            "43     \t [-3.13511792 -2.31982369]. \t  -32.850753614064445 \t -7.946270791601196\n",
            "44     \t [-5.05479084 -3.02500569]. \t  -35.41149128296962 \t -7.946270791601196\n",
            "45     \t [-2.83574967  0.2663016 ]. \t  -24.003961688660826 \t -7.946270791601196\n",
            "46     \t [1.74416935 2.35181501]. \t  -34.909157912722165 \t -7.946270791601196\n",
            "47     \t [ 1.31228945 -4.1296772 ]. \t  -35.730704064350114 \t -7.946270791601196\n",
            "48     \t [-1.90676855  0.25620599]. \t  -15.758489377398769 \t -7.946270791601196\n",
            "49     \t [-0.84284601 -0.12100559]. \t  -7.970356515953692 \t -7.946270791601196\n",
            "50     \t [-4.5923221  -2.30753018]. \t  -58.3148451066831 \t -7.946270791601196\n",
            "51     \t [-4.47963929 -3.29296355]. \t  -63.49587082834549 \t -7.946270791601196\n",
            "52     \t [-5.12 -5.12]. \t  -57.849427451571785 \t -7.946270791601196\n",
            "53     \t [-2.28098811 -0.21725135]. \t  -25.141697598471332 \t -7.946270791601196\n",
            "54     \t [-0.96044163 -4.18166117]. \t  -24.552921374188273 \t -7.946270791601196\n",
            "55     \t [-3.87377409  1.29192968]. \t  -32.26296486654816 \t -7.946270791601196\n",
            "56     \t [ 0.48528551 -2.28405067]. \t  -37.532863621792124 \t -7.946270791601196\n",
            "57     \t [ 1.12911371 -0.92429089]. \t  \u001b[92m-6.353503217043878\u001b[0m \t -6.353503217043878\n",
            "58     \t [2.01828025 1.11944075]. \t  -8.078796955315791 \t -6.353503217043878\n",
            "59     \t [ 3.21729455 -2.71216818]. \t  -38.02104985742018 \t -6.353503217043878\n",
            "60     \t [ 1.25352349 -3.13773597]. \t  -25.15487804265485 \t -6.353503217043878\n",
            "61     \t [ 3.36197673 -1.82718008]. \t  -36.44939538518789 \t -6.353503217043878\n",
            "62     \t [ 4.5080184  -2.38155909]. \t  -63.33775919579998 \t -6.353503217043878\n",
            "63     \t [ 2.01929525 -4.71254732]. \t  -38.690618528147795 \t -6.353503217043878\n",
            "64     \t [-2.42941685 -4.06870324]. \t  -42.406359937292024 \t -6.353503217043878\n",
            "65     \t [-1.61249861 -4.88698787]. \t  -46.50379650264597 \t -6.353503217043878\n",
            "66     \t [ 4.8818638  -4.03915016]. \t  -43.079006405466686 \t -6.353503217043878\n",
            "67     \t [1.40043939 0.56844368]. \t  -39.48019786492275 \t -6.353503217043878\n",
            "68     \t [-3.98047191 -1.12191902]. \t  -19.97142068512744 \t -6.353503217043878\n",
            "69     \t [ 4.05978142 -0.2298161 ]. \t  -25.967033652925302 \t -6.353503217043878\n",
            "70     \t [ 4.92625307 -0.2008433 ]. \t  -32.32304044910741 \t -6.353503217043878\n",
            "71     \t [ 1.18649003 -1.37856489]. \t  -26.6504799238547 \t -6.353503217043878\n",
            "72     \t [2.0127716 0.0571093]. \t  \u001b[92m-4.7236018176569825\u001b[0m \t -4.7236018176569825\n",
            "73     \t [ 2.45829411 -3.89201244]. \t  -43.06447726581606 \t -4.7236018176569825\n",
            "74     \t [ 2.52202049 -0.53771897]. \t  -46.27464398110102 \t -4.7236018176569825\n",
            "75     \t [-0.53718032  1.11867612]. \t  -23.92199230568399 \t -4.7236018176569825\n",
            "76     \t [-3.68047529 -1.40206834]. \t  -47.90831870929716 \t -4.7236018176569825\n",
            "77     \t [-5.12      -1.2744454]. \t  -42.078842767576205 \t -4.7236018176569825\n",
            "78     \t [-4.34179742 -0.44582836]. \t  -53.92955682235787 \t -4.7236018176569825\n",
            "79     \t [-5.04925599  1.12683599]. \t  -30.250810199005244 \t -4.7236018176569825\n",
            "80     \t [ 1.19233762 -2.10041722]. \t  -14.2143972765043 \t -4.7236018176569825\n",
            "81     \t [ 2.44332478 -1.38217668]. \t  -44.63550431779259 \t -4.7236018176569825\n",
            "82     \t [2.1322616  3.17012289]. \t  -23.044252810108876 \t -4.7236018176569825\n",
            "83     \t [4.36134537 0.32138444]. \t  -49.900104418768805 \t -4.7236018176569825\n",
            "84     \t [-1.50934086 -4.34119494]. \t  -56.5283744759241 \t -4.7236018176569825\n",
            "85     \t [ 4.35456148 -4.87949685]. \t  -61.610962993418944 \t -4.7236018176569825\n",
            "86     \t [ 0.83618167 -5.01165949]. \t  -30.688572348172585 \t -4.7236018176569825\n",
            "87     \t [5.08992642 2.51622586]. \t  -53.741063884998766 \t -4.7236018176569825\n",
            "88     \t [2.46788009 2.34384146]. \t  -46.941547674508044 \t -4.7236018176569825\n",
            "89     \t [-1.43716933  2.72784065]. \t  -40.125213674689235 \t -4.7236018176569825\n",
            "90     \t [0.62189443 3.92823855]. \t  -34.0249064485392 \t -4.7236018176569825\n",
            "91     \t [ 1.83639747 -0.44242728]. \t  -27.755126739301325 \t -4.7236018176569825\n",
            "92     \t [2.93619809 0.56452977]. \t  -28.922010664533367 \t -4.7236018176569825\n",
            "93     \t [-1.26612237 -0.27499054]. \t  -24.253710067983597 \t -4.7236018176569825\n",
            "94     \t [-0.39781691  3.3171562 ]. \t  -43.26601594246641 \t -4.7236018176569825\n",
            "95     \t [ 3.85368127 -1.92423733]. \t  -23.601950661995886 \t -4.7236018176569825\n",
            "96     \t [ 2.61430401 -0.0245329 ]. \t  -24.483713428706757 \t -4.7236018176569825\n",
            "97     \t [0.69811059 2.36605648]. \t  -35.9512010276809 \t -4.7236018176569825\n",
            "98     \t [2.01666962 0.77721635]. \t  -13.024090703757349 \t -4.7236018176569825\n",
            "99     \t [1.12225058 3.33886088]. \t  -30.51297806027545 \t -4.7236018176569825\n",
            "100    \t [3.71821336 1.22428638]. \t  -35.69933506604409 \t -4.7236018176569825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrvkDXP0uvWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149c6bdd-5fda-49e6-9123-ef254e563e9f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.58729753 0.42762882]. \t  -38.044660353205785 \t -26.56648173948617\n",
            "init   \t [ 0.61873954 -1.20782752]. \t  -26.56648173948617 \t -26.56648173948617\n",
            "init   \t [-3.90047561  3.54303094]. \t  -49.29579717740555 \t -26.56648173948617\n",
            "init   \t [2.45495411 4.42734146]. \t  -64.2061730192077 \t -26.56648173948617\n",
            "init   \t [3.67951879 2.13099137]. \t  -35.56518260713154 \t -26.56648173948617\n",
            "1      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -26.56648173948617\n",
            "2      \t [ 2.73639949 -5.12      ]. \t  -47.2661015636547 \t -26.56648173948617\n",
            "3      \t [-5.12       -0.67939981]. \t  -43.678182405521426 \t -26.56648173948617\n",
            "4      \t [ 4.92333906 -1.75959437]. \t  -37.870770463318976 \t -26.56648173948617\n",
            "5      \t [-1.18540045 -4.83463217]. \t  -35.7599597974686 \t -26.56648173948617\n",
            "6      \t [-0.79412696  4.51378312]. \t  -48.230206955681915 \t -26.56648173948617\n",
            "7      \t [-2.27088916 -1.79767277]. \t  -26.74652701170146 \t -26.56648173948617\n",
            "8      \t [-2.0720228   1.07619305]. \t  \u001b[92m-7.582260282917048\u001b[0m \t -7.582260282917048\n",
            "9      \t [ 2.3777653  -2.80707118]. \t  -37.21673824087282 \t -7.582260282917048\n",
            "10     \t [4.93733356 3.73341919]. \t  -50.12081265094343 \t -7.582260282917048\n",
            "11     \t [ 4.99260986 -3.88956432]. \t  -42.37798791709419 \t -7.582260282917048\n",
            "12     \t [-1.70101179  1.62411192]. \t  -35.67124101701307 \t -7.582260282917048\n",
            "13     \t [-2.73470653  0.51243388]. \t  -38.67014557795256 \t -7.582260282917048\n",
            "14     \t [2.88480569 0.20108374]. \t  -17.844169113906432 \t -7.582260282917048\n",
            "15     \t [-1.53272509  0.33767008]. \t  -37.48671186237569 \t -7.582260282917048\n",
            "16     \t [-3.41332716 -3.30233411]. \t  -54.33896552237682 \t -7.582260282917048\n",
            "17     \t [5.11337923 0.2313637 ]. \t  -37.46383119014373 \t -7.582260282917048\n",
            "18     \t [-2.4168688   1.02668214]. \t  -25.702130020635277 \t -7.582260282917048\n",
            "19     \t [-2.1022277   0.91645538]. \t  -8.598535492124784 \t -7.582260282917048\n",
            "20     \t [ 0.389215   -3.82618972]. \t  -37.85843298589019 \t -7.582260282917048\n",
            "21     \t [1.67637546 2.38933378]. \t  -40.66026640342736 \t -7.582260282917048\n",
            "22     \t [-3.57164611 -0.37263478]. \t  -48.86446935386695 \t -7.582260282917048\n",
            "23     \t [-4.65989181  5.01737593]. \t  -62.31219722200768 \t -7.582260282917048\n",
            "24     \t [-4.97900942  1.2772519 ]. \t  -38.21268575276821 \t -7.582260282917048\n",
            "25     \t [-0.71012131 -2.56963631]. \t  -38.64479485157471 \t -7.582260282917048\n",
            "26     \t [ 2.07474945 -1.0799143 ]. \t  -7.787939520995682 \t -7.582260282917048\n",
            "27     \t [ 2.37375129 -1.01127875]. \t  -23.697851899243755 \t -7.582260282917048\n",
            "28     \t [ 1.42417112 -1.08285707]. \t  -23.41196596740658 \t -7.582260282917048\n",
            "29     \t [ 1.92559248 -1.92357002]. \t  -9.612208203020884 \t -7.582260282917048\n",
            "30     \t [2.51787021 0.80393463]. \t  -33.59868925840011 \t -7.582260282917048\n",
            "31     \t [3.87539544 0.4468187 ]. \t  -37.576622881475615 \t -7.582260282917048\n",
            "32     \t [-0.06270803 -1.74756478]. \t  -13.977132819818053 \t -7.582260282917048\n",
            "33     \t [-1.63043094  0.92826517]. \t  -21.344366303292915 \t -7.582260282917048\n",
            "34     \t [-0.34454915 -1.37687728]. \t  -34.76586610227167 \t -7.582260282917048\n",
            "35     \t [ 0.42739726 -2.27493495]. \t  -35.89573929954167 \t -7.582260282917048\n",
            "36     \t [-2.32203398 -0.81289056]. \t  -26.57621630064433 \t -7.582260282917048\n",
            "37     \t [ 1.1738972  -4.76504354]. \t  -38.5383335051751 \t -7.582260282917048\n",
            "38     \t [-2.56838294 -4.94761536]. \t  -50.70335285397977 \t -7.582260282917048\n",
            "39     \t [ 2.00542714 -1.70426624]. \t  -19.766228830472418 \t -7.582260282917048\n",
            "40     \t [ 1.66101346 -0.78179926]. \t  -26.689831109916796 \t -7.582260282917048\n",
            "41     \t [ 1.27160671 -2.16313761]. \t  -22.458786173889845 \t -7.582260282917048\n",
            "42     \t [0.4988678  1.02706821]. \t  -21.44776371439287 \t -7.582260282917048\n",
            "43     \t [ 1.79142166 -2.61047368]. \t  -35.1365737944693 \t -7.582260282917048\n",
            "44     \t [-0.36748296 -4.16310152]. \t  -39.0030677906451 \t -7.582260282917048\n",
            "45     \t [ 2.38579611 -0.42742836]. \t  -42.38710870807003 \t -7.582260282917048\n",
            "46     \t [2.58661263 1.43075255]. \t  -46.361320402576844 \t -7.582260282917048\n",
            "47     \t [3.24802642 0.68425609]. \t  -34.90821347204846 \t -7.582260282917048\n",
            "48     \t [ 3.88907738 -0.43161597]. \t  -36.734198194655086 \t -7.582260282917048\n",
            "49     \t [3.55981794 2.83954974]. \t  -44.70295666635696 \t -7.582260282917048\n",
            "50     \t [4.65024718 1.84315896]. \t  -45.36252834966585 \t -7.582260282917048\n",
            "51     \t [ 3.45828583 -0.16169293]. \t  -36.37621511548231 \t -7.582260282917048\n",
            "52     \t [-5.11041598 -2.34832095]. \t  -49.734694318473196 \t -7.582260282917048\n",
            "53     \t [ 0.28952041 -4.87590313]. \t  -39.20487552802784 \t -7.582260282917048\n",
            "54     \t [ 4.54401816 -0.1790928 ]. \t  -45.99084111082012 \t -7.582260282917048\n",
            "55     \t [0.07400832 0.19245334]. \t  \u001b[92m-7.566845067366542\u001b[0m \t -7.566845067366542\n",
            "56     \t [ 1.62788746 -2.12496244]. \t  -27.034371259826226 \t -7.566845067366542\n",
            "57     \t [-1.6653885  -2.97312482]. \t  -26.82461115619329 \t -7.566845067366542\n",
            "58     \t [-1.30947213 -2.50069494]. \t  -41.61848418600241 \t -7.566845067366542\n",
            "59     \t [ 2.82847487 -2.48573003]. \t  -39.40561242577757 \t -7.566845067366542\n",
            "60     \t [-1.37170186 -3.82119808]. \t  -39.08036369192858 \t -7.566845067366542\n",
            "61     \t [-2.92249886 -2.2623453 ]. \t  -25.59648318953175 \t -7.566845067366542\n",
            "62     \t [ 0.56346259 -1.78084984]. \t  -30.77816019834436 \t -7.566845067366542\n",
            "63     \t [-2.5530655  -3.07193672]. \t  -36.4084333247347 \t -7.566845067366542\n",
            "64     \t [-2.73840551 -1.68361655]. \t  -35.11239219773767 \t -7.566845067366542\n",
            "65     \t [-5.03441535 -3.37130096]. \t  -53.848721549717425 \t -7.566845067366542\n",
            "66     \t [ 2.43632701 -2.49906536]. \t  -51.39118490600554 \t -7.566845067366542\n",
            "67     \t [ 3.68696632 -3.51930963]. \t  -59.763543332546774 \t -7.566845067366542\n",
            "68     \t [ 1.8600592  -3.71440962]. \t  -33.09717236257632 \t -7.566845067366542\n",
            "69     \t [ 3.78236188 -2.97377251]. \t  -31.265685402724337 \t -7.566845067366542\n",
            "70     \t [ 3.91107853 -2.09811969]. \t  -23.06019879328032 \t -7.566845067366542\n",
            "71     \t [ 4.91308641 -4.92192074]. \t  -50.99755939759251 \t -7.566845067366542\n",
            "72     \t [4.68117357 0.86489646]. \t  -40.24415263751852 \t -7.566845067366542\n",
            "73     \t [ 2.69895255 -3.54638178]. \t  -52.592216757113775 \t -7.566845067366542\n",
            "74     \t [ 0.70703805 -3.45626045]. \t  -44.737085455701724 \t -7.566845067366542\n",
            "75     \t [ 3.20877976 -2.83597175]. \t  -30.635043912165138 \t -7.566845067366542\n",
            "76     \t [ 3.92847919 -4.71419202]. \t  -50.88033106059134 \t -7.566845067366542\n",
            "77     \t [4.25129781 2.11019841]. \t  -34.910833137709204 \t -7.566845067366542\n",
            "78     \t [ 1.89913019 -3.40197245]. \t  -35.28457886100801 \t -7.566845067366542\n",
            "79     \t [-0.91090772 -1.70525539]. \t  -18.038444643049655 \t -7.566845067366542\n",
            "80     \t [ 4.17321121 -3.71393923]. \t  -48.815705756375046 \t -7.566845067366542\n",
            "81     \t [ 3.43477806 -1.44179905]. \t  -52.387265896532085 \t -7.566845067366542\n",
            "82     \t [ 1.81650555 -4.59173198]. \t  -48.71004856333874 \t -7.566845067366542\n",
            "83     \t [ 5.03443261 -2.58813054]. \t  -50.78268330387078 \t -7.566845067366542\n",
            "84     \t [ 3.46529231 -2.21277316]. \t  -44.350012187010336 \t -7.566845067366542\n",
            "85     \t [ 4.0626609  -1.17174522]. \t  -23.922121487455325 \t -7.566845067366542\n",
            "86     \t [ 3.56879813 -4.98560014]. \t  -56.7135967149291 \t -7.566845067366542\n",
            "87     \t [ 0.79087066 -4.3073222 ]. \t  -40.16294926941291 \t -7.566845067366542\n",
            "88     \t [-1.0144632  -3.26774368]. \t  -22.861107702822093 \t -7.566845067366542\n",
            "89     \t [ 4.54683902 -2.47888853]. \t  -66.30084524628691 \t -7.566845067366542\n",
            "90     \t [-4.1608129  -0.01158508]. \t  -22.023905925931444 \t -7.566845067366542\n",
            "91     \t [1.18989464 3.67382953]. \t  -35.8307817652523 \t -7.566845067366542\n",
            "92     \t [5.08641945 4.63751582]. \t  -65.310261329266 \t -7.566845067366542\n",
            "93     \t [ 0.9710134  -2.87948247]. \t  -12.132293776561557 \t -7.566845067366542\n",
            "94     \t [-0.46014822 -5.01270378]. \t  -45.05891833784578 \t -7.566845067366542\n",
            "95     \t [ 4.03146274 -1.59270981]. \t  -37.33500887693835 \t -7.566845067366542\n",
            "96     \t [ 0.47980469 -0.42528419]. \t  -39.24883828381323 \t -7.566845067366542\n",
            "97     \t [-4.29700089 -4.33201553]. \t  -65.06911332612033 \t -7.566845067366542\n",
            "98     \t [ 3.50242737 -0.61401784]. \t  -50.18463024524639 \t -7.566845067366542\n",
            "99     \t [1.49602644 1.68001112]. \t  -39.314576694531766 \t -7.566845067366542\n",
            "100    \t [ 1.21116168 -4.0406431 ]. \t  -25.701869734535823 \t -7.566845067366542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO3I_9cbuvY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03fc4807-5b5f-4327-d6bd-d80812889603"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 4.02288795 -1.72052679]. \t  -31.08835710146886 \t -17.28954482757088\n",
            "init   \t [ 3.28938622 -4.69302655]. \t  -58.797867722203385 \t -17.28954482757088\n",
            "init   \t [-4.0175956   0.97333314]. \t  -17.28954482757088 \t -17.28954482757088\n",
            "init   \t [ 0.30532979 -0.83141193]. \t  -19.296253155889353 \t -17.28954482757088\n",
            "init   \t [-1.68542362  1.25459899]. \t  -28.650630936276173 \t -17.28954482757088\n",
            "1      \t [3.94510418 4.87990293]. \t  -42.68076688045528 \t -17.28954482757088\n",
            "2      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -17.28954482757088\n",
            "3      \t [-3.94018201  5.05044227]. \t  -42.22809151092162 \t -17.28954482757088\n",
            "4      \t [-0.95959198 -4.95868608]. \t  -26.16498922741133 \t -17.28954482757088\n",
            "5      \t [0.23252298 4.130865  ]. \t  -29.21645315585864 \t -17.28954482757088\n",
            "6      \t [-3.07622716 -2.13447075]. \t  -18.50631708551415 \t -17.28954482757088\n",
            "7      \t [2.94113029 1.50964659]. \t  -31.587243379322928 \t -17.28954482757088\n",
            "8      \t [-5.12       -1.28775675]. \t  -42.93316784315402 \t -17.28954482757088\n",
            "9      \t [-5.11443067  2.68557657]. \t  -49.783208806301474 \t -17.28954482757088\n",
            "10     \t [ 0.83632322 -3.06788167]. \t  \u001b[92m-15.845396489320052\u001b[0m \t -15.845396489320052\n",
            "11     \t [4.80256666 0.94543272]. \t  -31.297370463426564 \t -15.845396489320052\n",
            "12     \t [-2.01133516  3.54701967]. \t  -36.218929854026946 \t -15.845396489320052\n",
            "13     \t [0.58282148 1.29249716]. \t  -33.32507741896706 \t -15.845396489320052\n",
            "14     \t [-1.18575523 -2.58781885]. \t  -32.690854323137344 \t -15.845396489320052\n",
            "15     \t [ 1.92410421 -1.7175528 ]. \t  -19.792417325288078 \t -15.845396489320052\n",
            "16     \t [-2.79301029 -5.12      ]. \t  -44.0559772952378 \t -15.845396489320052\n",
            "17     \t [ 0.49259262 -4.99380046]. \t  -45.1774473977748 \t -15.845396489320052\n",
            "18     \t [-2.55683155 -0.53466664]. \t  -45.956174977477005 \t -15.845396489320052\n",
            "19     \t [2.00541658 3.71078941]. \t  -30.236271372578877 \t -15.845396489320052\n",
            "20     \t [ 5.09101626 -2.98953593]. \t  -36.468484842285456 \t -15.845396489320052\n",
            "21     \t [-3.50556578 -3.19851201]. \t  -49.334405242786914 \t -15.845396489320052\n",
            "22     \t [4.81833719 2.92776687]. \t  -38.63752366567171 \t -15.845396489320052\n",
            "23     \t [ 2.7842996  -0.53939001]. \t  -35.60009382398926 \t -15.845396489320052\n",
            "24     \t [ 0.92008935 -2.78269222]. \t  -17.784472520983176 \t -15.845396489320052\n",
            "25     \t [-0.61415331  5.08220104]. \t  -45.046530440148274 \t -15.845396489320052\n",
            "26     \t [ 1.03934076 -3.42388565]. \t  -31.985232951781555 \t -15.845396489320052\n",
            "27     \t [-0.01317896 -2.71876808]. \t  -19.37592694245975 \t -15.845396489320052\n",
            "28     \t [ 0.21909751 -3.09737048]. \t  -19.526050077086076 \t -15.845396489320052\n",
            "29     \t [ 0.44697575 -2.97409275]. \t  -28.62734201340966 \t -15.845396489320052\n",
            "30     \t [ 1.75023022 -2.84753355]. \t  -25.40551094124499 \t -15.845396489320052\n",
            "31     \t [-0.69913693 -3.45274592]. \t  -45.11440748295823 \t -15.845396489320052\n",
            "32     \t [-0.73312345 -2.15166778]. \t  -20.43278833435115 \t -15.845396489320052\n",
            "33     \t [ 0.63948753 -0.23336229]. \t  -25.81894402144048 \t -15.845396489320052\n",
            "34     \t [-0.71332036 -2.84910485]. \t  -25.078269126930707 \t -15.845396489320052\n",
            "35     \t [-2.63850927 -1.96724627]. \t  -27.488935863672115 \t -15.845396489320052\n",
            "36     \t [-2.1594274  -2.93215562]. \t  -18.766946269746565 \t -15.845396489320052\n",
            "37     \t [-0.53660559 -4.87231229]. \t  -46.8133836233991 \t -15.845396489320052\n",
            "38     \t [ 1.21812848 -3.01315553]. \t  -18.60789714644278 \t -15.845396489320052\n",
            "39     \t [ 2.53683957 -2.39325125]. \t  -49.73023890998492 \t -15.845396489320052\n",
            "40     \t [ 1.61676344 -2.33417734]. \t  -40.53554140038664 \t -15.845396489320052\n",
            "41     \t [ 0.11142804 -3.99393742]. \t  -18.3235771550463 \t -15.845396489320052\n",
            "42     \t [ 1.62557253 -1.22045576]. \t  -29.331907255968304 \t -15.845396489320052\n",
            "43     \t [ 5.03303693 -1.20006788]. \t  -33.90017917396216 \t -15.845396489320052\n",
            "44     \t [ 3.06607902 -3.68211972]. \t  -37.94534469111061 \t -15.845396489320052\n",
            "45     \t [-0.0557351  -1.52306873]. \t  -22.824921341729866 \t -15.845396489320052\n",
            "46     \t [ 2.25926545 -1.12045437]. \t  -19.671421232544674 \t -15.845396489320052\n",
            "47     \t [3.60829555 0.21789053]. \t  -38.836387633359294 \t -15.845396489320052\n",
            "48     \t [ 0.4180151  -2.08770654]. \t  -24.71594640831747 \t -15.845396489320052\n",
            "49     \t [ 2.61405825 -3.25836526]. \t  -45.515717532953076 \t -15.845396489320052\n",
            "50     \t [-4.11167197 -2.22988562]. \t  -32.98002905352818 \t -15.845396489320052\n",
            "51     \t [ 4.12103642 -4.28570152]. \t  -50.32965155097466 \t -15.845396489320052\n",
            "52     \t [ 4.24762654 -1.28759778]. \t  -41.89154280572083 \t -15.845396489320052\n",
            "53     \t [-2.05509316 -2.6469145 ]. \t  -27.85632121523649 \t -15.845396489320052\n",
            "54     \t [-2.64591403 -3.89755836]. \t  -40.27636529417981 \t -15.845396489320052\n",
            "55     \t [-3.0718843  -1.39497989]. \t  -30.285989654313397 \t -15.845396489320052\n",
            "56     \t [ 4.71857603 -1.87706574]. \t  -40.5877108238079 \t -15.845396489320052\n",
            "57     \t [-0.09800408 -0.23125513]. \t  \u001b[92m-10.724785064160107\u001b[0m \t -10.724785064160107\n",
            "58     \t [2.83295248 0.41970049]. \t  -31.976491461889246 \t -10.724785064160107\n",
            "59     \t [ 2.96614744 -0.89950221]. \t  -11.760747496877759 \t -10.724785064160107\n",
            "60     \t [2.26052817 2.19588465]. \t  -27.25789013967765 \t -10.724785064160107\n",
            "61     \t [-0.03176137 -3.49653741]. \t  -32.422882266891335 \t -10.724785064160107\n",
            "62     \t [-1.61975477 -4.51910072]. \t  -60.274172428370406 \t -10.724785064160107\n",
            "63     \t [-1.55590764 -1.62141929]. \t  -41.66753780526701 \t -10.724785064160107\n",
            "64     \t [-1.90526014 -3.42746616]. \t  -36.07689868962823 \t -10.724785064160107\n",
            "65     \t [ 0.57261832 -4.29460546]. \t  -50.51462774010601 \t -10.724785064160107\n",
            "66     \t [0.43613479 0.44713595]. \t  -39.04919343391195 \t -10.724785064160107\n",
            "67     \t [0.32687042 2.29009946]. \t  -32.48865292695414 \t -10.724785064160107\n",
            "68     \t [-5.12       -2.14345959]. \t  -37.31387372436512 \t -10.724785064160107\n",
            "69     \t [-3.14176305 -3.63388398]. \t  -43.45284577214097 \t -10.724785064160107\n",
            "70     \t [ 3.54482597 -4.16650588]. \t  -54.522797783255065 \t -10.724785064160107\n",
            "71     \t [ 4.69894369 -4.97542078]. \t  -60.10712207024859 \t -10.724785064160107\n",
            "72     \t [-2.85899671 -3.41556634]. \t  -42.13989373575988 \t -10.724785064160107\n",
            "73     \t [-4.71422993 -3.39314317]. \t  -63.79551809690677 \t -10.724785064160107\n",
            "74     \t [ 4.72101032 -3.74986826]. \t  -58.169147792536556 \t -10.724785064160107\n",
            "75     \t [3.34473534 2.13744999]. \t  -34.8661497103874 \t -10.724785064160107\n",
            "76     \t [ 1.02329711 -2.06448222]. \t  \u001b[92m-6.225749956596754\u001b[0m \t -6.225749956596754\n",
            "77     \t [-4.35953585 -2.73532044]. \t  -53.7603092599423 \t -6.225749956596754\n",
            "78     \t [1.38853775 4.65849512]. \t  -56.713720097449475 \t -6.225749956596754\n",
            "79     \t [ 3.42423022 -0.51503405]. \t  -50.83403389555508 \t -6.225749956596754\n",
            "80     \t [-3.40969072 -4.46636078]. \t  -69.78468201723214 \t -6.225749956596754\n",
            "81     \t [-2.65415518 -1.38135043]. \t  -41.964832816790796 \t -6.225749956596754\n",
            "82     \t [-1.66378965  0.96140627]. \t  -19.14079546093275 \t -6.225749956596754\n",
            "83     \t [-2.88119588  0.15498561]. \t  -15.362812185014853 \t -6.225749956596754\n",
            "84     \t [-1.91964172  0.21546539]. \t  -12.82636371211422 \t -6.225749956596754\n",
            "85     \t [ 2.6978057  -1.73009186]. \t  -34.73997506731769 \t -6.225749956596754\n",
            "86     \t [3.63141509 1.00780992]. \t  -30.995280705050966 \t -6.225749956596754\n",
            "87     \t [-4.55992057  1.28149684]. \t  -53.70082287624954 \t -6.225749956596754\n",
            "88     \t [1.21308377 1.67169993]. \t  -26.69104625090642 \t -6.225749956596754\n",
            "89     \t [4.96954288 0.38849791]. \t  -42.674467518092335 \t -6.225749956596754\n",
            "90     \t [-1.03138023 -1.89416241]. \t  -6.976158047727759 \t -6.225749956596754\n",
            "91     \t [ 0.50943727 -3.4800102 ]. \t  -52.27364959646884 \t -6.225749956596754\n",
            "92     \t [-0.71767658  0.35235057]. \t  -28.65290908722026 \t -6.225749956596754\n",
            "93     \t [ 0.43162355 -1.25964377]. \t  -31.469799578859018 \t -6.225749956596754\n",
            "94     \t [-0.80040898  1.07346221]. \t  -9.724862413365882 \t -6.225749956596754\n",
            "95     \t [-2.65462125  2.44297569]. \t  -48.020607930828504 \t -6.225749956596754\n",
            "96     \t [-1.9265482  -5.02920592]. \t  -30.21859536594548 \t -6.225749956596754\n",
            "97     \t [-1.3788777  -4.85230924]. \t  -46.69282906257635 \t -6.225749956596754\n",
            "98     \t [ 2.67610704 -4.25183637]. \t  -49.832861088801806 \t -6.225749956596754\n",
            "99     \t [-0.43223521 -4.40232677]. \t  -56.84971573653914 \t -6.225749956596754\n",
            "100    \t [-4.27763307 -1.6378122 ]. \t  -49.187651983231994 \t -6.225749956596754\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rQbLZD8uvbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7c8da09-0d6c-4628-e869-0e5d8251ca37"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-3.55672335 -2.02386832]. \t  -36.23014472109487 \t -36.23014472109487\n",
            "init   \t [-4.48474711 -0.4110301 ]. \t  -58.713796772117476 \t -36.23014472109487\n",
            "init   \t [3.43299466 4.37244977]. \t  -66.98740910066293 \t -36.23014472109487\n",
            "init   \t [2.3243672  2.74940131]. \t  -37.50394977069271 \t -36.23014472109487\n",
            "init   \t [-2.36334012  1.47485995]. \t  -44.17062022214734 \t -36.23014472109487\n",
            "1      \t [ 4.83286922 -4.13539777]. \t  -48.88906404842241 \t -36.23014472109487\n",
            "2      \t [ 0.0160484 -5.12     ]. \t  \u001b[92m-28.975766783034203\u001b[0m \t -28.975766783034203\n",
            "3      \t [ 0.88371363 -1.07198128]. \t  \u001b[92m-5.488111610345253\u001b[0m \t -5.488111610345253\n",
            "4      \t [-4.04476176  5.04009339]. \t  -42.4711660390786 \t -5.488111610345253\n",
            "5      \t [ 4.65593585 -0.40278571]. \t  -55.6038850237184 \t -5.488111610345253\n",
            "6      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -5.488111610345253\n",
            "7      \t [-0.69301343  3.71550095]. \t  -39.94047206917946 \t -5.488111610345253\n",
            "8      \t [-0.71635811 -2.32486504]. \t  -32.54859074764826 \t -5.488111610345253\n",
            "9      \t [ 2.12885841 -2.82956593]. \t  -20.847280389342234 \t -5.488111610345253\n",
            "10     \t [0.87145363 0.18353642]. \t  -9.825646730336132 \t -5.488111610345253\n",
            "11     \t [-2.41204687 -4.79314299]. \t  -54.62600348941287 \t -5.488111610345253\n",
            "12     \t [ 1.24675688 -0.90098238]. \t  -14.036118288206644 \t -5.488111610345253\n",
            "13     \t [ 0.2100079  -1.17811539]. \t  -14.581005351112236 \t -5.488111610345253\n",
            "14     \t [5.04851009 2.33834146]. \t  -46.68621045246245 \t -5.488111610345253\n",
            "15     \t [-0.02003215  1.36444101]. \t  -18.52793896684685 \t -5.488111610345253\n",
            "16     \t [-4.73130874  2.2574687 ]. \t  -49.12225606704074 \t -5.488111610345253\n",
            "17     \t [ 1.00537312 -2.14845219]. \t  -9.676068261767803 \t -5.488111610345253\n",
            "18     \t [ 2.13175509 -5.10617855]. \t  -35.99682236970344 \t -5.488111610345253\n",
            "19     \t [ 0.74814473 -1.15341377]. \t  -16.30366652312582 \t -5.488111610345253\n",
            "20     \t [-0.13808667 -0.47428404]. \t  -23.647353000093723 \t -5.488111610345253\n",
            "21     \t [ 1.05930296 -3.26283463]. \t  -23.259964058401263 \t -5.488111610345253\n",
            "22     \t [-1.07711818 -1.46705063]. \t  -24.25002781482017 \t -5.488111610345253\n",
            "23     \t [ 1.51114464 -2.31214713]. \t  -41.4114170386789 \t -5.488111610345253\n",
            "24     \t [ 0.44104073 -2.77412696]. \t  -35.701793157629595 \t -5.488111610345253\n",
            "25     \t [ 3.49880552 -3.45064243]. \t  -63.671252766862395 \t -5.488111610345253\n",
            "26     \t [ 1.88729208 -3.24927267]. \t  -26.478376473305023 \t -5.488111610345253\n",
            "27     \t [-0.3225066   2.15136497]. \t  -23.323893010972235 \t -5.488111610345253\n",
            "28     \t [ 0.62891379 -0.26173924]. \t  -28.09603227239225 \t -5.488111610345253\n",
            "29     \t [-0.58548928 -1.46779031]. \t  -40.88484737443466 \t -5.488111610345253\n",
            "30     \t [-1.93724302 -2.48668557]. \t  -30.668939350724287 \t -5.488111610345253\n",
            "31     \t [1.04640327 0.81581976]. \t  -8.16385954225484 \t -5.488111610345253\n",
            "32     \t [-0.67596428 -0.91075659]. \t  -17.303587317405622 \t -5.488111610345253\n",
            "33     \t [-2.68072267 -1.14005283]. \t  -26.330977001621626 \t -5.488111610345253\n",
            "34     \t [ 2.90409335 -3.10683979]. \t  -22.017386733691136 \t -5.488111610345253\n",
            "35     \t [1.24359809 0.34269721]. \t  -26.76242278935297 \t -5.488111610345253\n",
            "36     \t [-2.0988899  -1.51873228]. \t  -28.511731325715854 \t -5.488111610345253\n",
            "37     \t [-1.29769263 -2.98325306]. \t  -23.591083761347072 \t -5.488111610345253\n",
            "38     \t [-1.5673367  -1.98889885]. \t  -25.554829212896124 \t -5.488111610345253\n",
            "39     \t [-3.31668714 -3.28980824]. \t  -48.36702012176618 \t -5.488111610345253\n",
            "40     \t [-2.11986596 -0.79064437]. \t  -15.297408131115272 \t -5.488111610345253\n",
            "41     \t [ 1.10180855 -2.77460731]. \t  -19.3496009121772 \t -5.488111610345253\n",
            "42     \t [ 0.93121432 -4.47267115]. \t  -41.64439098567885 \t -5.488111610345253\n",
            "43     \t [-2.64941203 -3.34466294]. \t  -49.71715971019694 \t -5.488111610345253\n",
            "44     \t [ 3.22449572 -5.11401382]. \t  -47.41293465944089 \t -5.488111610345253\n",
            "45     \t [ 0.00657361 -0.91498774]. \t  \u001b[92m-2.238746313840034\u001b[0m \t -2.238746313840034\n",
            "46     \t [-1.31163785  1.88059629]. \t  -21.718485242983576 \t -2.238746313840034\n",
            "47     \t [-1.50657968 -1.39631245]. \t  -42.16275000919254 \t -2.238746313840034\n",
            "48     \t [-2.87890879 -2.42085241]. \t  -35.69480433169541 \t -2.238746313840034\n",
            "49     \t [-1.6941785   0.63387988]. \t  -33.373689587968215 \t -2.238746313840034\n",
            "50     \t [-4.62925168 -2.61079069]. \t  -62.79923910762944 \t -2.238746313840034\n",
            "51     \t [-3.74186399 -4.28528287]. \t  -55.07494711711097 \t -2.238746313840034\n",
            "52     \t [-5.03107465 -0.330797  ]. \t  -40.47250146222364 \t -2.238746313840034\n",
            "53     \t [-0.9149973   0.50681133]. \t  -22.477587444664685 \t -2.238746313840034\n",
            "54     \t [-2.12450803  0.02911154]. \t  -7.588310246803951 \t -2.238746313840034\n",
            "55     \t [ 1.25384881 -3.80393693]. \t  -32.959419143474655 \t -2.238746313840034\n",
            "56     \t [0.91680121 0.41996369]. \t  -21.11438350890625 \t -2.238746313840034\n",
            "57     \t [1.17600401 2.00417531]. \t  -10.919538214799905 \t -2.238746313840034\n",
            "58     \t [0.9537211  1.48370054]. \t  -23.478346873530135 \t -2.238746313840034\n",
            "59     \t [1.38775514 1.53351707]. \t  -41.67106927863101 \t -2.238746313840034\n",
            "60     \t [1.0564556  3.71017225]. \t  -27.98045703146099 \t -2.238746313840034\n",
            "61     \t [1.1075998  3.04891313]. \t  -13.190820771171879 \t -2.238746313840034\n",
            "62     \t [-0.63857737  1.35094077]. \t  -34.601250386723436 \t -2.238746313840034\n",
            "63     \t [0.96794264 4.97644779]. \t  -26.013410478852315 \t -2.238746313840034\n",
            "64     \t [-2.11976889  2.67393477]. \t  -28.94316687408723 \t -2.238746313840034\n",
            "65     \t [1.45645256 2.56516752]. \t  -47.502698670118484 \t -2.238746313840034\n",
            "66     \t [3.84001036 3.46345565]. \t  -51.119927937301995 \t -2.238746313840034\n",
            "67     \t [2.8413357  2.96972914]. \t  -21.64387239857719 \t -2.238746313840034\n",
            "68     \t [0.18741161 3.57362109]. \t  -37.922990164997756 \t -2.238746313840034\n",
            "69     \t [5.11356142 3.38424745]. \t  -57.51082005067322 \t -2.238746313840034\n",
            "70     \t [-1.1961137   4.43948285]. \t  -47.10399253807862 \t -2.238746313840034\n",
            "71     \t [ 1.0093147  -0.66650905]. \t  -16.4886462244772 \t -2.238746313840034\n",
            "72     \t [0.06381786 2.23344692]. \t  -14.747379533299803 \t -2.238746313840034\n",
            "73     \t [ 1.74318783 -1.77515659]. \t  -25.04371398924042 \t -2.238746313840034\n",
            "74     \t [4.44550124 3.6036384 ]. \t  -70.12181582902738 \t -2.238746313840034\n",
            "75     \t [0.69624601 2.00086245]. \t  -17.80197113867162 \t -2.238746313840034\n",
            "76     \t [-1.90650287  2.35378801]. \t  -26.92028047321693 \t -2.238746313840034\n",
            "77     \t [-2.62766647  3.75652509]. \t  -47.55785580069755 \t -2.238746313840034\n",
            "78     \t [0.79102019 2.65665567]. \t  -30.669089379853713 \t -2.238746313840034\n",
            "79     \t [-2.70011583  3.13366326]. \t  -33.51783173469893 \t -2.238746313840034\n",
            "80     \t [-1.60024777  3.18959387]. \t  -37.11035197201317 \t -2.238746313840034\n",
            "81     \t [-2.36294435  3.33750131]. \t  -48.4631621103831 \t -2.238746313840034\n",
            "82     \t [-2.87018649  4.58391922]. \t  -51.038067309836705 \t -2.238746313840034\n",
            "83     \t [-3.79256584  1.33353677]. \t  -38.53022111817837 \t -2.238746313840034\n",
            "84     \t [-3.880447    4.14303057]. \t  -38.68731272100571 \t -2.238746313840034\n",
            "85     \t [-4.76322313 -1.44519773]. \t  -53.360021370487615 \t -2.238746313840034\n",
            "86     \t [-2.88408272 -1.85445714]. \t  -18.191988670918224 \t -2.238746313840034\n",
            "87     \t [-1.10890992 -2.61667334]. \t  -27.759218554941743 \t -2.238746313840034\n",
            "88     \t [-1.1227374 -4.8798399]. \t  -30.619705818278913 \t -2.238746313840034\n",
            "89     \t [-4.16737795 -2.51853926]. \t  -48.68106467816446 \t -2.238746313840034\n",
            "90     \t [-1.08447635 -0.0833634 ]. \t  -3.8996069212642936 \t -2.238746313840034\n",
            "91     \t [-1.67796722 -4.07801833]. \t  -34.996447452432584 \t -2.238746313840034\n",
            "92     \t [-5.04924733 -3.79262446]. \t  -47.707553634945015 \t -2.238746313840034\n",
            "93     \t [-0.07682372  1.63439304]. \t  -20.461359888130602 \t -2.238746313840034\n",
            "94     \t [1.74729814 3.22117124]. \t  -31.797274458013238 \t -2.238746313840034\n",
            "95     \t [-0.66946012  2.79658474]. \t  -30.230908300857692 \t -2.238746313840034\n",
            "96     \t [ 1.18648385 -1.2718039 ]. \t  -20.505184353702194 \t -2.238746313840034\n",
            "97     \t [ 3.46638742 -1.51432215]. \t  -54.04636381827271 \t -2.238746313840034\n",
            "98     \t [-0.01841115 -4.09088218]. \t  -18.389042003170168 \t -2.238746313840034\n",
            "99     \t [ 2.6928993  -1.60035059]. \t  -41.40129945322572 \t -2.238746313840034\n",
            "100    \t [ 4.5757327  -1.79927282]. \t  -50.01709907050444 \t -2.238746313840034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YRio_skuvd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "129a86c7-b3b6-4a4f-8ecc-f70ba04fe457"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-3.99872128 -4.68771825]. \t  -51.77895852895087 \t -5.803967830456797\n",
            "init   \t [ 1.02085207 -2.04932398]. \t  -5.803967830456797 \t -5.803967830456797\n",
            "init   \t [ 3.03730465 -1.37015168]. \t  -28.228393086698507 \t -5.803967830456797\n",
            "init   \t [ 3.89742944 -2.07010168]. \t  -22.435544673579564 \t -5.803967830456797\n",
            "init   \t [-1.74998258  0.70366122]. \t  -26.429262404618022 \t -5.803967830456797\n",
            "1      \t [4.35939273 4.99968846]. \t  -60.346002738676276 \t -5.803967830456797\n",
            "2      \t [-4.66270521  4.43201236]. \t  -75.69892251377243 \t -5.803967830456797\n",
            "3      \t [-0.38971611  4.92355286]. \t  -43.2185836938886 \t -5.803967830456797\n",
            "4      \t [ 0.32090187 -5.12      ]. \t  -43.33668904989738 \t -5.803967830456797\n",
            "5      \t [-4.82824724 -1.16082392]. \t  -34.62425722377728 \t -5.803967830456797\n",
            "6      \t [ 4.81151197 -5.12      ]. \t  -58.30595444784639 \t -5.803967830456797\n",
            "7      \t [4.97539415 1.49820537]. \t  -47.11780328862186 \t -5.803967830456797\n",
            "8      \t [-1.50247309 -2.34560268]. \t  -43.41018284632621 \t -5.803967830456797\n",
            "9      \t [1.2211937  1.77343529]. \t  -21.36913202820945 \t -5.803967830456797\n",
            "10     \t [-4.13875266  1.39584705]. \t  -40.57728996490613 \t -5.803967830456797\n",
            "11     \t [ 0.36804349 -0.34981314]. \t  -32.88151831873741 \t -5.803967830456797\n",
            "12     \t [ 1.41398288 -2.9001264 ]. \t  -30.88995822918296 \t -5.803967830456797\n",
            "13     \t [ 0.69415004 -1.58640277]. \t  -34.998316458413136 \t -5.803967830456797\n",
            "14     \t [-1.68237598  2.88626817]. \t  -27.7296105461826 \t -5.803967830456797\n",
            "15     \t [1.81403831 3.68505729]. \t  -36.922605650289576 \t -5.803967830456797\n",
            "16     \t [ 1.41098863 -1.76330868]. \t  -32.741311838729224 \t -5.803967830456797\n",
            "17     \t [ 0.529921   -2.70248641]. \t  -40.34926973475286 \t -5.803967830456797\n",
            "18     \t [0.34135358 0.47924119]. \t  -35.69113499161949 \t -5.803967830456797\n",
            "19     \t [ 3.87001876 -0.67104578]. \t  -33.340869916277356 \t -5.803967830456797\n",
            "20     \t [2.56907143 1.56690104]. \t  -47.257719847100915 \t -5.803967830456797\n",
            "21     \t [-1.7754596 -4.5711204]. \t  -51.47261154430345 \t -5.803967830456797\n",
            "22     \t [-2.46516174 -0.87812638]. \t  -29.400911783830956 \t -5.803967830456797\n",
            "23     \t [-2.75309102  4.9186908 ]. \t  -42.85568492166179 \t -5.803967830456797\n",
            "24     \t [0.50867348 2.35904035]. \t  -42.136638845624276 \t -5.803967830456797\n",
            "25     \t [-4.86565074 -3.37715162]. \t  -55.601990052158236 \t -5.803967830456797\n",
            "26     \t [ 5.04211097 -2.84797378]. \t  -38.10746032338191 \t -5.803967830456797\n",
            "27     \t [ 2.29542176 -4.78480621]. \t  -48.80913248979635 \t -5.803967830456797\n",
            "28     \t [-0.67266989 -0.11722653]. \t  -17.72822449626706 \t -5.803967830456797\n",
            "29     \t [2.63221371 4.94577673]. \t  -48.70744802155118 \t -5.803967830456797\n",
            "30     \t [4.91275916 3.03534061]. \t  -35.0591198497657 \t -5.803967830456797\n",
            "31     \t [-5.07732719  2.25429032]. \t  -42.287880424857335 \t -5.803967830456797\n",
            "32     \t [-3.27748301 -2.47798437]. \t  -48.50501974417704 \t -5.803967830456797\n",
            "33     \t [-2.45147196  2.09366159]. \t  -31.61409015459226 \t -5.803967830456797\n",
            "34     \t [-3.31569538 -0.12311223]. \t  -27.866091991636814 \t -5.803967830456797\n",
            "35     \t [ 3.29849276 -3.16245474]. \t  -38.65372927408432 \t -5.803967830456797\n",
            "36     \t [ 5.0144765  -1.56803643]. \t  -46.7451624523903 \t -5.803967830456797\n",
            "37     \t [ 1.1614418  -2.36954605]. \t  -28.50678618412221 \t -5.803967830456797\n",
            "38     \t [ 1.61813013 -4.16546615]. \t  -42.27387833876279 \t -5.803967830456797\n",
            "39     \t [ 2.07208993 -3.71440315]. \t  -31.31678145542991 \t -5.803967830456797\n",
            "40     \t [ 2.94348215 -0.64029737]. \t  -26.05782502697333 \t -5.803967830456797\n",
            "41     \t [1.92044801 2.33109334]. \t  -25.22314565219039 \t -5.803967830456797\n",
            "42     \t [1.54233594 0.55565585]. \t  -41.73061183047132 \t -5.803967830456797\n",
            "43     \t [ 1.0486525  -4.03848145]. \t  -18.163498781839618 \t -5.803967830456797\n",
            "44     \t [ 3.55593074 -4.76605473]. \t  -63.74172021899464 \t -5.803967830456797\n",
            "45     \t [0.10009306 0.08175241]. \t  \u001b[92m-3.2204780951840597\u001b[0m \t -3.2204780951840597\n",
            "46     \t [ 1.10643929 -3.79882406]. \t  -24.789655477048296 \t -3.2204780951840597\n",
            "47     \t [-1.22179593 -3.64764694]. \t  -39.03207679449522 \t -3.2204780951840597\n",
            "48     \t [ 2.01079245 -3.06254856]. \t  -14.20784565812027 \t -3.2204780951840597\n",
            "49     \t [0.99723914 4.38775597]. \t  -37.86288788017224 \t -3.2204780951840597\n",
            "50     \t [ 0.75353176 -4.58361482]. \t  -50.00684365759888 \t -3.2204780951840597\n",
            "51     \t [ 0.83077228 -1.95407376]. \t  -10.062047501072243 \t -3.2204780951840597\n",
            "52     \t [-0.42743323 -3.569142  ]. \t  -50.97099576225636 \t -3.2204780951840597\n",
            "53     \t [-0.846762   -4.90685567]. \t  -30.746406821759948 \t -3.2204780951840597\n",
            "54     \t [ 3.62987839 -1.44791247]. \t  -51.59272343761734 \t -3.2204780951840597\n",
            "55     \t [-0.71657939 -5.10402348]. \t  -40.71001514035196 \t -3.2204780951840597\n",
            "56     \t [4.22266944 0.16856066]. \t  -31.253959161262756 \t -3.2204780951840597\n",
            "57     \t [ 4.92756863 -0.05258133]. \t  -25.842328302667834 \t -3.2204780951840597\n",
            "58     \t [ 1.02651718 -1.13768917]. \t  -6.001114211551766 \t -3.2204780951840597\n",
            "59     \t [ 4.09136406 -2.75761112]. \t  -35.468594616752576 \t -3.2204780951840597\n",
            "60     \t [-0.22815849  0.09163535]. \t  -10.304637246717402 \t -3.2204780951840597\n",
            "61     \t [-0.56300568  3.60362376]. \t  -50.48392260295623 \t -3.2204780951840597\n",
            "62     \t [ 0.94315475 -1.96030775]. \t  -5.67282242692238 \t -3.2204780951840597\n",
            "63     \t [-0.49847559 -3.00570173]. \t  -29.288678556498176 \t -3.2204780951840597\n",
            "64     \t [-1.43566074 -1.05348318]. \t  -22.924240171564417 \t -3.2204780951840597\n",
            "65     \t [-2.47376681  0.16090432]. \t  -30.699671368689472 \t -3.2204780951840597\n",
            "66     \t [ 3.52721262 -2.30864035]. \t  -51.22691281847528 \t -3.2204780951840597\n",
            "67     \t [-1.28051027 -4.27464909]. \t  -43.36019187883934 \t -3.2204780951840597\n",
            "68     \t [-2.571286   -2.66848687]. \t  -47.64653639538962 \t -3.2204780951840597\n",
            "69     \t [1.81041553 1.4712851 ]. \t  -31.574458225065733 \t -3.2204780951840597\n",
            "70     \t [1.46518164 3.13523144]. \t  -35.13588079634401 \t -3.2204780951840597\n",
            "71     \t [-3.92537174 -1.130209  ]. \t  -20.92937864917531 \t -3.2204780951840597\n",
            "72     \t [ 1.24592195 -1.48594569]. \t  -33.46518834157471 \t -3.2204780951840597\n",
            "73     \t [ 1.04684729 -0.37136522]. \t  -18.57164171118864 \t -3.2204780951840597\n",
            "74     \t [-1.62231396 -0.52037421]. \t  -40.01025744332049 \t -3.2204780951840597\n",
            "75     \t [2.68364871 0.4220033 ]. \t  -40.2523228898152 \t -3.2204780951840597\n",
            "76     \t [ 3.40343858 -3.7332179 ]. \t  -54.78807171697173 \t -3.2204780951840597\n",
            "77     \t [-2.69940842  0.99504648]. \t  -21.40726554514262 \t -3.2204780951840597\n",
            "78     \t [-2.36442609 -1.69665313]. \t  -38.34463925303925 \t -3.2204780951840597\n",
            "79     \t [-2.34225412 -3.26924541]. \t  -42.857709034665 \t -3.2204780951840597\n",
            "80     \t [3.63461383 2.21573626]. \t  -42.61494142261167 \t -3.2204780951840597\n",
            "81     \t [ 4.52441835 -2.20719334]. \t  -52.567287494191504 \t -3.2204780951840597\n",
            "82     \t [-3.1519042  -0.92110705]. \t  -16.205934346783483 \t -3.2204780951840597\n",
            "83     \t [-2.701236   -5.08081039]. \t  -47.38910505390692 \t -3.2204780951840597\n",
            "84     \t [ 1.67186261 -3.53001447]. \t  -49.793482510085234 \t -3.2204780951840597\n",
            "85     \t [-2.07066647  0.78483137]. \t  -13.702167553483637 \t -3.2204780951840597\n",
            "86     \t [ 2.7220117  -4.39085469]. \t  -56.17771139482692 \t -3.2204780951840597\n",
            "87     \t [2.34233138 3.37477414]. \t  -49.41801323858982 \t -3.2204780951840597\n",
            "88     \t [ 3.25067072 -0.35111238]. \t  -36.66653560825821 \t -3.2204780951840597\n",
            "89     \t [ 2.64644743 -3.91453043]. \t  -39.79182611734964 \t -3.2204780951840597\n",
            "90     \t [ 0.35150563 -4.10701712]. \t  -35.1220304241371 \t -3.2204780951840597\n",
            "91     \t [-1.55341818 -5.06236228]. \t  -48.24052588603393 \t -3.2204780951840597\n",
            "92     \t [ 4.82278954 -4.23521985]. \t  -55.853334579044294 \t -3.2204780951840597\n",
            "93     \t [ 1.79760919 -2.46800385]. \t  -36.174080792716914 \t -3.2204780951840597\n",
            "94     \t [1.99163556 0.6999218 ]. \t  -17.565152480840922 \t -3.2204780951840597\n",
            "95     \t [0.33063961 0.14458922]. \t  -18.833499214864474 \t -3.2204780951840597\n",
            "96     \t [-3.45131063  1.79674108]. \t  -41.78071976044242 \t -3.2204780951840597\n",
            "97     \t [ 4.26860545 -0.35818755]. \t  -45.80172789406075 \t -3.2204780951840597\n",
            "98     \t [3.50993807 0.79506002]. \t  -40.1387643056008 \t -3.2204780951840597\n",
            "99     \t [4.60161842 0.63731176]. \t  -56.114509771859865 \t -3.2204780951840597\n",
            "100    \t [0.461364   2.96133469]. \t  -28.982802721205005 \t -3.2204780951840597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejw6v-Ihuvf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "630b1c32-e8d9-4cbd-cd87-af5fcaea97ce"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.76413244 -0.12207719]. \t  -15.040540034920513 \t -15.040540034920513\n",
            "init   \t [ 3.33307058 -4.79798899]. \t  -56.14599841787976 \t -15.040540034920513\n",
            "init   \t [3.15443162 0.67192238]. \t  -29.462925809392473 \t -15.040540034920513\n",
            "init   \t [-2.07234561 -4.64183582]. \t  -43.141657603020725 \t -15.040540034920513\n",
            "init   \t [ 5.02402457 -5.05010449]. \t  -51.34955862002178 \t -15.040540034920513\n",
            "1      \t [-4.74552942  3.02987162]. \t  -42.15664864154001 \t -15.040540034920513\n",
            "2      \t [0.32595061 4.76020431]. \t  -46.71811490423538 \t -15.040540034920513\n",
            "3      \t [-5.12       -1.45183685]. \t  -50.578139316560346 \t -15.040540034920513\n",
            "4      \t [5.04843149 5.07855462]. \t  -52.93136469570379 \t -15.040540034920513\n",
            "5      \t [-1.57606276  0.8622242 ]. \t  -25.625662247467787 \t -15.040540034920513\n",
            "6      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -15.040540034920513\n",
            "7      \t [ 0.5833589  -2.60728524]. \t  -43.61042867027566 \t -15.040540034920513\n",
            "8      \t [ 4.41058562 -1.57146363]. \t  -59.39443429185532 \t -15.040540034920513\n",
            "9      \t [-3.08233087  4.87832211]. \t  -37.390100994422596 \t -15.040540034920513\n",
            "10     \t [2.56328199 3.08959207]. \t  -36.8788894340649 \t -15.040540034920513\n",
            "11     \t [-2.27793803 -1.73439151]. \t  -30.922651525911224 \t -15.040540034920513\n",
            "12     \t [0.45023609 1.59843088]. \t  -40.42055870234715 \t -15.040540034920513\n",
            "13     \t [4.9159632  2.20135353]. \t  -37.365405440809184 \t -15.040540034920513\n",
            "14     \t [-4.71432391  0.83021971]. \t  -40.30735467861459 \t -15.040540034920513\n",
            "15     \t [-2.56315594  2.09385865]. \t  -31.866056085191016 \t -15.040540034920513\n",
            "16     \t [ 0.18822628 -5.12      ]. \t  -35.17550793635441 \t -15.040540034920513\n",
            "17     \t [-0.1246342  -0.70157579]. \t  -16.41629723801411 \t -15.040540034920513\n",
            "18     \t [ 2.15230245 -1.41565848]. \t  -29.50469902546012 \t -15.040540034920513\n",
            "19     \t [2.89350215 4.84334806]. \t  -38.452170393034905 \t -15.040540034920513\n",
            "20     \t [-1.43092992  3.49235578]. \t  -53.305570587789845 \t -15.040540034920513\n",
            "21     \t [ 0.80319181 -0.24283894]. \t  -16.97402907548052 \t -15.040540034920513\n",
            "22     \t [-3.80488155 -3.25531469]. \t  -42.02769106042497 \t -15.040540034920513\n",
            "23     \t [-2.96619252 -0.04863703]. \t  \u001b[92m-9.488745406433301\u001b[0m \t -9.488745406433301\n",
            "24     \t [-2.76637349  0.35543613]. \t  -32.90289410823794 \t -9.488745406433301\n",
            "25     \t [-3.22613043 -0.7751921 ]. \t  -27.938422215329307 \t -9.488745406433301\n",
            "26     \t [-2.4567465  -0.48631269]. \t  -45.86811968958581 \t -9.488745406433301\n",
            "27     \t [-3.20679669 -0.21875373]. \t  -25.699395914849234 \t -9.488745406433301\n",
            "28     \t [-1.07509429  1.13844302]. \t  \u001b[92m-7.0951948718089595\u001b[0m \t -7.0951948718089595\n",
            "29     \t [-3.1548749 -2.0734234]. \t  -19.67038723319734 \t -7.0951948718089595\n",
            "30     \t [-1.39209851 -2.88315164]. \t  -30.615203767463314 \t -7.0951948718089595\n",
            "31     \t [-1.20403835  1.9893733 ]. \t  -12.581714216388963 \t -7.0951948718089595\n",
            "32     \t [-2.20214766  2.53699282]. \t  -38.05532037285464 \t -7.0951948718089595\n",
            "33     \t [-4.6605601   1.42426203]. \t  -57.96678287962769 \t -7.0951948718089595\n",
            "34     \t [-4.90825685  4.20862063]. \t  -50.84863575464086 \t -7.0951948718089595\n",
            "35     \t [-1.42744562  1.55955604]. \t  -42.75658552822914 \t -7.0951948718089595\n",
            "36     \t [-0.4347815   2.49371754]. \t  -45.57195752304749 \t -7.0951948718089595\n",
            "37     \t [-0.96204315  0.53954319]. \t  -21.192605433397993 \t -7.0951948718089595\n",
            "38     \t [-3.29652534  2.4114897 ]. \t  -48.05724010466869 \t -7.0951948718089595\n",
            "39     \t [ 0.31346598 -1.41999749]. \t  -34.76048213576635 \t -7.0951948718089595\n",
            "40     \t [-2.21585446 -2.39488738]. \t  -36.413700729475195 \t -7.0951948718089595\n",
            "41     \t [-0.99564053 -2.20162346]. \t  -12.849198973819703 \t -7.0951948718089595\n",
            "42     \t [-0.47759838  1.23269157]. \t  -30.563355962035782 \t -7.0951948718089595\n",
            "43     \t [1.19589197 1.92542894]. \t  -12.880570567263343 \t -7.0951948718089595\n",
            "44     \t [1.66373211 0.75425257]. \t  -28.228558510128448 \t -7.0951948718089595\n",
            "45     \t [ 1.15333436 -3.3465902 ]. \t  -32.52595287170336 \t -7.0951948718089595\n",
            "46     \t [-5.02634344 -3.12649175]. \t  -38.17127089456905 \t -7.0951948718089595\n",
            "47     \t [-1.1712274   0.91692243]. \t  -8.794428813478518 \t -7.0951948718089595\n",
            "48     \t [-5.03885502  0.57270736]. \t  -44.989118010235735 \t -7.0951948718089595\n",
            "49     \t [-0.95523106  2.04321562]. \t  \u001b[92m-5.846607271395669\u001b[0m \t -5.846607271395669\n",
            "50     \t [4.7459442  0.11589821]. \t  -35.32855496763681 \t -5.846607271395669\n",
            "51     \t [3.60681473 3.24059258]. \t  -50.75096401493143 \t -5.846607271395669\n",
            "52     \t [-1.02887351  2.13340082]. \t  -9.085936602100485 \t -5.846607271395669\n",
            "53     \t [0.58811569 3.12854771]. \t  -31.728112414255087 \t -5.846607271395669\n",
            "54     \t [-1.35395891  4.98618596]. \t  -42.81015248791127 \t -5.846607271395669\n",
            "55     \t [-0.49208474 -2.97043873]. \t  -29.225288752195816 \t -5.846607271395669\n",
            "56     \t [-2.90556351 -0.34328214]. \t  -25.80072256946525 \t -5.846607271395669\n",
            "57     \t [-3.63456729 -1.70642011]. \t  -45.45955295171681 \t -5.846607271395669\n",
            "58     \t [-2.18954541 -0.05966691]. \t  -11.784433111970806 \t -5.846607271395669\n",
            "59     \t [-3.37085457 -4.07225315]. \t  -45.84333134189622 \t -5.846607271395669\n",
            "60     \t [-2.97999812 -1.84876711]. \t  -16.562189806861056 \t -5.846607271395669\n",
            "61     \t [-2.91955537 -3.31099726]. \t  -34.47636466595052 \t -5.846607271395669\n",
            "62     \t [-1.4052995  -0.27590835]. \t  -31.953050111895426 \t -5.846607271395669\n",
            "63     \t [-3.58634753 -4.84917707]. \t  -59.10447661563274 \t -5.846607271395669\n",
            "64     \t [-0.44981965  3.80362851]. \t  -40.87080954958236 \t -5.846607271395669\n",
            "65     \t [-1.84125424  0.23801728]. \t  -17.270044405420506 \t -5.846607271395669\n",
            "66     \t [-0.887171   -1.52291489]. \t  -25.412253914307882 \t -5.846607271395669\n",
            "67     \t [ 1.09233613 -1.32348737]. \t  -19.03611351311256 \t -5.846607271395669\n",
            "68     \t [-1.66394331 -1.39203302]. \t  -37.63983798685264 \t -5.846607271395669\n",
            "69     \t [0.79935666 2.1300746 ]. \t  -15.282434476570563 \t -5.846607271395669\n",
            "70     \t [-3.87400249 -1.07275667]. \t  -20.15892188750184 \t -5.846607271395669\n",
            "71     \t [ 1.78868558 -4.15193994]. \t  -32.25237198351974 \t -5.846607271395669\n",
            "72     \t [-0.90578206  1.36735041]. \t  -21.114933340011973 \t -5.846607271395669\n",
            "73     \t [-0.16634902  0.4883795 ]. \t  -25.222269097171527 \t -5.846607271395669\n",
            "74     \t [-1.25762575  3.9788283 ]. \t  -27.98000281280642 \t -5.846607271395669\n",
            "75     \t [-1.88466113  3.81221799]. \t  -26.787482729994117 \t -5.846607271395669\n",
            "76     \t [ 1.0119697  -0.80803673]. \t  -8.138998626575317 \t -5.846607271395669\n",
            "77     \t [ 2.45354236 -2.37076082]. \t  -48.09759972609626 \t -5.846607271395669\n",
            "78     \t [ 1.56750843 -2.2994247 ]. \t  -39.914023457776196 \t -5.846607271395669\n",
            "79     \t [-2.8731378   2.34040306]. \t  -32.12419038710145 \t -5.846607271395669\n",
            "80     \t [0.16527367 4.1123614 ]. \t  -24.253513216007356 \t -5.846607271395669\n",
            "81     \t [-3.89928727  1.96590852]. \t  -21.234009760887197 \t -5.846607271395669\n",
            "82     \t [-0.29961515  5.08948829]. \t  -40.599360657566336 \t -5.846607271395669\n",
            "83     \t [ 1.34032722 -0.15867223]. \t  -21.768750035995918 \t -5.846607271395669\n",
            "84     \t [ 2.98357349 -1.07481159]. \t  -11.194712311052074 \t -5.846607271395669\n",
            "85     \t [2.28042426 0.09454747]. \t  -18.82251116489805 \t -5.846607271395669\n",
            "86     \t [3.67563336 1.072806  ]. \t  -30.193804073739496 \t -5.846607271395669\n",
            "87     \t [ 3.44384215 -2.75311052]. \t  -48.62815459054413 \t -5.846607271395669\n",
            "88     \t [-3.12847056 -2.92958513]. \t  -22.417450300782214 \t -5.846607271395669\n",
            "89     \t [2.12443869 1.97444288]. \t  -11.44435523952528 \t -5.846607271395669\n",
            "90     \t [-2.93305365  5.11756012]. \t  -38.270164424827925 \t -5.846607271395669\n",
            "91     \t [-3.81815181  4.8646729 ]. \t  -47.49320241850316 \t -5.846607271395669\n",
            "92     \t [ 0.81337639 -3.97084439]. \t  -22.718861242777173 \t -5.846607271395669\n",
            "93     \t [ 0.76080175 -0.84486702]. \t  -15.000525317474587 \t -5.846607271395669\n",
            "94     \t [ 1.93794103 -0.68039828]. \t  -19.204329608236783 \t -5.846607271395669\n",
            "95     \t [1.08195748 5.02176905]. \t  -27.779039534087005 \t -5.846607271395669\n",
            "96     \t [-0.25757176 -2.31327628]. \t  -29.765009694860886 \t -5.846607271395669\n",
            "97     \t [2.2845364 0.9343041]. \t  -19.084933034243562 \t -5.846607271395669\n",
            "98     \t [ 3.98065029 -0.1945726 ]. \t  -22.544616093319384 \t -5.846607271395669\n",
            "99     \t [ 3.32936938 -1.63013098]. \t  -45.36427391700312 \t -5.846607271395669\n",
            "100    \t [ 2.73231342 -3.30429073]. \t  -42.83828411504665 \t -5.846607271395669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4xG5dbuuvig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1db19e-875b-47ce-dcc3-f475ae64f7e5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.57275736 -3.9423289 ]. \t  -37.63803811714605 \t -29.87920356309123\n",
            "init   \t [ 4.61089653 -0.18236005]. \t  -44.839569549068806 \t -29.87920356309123\n",
            "init   \t [ 3.81413924 -2.94571335]. \t  -29.87920356309123 \t -29.87920356309123\n",
            "init   \t [-4.70313344 -1.05272872]. \t  -36.67385216521674 \t -29.87920356309123\n",
            "init   \t [-2.7327263   3.49942502]. \t  -50.7969122914016 \t -29.87920356309123\n",
            "1      \t [4.08205924 5.11352276]. \t  -46.549151606379965 \t -29.87920356309123\n",
            "2      \t [-3.07793615 -5.12      ]. \t  -39.57360484062926 \t -29.87920356309123\n",
            "3      \t [0.17274586 0.09447728]. \t  \u001b[92m-7.0839049606070965\u001b[0m \t -7.0839049606070965\n",
            "4      \t [0.85935797 3.20380684]. \t  -21.79794502444777 \t -7.0839049606070965\n",
            "5      \t [-1.37259247 -2.19744722]. \t  -30.433771852271303 \t -7.0839049606070965\n",
            "6      \t [-5.00655854  1.7294695 ]. \t  -39.35157731342374 \t -7.0839049606070965\n",
            "7      \t [-1.61280282  0.56293364]. \t  -39.738050468053856 \t -7.0839049606070965\n",
            "8      \t [ 1.32901126 -0.1285568 ]. \t  -19.634511111330593 \t -7.0839049606070965\n",
            "9      \t [3.74449581 2.33336143]. \t  -44.813122964494326 \t -7.0839049606070965\n",
            "10     \t [-5.12      -3.7126288]. \t  -55.03491000222165 \t -7.0839049606070965\n",
            "11     \t [-0.44072074  4.96485203]. \t  -44.40119528831736 \t -7.0839049606070965\n",
            "12     \t [-4.86795547  4.08243168]. \t  -44.923533459498216 \t -7.0839049606070965\n",
            "13     \t [ 3.7623448 -5.12     ]. \t  -52.305082754895224 \t -7.0839049606070965\n",
            "14     \t [0.04196255 1.15089179]. \t  \u001b[92m-5.839453989409604\u001b[0m \t -5.839453989409604\n",
            "15     \t [-0.68518779 -4.91292423]. \t  -40.02665522182387 \t -5.839453989409604\n",
            "16     \t [ 0.14585732 -0.89370534]. \t  -6.882132929831754 \t -5.839453989409604\n",
            "17     \t [1.98886272 4.96654695]. \t  -28.866728526829085 \t -5.839453989409604\n",
            "18     \t [1.66515128 1.86403752]. \t  -24.76195976840413 \t -5.839453989409604\n",
            "19     \t [ 0.82249056 -1.84172728]. \t  -14.220006949489484 \t -5.839453989409604\n",
            "20     \t [-0.0156024   1.44310261]. \t  -21.498559189504896 \t -5.839453989409604\n",
            "21     \t [0.69211825 0.63213016]. \t  -31.18308623871346 \t -5.839453989409604\n",
            "22     \t [-0.20759596  0.40755902]. \t  -25.936395143084233 \t -5.839453989409604\n",
            "23     \t [ 1.09291582 -0.89254745]. \t  -5.841237312716197 \t -5.839453989409604\n",
            "24     \t [-2.64639964  0.41841864]. \t  -41.952619717416134 \t -5.839453989409604\n",
            "25     \t [ 0.43848545 -0.87963979]. \t  -22.9541740086028 \t -5.839453989409604\n",
            "26     \t [ 1.93761687 -0.78514755]. \t  -12.938733922292506 \t -5.839453989409604\n",
            "27     \t [-0.8349336  -1.14120301]. \t  -10.596818642975853 \t -5.839453989409604\n",
            "28     \t [1.98533825 2.74815642]. \t  -21.65216765781304 \t -5.839453989409604\n",
            "29     \t [ 1.55214857 -2.52136109]. \t  -48.14447581219384 \t -5.839453989409604\n",
            "30     \t [-0.55892022 -2.45489712]. \t  -45.26256895490933 \t -5.839453989409604\n",
            "31     \t [-2.67184575 -2.55000769]. \t  -47.86730295197906 \t -5.839453989409604\n",
            "32     \t [ 1.45788053 -0.7361564 ]. \t  -33.18791998398324 \t -5.839453989409604\n",
            "33     \t [2.66438809 0.49457463]. \t  -42.461228116674974 \t -5.839453989409604\n",
            "34     \t [ 3.24319143 -1.33040852]. \t  -36.70062673414901 \t -5.839453989409604\n",
            "35     \t [ 1.5729442  -1.26540483]. \t  -34.009768581478745 \t -5.839453989409604\n",
            "36     \t [ 2.77433591 -0.64552809]. \t  -32.69334603364738 \t -5.839453989409604\n",
            "37     \t [-1.32643771 -3.27554735]. \t  -38.70715728216498 \t -5.839453989409604\n",
            "38     \t [-0.65597893 -0.52117825]. \t  -36.183387635320265 \t -5.839453989409604\n",
            "39     \t [-2.22327338  1.61807521]. \t  -33.26164950521402 \t -5.839453989409604\n",
            "40     \t [-0.51997926 -1.03154423]. \t  -21.45154280806048 \t -5.839453989409604\n",
            "41     \t [-2.45180042 -0.9839601 ]. \t  -26.57515765320194 \t -5.839453989409604\n",
            "42     \t [1.99555011 0.20463251]. \t  -11.21592617189563 \t -5.839453989409604\n",
            "43     \t [-0.3161024  -3.16574693]. \t  -29.106863634323002 \t -5.839453989409604\n",
            "44     \t [2.87708049 1.95593907]. \t  -15.321164089489297 \t -5.839453989409604\n",
            "45     \t [3.85481418 1.481249  ]. \t  -40.86452716462977 \t -5.839453989409604\n",
            "46     \t [-3.46186883 -3.1930448 ]. \t  -48.39172916331317 \t -5.839453989409604\n",
            "47     \t [-1.80547971 -1.30099812]. \t  -24.686383034762123 \t -5.839453989409604\n",
            "48     \t [-3.42764965 -2.3212876 ]. \t  -50.452422462547375 \t -5.839453989409604\n",
            "49     \t [ 1.24671349 -3.05197662]. \t  -21.190918920606 \t -5.839453989409604\n",
            "50     \t [ 2.09063549 -3.5543312 ]. \t  -38.00516928969581 \t -5.839453989409604\n",
            "51     \t [-2.55562422 -0.4694297 ]. \t  -45.96312413028679 \t -5.839453989409604\n",
            "52     \t [-3.16468628 -1.27194245]. \t  -27.900027380533388 \t -5.839453989409604\n",
            "53     \t [3.03015153 1.15419092]. \t  -15.030084304067385 \t -5.839453989409604\n",
            "54     \t [-2.65606266 -3.57409475]. \t  -54.33005121732497 \t -5.839453989409604\n",
            "55     \t [-0.13776081 -0.46447108]. \t  -23.504564630257914 \t -5.839453989409604\n",
            "56     \t [4.92784057 0.99934184]. \t  -26.292713096351477 \t -5.839453989409604\n",
            "57     \t [-1.98688311 -0.53655354]. \t  -24.00694645037511 \t -5.839453989409604\n",
            "58     \t [ 3.22186516 -2.37756562]. \t  -41.45879368481959 \t -5.839453989409604\n",
            "59     \t [ 5.12       -1.18876333]. \t  -36.58449254718492 \t -5.839453989409604\n",
            "60     \t [ 0.78449783 -1.45144639]. \t  -30.10975953152938 \t -5.839453989409604\n",
            "61     \t [-3.75250381  0.63226418]. \t  -41.064807252608524 \t -5.839453989409604\n",
            "62     \t [2.30087677 1.46075395]. \t  -40.26785578224564 \t -5.839453989409604\n",
            "63     \t [ 1.89891443 -4.73247937]. \t  -39.05097802619461 \t -5.839453989409604\n",
            "64     \t [ 2.82078149 -1.3831099 ]. \t  -32.98967806667047 \t -5.839453989409604\n",
            "65     \t [ 3.6414629 -1.8245315]. \t  -38.37865864205652 \t -5.839453989409604\n",
            "66     \t [ 0.64703325 -5.0670452 ]. \t  -42.995472714200375 \t -5.839453989409604\n",
            "67     \t [ 0.93697402 -2.45003918]. \t  -27.165840079279373 \t -5.839453989409604\n",
            "68     \t [-2.78523292  1.05381151]. \t  -17.23848836208554 \t -5.839453989409604\n",
            "69     \t [5.11037337 0.22622317]. \t  -36.98854353822173 \t -5.839453989409604\n",
            "70     \t [1.50957416 2.36675683]. \t  -44.55777870587254 \t -5.839453989409604\n",
            "71     \t [0.10757888 4.04057194]. \t  -18.859758549618142 \t -5.839453989409604\n",
            "72     \t [-0.60787207  1.25130053]. \t  -29.806641339060015 \t -5.839453989409604\n",
            "73     \t [-0.51127517 -3.92951862]. \t  -36.64208394718461 \t -5.839453989409604\n",
            "74     \t [-4.94466175 -1.17121693]. \t  -31.669449318242908 \t -5.839453989409604\n",
            "75     \t [-3.66148354  0.02605204]. \t  -28.820146593934737 \t -5.839453989409604\n",
            "76     \t [-5.03731415 -0.33797522]. \t  -41.0127640745938 \t -5.839453989409604\n",
            "77     \t [2.57549373 2.65694578]. \t  -48.10776736943323 \t -5.839453989409604\n",
            "78     \t [1.01636293 4.02395392]. \t  -17.391051161178225 \t -5.839453989409604\n",
            "79     \t [ 2.46678056 -0.78346974]. \t  -34.39429305839977 \t -5.839453989409604\n",
            "80     \t [ 5.05492838 -3.22586465]. \t  -45.03750952322572 \t -5.839453989409604\n",
            "81     \t [ 4.25127009 -3.51428536]. \t  -60.4630453014158 \t -5.839453989409604\n",
            "82     \t [3.74507308 0.05471142]. \t  -24.92314933649796 \t -5.839453989409604\n",
            "83     \t [4.7364216  3.13772941]. \t  -46.647643918535735 \t -5.839453989409604\n",
            "84     \t [ 4.51715798 -2.76535786]. \t  -57.030399370042964 \t -5.839453989409604\n",
            "85     \t [5.07952887 1.55288354]. \t  -48.88879169746546 \t -5.839453989409604\n",
            "86     \t [3.55776319 3.59787765]. \t  -63.118814978213166 \t -5.839453989409604\n",
            "87     \t [-1.55013215 -0.69365615]. \t  -35.85876694555717 \t -5.839453989409604\n",
            "88     \t [-2.36491559  2.32056165]. \t  -41.87666887487329 \t -5.839453989409604\n",
            "89     \t [ 1.93050985 -5.09429623]. \t  -32.321278749017125 \t -5.839453989409604\n",
            "90     \t [2.08744967 2.00744163]. \t  -9.870143611294914 \t -5.839453989409604\n",
            "91     \t [2.41808801 3.66137991]. \t  -53.242275690173074 \t -5.839453989409604\n",
            "92     \t [ 3.97429031 -0.6101806 ]. \t  -33.995388662745924 \t -5.839453989409604\n",
            "93     \t [-0.9532609  -3.36147321]. \t  -29.08161769886105 \t -5.839453989409604\n",
            "94     \t [-4.26625617  0.03240522]. \t  -29.42818832867258 \t -5.839453989409604\n",
            "95     \t [0.90358379 0.20304821]. \t  -9.729765616441059 \t -5.839453989409604\n",
            "96     \t [-1.04254472  2.34183095]. \t  -22.381283958262387 \t -5.839453989409604\n",
            "97     \t [-0.91941581  1.89323895]. \t  -7.851104734519533 \t -5.839453989409604\n",
            "98     \t [-0.97243541  3.92608862]. \t  -17.568498376767934 \t -5.839453989409604\n",
            "99     \t [ 2.81822614 -4.77672204]. \t  -44.93167145368438 \t -5.839453989409604\n",
            "100    \t [ 2.24404663 -0.32097556]. \t  -29.077972045003886 \t -5.839453989409604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSj_CQIAuvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5311b2f4-37a7-44c4-9f7c-141c0628661f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [4.92580518 3.28957798]. \t  -48.613167836867184 \t -11.082229452321341\n",
            "init   \t [ 1.4939506  -0.78143474]. \t  -30.873019488911932 \t -11.082229452321341\n",
            "init   \t [-3.04833558 -0.08287467]. \t  -11.082229452321341 \t -11.082229452321341\n",
            "init   \t [-3.70116269 -0.4861846 ]. \t  -46.917938977325136 \t -11.082229452321341\n",
            "init   \t [-3.95535741 -5.09005324]. \t  -43.50282995110477 \t -11.082229452321341\n",
            "1      \t [-1.89231656  0.02090104]. \t  \u001b[92m-5.870314035667901\u001b[0m \t -5.870314035667901\n",
            "2      \t [-3.03986331  1.26049881]. \t  -21.80084381406698 \t -5.870314035667901\n",
            "3      \t [-2.43671339 -0.16963358]. \t  -30.34841730721483 \t -5.870314035667901\n",
            "4      \t [-1.33919011  0.31482816]. \t  -31.16934794656118 \t -5.870314035667901\n",
            "5      \t [-3.2507612   0.38549152]. \t  -38.285377426622276 \t -5.870314035667901\n",
            "6      \t [-2.35775092  1.69126954]. \t  -38.291068271660706 \t -5.870314035667901\n",
            "7      \t [-3.34508684  2.22016338]. \t  -39.88034862754407 \t -5.870314035667901\n",
            "8      \t [-2.40388092  0.51325951]. \t  -44.23847365213334 \t -5.870314035667901\n",
            "9      \t [-1.16147503 -0.60681689]. \t  -24.268531346655514 \t -5.870314035667901\n",
            "10     \t [ 4.66843567 -4.7395977 ]. \t  -69.81464286553684 \t -5.870314035667901\n",
            "11     \t [ 0.13841943 -5.12      ]. \t  -32.49342947985242 \t -5.870314035667901\n",
            "12     \t [0.27295152 5.1033554 ]. \t  -39.5913712873193 \t -5.870314035667901\n",
            "13     \t [ 4.98257069 -1.28735721]. \t  -38.86893192980003 \t -5.870314035667901\n",
            "14     \t [-4.95648797  5.08269127]. \t  -52.091568722755326 \t -5.870314035667901\n",
            "15     \t [-1.5593812  -0.20479466]. \t  -28.983319737030335 \t -5.870314035667901\n",
            "16     \t [-0.40546774 -1.55380636]. \t  -40.29995515611784 \t -5.870314035667901\n",
            "17     \t [-0.41935346 -1.24106206]. \t  -29.89822445179403 \t -5.870314035667901\n",
            "18     \t [-1.32954584 -2.23864385]. \t  -30.858807000922862 \t -5.870314035667901\n",
            "19     \t [-4.20806503  1.47325515]. \t  -47.13296151475289 \t -5.870314035667901\n",
            "20     \t [-0.95364996 -1.54664586]. \t  -23.29620831242899 \t -5.870314035667901\n",
            "21     \t [-0.25017066 -3.1274648 ]. \t  -22.893628543532586 \t -5.870314035667901\n",
            "22     \t [-1.89733684 -3.36065613]. \t  -33.30916020672482 \t -5.870314035667901\n",
            "23     \t [-3.68634734  1.52914022]. \t  -49.65392249456305 \t -5.870314035667901\n",
            "24     \t [-5.05197018  1.24594999]. \t  -37.34876735955228 \t -5.870314035667901\n",
            "25     \t [-2.80143396  1.85876984]. \t  -21.81283108249293 \t -5.870314035667901\n",
            "26     \t [-3.76196247  4.00327726]. \t  -39.42979401170691 \t -5.870314035667901\n",
            "27     \t [-4.83829514  3.18646483]. \t  -44.40829569560296 \t -5.870314035667901\n",
            "28     \t [ 2.24815145 -0.82695498]. \t  -20.972887322284446 \t -5.870314035667901\n",
            "29     \t [ 0.80197478 -2.58423752]. \t  -32.74522047882864 \t -5.870314035667901\n",
            "30     \t [-0.78490274 -0.98153529]. \t  -9.471240040980794 \t -5.870314035667901\n",
            "31     \t [-1.7482841   0.77924272]. \t  -21.944474010960192 \t -5.870314035667901\n",
            "32     \t [-3.5830942   2.99621943]. \t  -40.486472988987494 \t -5.870314035667901\n",
            "33     \t [-4.58856553  2.2547285 ]. \t  -54.927021021546096 \t -5.870314035667901\n",
            "34     \t [0.22679386 0.57133158]. \t  -27.937262323886856 \t -5.870314035667901\n",
            "35     \t [ 0.6472419 -1.5504203]. \t  -38.342259422146185 \t -5.870314035667901\n",
            "36     \t [ 0.04017898 -0.34165389]. \t  -15.88102977875036 \t -5.870314035667901\n",
            "37     \t [-0.26882169  2.18201212]. \t  -21.870226297171037 \t -5.870314035667901\n",
            "38     \t [2.33676397 0.5524391 ]. \t  -40.41324559987487 \t -5.870314035667901\n",
            "39     \t [-3.30161811 -0.26893854]. \t  -35.346848752052 \t -5.870314035667901\n",
            "40     \t [ 1.74538223 -3.67110918]. \t  -41.56985295303694 \t -5.870314035667901\n",
            "41     \t [-4.4667727  -2.83856292]. \t  -52.51054466737926 \t -5.870314035667901\n",
            "42     \t [-4.88124809 -1.73862296]. \t  -40.220480916139955 \t -5.870314035667901\n",
            "43     \t [-1.04157173  1.60263777]. \t  -21.98416537874879 \t -5.870314035667901\n",
            "44     \t [0.55671763 2.77741677]. \t  -35.6815425824312 \t -5.870314035667901\n",
            "45     \t [-5.12       -1.10946471]. \t  -32.42909833333512 \t -5.870314035667901\n",
            "46     \t [-4.67724277 -1.10810585]. \t  -39.73795212280514 \t -5.870314035667901\n",
            "47     \t [0.93166856 0.44167515]. \t  -21.3066679347042 \t -5.870314035667901\n",
            "48     \t [-5.07169638 -2.85717248]. \t  -38.646810379049825 \t -5.870314035667901\n",
            "49     \t [-5.09202753  3.89063398]. \t  -44.96097958782097 \t -5.870314035667901\n",
            "50     \t [ 0.14650521 -2.74055676]. \t  -22.071034638799727 \t -5.870314035667901\n",
            "51     \t [2.74072895 3.94921676]. \t  -34.194853593440975 \t -5.870314035667901\n",
            "52     \t [-3.22890264  5.00985579]. \t  -44.22192758128832 \t -5.870314035667901\n",
            "53     \t [-1.39203211  1.12165505]. \t  -23.763645464285585 \t -5.870314035667901\n",
            "54     \t [-4.78351506 -0.4696155 ]. \t  -50.83058785384455 \t -5.870314035667901\n",
            "55     \t [-0.90048478  2.26679269]. \t  -18.894345102808664 \t -5.870314035667901\n",
            "56     \t [0.39280197 1.73156114]. \t  -32.12470767451734 \t -5.870314035667901\n",
            "57     \t [4.67468691 1.0030065 ]. \t  -37.417927879336006 \t -5.870314035667901\n",
            "58     \t [-1.18183897 -4.40691742]. \t  -45.00262346071061 \t -5.870314035667901\n",
            "59     \t [1.50872421 3.17749404]. \t  -37.95796901764115 \t -5.870314035667901\n",
            "60     \t [ 4.05087924 -2.23981595]. \t  -31.29360295200288 \t -5.870314035667901\n",
            "61     \t [4.88561166 4.91707469]. \t  -51.84731042732848 \t -5.870314035667901\n",
            "62     \t [3.7593778  2.56693034]. \t  -49.261872788663844 \t -5.870314035667901\n",
            "63     \t [-2.90850545  1.69885378]. \t  -26.11150098591939 \t -5.870314035667901\n",
            "64     \t [-1.78600899  0.21845427]. \t  -19.025168454247684 \t -5.870314035667901\n",
            "65     \t [-2.60246304  3.09028929]. \t  -35.88741692169955 \t -5.870314035667901\n",
            "66     \t [-3.9685605   4.72694258]. \t  -49.73161334793555 \t -5.870314035667901\n",
            "67     \t [1.57525322 0.68380216]. \t  -35.89227727942975 \t -5.870314035667901\n",
            "68     \t [ 1.96475429 -0.24541005]. \t  -13.876341927685083 \t -5.870314035667901\n",
            "69     \t [ 3.56562454 -0.06347442]. \t  -32.6644057742997 \t -5.870314035667901\n",
            "70     \t [-0.55760334  0.58411085]. \t  -38.63997457629825 \t -5.870314035667901\n",
            "71     \t [-2.84645538  2.66391013]. \t  -34.65173266305247 \t -5.870314035667901\n",
            "72     \t [-1.89685902  3.99411196]. \t  -21.58524650224738 \t -5.870314035667901\n",
            "73     \t [-4.21814045  3.06489661]. \t  -36.01772360399493 \t -5.870314035667901\n",
            "74     \t [-2.29331148  3.90116256]. \t  -35.03333397031997 \t -5.870314035667901\n",
            "75     \t [-2.2529462   2.74463579]. \t  -33.130876123798174 \t -5.870314035667901\n",
            "76     \t [-1.15149669 -3.09353432]. \t  -16.772168025468215 \t -5.870314035667901\n",
            "77     \t [-0.0962529  -0.08295514]. \t  \u001b[92m-3.1177339188833066\u001b[0m \t -3.1177339188833066\n",
            "78     \t [-1.00322056  4.48775312]. \t  -41.1188353417323 \t -3.1177339188833066\n",
            "79     \t [0.90118256 0.88652475]. \t  -5.900315104261867 \t -3.1177339188833066\n",
            "80     \t [3.19834244 4.58622711]. \t  -56.64173959632752 \t -3.1177339188833066\n",
            "81     \t [0.58998612 4.06318459]. \t  -36.079049832099436 \t -3.1177339188833066\n",
            "82     \t [-5.02811109 -2.365616  ]. \t  -47.67572236455807 \t -3.1177339188833066\n",
            "83     \t [-4.66939711 -4.31373122]. \t  -69.16043430231525 \t -3.1177339188833066\n",
            "84     \t [ 4.478002   -0.12491166]. \t  -42.897744751589435 \t -3.1177339188833066\n",
            "85     \t [-3.10974372 -5.04046627]. \t  -37.6829156184347 \t -3.1177339188833066\n",
            "86     \t [0.91016187 2.90692843]. \t  -12.491599966949668 \t -3.1177339188833066\n",
            "87     \t [-0.03620434 -3.77129183]. \t  -23.147753188209187 \t -3.1177339188833066\n",
            "88     \t [ 2.3702754  -2.82189792]. \t  -36.074000516649754 \t -3.1177339188833066\n",
            "89     \t [-0.85199786  1.87697853]. \t  -11.111583972101856 \t -3.1177339188833066\n",
            "90     \t [-0.55041058  4.22191031]. \t  -45.874261136342206 \t -3.1177339188833066\n",
            "91     \t [1.00534544 4.1499809 ]. \t  -22.35987732451341 \t -3.1177339188833066\n",
            "92     \t [3.14215099 3.81570189]. \t  -34.151253951028785 \t -3.1177339188833066\n",
            "93     \t [-1.1192669  3.1572844]. \t  -18.39851709598948 \t -3.1177339188833066\n",
            "94     \t [-1.84202289  3.39870295]. \t  -37.52108151427881 \t -3.1177339188833066\n",
            "95     \t [0.93015226 4.91571839]. \t  -27.346821243891796 \t -3.1177339188833066\n",
            "96     \t [-0.57357119  3.40860789]. \t  -49.2941429899214 \t -3.1177339188833066\n",
            "97     \t [ 4.71916236 -2.64381857]. \t  -57.37331062141704 \t -3.1177339188833066\n",
            "98     \t [-0.10942634  2.68443554]. \t  -23.49411349670578 \t -3.1177339188833066\n",
            "99     \t [1.26469649 2.70071269]. \t  -32.86295316613385 \t -3.1177339188833066\n",
            "100    \t [-0.26393049 -0.2894339 ]. \t  -23.480024593843368 \t -3.1177339188833066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98Nt7Tguvna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd14eda-e3a7-4381-d0cb-200f0f93056d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-3.15884083  1.25039382]. \t  -26.146884045802807 \t -26.146884045802807\n",
            "init   \t [-0.63766795  2.9220719 ]. \t  -26.606539329930673 \t -26.146884045802807\n",
            "init   \t [ 2.86695228 -2.32865172]. \t  -31.680531551175278 \t -26.146884045802807\n",
            "init   \t [-2.28900603  3.0911711 ]. \t  -28.81762335775992 \t -26.146884045802807\n",
            "init   \t [4.69134698 3.84955018]. \t  -54.575233812225456 \t -26.146884045802807\n",
            "1      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -26.146884045802807\n",
            "2      \t [-0.52537473 -5.12      ]. \t  -49.07390514803535 \t -26.146884045802807\n",
            "3      \t [-0.60117407 -1.40941365]. \t  -38.817928514699744 \t -26.146884045802807\n",
            "4      \t [-5.12       -1.44099077]. \t  -50.3216690321195 \t -26.146884045802807\n",
            "5      \t [-5.12        4.55512019]. \t  -69.08008059286784 \t -26.146884045802807\n",
            "6      \t [2.0584221  1.36049198]. \t  \u001b[92m-23.152263244104873\u001b[0m \t -23.152263244104873\n",
            "7      \t [ 4.26677715 -4.73392291]. \t  -62.676037838021976 \t -23.152263244104873\n",
            "8      \t [ 4.42356517 -0.02120562]. \t  -38.52578518245134 \t -23.152263244104873\n",
            "9      \t [-2.72531484 -3.3764226 ]. \t  -47.50636268059132 \t -23.152263244104873\n",
            "10     \t [0.95575217 4.87282703]. \t  -28.068021404298527 \t -23.152263244104873\n",
            "11     \t [-1.66165331  5.07700354]. \t  -44.955118305234755 \t -23.152263244104873\n",
            "12     \t [-5.12       1.4081957]. \t  -49.28972145212175 \t -23.152263244104873\n",
            "13     \t [-0.19576757  0.8568735 ]. \t  \u001b[92m-11.208936559436385\u001b[0m \t -11.208936559436385\n",
            "14     \t [ 1.45876255 -3.93605349]. \t  -38.083075029741536 \t -11.208936559436385\n",
            "15     \t [-2.64989098 -0.77506649]. \t  -31.937572043213102 \t -11.208936559436385\n",
            "16     \t [1.92225436 3.42811812]. \t  -35.6139261702357 \t -11.208936559436385\n",
            "17     \t [ 1.76524223 -0.92314815]. \t  -14.155409286174054 \t -11.208936559436385\n",
            "18     \t [-1.07190581  0.77634482]. \t  \u001b[92m-11.107310380162271\u001b[0m \t -11.107310380162271\n",
            "19     \t [-0.81581744  0.95530873]. \t  \u001b[92m-7.951278623115167\u001b[0m \t -7.951278623115167\n",
            "20     \t [-0.53258485  1.25334045]. \t  -31.855525679578868 \t -7.951278623115167\n",
            "21     \t [-0.01295215 -0.26693591]. \t  -11.166625834886634 \t -7.951278623115167\n",
            "22     \t [ 4.89245972 -1.872057  ]. \t  -32.69892588289728 \t -7.951278623115167\n",
            "23     \t [-0.29216838  3.66328676]. \t  -41.30643224874567 \t -7.951278623115167\n",
            "24     \t [4.42755788 1.88706343]. \t  -44.559935599346666 \t -7.951278623115167\n",
            "25     \t [ 2.17902421 -0.19245186]. \t  -16.934418395155674 \t -7.951278623115167\n",
            "26     \t [2.31874049 4.74768616]. \t  -52.248514132279226 \t -7.951278623115167\n",
            "27     \t [-0.12063219  0.09181928]. \t  \u001b[92m-4.379066161713572\u001b[0m \t -4.379066161713572\n",
            "28     \t [ 0.38119017 -0.95822991]. \t  -18.746608995698775 \t -4.379066161713572\n",
            "29     \t [-0.0562475  0.3159467]. \t  -14.747017499559588 \t -4.379066161713572\n",
            "30     \t [-0.57822884 -0.14470018]. \t  -23.027477084017203 \t -4.379066161713572\n",
            "31     \t [-1.05430994 -2.93214872]. \t  -11.180731741392934 \t -4.379066161713572\n",
            "32     \t [-0.79288109  0.58822046]. \t  -26.815506842264842 \t -4.379066161713572\n",
            "33     \t [-1.98812432  1.34607106]. \t  -21.468737378884924 \t -4.379066161713572\n",
            "34     \t [ 1.0108431  -2.13952774]. \t  -9.225506227649337 \t -4.379066161713572\n",
            "35     \t [ 2.80667606 -0.61616637]. \t  -32.22326274228547 \t -4.379066161713572\n",
            "36     \t [-3.9703256   0.44325955]. \t  -35.50448202128924 \t -4.379066161713572\n",
            "37     \t [ 0.21053281 -1.83622767]. \t  -15.804939772571728 \t -4.379066161713572\n",
            "38     \t [ 1.06087815 -1.53585757]. \t  -23.95428089521237 \t -4.379066161713572\n",
            "39     \t [ 1.15500168 -2.8864032 ]. \t  -16.485482290390955 \t -4.379066161713572\n",
            "40     \t [-4.04132817  1.84641842]. \t  -24.382532790478592 \t -4.379066161713572\n",
            "41     \t [1.39279667 2.06679048]. \t  -24.895163004377856 \t -4.379066161713572\n",
            "42     \t [-2.05684489  0.72424311]. \t  -16.99752662677102 \t -4.379066161713572\n",
            "43     \t [2.65008388 1.65495996]. \t  -41.25833918287854 \t -4.379066161713572\n",
            "44     \t [-2.19503432 -2.10569521]. \t  -17.992075576313695 \t -4.379066161713572\n",
            "45     \t [-0.02536457 -0.90177737]. \t  \u001b[92m-2.7852665290751393\u001b[0m \t -2.7852665290751393\n",
            "46     \t [ 1.83303505 -5.12      ]. \t  -37.30097058912596 \t -2.7852665290751393\n",
            "47     \t [-1.61763609  1.19514797]. \t  -28.057048263385337 \t -2.7852665290751393\n",
            "48     \t [-3.90348032 -0.63478484]. \t  -34.04660455862844 \t -2.7852665290751393\n",
            "49     \t [-1.46101673 -3.6144596 ]. \t  -52.423930773665376 \t -2.7852665290751393\n",
            "50     \t [-2.81690934  2.10566921]. \t  -20.411820692719342 \t -2.7852665290751393\n",
            "51     \t [-1.62511372  3.53758945]. \t  -51.943933026283254 \t -2.7852665290751393\n",
            "52     \t [-4.75715833  1.98702467]. \t  -36.16241732881193 \t -2.7852665290751393\n",
            "53     \t [-4.72278104  0.35313878]. \t  -50.16751056057116 \t -2.7852665290751393\n",
            "54     \t [-1.79319857 -1.65807844]. \t  -28.743558217913048 \t -2.7852665290751393\n",
            "55     \t [-3.32340846 -1.26071709]. \t  -37.75789625288967 \t -2.7852665290751393\n",
            "56     \t [-2.64927699  1.35277602]. \t  -40.781277378515554 \t -2.7852665290751393\n",
            "57     \t [-2.20058298  2.35452741]. \t  -33.4366330026977 \t -2.7852665290751393\n",
            "58     \t [-2.06461617 -2.68345781]. \t  -26.336708269970224 \t -2.7852665290751393\n",
            "59     \t [-3.97423396  1.18108629]. \t  -23.124324323038856 \t -2.7852665290751393\n",
            "60     \t [-0.13411166  3.03095151]. \t  -12.738274998856618 \t -2.7852665290751393\n",
            "61     \t [-2.81554131  4.72874963]. \t  -47.61691208185622 \t -2.7852665290751393\n",
            "62     \t [-5.02334207  2.72383728]. \t  -44.39707077080579 \t -2.7852665290751393\n",
            "63     \t [-5.05360368 -0.84498161]. \t  -31.194862213780755 \t -2.7852665290751393\n",
            "64     \t [-3.5873662  -2.67678195]. \t  -53.005011736336726 \t -2.7852665290751393\n",
            "65     \t [-3.46039435  3.68182945]. \t  -59.37566695128865 \t -2.7852665290751393\n",
            "66     \t [ 2.28878121 -0.86384299]. \t  -21.838984430911594 \t -2.7852665290751393\n",
            "67     \t [ 3.02053532 -2.96309889]. \t  -18.25429861497306 \t -2.7852665290751393\n",
            "68     \t [ 2.02711115 -0.56112819]. \t  -23.840216368236533 \t -2.7852665290751393\n",
            "69     \t [2.4294631  0.50273429]. \t  -45.18741272994489 \t -2.7852665290751393\n",
            "70     \t [ 1.71261956 -2.54630544]. \t  -41.32361567794335 \t -2.7852665290751393\n",
            "71     \t [2.56962725 2.29326167]. \t  -43.605112053951835 \t -2.7852665290751393\n",
            "72     \t [ 3.16616439 -3.91840691]. \t  -31.6367968565732 \t -2.7852665290751393\n",
            "73     \t [ 1.07470967 -4.85710195]. \t  -29.595219620852532 \t -2.7852665290751393\n",
            "74     \t [-5.07980269 -0.21715951]. \t  -35.033703744712604 \t -2.7852665290751393\n",
            "75     \t [-1.50435399 -0.83644269]. \t  -27.790747961276875 \t -2.7852665290751393\n",
            "76     \t [ 3.81979252 -2.98862339]. \t  -29.302228235579996 \t -2.7852665290751393\n",
            "77     \t [1.88944688 1.98192609]. \t  -9.879519261449495 \t -2.7852665290751393\n",
            "78     \t [-0.82904951  3.29822571]. \t  -29.784452650379343 \t -2.7852665290751393\n",
            "79     \t [-2.28012132  3.96285433]. \t  -33.05559543486265 \t -2.7852665290751393\n",
            "80     \t [-4.28804555 -1.12678587]. \t  -35.03347167016901 \t -2.7852665290751393\n",
            "81     \t [ 1.77067085 -4.63725193]. \t  -49.8505659436293 \t -2.7852665290751393\n",
            "82     \t [-0.21948905  5.02638313]. \t  -33.54444819487259 \t -2.7852665290751393\n",
            "83     \t [-3.50028296 -4.21203664]. \t  -57.63046439400906 \t -2.7852665290751393\n",
            "84     \t [-0.07541132 -0.40667905]. \t  -19.60241511212343 \t -2.7852665290751393\n",
            "85     \t [ 1.63701227 -2.11066166]. \t  -25.97388842578516 \t -2.7852665290751393\n",
            "86     \t [ 0.07359759 -1.36703568]. \t  -19.63299882856986 \t -2.7852665290751393\n",
            "87     \t [ 0.45059142 -3.30971595]. \t  -44.343880615412715 \t -2.7852665290751393\n",
            "88     \t [-0.12427455 -2.77253337]. \t  -19.188072599782505 \t -2.7852665290751393\n",
            "89     \t [ 0.33749387 -2.58976326]. \t  -40.49667326180284 \t -2.7852665290751393\n",
            "90     \t [-0.29655296 -4.10400444]. \t  -31.874539387958535 \t -2.7852665290751393\n",
            "91     \t [ 0.54859811 -3.92747775]. \t  -36.28379589376167 \t -2.7852665290751393\n",
            "92     \t [-4.47664818 -3.34657678]. \t  -66.83500592164621 \t -2.7852665290751393\n",
            "93     \t [2.41139927 3.78471062]. \t  -46.465257954533 \t -2.7852665290751393\n",
            "94     \t [-0.56647889 -3.34879229]. \t  -46.491848691412194 \t -2.7852665290751393\n",
            "95     \t [-2.14746    -4.42840983]. \t  -47.22146462985425 \t -2.7852665290751393\n",
            "96     \t [0.32069164 4.25261693]. \t  -42.649086542554535 \t -2.7852665290751393\n",
            "97     \t [-0.88506095 -1.14014049]. \t  -8.212174253927593 \t -2.7852665290751393\n",
            "98     \t [ 4.81940896 -0.22225366]. \t  -37.31740079245908 \t -2.7852665290751393\n",
            "99     \t [-3.89787926  4.5883842 ]. \t  -56.732842995606674 \t -2.7852665290751393\n",
            "100    \t [-0.78366634 -0.60398137]. \t  -26.819963507631904 \t -2.7852665290751393\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpn-kmNuvqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a915a352-4174-4b34-f15b-1294828dffe7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.64499773 -0.88472932]. \t  -22.128094062039295 \t -22.128094062039295\n",
            "init   \t [ 1.72137019 -2.76537936]. \t  -31.434805818064365 \t -22.128094062039295\n",
            "init   \t [3.15062616 1.40102108]. \t  -34.17104049244217 \t -22.128094062039295\n",
            "init   \t [-3.35673495 -2.45364871]. \t  -53.08180488505975 \t -22.128094062039295\n",
            "init   \t [ 4.2528767  -0.38076919]. \t  -45.735368766041006 \t -22.128094062039295\n",
            "1      \t [-3.68998462  4.94754404]. \t  -52.31457314781095 \t -22.128094062039295\n",
            "2      \t [4.71654829 5.07173657]. \t  -61.05345619973109 \t -22.128094062039295\n",
            "3      \t [-1.66980886  1.47209369]. \t  -39.6300509938484 \t -22.128094062039295\n",
            "4      \t [0.04782765 4.79025443]. \t  -30.89458896926171 \t -22.128094062039295\n",
            "5      \t [ 4.29443739 -5.0571299 ]. \t  -57.41006843935719 \t -22.128094062039295\n",
            "6      \t [-5.12        0.44326999]. \t  -48.492633783709 \t -22.128094062039295\n",
            "7      \t [-1.11475007 -5.12      ]. \t  -32.655895090232335 \t -22.128094062039295\n",
            "8      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -22.128094062039295\n",
            "9      \t [-0.86024046 -1.23861073]. \t  \u001b[92m-15.173298115250663\u001b[0m \t -15.173298115250663\n",
            "10     \t [0.85285274 2.14777934]. \t  \u001b[92m-13.328245406046154\u001b[0m \t -13.328245406046154\n",
            "11     \t [ 1.41299217 -5.12      ]. \t  -49.46377899875385 \t -13.328245406046154\n",
            "12     \t [1.98898819 4.32751198]. \t  -37.387323536745356 \t -13.328245406046154\n",
            "13     \t [-4.72477049  2.51991663]. \t  -60.17382423698455 \t -13.328245406046154\n",
            "14     \t [ 4.78368497 -2.79191851]. \t  -45.97425602633363 \t -13.328245406046154\n",
            "15     \t [4.69592825 2.63572416]. \t  -58.91015040539969 \t -13.328245406046154\n",
            "16     \t [-0.91110783 -2.66278276]. \t  -24.649986988147916 \t -13.328245406046154\n",
            "17     \t [0.27722808 0.63374957]. \t  -28.852799708677903 \t -13.328245406046154\n",
            "18     \t [-1.90313003  3.76829294]. \t  -28.47089025694777 \t -13.328245406046154\n",
            "19     \t [-3.08937673 -0.09923627]. \t  \u001b[92m-12.971615999316162\u001b[0m \t -12.971615999316162\n",
            "20     \t [-1.55400054 -0.89117354]. \t  -24.887078615605738 \t -12.971615999316162\n",
            "21     \t [-0.40907909 -1.24203583]. \t  -29.62193779823948 \t -12.971615999316162\n",
            "22     \t [0.38423875 2.58230523]. \t  -42.977705595658946 \t -12.971615999316162\n",
            "23     \t [1.53346907 1.37596963]. \t  -41.138535005625634 \t -12.971615999316162\n",
            "24     \t [-1.37675274 -1.79109935]. \t  -29.698251645484337 \t -12.971615999316162\n",
            "25     \t [1.04121979 1.99697682]. \t  \u001b[92m-5.407371822823926\u001b[0m \t -5.407371822823926\n",
            "26     \t [-5.08889766 -1.96279181]. \t  -31.54127900931414 \t -5.407371822823926\n",
            "27     \t [1.79798777 2.13488086]. \t  -18.202062184579244 \t -5.407371822823926\n",
            "28     \t [-3.03282334 -4.62460446]. \t  -47.885515128069805 \t -5.407371822823926\n",
            "29     \t [ 2.30175983 -1.03283514]. \t  -19.772050694556963 \t -5.407371822823926\n",
            "30     \t [-3.04176008  0.50210625]. \t  -29.845801881290207 \t -5.407371822823926\n",
            "31     \t [5.0010626  0.91213924]. \t  -27.328312031940634 \t -5.407371822823926\n",
            "32     \t [-0.38196995 -4.20520225]. \t  -42.42577741908669 \t -5.407371822823926\n",
            "33     \t [-3.62757808 -0.63285432]. \t  -47.2290739846962 \t -5.407371822823926\n",
            "34     \t [ 3.06622956 -2.98426698]. \t  -19.209842728339478 \t -5.407371822823926\n",
            "35     \t [ 2.67145911 -3.99929572]. \t  -37.868154824286705 \t -5.407371822823926\n",
            "36     \t [-4.93263459 -3.1871614 ]. \t  -41.52489096571722 \t -5.407371822823926\n",
            "37     \t [2.55084909 5.10449115]. \t  -54.135499088900445 \t -5.407371822823926\n",
            "38     \t [-1.92361638  5.00598557]. \t  -29.897001325977005 \t -5.407371822823926\n",
            "39     \t [-5.06037633  4.53016305]. \t  -66.6616990146782 \t -5.407371822823926\n",
            "40     \t [1.17513218 2.18837794]. \t  -17.861617767876133 \t -5.407371822823926\n",
            "41     \t [-0.12147992  1.61510932]. \t  -22.894183496934815 \t -5.407371822823926\n",
            "42     \t [3.28198274 2.10004892]. \t  -29.08929059768451 \t -5.407371822823926\n",
            "43     \t [-2.58843791 -0.25906265]. \t  -35.83170202171351 \t -5.407371822823926\n",
            "44     \t [ 2.1359618  -1.68015222]. \t  -25.066713076953896 \t -5.407371822823926\n",
            "45     \t [-1.40474147  2.55081427]. \t  -46.2362472911591 \t -5.407371822823926\n",
            "46     \t [0.59059631 1.60914826]. \t  -39.10044089648183 \t -5.407371822823926\n",
            "47     \t [-0.4652063  -0.40356713]. \t  -38.36113721541547 \t -5.407371822823926\n",
            "48     \t [-0.84116363  1.12691241]. \t  -9.572039079109778 \t -5.407371822823926\n",
            "49     \t [-0.77651351  1.5388653 ]. \t  -31.016197233376936 \t -5.407371822823926\n",
            "50     \t [-0.45992269  0.4065769 ]. \t  -38.387545860250576 \t -5.407371822823926\n",
            "51     \t [ 0.63883943 -0.38296498]. \t  -34.40097621251242 \t -5.407371822823926\n",
            "52     \t [1.55721086 1.75959664]. \t  -34.279323151773546 \t -5.407371822823926\n",
            "53     \t [2.4419559  2.71524338]. \t  -44.8445025504026 \t -5.407371822823926\n",
            "54     \t [1.64313999 0.15442496]. \t  -23.29406958979908 \t -5.407371822823926\n",
            "55     \t [-0.29794521  2.37147532]. \t  -35.5925445971408 \t -5.407371822823926\n",
            "56     \t [-2.81833205  2.64537674]. \t  -36.8886285260117 \t -5.407371822823926\n",
            "57     \t [ 1.49592782 -1.54485372]. \t  -44.22659699603025 \t -5.407371822823926\n",
            "58     \t [1.65701662 4.69881826]. \t  -53.50088580413434 \t -5.407371822823926\n",
            "59     \t [-0.74952505  3.60319104]. \t  -41.545316912796864 \t -5.407371822823926\n",
            "60     \t [-1.84929112  2.62085846]. \t  -31.699672935370177 \t -5.407371822823926\n",
            "61     \t [1.18203457 0.52982347]. \t  -27.36108454140387 \t -5.407371822823926\n",
            "62     \t [-0.34342607  0.97067792]. \t  -16.768159931012818 \t -5.407371822823926\n",
            "63     \t [0.08254582 0.12488648]. \t  \u001b[92m-4.261412329502119\u001b[0m \t -4.261412329502119\n",
            "64     \t [-2.21543721  0.77337899]. \t  -21.88799339592494 \t -4.261412329502119\n",
            "65     \t [1.98204524 0.47575133]. \t  -24.102567302306625 \t -4.261412329502119\n",
            "66     \t [-3.70157229  2.67414763]. \t  -48.43632710271027 \t -4.261412329502119\n",
            "67     \t [4.26823013 1.10803177]. \t  -32.80508298092954 \t -4.261412329502119\n",
            "68     \t [-1.63363109  0.70814653]. \t  -32.44713942676445 \t -4.261412329502119\n",
            "69     \t [-2.65767781  0.94799908]. \t  -23.971876333785854 \t -4.261412329502119\n",
            "70     \t [-1.39712566  2.98841557]. \t  -28.89176815362285 \t -4.261412329502119\n",
            "71     \t [-3.12749057  4.12343045]. \t  -32.68386623669688 \t -4.261412329502119\n",
            "72     \t [2.16003488 2.42887373]. \t  -34.2266744175987 \t -4.261412329502119\n",
            "73     \t [3.23419685 3.7442823 ]. \t  -43.84754549689783 \t -4.261412329502119\n",
            "74     \t [ 5.05390594 -0.58588282]. \t  -45.03240515841137 \t -4.261412329502119\n",
            "75     \t [3.72010692 1.92718496]. \t  -30.448901286420615 \t -4.261412329502119\n",
            "76     \t [ 2.76627419 -1.92420273]. \t  -21.446860985689366 \t -4.261412329502119\n",
            "77     \t [2.2274729  4.63057868]. \t  -51.81211702719457 \t -4.261412329502119\n",
            "78     \t [-0.08723446  4.05542733]. \t  -18.5193145527676 \t -4.261412329502119\n",
            "79     \t [3.86446717 2.7752271 ]. \t  -34.469586547503695 \t -4.261412329502119\n",
            "80     \t [1.68860887 0.56283969]. \t  -36.161167886019406 \t -4.261412329502119\n",
            "81     \t [2.25602521e+00 1.66269649e-03]. \t  -15.468682571214682 \t -4.261412329502119\n",
            "82     \t [ 0.66129195 -2.3431403 ]. \t  -36.74097601433075 \t -4.261412329502119\n",
            "83     \t [-2.05740182 -3.23702684]. \t  -24.540400834101305 \t -4.261412329502119\n",
            "84     \t [ 1.63257498 -2.40694136]. \t  -43.52405883338022 \t -4.261412329502119\n",
            "85     \t [ 1.63152437 -3.48512442]. \t  -51.53965456566709 \t -4.261412329502119\n",
            "86     \t [-2.34120161  4.45511812]. \t  -60.35616814953608 \t -4.261412329502119\n",
            "87     \t [ 2.45117375 -3.65820578]. \t  -54.376928354448225 \t -4.261412329502119\n",
            "88     \t [ 3.50343505 -4.2508276 ]. \t  -60.393262785459456 \t -4.261412329502119\n",
            "89     \t [ 1.96450878 -0.54859178]. \t  -23.94539414922487 \t -4.261412329502119\n",
            "90     \t [ 3.132239   -1.29283365]. \t  -27.399040422966305 \t -4.261412329502119\n",
            "91     \t [ 2.68004087 -4.96650339]. \t  -46.32490654257594 \t -4.261412329502119\n",
            "92     \t [3.04533379 0.46669906]. \t  -29.676700241671405 \t -4.261412329502119\n",
            "93     \t [2.60355117 3.77823845]. \t  -47.24560123089298 \t -4.261412329502119\n",
            "94     \t [-1.55036138  4.35202767]. \t  -56.82773228045329 \t -4.261412329502119\n",
            "95     \t [5.08698648 2.37378936]. \t  -49.98616787203251 \t -4.261412329502119\n",
            "96     \t [4.57877585 0.53711912]. \t  -59.782804940890415 \t -4.261412329502119\n",
            "97     \t [-2.39012726 -1.97587721]. \t  -27.441670471827834 \t -4.261412329502119\n",
            "98     \t [0.00628662 2.32691586]. \t  -20.06920751596877 \t -4.261412329502119\n",
            "99     \t [3.31230002 2.94796945]. \t  -34.00671105823686 \t -4.261412329502119\n",
            "100    \t [3.53118728 5.04751986]. \t  -58.19779793792328 \t -4.261412329502119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NdFRXtPuvsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd5e18a-0120-40ec-8e10-13deeba06c46"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.51092275 0.07321281]. \t  -23.304224567038325 \t -16.03632029095263\n",
            "init   \t [0.29021573 4.05796049]. \t  -29.707120177200387 \t -16.03632029095263\n",
            "init   \t [2.04790979 2.19440232]. \t  -16.03632029095263 \t -16.03632029095263\n",
            "init   \t [ 2.22554503 -2.83832871]. \t  -26.209337872186715 \t -16.03632029095263\n",
            "init   \t [-3.32641768 -0.44194316]. \t  -45.22147500850562 \t -16.03632029095263\n",
            "1      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -16.03632029095263\n",
            "2      \t [-4.33898592  4.11681987]. \t  -53.654362587970155 \t -16.03632029095263\n",
            "3      \t [-0.94595467 -5.12      ]. \t  -30.39058682973107 \t -16.03632029095263\n",
            "4      \t [4.90419229 3.03713839]. \t  -35.304159856002975 \t -16.03632029095263\n",
            "5      \t [ 4.75658075 -1.60749822]. \t  -52.60012191203161 \t -16.03632029095263\n",
            "6      \t [ 4.96130655 -4.94676174]. \t  -49.93336717132176 \t -16.03632029095263\n",
            "7      \t [-1.7572216   1.93119409]. \t  -17.28378727476177 \t -16.03632029095263\n",
            "8      \t [-0.99139078 -2.44237136]. \t  -26.314240758379796 \t -16.03632029095263\n",
            "9      \t [-5.12       -2.29785314]. \t  -47.16644604226388 \t -16.03632029095263\n",
            "10     \t [ 1.3662165 -5.12     ]. \t  -47.46152099939072 \t -16.03632029095263\n",
            "11     \t [-2.12482046  5.10318183]. \t  -35.50723585423232 \t -16.03632029095263\n",
            "12     \t [2.65520299 4.68074164]. \t  -58.78530894407301 \t -16.03632029095263\n",
            "13     \t [-5.12       1.2611059]. \t  -41.21233964100173 \t -16.03632029095263\n",
            "14     \t [3.90734235 0.79183838]. \t  -24.94309080927643 \t -16.03632029095263\n",
            "15     \t [-0.62874385 -0.00445281]. \t  -17.30204399114002 \t -16.03632029095263\n",
            "16     \t [-2.80551601 -4.14802871]. \t  -35.681591964654714 \t -16.03632029095263\n",
            "17     \t [-0.30653661  2.12063392]. \t  -20.806749932396556 \t -16.03632029095263\n",
            "18     \t [ 0.72700192 -1.80993095]. \t  -21.567158945598695 \t -16.03632029095263\n",
            "19     \t [-2.90624218  2.0001568 ]. \t  \u001b[92m-14.132452586562442\u001b[0m \t -14.132452586562442\n",
            "20     \t [-2.69536958  2.98901806]. \t  -29.58856406643492 \t -14.132452586562442\n",
            "21     \t [4.56066364 5.0763808 ]. \t  -66.98128618510383 \t -14.132452586562442\n",
            "22     \t [-2.58855258  1.49586885]. \t  -47.42651925962695 \t -14.132452586562442\n",
            "23     \t [-3.75070758  2.14637515]. \t  -32.56970480955459 \t -14.132452586562442\n",
            "24     \t [-2.65640021  2.02059474]. \t  -26.770723287611887 \t -14.132452586562442\n",
            "25     \t [-0.9330005   1.65902893]. \t  -19.905621918768365 \t -14.132452586562442\n",
            "26     \t [-1.03353422  2.61008205]. \t  -25.803722485444048 \t -14.132452586562442\n",
            "27     \t [0.72673261 2.14687522]. \t  -20.558396076361323 \t -14.132452586562442\n",
            "28     \t [-0.47286954 -0.80883132]. \t  -27.119999220976066 \t -14.132452586562442\n",
            "29     \t [-3.24435694  1.63880122]. \t  -39.289126594469735 \t -14.132452586562442\n",
            "30     \t [-2.96327767  2.33591893]. \t  -29.642568475126893 \t -14.132452586562442\n",
            "31     \t [-3.10683325  3.99748832]. \t  -27.803142019886252 \t -14.132452586562442\n",
            "32     \t [-2.28360565  3.8400205 ]. \t  -36.69710710103804 \t -14.132452586562442\n",
            "33     \t [-3.22327198 -1.43954286]. \t  -40.07743032297111 \t -14.132452586562442\n",
            "34     \t [-2.78401592  3.52109403]. \t  -47.94009662601741 \t -14.132452586562442\n",
            "35     \t [-1.95457841  2.96331227]. \t  \u001b[92m-13.270596729378234\u001b[0m \t -13.270596729378234\n",
            "36     \t [-2.95098481  4.96837465]. \t  -34.06032935045383 \t -13.270596729378234\n",
            "37     \t [-5.01000039  1.7890186 ]. \t  -35.893299864455464 \t -13.270596729378234\n",
            "38     \t [-3.36196075  4.13316751]. \t  -48.15548791458957 \t -13.270596729378234\n",
            "39     \t [-5.11161431  3.03817251]. \t  -38.00526151137795 \t -13.270596729378234\n",
            "40     \t [-2.32436954  4.44779192]. \t  -59.15689934791702 \t -13.270596729378234\n",
            "41     \t [-4.97041594  4.08540992]. \t  -42.97359208354223 \t -13.270596729378234\n",
            "42     \t [-1.66054508  0.32463586]. \t  -32.711625295818976 \t -13.270596729378234\n",
            "43     \t [-4.91101477 -0.64282216]. \t  -42.29063094258831 \t -13.270596729378234\n",
            "44     \t [-0.28154581  2.78035407]. \t  -27.883104563946656 \t -13.270596729378234\n",
            "45     \t [0.20546847 5.02211785]. \t  -32.59866084199459 \t -13.270596729378234\n",
            "46     \t [-0.3472913  -0.25948402]. \t  -26.52280854436536 \t -13.270596729378234\n",
            "47     \t [-1.02391916 -0.74657352]. \t  \u001b[92m-11.933778920709138\u001b[0m \t -11.933778920709138\n",
            "48     \t [-0.7572827   2.07657676]. \t  -15.563570794145088 \t -11.933778920709138\n",
            "49     \t [-2.72369365  4.16217005]. \t  -41.14487727099058 \t -11.933778920709138\n",
            "50     \t [-0.46373644  3.83571437]. \t  -39.54030033531307 \t -11.933778920709138\n",
            "51     \t [-1.23717452  4.00286318]. \t  -26.750155638851872 \t -11.933778920709138\n",
            "52     \t [-1.62218497  1.12542293]. \t  -24.04083199877693 \t -11.933778920709138\n",
            "53     \t [-4.59269104  3.42983118]. \t  -70.2517795453542 \t -11.933778920709138\n",
            "54     \t [1.11634127 1.07306461]. \t  \u001b[92m-5.987959862412701\u001b[0m \t -5.987959862412701\n",
            "55     \t [-2.31982357 -0.78672261]. \t  -27.961343264753186 \t -5.987959862412701\n",
            "56     \t [2.7083875  2.30281093]. \t  -38.48085530619615 \t -5.987959862412701\n",
            "57     \t [0.51808123 1.30138726]. \t  -35.07050258769941 \t -5.987959862412701\n",
            "58     \t [1.91499217 1.31441708]. \t  -20.72555625843176 \t -5.987959862412701\n",
            "59     \t [-1.42746297 -0.2703172 ]. \t  -32.36307702020685 \t -5.987959862412701\n",
            "60     \t [ 0.77829829 -2.89868695]. \t  -19.19805161319117 \t -5.987959862412701\n",
            "61     \t [4.92114574 0.63977494]. \t  -42.2145961295937 \t -5.987959862412701\n",
            "62     \t [ 1.39870604 -2.3029591 ]. \t  -38.56856565755072 \t -5.987959862412701\n",
            "63     \t [ 3.00325826 -3.37788309]. \t  -37.62974289548558 \t -5.987959862412701\n",
            "64     \t [0.93225068 4.79086123]. \t  -32.17459264173737 \t -5.987959862412701\n",
            "65     \t [1.35675839 0.84523571]. \t  -23.138133034697816 \t -5.987959862412701\n",
            "66     \t [ 1.198773   -0.81445193]. \t  -14.997138187522738 \t -5.987959862412701\n",
            "67     \t [1.33722179 1.51098138]. \t  -39.25750571218604 \t -5.987959862412701\n",
            "68     \t [ 2.46277913 -1.09917161]. \t  -28.88058376202595 \t -5.987959862412701\n",
            "69     \t [2.47488763 1.69576874]. \t  -42.21837098239657 \t -5.987959862412701\n",
            "70     \t [2.33897783 2.81817637]. \t  -34.563027348651254 \t -5.987959862412701\n",
            "71     \t [ 3.24016932 -0.14764302]. \t  -23.906194372255435 \t -5.987959862412701\n",
            "72     \t [-0.97459174 -1.71565854]. \t  -16.161508231253663 \t -5.987959862412701\n",
            "73     \t [0.63711861 0.42513927]. \t  -36.013381771095446 \t -5.987959862412701\n",
            "74     \t [ 1.4822128  -0.61589715]. \t  -39.977611139462866 \t -5.987959862412701\n",
            "75     \t [ 2.98343709 -1.92549547]. \t  -13.738376123396382 \t -5.987959862412701\n",
            "76     \t [-1.49679882 -2.06125671]. \t  -27.218755526090114 \t -5.987959862412701\n",
            "77     \t [ 0.65648559 -0.84589005]. \t  -21.022893055346714 \t -5.987959862412701\n",
            "78     \t [-2.84670748 -0.81320713]. \t  -19.187963400904206 \t -5.987959862412701\n",
            "79     \t [2.61837318 0.07601714]. \t  -25.340063133720047 \t -5.987959862412701\n",
            "80     \t [-2.86185029  5.10568966]. \t  -39.91995193573708 \t -5.987959862412701\n",
            "81     \t [ 2.99973272 -1.26644269]. \t  -21.633575624928383 \t -5.987959862412701\n",
            "82     \t [-0.00753362 -3.53972305]. \t  -32.23104151038311 \t -5.987959862412701\n",
            "83     \t [-4.51716825  4.89461324]. \t  -66.41729040299278 \t -5.987959862412701\n",
            "84     \t [-1.30149104  1.79542489]. \t  -25.281024164981368 \t -5.987959862412701\n",
            "85     \t [ 2.31967104 -1.65542319]. \t  -37.95920318412442 \t -5.987959862412701\n",
            "86     \t [-4.28255035 -1.08097323]. \t  -32.806267168484474 \t -5.987959862412701\n",
            "87     \t [3.68378416 3.1074924 ]. \t  -39.463645043611976 \t -5.987959862412701\n",
            "88     \t [4.36219153 0.255178  ]. \t  -45.89884702747856 \t -5.987959862412701\n",
            "89     \t [4.5745973  1.48395521]. \t  -61.99981456805236 \t -5.987959862412701\n",
            "90     \t [ 5.12       -2.60099754]. \t  -53.743072325890324 \t -5.987959862412701\n",
            "91     \t [-3.60162339 -0.88089511]. \t  -34.449392325370496 \t -5.987959862412701\n",
            "92     \t [-3.97460458 -2.6936867 ]. \t  -36.645364985592224 \t -5.987959862412701\n",
            "93     \t [-3.31013958 -2.36611653]. \t  -46.9105083493364 \t -5.987959862412701\n",
            "94     \t [2.4413387 5.0933039]. \t  -52.90004325365118 \t -5.987959862412701\n",
            "95     \t [ 0.99352796 -0.31093108]. \t  -14.827618487786118 \t -5.987959862412701\n",
            "96     \t [ 1.2319498  -4.19370434]. \t  -34.50929680482356 \t -5.987959862412701\n",
            "97     \t [ 0.94655247 -1.41357327]. \t  -22.014191896781025 \t -5.987959862412701\n",
            "98     \t [-0.37513661 -2.382681  ]. \t  -40.29899121304487 \t -5.987959862412701\n",
            "99     \t [-4.35835998 -1.9440611 ]. \t  -39.68051927923809 \t -5.987959862412701\n",
            "100    \t [-4.82423208 -3.63991972]. \t  -58.40349501947909 \t -5.987959862412701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86panpOuvum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82be621-6a10-4f75-fb12-6cd01a7edd5b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.79751023 -2.50366638]. \t  -33.96069680534386 \t -17.757681446799715\n",
            "init   \t [-2.53412486  2.14942059]. \t  -34.905546618784385 \t -17.757681446799715\n",
            "init   \t [-0.5370638  -2.79610411]. \t  -34.98023435189899 \t -17.757681446799715\n",
            "init   \t [-1.00649405  3.91431921]. \t  -17.757681446799715 \t -17.757681446799715\n",
            "init   \t [-0.6429341   3.87507385]. \t  -34.58633031923943 \t -17.757681446799715\n",
            "1      \t [-2.1017041  4.4904548]. \t  -46.53659290860239 \t -17.757681446799715\n",
            "2      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -17.757681446799715\n",
            "3      \t [5.03078683 3.03139074]. \t  -34.87853335673067 \t -17.757681446799715\n",
            "4      \t [-5.12       -0.92799914]. \t  -30.79186718310666 \t -17.757681446799715\n",
            "5      \t [ 4.97947456 -4.77032287]. \t  -56.360735805823744 \t -17.757681446799715\n",
            "6      \t [ 4.58531299 -0.79578072]. \t  -47.41877174720946 \t -17.757681446799715\n",
            "7      \t [1.13159289 0.65399908]. \t  -20.608783321181246 \t -17.757681446799715\n",
            "8      \t [-1.89076293 -5.12      ]. \t  -34.76409852436383 \t -17.757681446799715\n",
            "9      \t [-5.07627791  3.5294828 ]. \t  -59.18143004845064 \t -17.757681446799715\n",
            "10     \t [ 1.26116961 -5.06809896]. \t  -38.87892588537109 \t -17.757681446799715\n",
            "11     \t [-2.4899733  -0.66423702]. \t  -41.75295873527785 \t -17.757681446799715\n",
            "12     \t [3.0503262  5.04761177]. \t  -35.72280018586936 \t -17.757681446799715\n",
            "13     \t [2.54742634 2.27495467]. \t  -42.78562098342792 \t -17.757681446799715\n",
            "14     \t [-3.64193262 -2.95243356]. \t  -38.70404400913419 \t -17.757681446799715\n",
            "15     \t [-1.46431426  3.23472255]. \t  -41.39888744683072 \t -17.757681446799715\n",
            "16     \t [-1.01132276  4.6208743 ]. \t  -39.65252054100676 \t -17.757681446799715\n",
            "17     \t [-3.58597884  1.29269759]. \t  -45.75696490818683 \t -17.757681446799715\n",
            "18     \t [ 3.04144621 -2.89179203]. \t  -20.17360792732634 \t -17.757681446799715\n",
            "19     \t [-0.43608043  0.01000714]. \t  -19.414322307054793 \t -17.757681446799715\n",
            "20     \t [ 1.86343381 -0.46455023]. \t  -26.902172712637487 \t -17.757681446799715\n",
            "21     \t [ 5.07827321 -2.53484893]. \t  -53.16073272073709 \t -17.757681446799715\n",
            "22     \t [ 2.71497754 -4.17754477]. \t  -42.60892739207959 \t -17.757681446799715\n",
            "23     \t [4.27647135 1.01758766]. \t  -31.040274470216687 \t -17.757681446799715\n",
            "24     \t [4.9529446  4.77085405]. \t  -56.42004942452951 \t -17.757681446799715\n",
            "25     \t [0.22785145 1.04965454]. \t  \u001b[92m-10.249293711656186\u001b[0m \t -10.249293711656186\n",
            "26     \t [-0.55253414  0.78939831]. \t  -27.938338719244516 \t -10.249293711656186\n",
            "27     \t [-0.30229016 -1.03861742]. \t  -14.689733183597426 \t -10.249293711656186\n",
            "28     \t [0.59122079 1.22806435]. \t  -28.885716606452114 \t -10.249293711656186\n",
            "29     \t [ 0.16258336 -0.16974263]. \t  \u001b[92m-10.003025855149286\u001b[0m \t -10.003025855149286\n",
            "30     \t [-0.00122981  0.15401418]. \t  \u001b[92m-4.352064430950285\u001b[0m \t -4.352064430950285\n",
            "31     \t [-4.55104548  5.08508556]. \t  -67.4554883965343 \t -4.352064430950285\n",
            "32     \t [-0.92800184 -1.84495453]. \t  -9.65247122750524 \t -4.352064430950285\n",
            "33     \t [ 2.13212774 -2.27468053]. \t  -24.51724915189912 \t -4.352064430950285\n",
            "34     \t [-1.09532275 -1.14679741]. \t  -8.216040314831313 \t -4.352064430950285\n",
            "35     \t [-1.89444317 -2.35465513]. \t  -27.36519202648973 \t -4.352064430950285\n",
            "36     \t [0.03162731 0.50021849]. \t  -20.44800911905955 \t -4.352064430950285\n",
            "37     \t [-0.11555142  1.6228846 ]. \t  -22.33337840047016 \t -4.352064430950285\n",
            "38     \t [-1.26378631  1.35495068]. \t  -30.424805643664897 \t -4.352064430950285\n",
            "39     \t [2.05469512 0.64795908]. \t  -21.207454089552282 \t -4.352064430950285\n",
            "40     \t [ 2.58209987 -0.90428455]. \t  -27.938259234098705 \t -4.352064430950285\n",
            "41     \t [1.88289394 0.15788879]. \t  -10.687477704536347 \t -4.352064430950285\n",
            "42     \t [-2.50369006 -1.68290691]. \t  -43.189780542273404 \t -4.352064430950285\n",
            "43     \t [-0.10062399  1.18865189]. \t  -9.596087254530264 \t -4.352064430950285\n",
            "44     \t [3.32965082 2.04151261]. \t  -30.39088866296241 \t -4.352064430950285\n",
            "45     \t [-2.89218958  2.93937215]. \t  -19.929404422614663 \t -4.352064430950285\n",
            "46     \t [ 0.27049733 -3.33388024]. \t  -37.50198387878304 \t -4.352064430950285\n",
            "47     \t [-1.90770348 -3.61853352]. \t  -35.720482467161446 \t -4.352064430950285\n",
            "48     \t [-0.76031507 -1.50170065]. \t  -32.18495155820484 \t -4.352064430950285\n",
            "49     \t [-1.50210527 -1.98441609]. \t  -26.241252596924895 \t -4.352064430950285\n",
            "50     \t [-2.6077245  -3.14035786]. \t  -38.10065267154073 \t -4.352064430950285\n",
            "51     \t [ 1.02193821 -3.58414552]. \t  -32.61993417153835 \t -4.352064430950285\n",
            "52     \t [-1.20590489 -2.5218785 ]. \t  -34.984472997112626 \t -4.352064430950285\n",
            "53     \t [-0.23385532 -2.38068354]. \t  -32.028699967148206 \t -4.352064430950285\n",
            "54     \t [-3.135537   -2.34751381]. \t  -34.5054038627098 \t -4.352064430950285\n",
            "55     \t [-4.84904425 -3.43299699]. \t  -58.59637309281978 \t -4.352064430950285\n",
            "56     \t [-1.76502313 -0.91584063]. \t  -14.377353829319645 \t -4.352064430950285\n",
            "57     \t [2.56009327 3.24057195]. \t  -45.758954450357194 \t -4.352064430950285\n",
            "58     \t [ 2.12560226 -0.84570932]. \t  -12.531510695281785 \t -4.352064430950285\n",
            "59     \t [ 1.55286302 -3.07499706]. \t  -32.4102821174629 \t -4.352064430950285\n",
            "60     \t [-0.37612856 -0.57640231]. \t  -36.464461275433315 \t -4.352064430950285\n",
            "61     \t [ 0.70596245 -1.79462932]. \t  -23.68332869737933 \t -4.352064430950285\n",
            "62     \t [-3.51223231 -0.47001267]. \t  -52.35018818678378 \t -4.352064430950285\n",
            "63     \t [ 0.15192637 -1.03128543]. \t  -5.499709749017807 \t -4.352064430950285\n",
            "64     \t [-1.88669315  0.56423324]. \t  -25.503505580914474 \t -4.352064430950285\n",
            "65     \t [-1.80982132 -4.40955874]. \t  -47.47724742111777 \t -4.352064430950285\n",
            "66     \t [-4.50657678 -2.92873519]. \t  -49.86403908459047 \t -4.352064430950285\n",
            "67     \t [-4.13652847 -4.15016342]. \t  -41.924405125455394 \t -4.352064430950285\n",
            "68     \t [-4.26497113 -3.41917996]. \t  -59.55817601497764 \t -4.352064430950285\n",
            "69     \t [-1.2069298   0.31177781]. \t  -22.665486724389407 \t -4.352064430950285\n",
            "70     \t [-2.8905911   0.80952399]. \t  -17.628672053286202 \t -4.352064430950285\n",
            "71     \t [2.68999012 1.44486159]. \t  -42.41135284980274 \t -4.352064430950285\n",
            "72     \t [ 0.33817679 -4.43709362]. \t  -54.29235292387487 \t -4.352064430950285\n",
            "73     \t [-1.99599379  1.23823449]. \t  -14.781808013414874 \t -4.352064430950285\n",
            "74     \t [-3.80808713 -5.12      ]. \t  -49.857008094964314 \t -4.352064430950285\n",
            "75     \t [3.0240555  2.64418064]. \t  -32.420277716544746 \t -4.352064430950285\n",
            "76     \t [-4.71230854  2.18062562]. \t  -45.08493711228544 \t -4.352064430950285\n",
            "77     \t [ 1.36529606 -2.18017001]. \t  -28.996110237256314 \t -4.352064430950285\n",
            "78     \t [4.51487866 2.02904121]. \t  -44.62348930331223 \t -4.352064430950285\n",
            "79     \t [0.27210648 4.62845082]. \t  -49.797229543869875 \t -4.352064430950285\n",
            "80     \t [-0.94703741 -2.16028015]. \t  -10.768893924120274 \t -4.352064430950285\n",
            "81     \t [-1.00933338 -4.26686587]. \t  -30.299819790056105 \t -4.352064430950285\n",
            "82     \t [-2.61505263 -2.65395535]. \t  -47.055902141163315 \t -4.352064430950285\n",
            "83     \t [-3.51110961 -1.47217159]. \t  -54.31835157783239 \t -4.352064430950285\n",
            "84     \t [-3.00317333 -0.19850906]. \t  -15.881317012915527 \t -4.352064430950285\n",
            "85     \t [ 1.66726082 -4.04644546]. \t  -34.54391132618639 \t -4.352064430950285\n",
            "86     \t [-3.52360542 -4.37016684]. \t  -68.25747385953765 \t -4.352064430950285\n",
            "87     \t [-4.74309194  0.2074969 ]. \t  -40.33496808066906 \t -4.352064430950285\n",
            "88     \t [-3.10783483 -1.37314089]. \t  -30.741009497025587 \t -4.352064430950285\n",
            "89     \t [-5.12       -4.23434673]. \t  -55.87246693517931 \t -4.352064430950285\n",
            "90     \t [ 3.27167695 -5.02326844]. \t  -47.401573511485715 \t -4.352064430950285\n",
            "91     \t [-4.26762686 -2.43097358]. \t  -54.301684188190265 \t -4.352064430950285\n",
            "92     \t [-4.82485232 -0.49008646]. \t  -48.96835624149266 \t -4.352064430950285\n",
            "93     \t [-4.50475881 -5.05299773]. \t  -66.37049080403612 \t -4.352064430950285\n",
            "94     \t [-0.11389905 -1.24025213]. \t  -13.392427908508717 \t -4.352064430950285\n",
            "95     \t [-1.41274415  1.9272153 ]. \t  -25.272020082045785 \t -4.352064430950285\n",
            "96     \t [ 2.26170005 -3.20502311]. \t  -33.33341742058387 \t -4.352064430950285\n",
            "97     \t [-0.93979991 -5.09812064]. \t  -29.421916478423434 \t -4.352064430950285\n",
            "98     \t [ 3.45864628 -2.23284129]. \t  -45.536116828976354 \t -4.352064430950285\n",
            "99     \t [ 3.71157902 -3.96878108]. \t  -42.109490967018885 \t -4.352064430950285\n",
            "100    \t [-4.14316074  0.21528273]. \t  -28.82806264847789 \t -4.352064430950285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "any0xrgYuvxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60126aac-d396-406e-aa4c-8dd0a15e9208"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.72683092 -1.88041214]. \t  -31.568449729559287 \t -13.57707882297342\n",
            "init   \t [1.76282397 0.08101548]. \t  -13.57707882297342 \t -13.57707882297342\n",
            "init   \t [ 2.88420277 -2.21509961]. \t  -23.5820775563595 \t -13.57707882297342\n",
            "init   \t [-2.72682205  0.65330879]. \t  -35.02194386260499 \t -13.57707882297342\n",
            "init   \t [3.84024949 2.2419755 ]. \t  -33.89849286588301 \t -13.57707882297342\n",
            "1      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -13.57707882297342\n",
            "2      \t [-0.4833691   4.77499064]. \t  -51.41587121018412 \t -13.57707882297342\n",
            "3      \t [-0.22484127 -5.12      ]. \t  -37.401072980320734 \t -13.57707882297342\n",
            "4      \t [-5.12        4.14969441]. \t  -50.25130191228371 \t -13.57707882297342\n",
            "5      \t [ 3.66651022 -5.12      ]. \t  -57.376521334887286 \t -13.57707882297342\n",
            "6      \t [-5.12      -1.6409068]. \t  -47.94752464022621 \t -13.57707882297342\n",
            "7      \t [-0.72092522 -1.88156692]. \t  -18.519978017055124 \t -13.57707882297342\n",
            "8      \t [0.24901982 1.72107202]. \t  -24.770120683171964 \t -13.57707882297342\n",
            "9      \t [-2.60927994 -3.62302479]. \t  -54.826820004858455 \t -13.57707882297342\n",
            "10     \t [5.08518038 4.5561182 ]. \t  -67.40040360776578 \t -13.57707882297342\n",
            "11     \t [-5.12        1.33873602]. \t  -46.007972365765625 \t -13.57707882297342\n",
            "12     \t [2.15753098 4.06444075]. \t  -26.494572385246904 \t -13.57707882297342\n",
            "13     \t [ 1.2168406  -2.69882486]. \t  -29.856238637931312 \t -13.57707882297342\n",
            "14     \t [-2.07999018  2.78723749]. \t  -21.013275540911188 \t -13.57707882297342\n",
            "15     \t [4.18123734 0.17541823]. \t  -28.809722112686668 \t -13.57707882297342\n",
            "16     \t [-0.11567257 -0.38611305]. \t  -20.236541742798956 \t -13.57707882297342\n",
            "17     \t [-2.93516572  5.02582075]. \t  -34.82371060628865 \t -13.57707882297342\n",
            "18     \t [ 4.98785736 -3.21228826]. \t  -42.879218821389934 \t -13.57707882297342\n",
            "19     \t [-2.08493099 -1.45327914]. \t  -27.42154101886384 \t -13.57707882297342\n",
            "20     \t [1.85235855 1.78495329]. \t  -18.441629889481174 \t -13.57707882297342\n",
            "21     \t [1.77218244 0.03736658]. \t  \u001b[92m-12.027118065195445\u001b[0m \t -12.027118065195445\n",
            "22     \t [ 1.18384273 -1.10528897]. \t  \u001b[92m-10.694631865915664\u001b[0m \t -10.694631865915664\n",
            "23     \t [ 1.34966405 -0.60280803]. \t  -36.03094371522893 \t -10.694631865915664\n",
            "24     \t [2.53085361 0.10142932]. \t  -28.191131786714735 \t -10.694631865915664\n",
            "25     \t [ 1.95885154 -1.23371092]. \t  -14.669820861328622 \t -10.694631865915664\n",
            "26     \t [-0.33770534 -1.32772506]. \t  -31.80467102324913 \t -10.694631865915664\n",
            "27     \t [-0.76067956  0.32419726]. \t  -24.508131063390387 \t -10.694631865915664\n",
            "28     \t [-1.08248002 -2.71513042]. \t  -22.030193879400766 \t -10.694631865915664\n",
            "29     \t [ 0.78719308 -1.98365016]. \t  -12.291562777876994 \t -10.694631865915664\n",
            "30     \t [ 1.30408281 -1.79900214]. \t  -25.239661387758762 \t -10.694631865915664\n",
            "31     \t [ 1.38908098 -3.86061011]. \t  -38.09832324670143 \t -10.694631865915664\n",
            "32     \t [4.7643749 0.0868051]. \t  -33.255695661341434 \t -10.694631865915664\n",
            "33     \t [0.8900007 2.741483 ]. \t  -21.137553515636327 \t -10.694631865915664\n",
            "34     \t [ 2.05435848 -0.11165372]. \t  \u001b[92m-7.171985015815268\u001b[0m \t -7.171985015815268\n",
            "35     \t [2.00250086 2.45396039]. \t  -29.61767402049714 \t -7.171985015815268\n",
            "36     \t [ 0.25153484 -3.14843775]. \t  -24.115385530113997 \t -7.171985015815268\n",
            "37     \t [3.46660766 0.71364863]. \t  -44.571584482146726 \t -7.171985015815268\n",
            "38     \t [2.85422543 4.78729873]. \t  -42.65213380647656 \t -7.171985015815268\n",
            "39     \t [-1.29223476 -2.19997377]. \t  -26.040669837998163 \t -7.171985015815268\n",
            "40     \t [-0.08572703 -2.63265501]. \t  -25.077062572260683 \t -7.171985015815268\n",
            "41     \t [-3.21160415 -0.80352534]. \t  -25.270848405457656 \t -7.171985015815268\n",
            "42     \t [ 0.31803427 -1.59377407]. \t  -35.100832575773914 \t -7.171985015815268\n",
            "43     \t [-1.13843901 -0.80339507]. \t  -12.199653869477983 \t -7.171985015815268\n",
            "44     \t [-0.63439636 -0.22851723]. \t  -25.75049274822363 \t -7.171985015815268\n",
            "45     \t [-1.23742465  1.47946333]. \t  -32.84758558385451 \t -7.171985015815268\n",
            "46     \t [-0.59470338  1.08163392]. \t  -21.091723139311867 \t -7.171985015815268\n",
            "47     \t [-1.57328427 -1.23661181]. \t  -32.122707153023484 \t -7.171985015815268\n",
            "48     \t [-3.0675386  -2.01200293]. \t  -14.37333918810273 \t -7.171985015815268\n",
            "49     \t [-1.8519402  -0.65491608]. \t  -23.507753860108167 \t -7.171985015815268\n",
            "50     \t [-2.88678205  2.73985367]. \t  -28.902703833702873 \t -7.171985015815268\n",
            "51     \t [ 0.67421042 -2.82152105]. \t  -28.65552149731377 \t -7.171985015815268\n",
            "52     \t [-1.90199945  0.67038036]. \t  -20.700219841415837 \t -7.171985015815268\n",
            "53     \t [ 1.79380962 -3.59420676]. \t  -41.71678326605355 \t -7.171985015815268\n",
            "54     \t [-0.21387195 -4.27504898]. \t  -37.63862184762466 \t -7.171985015815268\n",
            "55     \t [1.5529924  4.78886106]. \t  -52.37824033917959 \t -7.171985015815268\n",
            "56     \t [ 2.76792506 -1.4629555 ]. \t  -38.40810318826243 \t -7.171985015815268\n",
            "57     \t [1.50006899 2.02422658]. \t  -26.463330591757508 \t -7.171985015815268\n",
            "58     \t [0.16879261 2.7163677 ]. \t  -24.620755245005235 \t -7.171985015815268\n",
            "59     \t [-0.16501975  0.53149758]. \t  -25.02518111284599 \t -7.171985015815268\n",
            "60     \t [2.42743167 1.90017375]. \t  -30.384888942102762 \t -7.171985015815268\n",
            "61     \t [4.21441209 2.52320218]. \t  -51.80427508743782 \t -7.171985015815268\n",
            "62     \t [-2.66302936 -1.8135538 ]. \t  -31.68937773089054 \t -7.171985015815268\n",
            "63     \t [-4.05151041 -2.84105493]. \t  -29.591404050911912 \t -7.171985015815268\n",
            "64     \t [-3.65076486 -2.30568984]. \t  -47.911323817691574 \t -7.171985015815268\n",
            "65     \t [-3.62613448 -3.85076831]. \t  -49.0809140394557 \t -7.171985015815268\n",
            "66     \t [-4.30233301 -3.55711812]. \t  -63.75529923101757 \t -7.171985015815268\n",
            "67     \t [-3.40212106 -3.28210946]. \t  -52.51829192975486 \t -7.171985015815268\n",
            "68     \t [-4.77862881 -2.97023239]. \t  -40.04285955601216 \t -7.171985015815268\n",
            "69     \t [-3.81748978  1.71391302]. \t  -35.64420105984538 \t -7.171985015815268\n",
            "70     \t [-2.1074264 -5.0838942]. \t  -33.837451514339776 \t -7.171985015815268\n",
            "71     \t [-2.68493158 -1.25941964]. \t  -33.36192705952337 \t -7.171985015815268\n",
            "72     \t [-4.20630961  0.43130885]. \t  -44.25124749479872 \t -7.171985015815268\n",
            "73     \t [-3.09228681 -4.46027944]. \t  -50.78109301026808 \t -7.171985015815268\n",
            "74     \t [-3.87492784 -0.36009343]. \t  -34.455633699961105 \t -7.171985015815268\n",
            "75     \t [-3.38896826  0.26470552]. \t  -40.14148463314554 \t -7.171985015815268\n",
            "76     \t [-1.21824331  0.10868043]. \t  -11.756093975849623 \t -7.171985015815268\n",
            "77     \t [-2.85634694  1.56797637]. \t  -33.52322064625056 \t -7.171985015815268\n",
            "78     \t [-0.77873073 -2.51878342]. \t  -35.08571239813055 \t -7.171985015815268\n",
            "79     \t [-1.91256435 -2.96237621]. \t  -14.183181164321377 \t -7.171985015815268\n",
            "80     \t [-1.54389677 -4.07393762]. \t  -39.66245900898918 \t -7.171985015815268\n",
            "81     \t [-1.64400452 -3.37562746]. \t  -47.37487457598273 \t -7.171985015815268\n",
            "82     \t [-1.58632919 -4.65421305]. \t  -58.40440748100442 \t -7.171985015815268\n",
            "83     \t [ 0.60395065 -5.12      ]. \t  -47.231258692664795 \t -7.171985015815268\n",
            "84     \t [-1.93081883 -2.56295869]. \t  -30.45449023191432 \t -7.171985015815268\n",
            "85     \t [-4.94503194  0.78977279]. \t  -33.19452557742955 \t -7.171985015815268\n",
            "86     \t [-4.20800125  1.04319171]. \t  -26.553169703137932 \t -7.171985015815268\n",
            "87     \t [-3.91930326 -4.78442463]. \t  -47.36362695682431 \t -7.171985015815268\n",
            "88     \t [1.99414633 0.14489252]. \t  -7.869971122768624 \t -7.171985015815268\n",
            "89     \t [0.31295993 2.2569615 ]. \t  -29.48260043897678 \t -7.171985015815268\n",
            "90     \t [0.36551944 3.83233347]. \t  -36.512454466854294 \t -7.171985015815268\n",
            "91     \t [-1.40951977  0.85862836]. \t  -24.843462420829617 \t -7.171985015815268\n",
            "92     \t [0.88728116 3.48179056]. \t  -35.24963012469825 \t -7.171985015815268\n",
            "93     \t [ 1.63816721 -3.06748791]. \t  -29.441274015124126 \t -7.171985015815268\n",
            "94     \t [-0.7592038  -3.95718901]. \t  -26.017365906599025 \t -7.171985015815268\n",
            "95     \t [-3.85375295  2.72174588]. \t  -37.958290343815335 \t -7.171985015815268\n",
            "96     \t [-2.35795462 -4.65480048]. \t  -59.13300905776252 \t -7.171985015815268\n",
            "97     \t [ 1.83146262 -4.60664776]. \t  -47.5152674459232 \t -7.171985015815268\n",
            "98     \t [ 3.7842303  -2.44194556]. \t  -47.491356774046125 \t -7.171985015815268\n",
            "99     \t [ 5.02538744 -0.90413146]. \t  -27.958917928233088 \t -7.171985015815268\n",
            "100    \t [-2.21661369  2.20486733]. \t  -24.894528712309967 \t -7.171985015815268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reLyKt6Quvzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05a79db-f64e-4b2a-ba7f-d4f81fabd1b5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.55467805 2.36052095]. \t  -43.80415029879179 \t -20.520783967472433\n",
            "init   \t [-3.13991821  0.20149492]. \t  -20.520783967472433 \t -20.520783967472433\n",
            "init   \t [1.80616465 3.1175155 ]. \t  -22.129339239545118 \t -20.520783967472433\n",
            "init   \t [0.34317173 4.17827   ]. \t  -38.745269185045146 \t -20.520783967472433\n",
            "init   \t [-0.57875324 -2.49884457]. \t  -45.3794522559586 \t -20.520783967472433\n",
            "1      \t [3.48763249 4.56729682]. \t  -72.11288244713099 \t -20.520783967472433\n",
            "2      \t [ 4.18376978 -4.97326062]. \t  -48.335754328835264 \t -20.520783967472433\n",
            "3      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -20.520783967472433\n",
            "4      \t [-4.59286463  4.75822255]. \t  -71.56415226613798 \t -20.520783967472433\n",
            "5      \t [ 2.93549045 -1.03624219]. \t  \u001b[92m-10.7593162424175\u001b[0m \t -10.7593162424175\n",
            "6      \t [-5.11804298 -1.80151473]. \t  -38.88596809264451 \t -10.7593162424175\n",
            "7      \t [4.90419243 0.44557107]. \t  -45.42835980446217 \t -10.7593162424175\n",
            "8      \t [-1.82934396 -5.12      ]. \t  -37.48983842892195 \t -10.7593162424175\n",
            "9      \t [-2.12037042  2.57196839]. \t  -32.8321783924987 \t -10.7593162424175\n",
            "10     \t [ 1.19496173 -5.12      ]. \t  -36.9630059822656 \t -10.7593162424175\n",
            "11     \t [-5.12        1.57002417]. \t  -50.437313462442795 \t -10.7593162424175\n",
            "12     \t [-0.1064799   0.05256851]. \t  \u001b[92m-2.710424652459796\u001b[0m \t -2.710424652459796\n",
            "13     \t [-0.57015774  0.01049238]. \t  -19.39095902783496 \t -2.710424652459796\n",
            "14     \t [0.58557261 0.03121761]. \t  -19.124670489037378 \t -2.710424652459796\n",
            "15     \t [-0.08248001  0.08102534]. \t  \u001b[92m-2.594582007713207\u001b[0m \t -2.594582007713207\n",
            "16     \t [ 3.43594676 -2.38266047]. \t  -54.08690284250296 \t -2.594582007713207\n",
            "17     \t [4.5820686  2.43094463]. \t  -64.6779396329304 \t -2.594582007713207\n",
            "18     \t [-0.12177944 -0.12614863]. \t  -5.798195989817982 \t -2.594582007713207\n",
            "19     \t [ 2.58179338 -0.83993832]. \t  -30.72438523261907 \t -2.594582007713207\n",
            "20     \t [-1.77256984  5.07315555]. \t  -38.50347556802406 \t -2.594582007713207\n",
            "21     \t [ 3.52279344 -0.76512385]. \t  -41.94427998914542 \t -2.594582007713207\n",
            "22     \t [-3.35229884 -2.83637454]. \t  -40.112456366907864 \t -2.594582007713207\n",
            "23     \t [ 2.45936006 -1.92978891]. \t  -30.405675645956883 \t -2.594582007713207\n",
            "24     \t [-4.10929701 -0.31322769]. \t  -33.120257064066344 \t -2.594582007713207\n",
            "25     \t [-0.58532454 -4.24887757]. \t  -46.92206366017355 \t -2.594582007713207\n",
            "26     \t [ 2.94654044 -1.4177194 ]. \t  -29.944013441030734 \t -2.594582007713207\n",
            "27     \t [ 2.66661254 -3.57728792]. \t  -53.75464015121851 \t -2.594582007713207\n",
            "28     \t [ 1.51028369 -2.65172057]. \t  -45.08176190380857 \t -2.594582007713207\n",
            "29     \t [-3.49871528  2.33039712]. \t  -52.510821019019204 \t -2.594582007713207\n",
            "30     \t [ 3.92410485 -3.33722861]. \t  -42.86174210928388 \t -2.594582007713207\n",
            "31     \t [ 2.98120745 -0.62218605]. \t  -26.53931988464322 \t -2.594582007713207\n",
            "32     \t [ 4.99542124 -2.81865228]. \t  -38.72215121734042 \t -2.594582007713207\n",
            "33     \t [0.06287683 0.12086464]. \t  -3.5364634199674114 \t -2.594582007713207\n",
            "34     \t [5.1162827  4.94587153]. \t  -53.76320848174146 \t -2.594582007713207\n",
            "35     \t [1.50510718 4.55941106]. \t  -62.359751994439264 \t -2.594582007713207\n",
            "36     \t [-0.21531015  3.16151861]. \t  -22.601686047984657 \t -2.594582007713207\n",
            "37     \t [-2.96561205 -4.44892957]. \t  -48.30991142694378 \t -2.594582007713207\n",
            "38     \t [-0.02294807  0.0402411 ]. \t  \u001b[92m-0.42386219965042216\u001b[0m \t -0.42386219965042216\n",
            "39     \t [-2.10190019  0.77190673]. \t  -15.622308048680932 \t -0.42386219965042216\n",
            "40     \t [ 0.21041274 -0.2835968 ]. \t  -19.758235396866212 \t -0.42386219965042216\n",
            "41     \t [-2.13756099 -2.7776347 ]. \t  -24.065227230761618 \t -0.42386219965042216\n",
            "42     \t [-2.22725776 -0.6452295 ]. \t  -30.07058942944822 \t -0.42386219965042216\n",
            "43     \t [2.52738197 2.48925317]. \t  -52.413617540702994 \t -0.42386219965042216\n",
            "44     \t [-4.81962359 -2.89728672]. \t  -39.39786698119388 \t -0.42386219965042216\n",
            "45     \t [ 5.09451445 -5.07604772]. \t  -54.552436606930236 \t -0.42386219965042216\n",
            "46     \t [-1.66020306  3.65556908]. \t  -47.058174976233595 \t -0.42386219965042216\n",
            "47     \t [ 0.3870911  -3.31050636]. \t  -42.40744902129183 \t -0.42386219965042216\n",
            "48     \t [ 3.68609746 -3.21578233]. \t  -45.70323666924233 \t -0.42386219965042216\n",
            "49     \t [-0.85395424  3.70899599]. \t  -30.956820099891516 \t -0.42386219965042216\n",
            "50     \t [ 2.55482874 -2.75175163]. \t  -43.401678500123516 \t -0.42386219965042216\n",
            "51     \t [ 0.03251924 -0.00964808]. \t  \u001b[92m-0.22753646304498787\u001b[0m \t -0.22753646304498787\n",
            "52     \t [ 2.62821934 -4.82388359]. \t  -52.626696256679764 \t -0.22753646304498787\n",
            "53     \t [-0.42455382  3.412594  ]. \t  -49.252832814913134 \t -0.22753646304498787\n",
            "54     \t [0.31655091 2.73606621]. \t  -32.5213514350554 \t -0.22753646304498787\n",
            "55     \t [-0.97356547  4.23052147]. \t  -27.761940338659564 \t -0.22753646304498787\n",
            "56     \t [-1.49616598  2.02817859]. \t  -26.505446342001093 \t -0.22753646304498787\n",
            "57     \t [-0.62458207  2.57109111]. \t  -43.109091993844906 \t -0.22753646304498787\n",
            "58     \t [-2.82392382  1.73838415]. \t  -27.24616992233046 \t -0.22753646304498787\n",
            "59     \t [0.86962432 3.51065288]. \t  -36.23029278053478 \t -0.22753646304498787\n",
            "60     \t [0.18455823 4.77519893]. \t  -37.26295985184881 \t -0.22753646304498787\n",
            "61     \t [-2.43145172  1.87517498]. \t  -31.436131647630557 \t -0.22753646304498787\n",
            "62     \t [-3.39425388  1.01212536]. \t  -30.447094815200828 \t -0.22753646304498787\n",
            "63     \t [-2.08746815  0.10648999]. \t  -7.997591150453612 \t -0.22753646304498787\n",
            "64     \t [-1.19376877  2.85955698]. \t  -19.789318675216663 \t -0.22753646304498787\n",
            "65     \t [-3.35354603 -2.40176322]. \t  -51.22615915436691 \t -0.22753646304498787\n",
            "66     \t [2.82592303 3.07128371]. \t  -23.813487887538876 \t -0.22753646304498787\n",
            "67     \t [-4.76317194  4.78905818]. \t  -62.36667373106892 \t -0.22753646304498787\n",
            "68     \t [-4.64774374 -0.09371686]. \t  -39.28640018310186 \t -0.22753646304498787\n",
            "69     \t [2.45667807 3.40921129]. \t  -55.706429265634625 \t -0.22753646304498787\n",
            "70     \t [4.08789812 3.20467892]. \t  -35.65829469309266 \t -0.22753646304498787\n",
            "71     \t [0.64436756 3.60663859]. \t  -47.42150265921971 \t -0.22753646304498787\n",
            "72     \t [4.12770892 4.40622401]. \t  -57.8168840195733 \t -0.22753646304498787\n",
            "73     \t [-2.07737776  0.96354911]. \t  -6.663801435217769 \t -0.22753646304498787\n",
            "74     \t [0.90195472 4.20804602]. \t  -27.753800792409866 \t -0.22753646304498787\n",
            "75     \t [-2.37605672 -3.51461796]. \t  -55.073895430271676 \t -0.22753646304498787\n",
            "76     \t [-4.71678865  2.61377713]. \t  -58.703245992705035 \t -0.22753646304498787\n",
            "77     \t [-1.64334082  1.19567918]. \t  -26.994129235216487 \t -0.22753646304498787\n",
            "78     \t [3.35528587 3.1822327 ]. \t  -43.397356103590326 \t -0.22753646304498787\n",
            "79     \t [4.8779903  2.96918755]. \t  -35.59501752497236 \t -0.22753646304498787\n",
            "80     \t [2.52367632 0.66548758]. \t  -41.76538917256753 \t -0.22753646304498787\n",
            "81     \t [-2.05736352 -0.22110414]. \t  -13.11854100213931 \t -0.22753646304498787\n",
            "82     \t [-3.24545624 -0.70737921]. \t  -33.39396752507191 \t -0.22753646304498787\n",
            "83     \t [1.77077056 0.70697366]. \t  -25.00470315969732 \t -0.22753646304498787\n",
            "84     \t [0.86409862 4.75147499]. \t  -36.659975901184126 \t -0.22753646304498787\n",
            "85     \t [4.8552875  4.24401051]. \t  -55.0658662857671 \t -0.22753646304498787\n",
            "86     \t [-1.8520899  -4.13540841]. \t  -27.954414953589566 \t -0.22753646304498787\n",
            "87     \t [-3.75571636  1.839199  ]. \t  -31.813259233153477 \t -0.22753646304498787\n",
            "88     \t [-2.36209666  1.33270198]. \t  -38.796390374006705 \t -0.22753646304498787\n",
            "89     \t [-5.00287294  3.2030835 ]. \t  -42.384769088734515 \t -0.22753646304498787\n",
            "90     \t [-5.08961741  2.38616653]. \t  -50.69124025926375 \t -0.22753646304498787\n",
            "91     \t [-3.42341675  1.62979853]. \t  -50.09518185492452 \t -0.22753646304498787\n",
            "92     \t [-3.46907967  0.01818438]. \t  -31.911918303118497 \t -0.22753646304498787\n",
            "93     \t [-4.32462725  1.03124196]. \t  -34.47693436109129 \t -0.22753646304498787\n",
            "94     \t [-4.88604797 -1.17417935]. \t  -33.12188232132004 \t -0.22753646304498787\n",
            "95     \t [-4.71040859  1.78871586]. \t  -45.44080985889281 \t -0.22753646304498787\n",
            "96     \t [ 0.9459732  -1.62903355]. \t  -21.008881104020205 \t -0.22753646304498787\n",
            "97     \t [-4.44554765 -1.38647508]. \t  -58.667694553993165 \t -0.22753646304498787\n",
            "98     \t [-2.95599215 -1.05271594]. \t  -10.769508304442153 \t -0.22753646304498787\n",
            "99     \t [-2.72566413 -0.60247363]. \t  -37.313177177747086 \t -0.22753646304498787\n",
            "100    \t [-2.16014322 -2.27861988]. \t  -26.296223285720785 \t -0.22753646304498787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8b708f0-8ac9-41ef-a09b-905edcff5d4e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.82391708 4.79785639]. \t  -50.20079446939181 \t -13.871821018360485\n",
            "init   \t [3.78055209 0.31596228]. \t  -36.5114251593508 \t -13.871821018360485\n",
            "init   \t [-2.73686192 -5.00327624]. \t  -43.34985765011677 \t -13.871821018360485\n",
            "init   \t [-0.7119993  -0.99992207]. \t  -13.871821018360485 \t -13.871821018360485\n",
            "init   \t [ 0.23218863 -0.22126801]. \t  -17.190590355445654 \t -13.871821018360485\n",
            "1      \t [-4.44612211  4.9445944 ]. \t  -64.24932716942388 \t -13.871821018360485\n",
            "2      \t [ 3.30638279 -5.12      ]. \t  -53.325877992160855 \t -13.871821018360485\n",
            "3      \t [-5.12      -0.2349067]. \t  -38.032976010558706 \t -13.871821018360485\n",
            "4      \t [-0.33009428  3.88617833]. \t  -32.48420535846617 \t -13.871821018360485\n",
            "5      \t [ 0.28187413 -3.56856337]. \t  -43.889782058950296 \t -13.871821018360485\n",
            "6      \t [-2.48802432  1.48481659]. \t  -48.32117788966113 \t -13.871821018360485\n",
            "7      \t [ 5.09350871 -2.37578384]. \t  -50.37087750056072 \t -13.871821018360485\n",
            "8      \t [-4.68457863 -3.17368337]. \t  -51.39984744461357 \t -13.871821018360485\n",
            "9      \t [2.00806151 1.66491125]. \t  -21.912278885986503 \t -13.871821018360485\n",
            "10     \t [-2.29735436 -1.73145134]. \t  -32.3702270382159 \t -13.871821018360485\n",
            "11     \t [ 2.17658557 -1.72269993]. \t  -24.9612195966769 \t -13.871821018360485\n",
            "12     \t [4.98058096 2.73339332]. \t  -43.39350602796597 \t -13.871821018360485\n",
            "13     \t [-4.9340996   2.30050958]. \t  -43.6033570232228 \t -13.871821018360485\n",
            "14     \t [-5.12 -5.12]. \t  -57.849427451571785 \t -13.871821018360485\n",
            "15     \t [1.56537577 4.23362345]. \t  -48.51495852665746 \t -13.871821018360485\n",
            "16     \t [-0.01307743  1.57485217]. \t  -21.428347583764786 \t -13.871821018360485\n",
            "17     \t [-2.18901571  4.7450367 ]. \t  -43.88028920232122 \t -13.871821018360485\n",
            "18     \t [-0.45567798 -1.57769018]. \t  -41.143541620255434 \t -13.871821018360485\n",
            "19     \t [-1.25709875 -0.22494146]. \t  -20.508798530740876 \t -13.871821018360485\n",
            "20     \t [-0.33092596 -5.12      ]. \t  -43.90266391053686 \t -13.871821018360485\n",
            "21     \t [-0.71048801 -0.62051995]. \t  -30.614309921626543 \t -13.871821018360485\n",
            "22     \t [1.07395778 0.08379877]. \t  \u001b[92m-3.57520953542703\u001b[0m \t -3.57520953542703\n",
            "23     \t [-2.53729895 -0.35741453]. \t  -42.540505991402874 \t -3.57520953542703\n",
            "24     \t [-1.4806156  -1.25105359]. \t  -33.74947644603732 \t -3.57520953542703\n",
            "25     \t [-2.94030591 -2.52047721]. \t  -35.61072534220257 \t -3.57520953542703\n",
            "26     \t [-1.70234376  0.5522106 ]. \t  -35.61942336263071 \t -3.57520953542703\n",
            "27     \t [ 0.57702523 -1.08172139]. \t  -21.644205387965727 \t -3.57520953542703\n",
            "28     \t [ 2.05036497 -3.45114699]. \t  -36.14354629560965 \t -3.57520953542703\n",
            "29     \t [0.72142463 0.18228216]. \t  -18.211878021446623 \t -3.57520953542703\n",
            "30     \t [ 0.66058541 -0.28522735]. \t  -28.040274197178288 \t -3.57520953542703\n",
            "31     \t [2.5894598  0.25807243]. \t  -35.74030889810659 \t -3.57520953542703\n",
            "32     \t [1.43343282 0.92767009]. \t  -23.068324597563496 \t -3.57520953542703\n",
            "33     \t [2.21682961 2.7002746 ]. \t  -33.210471371707065 \t -3.57520953542703\n",
            "34     \t [1.74885231 0.06194814]. \t  -13.882423702541011 \t -3.57520953542703\n",
            "35     \t [2.73768066 1.7384572 ]. \t  -32.015022865779095 \t -3.57520953542703\n",
            "36     \t [-3.06202293  2.36429525]. \t  -32.295490149205065 \t -3.57520953542703\n",
            "37     \t [-0.94535446  2.44614448]. \t  -26.893912020678684 \t -3.57520953542703\n",
            "38     \t [ 1.90624746 -2.32144216]. \t  -25.047872299819744 \t -3.57520953542703\n",
            "39     \t [ 3.17324292 -2.2265625 ]. \t  -28.921752654897556 \t -3.57520953542703\n",
            "40     \t [0.33096227 2.49779911]. \t  -41.21801102363146 \t -3.57520953542703\n",
            "41     \t [-1.86587349 -0.52066628]. \t  -27.01420328188958 \t -3.57520953542703\n",
            "42     \t [-0.74687367  1.72592191]. \t  -25.24015371349896 \t -3.57520953542703\n",
            "43     \t [-1.33548447  0.34935258]. \t  -32.867052121528225 \t -3.57520953542703\n",
            "44     \t [-3.38537164  0.63007937]. \t  -46.21610855137516 \t -3.57520953542703\n",
            "45     \t [1.4269526  0.30518525]. \t  -34.49275168493112 \t -3.57520953542703\n",
            "46     \t [ 2.25213022 -0.2230742 ]. \t  -23.57195500695425 \t -3.57520953542703\n",
            "47     \t [ 1.53406831 -0.63560927]. \t  -39.113490197019004 \t -3.57520953542703\n",
            "48     \t [0.87091024 1.94979467]. \t  -8.166563237480165 \t -3.57520953542703\n",
            "49     \t [2.56669509 3.00883086]. \t  -34.79110430739858 \t -3.57520953542703\n",
            "50     \t [ 0.99002466 -4.12271716]. \t  -20.82482045710612 \t -3.57520953542703\n",
            "51     \t [0.87296021 1.41081393]. \t  -24.24316256306709 \t -3.57520953542703\n",
            "52     \t [3.6096933  1.48560783]. \t  -52.91345918180734 \t -3.57520953542703\n",
            "53     \t [ 0.23481911 -1.88655172]. \t  -15.096590270451776 \t -3.57520953542703\n",
            "54     \t [-2.27827504  0.26342899]. \t  -27.86994183151509 \t -3.57520953542703\n",
            "55     \t [-3.35662606 -1.30897755]. \t  -42.81127770455332 \t -3.57520953542703\n",
            "56     \t [1.67643774 2.80464007]. \t  -31.769587918312606 \t -3.57520953542703\n",
            "57     \t [-0.29213679  1.98947069]. \t  -16.68192504070487 \t -3.57520953542703\n",
            "58     \t [-3.63248638 -0.54983978]. \t  -49.74171008289327 \t -3.57520953542703\n",
            "59     \t [-2.18654906  3.10981977]. \t  -22.857663107554743 \t -3.57520953542703\n",
            "60     \t [ 0.18791871 -1.42715756]. \t  -27.2403660006103 \t -3.57520953542703\n",
            "61     \t [ 0.71916234 -1.61403507]. \t  -32.58885667386482 \t -3.57520953542703\n",
            "62     \t [ 2.59473388 -2.22228138]. \t  -38.21855008055236 \t -3.57520953542703\n",
            "63     \t [ 0.03645708 -2.81045263]. \t  -14.453512588979292 \t -3.57520953542703\n",
            "64     \t [ 0.87333394 -3.02684805]. \t  -13.0698074264394 \t -3.57520953542703\n",
            "65     \t [ 2.10371038 -4.24550702]. \t  -34.21670854567918 \t -3.57520953542703\n",
            "66     \t [ 3.10785831 -1.59423591]. \t  -32.70785873479316 \t -3.57520953542703\n",
            "67     \t [ 3.99178911 -2.67029691]. \t  -37.87935126536822 \t -3.57520953542703\n",
            "68     \t [-0.60427777  0.72197211]. \t  -30.567632971678464 \t -3.57520953542703\n",
            "69     \t [ 4.03214968 -2.23417949]. \t  -30.46072530783371 \t -3.57520953542703\n",
            "70     \t [0.81596436 4.98013818]. \t  -31.51832805595859 \t -3.57520953542703\n",
            "71     \t [-4.03595134  0.95751826]. \t  -17.813913642228155 \t -3.57520953542703\n",
            "72     \t [-1.9880933   2.39639748]. \t  -27.678268709612958 \t -3.57520953542703\n",
            "73     \t [5.12 5.12]. \t  -57.849427451571785 \t -3.57520953542703\n",
            "74     \t [-1.42879399  3.63920999]. \t  -50.71345673795493 \t -3.57520953542703\n",
            "75     \t [1.49596668 1.84174911]. \t  -30.17601125915005 \t -3.57520953542703\n",
            "76     \t [-0.47655192  4.44898379]. \t  -59.40286650437526 \t -3.57520953542703\n",
            "77     \t [-3.05296024  4.05830109]. \t  -27.00239659881607 \t -3.57520953542703\n",
            "78     \t [-1.84665478 -0.00208938]. \t  -7.704477804318046 \t -3.57520953542703\n",
            "79     \t [-2.6756552  -2.06065332]. \t  -26.62603395998375 \t -3.57520953542703\n",
            "80     \t [-2.97878396 -3.89659728]. \t  -26.182680475467745 \t -3.57520953542703\n",
            "81     \t [-2.97847752 -1.04874179]. \t  -10.52778649886853 \t -3.57520953542703\n",
            "82     \t [3.28259256 0.6949305 ]. \t  -36.68339789837508 \t -3.57520953542703\n",
            "83     \t [4.8853046  0.29320109]. \t  -39.11960884168767 \t -3.57520953542703\n",
            "84     \t [-3.90936798  2.17126874]. \t  -26.82810389480577 \t -3.57520953542703\n",
            "85     \t [-0.28203058  0.15967054]. \t  -16.728283907047917 \t -3.57520953542703\n",
            "86     \t [-5.06447782  0.83508595]. \t  -32.06071499792198 \t -3.57520953542703\n",
            "87     \t [ 1.52144968 -0.20194082]. \t  -29.290942820646162 \t -3.57520953542703\n",
            "88     \t [ 4.71431871 -3.57405276]. \t  -66.15876950495527 \t -3.57520953542703\n",
            "89     \t [1.22831738 3.49679598]. \t  -42.37617081792302 \t -3.57520953542703\n",
            "90     \t [2.20882498 4.95934597]. \t  -37.24015051527785 \t -3.57520953542703\n",
            "91     \t [4.25764212 0.32787236]. \t  -43.414962599218356 \t -3.57520953542703\n",
            "92     \t [ 4.28957592 -1.06004637]. \t  -32.68854950479599 \t -3.57520953542703\n",
            "93     \t [-3.1395672   2.99741238]. \t  -22.44751575079411 \t -3.57520953542703\n",
            "94     \t [2.06940203 3.15178094]. \t  -19.364986743457223 \t -3.57520953542703\n",
            "95     \t [ 0.44723982 -2.96181409]. \t  -28.714363647430538 \t -3.57520953542703\n",
            "96     \t [ 2.35239727 -4.94864132]. \t  -46.53802442461175 \t -3.57520953542703\n",
            "97     \t [-0.44981205 -4.87155897]. \t  -46.524780918374645 \t -3.57520953542703\n",
            "98     \t [ 5.06428365 -0.20104616]. \t  -33.46447138389678 \t -3.57520953542703\n",
            "99     \t [ 1.46277596 -3.49458385]. \t  -54.07377225421061 \t -3.57520953542703\n",
            "100    \t [ 0.2537814  -4.56607049]. \t  -50.30160095079971 \t -3.57520953542703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd11bb7-d145-4e51-fe8d-e09fc209ee98"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.60872271 2.67767798]. \t  -39.68588898862962 \t -22.797176954563277\n",
            "init   \t [-2.61599688  2.78210017]. \t  -40.03979647504547 \t -22.797176954563277\n",
            "init   \t [-1.49020588  3.03818137]. \t  -31.718714938133186 \t -22.797176954563277\n",
            "init   \t [-1.57695977 -0.57472578]. \t  -40.5884745864365 \t -22.797176954563277\n",
            "init   \t [-2.01417124 -0.59275346]. \t  -22.797176954563277 \t -22.797176954563277\n",
            "1      \t [-3.26052504 -0.77046733]. \t  -30.603010943333977 \t -22.797176954563277\n",
            "2      \t [ 5.03907422 -4.58487758]. \t  -65.3245710209712 \t -22.797176954563277\n",
            "3      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -22.797176954563277\n",
            "4      \t [-5.94389297e-04 -5.12000000e+00]. \t  -28.924783817358893 \t -22.797176954563277\n",
            "5      \t [5.03100623 1.64309585]. \t  -44.423123144574824 \t -22.797176954563277\n",
            "6      \t [ 2.21681314 -1.68210226]. \t  -29.81152185592379 \t -22.797176954563277\n",
            "7      \t [3.79232253 4.44223729]. \t  -60.83580158218214 \t -22.797176954563277\n",
            "8      \t [-2.33917048 -3.50657643]. \t  -53.073449809046515 \t -22.797176954563277\n",
            "9      \t [-3.60212257  5.10300826]. \t  -59.049411734035694 \t -22.797176954563277\n",
            "10     \t [-4.83889128  1.41846857]. \t  -48.843908087678095 \t -22.797176954563277\n",
            "11     \t [ 4.66361275 -1.94558338]. \t  -41.27866303105962 \t -22.797176954563277\n",
            "12     \t [ 1.94460183 -4.59140695]. \t  -43.85777239440516 \t -22.797176954563277\n",
            "13     \t [2.5585042  0.62487594]. \t  -43.3449427949992 \t -22.797176954563277\n",
            "14     \t [0.13986124 4.87430731]. \t  -30.357252460696746 \t -22.797176954563277\n",
            "15     \t [-5.02155022 -2.64649064]. \t  -48.36620767209054 \t -22.797176954563277\n",
            "16     \t [-2.59726868  0.41097586]. \t  -43.580525955501756 \t -22.797176954563277\n",
            "17     \t [-2.57347875 -1.19350563]. \t  -33.524728012884545 \t -22.797176954563277\n",
            "18     \t [ 0.30677995 -2.81949192]. \t  -27.307153418610675 \t -22.797176954563277\n",
            "19     \t [-4.74566637  3.31787773]. \t  -57.93868100648693 \t -22.797176954563277\n",
            "20     \t [-2.36987824 -5.12      ]. \t  -51.38092888491188 \t -22.797176954563277\n",
            "21     \t [1.94481248 4.69285355]. \t  -39.91431973124088 \t -22.797176954563277\n",
            "22     \t [-2.63899024 -0.47713637]. \t  -43.5119164572423 \t -22.797176954563277\n",
            "23     \t [-4.78502107 -0.77345336]. \t  -39.84364211105432 \t -22.797176954563277\n",
            "24     \t [-4.25155219 -1.16332576]. \t  -34.34587087051798 \t -22.797176954563277\n",
            "25     \t [-1.67828792  1.19568267]. \t  -25.254196066820285 \t -22.797176954563277\n",
            "26     \t [-3.79215116 -0.87867403]. \t  -25.302493880636483 \t -22.797176954563277\n",
            "27     \t [ 3.09620057 -3.28653878]. \t  -34.43530923522938 \t -22.797176954563277\n",
            "28     \t [-2.36345856  1.27822542]. \t  -35.524130388900694 \t -22.797176954563277\n",
            "29     \t [4.78385011 0.00643634]. \t  -30.782573277750032 \t -22.797176954563277\n",
            "30     \t [-1.68806623  4.71585757]. \t  -51.011637175474505 \t -22.797176954563277\n",
            "31     \t [-2.11645639  0.81178553]. \t  \u001b[92m-13.912779364041024\u001b[0m \t -13.912779364041024\n",
            "32     \t [-1.82360017  0.33255034]. \t  -23.932078220774883 \t -13.912779364041024\n",
            "33     \t [3.09642565 2.4482258 ]. \t  -36.837062713374 \t -13.912779364041024\n",
            "34     \t [ 1.43227168 -0.83756583]. \t  -26.632526691020836 \t -13.912779364041024\n",
            "35     \t [5.06242317 4.25571665]. \t  -54.85772151809728 \t -13.912779364041024\n",
            "36     \t [1.46751918 0.77965253]. \t  -30.701579810113557 \t -13.912779364041024\n",
            "37     \t [-0.75361722 -3.99139147]. \t  -26.28651196151955 \t -13.912779364041024\n",
            "38     \t [ 3.12869633 -4.99778611]. \t  -37.86262242418684 \t -13.912779364041024\n",
            "39     \t [-2.44129212  3.71759882]. \t  -51.129585499081294 \t -13.912779364041024\n",
            "40     \t [-4.9784845   5.01475114]. \t  -50.067195035390085 \t -13.912779364041024\n",
            "41     \t [-1.95401145  0.67608887]. \t  -19.16867446444656 \t -13.912779364041024\n",
            "42     \t [-1.02768277  1.73422084]. \t  -15.204352093324609 \t -13.912779364041024\n",
            "43     \t [-1.89440394  2.38063331]. \t  -28.6945087608877 \t -13.912779364041024\n",
            "44     \t [-3.72698538 -1.42215199]. \t  -46.18135648009872 \t -13.912779364041024\n",
            "45     \t [ 1.30464014 -3.31332718]. \t  -39.92111902250296 \t -13.912779364041024\n",
            "46     \t [-1.48536527  1.57165411]. \t  -43.637690923434064 \t -13.912779364041024\n",
            "47     \t [-0.22421047  1.32402172]. \t  -24.67503363745044 \t -13.912779364041024\n",
            "48     \t [-0.3374114   1.83668197]. \t  -23.526392925399 \t -13.912779364041024\n",
            "49     \t [-1.01425057  0.66607485]. \t  -16.54458791930877 \t -13.912779364041024\n",
            "50     \t [1.17547744 3.38265228]. \t  -35.713674970503156 \t -13.912779364041024\n",
            "51     \t [-1.48077704  5.08300652]. \t  -49.28630100820118 \t -13.912779364041024\n",
            "52     \t [-0.65907804  1.01018587]. \t  -16.882420546057844 \t -13.912779364041024\n",
            "53     \t [1.59990485 2.05400789]. \t  -25.442586225462478 \t -13.912779364041024\n",
            "54     \t [-0.86043803  1.57153373]. \t  -25.821469592621014 \t -13.912779364041024\n",
            "55     \t [-0.69255044  2.23792917]. \t  -28.262031075723858 \t -13.912779364041024\n",
            "56     \t [-0.39643684  0.10626317]. \t  -20.2723572393068 \t -13.912779364041024\n",
            "57     \t [-3.9963484   0.37625754]. \t  -33.2417199557818 \t -13.912779364041024\n",
            "58     \t [0.96985932 5.04103178]. \t  -26.8619101702429 \t -13.912779364041024\n",
            "59     \t [0.15651753 0.49824073]. \t  -24.730412409643748 \t -13.912779364041024\n",
            "60     \t [-2.73721856 -2.39450145]. \t  -41.91053369446465 \t -13.912779364041024\n",
            "61     \t [-1.63455701  2.69205896]. \t  -40.11358683955929 \t -13.912779364041024\n",
            "62     \t [-0.90174061  4.6563245 ]. \t  -39.89233524013365 \t -13.912779364041024\n",
            "63     \t [0.95074951 1.96454144]. \t  \u001b[92m-5.485493515343977\u001b[0m \t -5.485493515343977\n",
            "64     \t [-1.53204954  3.80253477]. \t  -43.363133878333855 \t -5.485493515343977\n",
            "65     \t [ 1.95132911 -0.0646386 ]. \t  \u001b[92m-5.089283152888152\u001b[0m \t -5.089283152888152\n",
            "66     \t [-2.68678977  4.8360293 ]. \t  -49.328068970516675 \t -5.089283152888152\n",
            "67     \t [-0.26909917  0.44675544]. \t  -30.914759717297244 \t -5.089283152888152\n",
            "68     \t [ 0.12957261 -1.14889756]. \t  -8.537982200800585 \t -5.089283152888152\n",
            "69     \t [ 0.82605961 -0.38770315]. \t  -23.84590653164713 \t -5.089283152888152\n",
            "70     \t [1.91118813 2.59904215]. \t  -30.050014084486513 \t -5.089283152888152\n",
            "71     \t [0.93937698 2.39117018]. \t  -25.06863090853872 \t -5.089283152888152\n",
            "72     \t [2.43177905 1.66960386]. \t  -42.635758323278026 \t -5.089283152888152\n",
            "73     \t [ 2.77463535 -0.0505978 ]. \t  -16.660559506399757 \t -5.089283152888152\n",
            "74     \t [-0.26627942 -0.61945943]. \t  -28.78861321703635 \t -5.089283152888152\n",
            "75     \t [ 0.80371229 -1.83182237]. \t  -15.772824883877178 \t -5.089283152888152\n",
            "76     \t [0.54855475 4.14804377]. \t  -41.06856823234489 \t -5.089283152888152\n",
            "77     \t [0.81756674 0.66578744]. \t  -22.04048864835492 \t -5.089283152888152\n",
            "78     \t [3.11633263 3.14072679]. \t  -25.791230678634545 \t -5.089283152888152\n",
            "79     \t [-3.23153652  4.16448729]. \t  -41.510172455787114 \t -5.089283152888152\n",
            "80     \t [-5.11209903  1.71483924]. \t  -43.645121325876026 \t -5.089283152888152\n",
            "81     \t [ 0.83951666 -4.08705912]. \t  -23.535395742743102 \t -5.089283152888152\n",
            "82     \t [ 2.10691009 -1.15208932]. \t  -12.167809870149089 \t -5.089283152888152\n",
            "83     \t [2.14003334 2.05651901]. \t  -13.060337911093768 \t -5.089283152888152\n",
            "84     \t [3.89727022 2.60443895]. \t  -41.90675470088911 \t -5.089283152888152\n",
            "85     \t [ 2.37405557 -2.98833153]. \t  -31.62211222646787 \t -5.089283152888152\n",
            "86     \t [-3.56700006 -0.04783282]. \t  -32.30093136928795 \t -5.089283152888152\n",
            "87     \t [-4.93447405  0.54330033]. \t  -45.11202063815604 \t -5.089283152888152\n",
            "88     \t [ 0.27966899 -0.67617524]. \t  -26.862794687881763 \t -5.089283152888152\n",
            "89     \t [-2.96910564 -1.51337804]. \t  -31.258406828246596 \t -5.089283152888152\n",
            "90     \t [ 0.01004835 -2.38343827]. \t  -23.136661261824656 \t -5.089283152888152\n",
            "91     \t [ 0.37450682 -2.05726917]. \t  -22.062177247507698 \t -5.089283152888152\n",
            "92     \t [-0.3554633  -3.25552064]. \t  -37.223617006600094 \t -5.089283152888152\n",
            "93     \t [ 0.40323665 -3.57572401]. \t  -50.04574621041637 \t -5.089283152888152\n",
            "94     \t [-1.29349065 -4.77001313]. \t  -45.870709896475056 \t -5.089283152888152\n",
            "95     \t [-5.11115283  3.59380148]. \t  -59.693431687311595 \t -5.089283152888152\n",
            "96     \t [2.58388994 2.91708056]. \t  -35.15533336163435 \t -5.089283152888152\n",
            "97     \t [ 1.00473091 -5.08127697]. \t  -28.109145365898097 \t -5.089283152888152\n",
            "98     \t [-2.33741191 -2.44391455]. \t  -46.04196529991329 \t -5.089283152888152\n",
            "99     \t [-3.55884098 -3.8830322 ]. \t  -49.64883981595021 \t -5.089283152888152\n",
            "100    \t [-4.4311704  4.9509533]. \t  -63.69764275884304 \t -5.089283152888152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9cf1cd-ed6a-4596-8af9-1e061270ba60"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.722097   0.66077445]. \t  -28.019395863326952 \t -20.02750192535725\n",
            "init   \t [-0.11835563 -1.6744678 ]. \t  -20.02750192535725 \t -20.02750192535725\n",
            "init   \t [-1.27110986  0.3280473 ]. \t  -27.755482301965866 \t -20.02750192535725\n",
            "init   \t [-4.42259161  0.86557758]. \t  -42.50859246850971 \t -20.02750192535725\n",
            "init   \t [-2.6839269  -3.47385261]. \t  -53.16973204675458 \t -20.02750192535725\n",
            "1      \t [ 5.02805185 -4.26101338]. \t  -54.28390661788428 \t -20.02750192535725\n",
            "2      \t [4.38721711 5.04009402]. \t  -62.55836483450742 \t -20.02750192535725\n",
            "3      \t [-1.01784443  4.95060886]. \t  -26.085006575018536 \t -20.02750192535725\n",
            "4      \t [4.91493053 0.32112343]. \t  -39.976020629187914 \t -20.02750192535725\n",
            "5      \t [-4.67359798  4.53251634]. \t  -76.79646008429751 \t -20.02750192535725\n",
            "6      \t [ 1.09471199 -5.12      ]. \t  -31.842144467222994 \t -20.02750192535725\n",
            "7      \t [-5.12 -5.12]. \t  -57.849427451571785 \t -20.02750192535725\n",
            "8      \t [ 2.66674322 -1.97666877]. \t  -26.12183007736462 \t -20.02750192535725\n",
            "9      \t [1.67293854 3.20535308]. \t  -34.959340302196736 \t -20.02750192535725\n",
            "10     \t [-5.12       -1.89435736]. \t  -34.63656600647248 \t -20.02750192535725\n",
            "11     \t [-2.10105553  2.69320567]. \t  -27.110018949126502 \t -20.02750192535725\n",
            "12     \t [4.69044434 2.61996041]. \t  -59.81112229947424 \t -20.02750192535725\n",
            "13     \t [-1.70225128 -5.12      ]. \t  -44.77770932779114 \t -20.02750192535725\n",
            "14     \t [2.53384458 0.64373981]. \t  -42.8009584464272 \t -20.02750192535725\n",
            "15     \t [-2.57411603 -1.14586333]. \t  -30.7881002062872 \t -20.02750192535725\n",
            "16     \t [ 4.88878989 -2.17520298]. \t  -36.446792339466825 \t -20.02750192535725\n",
            "17     \t [1.75037396 5.07403798]. \t  -39.84882956177728 \t -20.02750192535725\n",
            "18     \t [ 0.28476679 -3.01777832]. \t  -21.417532490610515 \t -20.02750192535725\n",
            "19     \t [ 2.90767723 -4.81702777]. \t  -39.206078201222326 \t -20.02750192535725\n",
            "20     \t [-0.34945828  2.73499317]. \t  -34.39410094546704 \t -20.02750192535725\n",
            "21     \t [-2.63071605  4.97042773]. \t  -48.61055105245904 \t -20.02750192535725\n",
            "22     \t [ 0.60312182 -1.21902518]. \t  -27.889163201781766 \t -20.02750192535725\n",
            "23     \t [-0.96114263 -1.69081902]. \t  \u001b[92m-17.712578186407903\u001b[0m \t -17.712578186407903\n",
            "24     \t [ 2.4476872  -3.24515718]. \t  -45.68263846579154 \t -17.712578186407903\n",
            "25     \t [-0.33769812 -3.22366901]. \t  -34.09478969916545 \t -17.712578186407903\n",
            "26     \t [ 0.37092058 -2.39462464]. \t  -40.64640326889239 \t -17.712578186407903\n",
            "27     \t [-0.63479939 -0.78115184]. \t  -25.690883024738056 \t -17.712578186407903\n",
            "28     \t [ 0.41203986 -3.51279955]. \t  -50.988476262464026 \t -17.712578186407903\n",
            "29     \t [-1.9862556   0.90549804]. \t  \u001b[92m-6.514049603237977\u001b[0m \t -6.514049603237977\n",
            "30     \t [-2.38207327  1.10499333]. \t  -26.37171577587768 \t -6.514049603237977\n",
            "31     \t [-1.40363176  1.18878297]. \t  -27.853330558069565 \t -6.514049603237977\n",
            "32     \t [-1.89727008  0.20746303]. \t  -13.013539281041862 \t -6.514049603237977\n",
            "33     \t [ 0.50586065 -0.48653449]. \t  -40.450061911016704 \t -6.514049603237977\n",
            "34     \t [-4.94413057  2.43081993]. \t  -50.033235363497894 \t -6.514049603237977\n",
            "35     \t [1.33339493 1.06851746]. \t  -18.835483721092565 \t -6.514049603237977\n",
            "36     \t [-2.06892761  0.57833862]. \t  -24.351060408528994 \t -6.514049603237977\n",
            "37     \t [-2.12422516  1.5516427 ]. \t  -29.292683013005025 \t -6.514049603237977\n",
            "38     \t [-1.77086078 -1.32493414]. \t  -28.120644027545683 \t -6.514049603237977\n",
            "39     \t [-2.35479522  3.52972264]. \t  -53.94902412578323 \t -6.514049603237977\n",
            "40     \t [-1.62531568 -0.86501869]. \t  -23.832937531973347 \t -6.514049603237977\n",
            "41     \t [-2.1417095  -2.33235377]. \t  -28.68228522725476 \t -6.514049603237977\n",
            "42     \t [-1.15152471  3.21390279]. \t  -23.606441116664882 \t -6.514049603237977\n",
            "43     \t [-3.63595262  1.76445708]. \t  -41.99444677304091 \t -6.514049603237977\n",
            "44     \t [-0.27799773 -1.40003504]. \t  -31.878935106298627 \t -6.514049603237977\n",
            "45     \t [-0.97702016 -2.15568603]. \t  -10.120476242881953 \t -6.514049603237977\n",
            "46     \t [-0.24508797 -2.49327621]. \t  -35.9589888912519 \t -6.514049603237977\n",
            "47     \t [-1.77819999 -2.16508504]. \t  -21.001173354226715 \t -6.514049603237977\n",
            "48     \t [-3.77359945 -2.65074935]. \t  -45.6288523354055 \t -6.514049603237977\n",
            "49     \t [-1.17313543 -0.41610167]. \t  -25.547860616177303 \t -6.514049603237977\n",
            "50     \t [-0.36936747 -0.08815649]. \t  -18.45587238454591 \t -6.514049603237977\n",
            "51     \t [ 3.23929154 -2.26260613]. \t  -35.731309500302004 \t -6.514049603237977\n",
            "52     \t [ 2.50880848 -0.88162717]. \t  -29.696784120196305 \t -6.514049603237977\n",
            "53     \t [-4.98200823  0.56091373]. \t  -44.475336042811975 \t -6.514049603237977\n",
            "54     \t [-1.22010537 -2.01870758]. \t  -13.765530201217674 \t -6.514049603237977\n",
            "55     \t [-4.81472847 -3.24947928]. \t  -49.75219260287155 \t -6.514049603237977\n",
            "56     \t [-1.75055856 -3.59045084]. \t  -44.3487645273282 \t -6.514049603237977\n",
            "57     \t [-3.30229153 -5.08513617]. \t  -51.38745607927086 \t -6.514049603237977\n",
            "58     \t [ 1.46869752 -5.11379921]. \t  -50.56443234277713 \t -6.514049603237977\n",
            "59     \t [-1.74510417  0.77881419]. \t  -22.158931010539938 \t -6.514049603237977\n",
            "60     \t [ 1.39428897 -0.05812537]. \t  -20.481030942438647 \t -6.514049603237977\n",
            "61     \t [-4.53092421  1.55773469]. \t  -62.11685508901702 \t -6.514049603237977\n",
            "62     \t [0.00226075 1.48549011]. \t  -22.16616524968932 \t -6.514049603237977\n",
            "63     \t [-3.88368068  2.55804041]. \t  -43.52289651373603 \t -6.514049603237977\n",
            "64     \t [3.39008894 0.89189781]. \t  -32.216286968588946 \t -6.514049603237977\n",
            "65     \t [-2.3453166  -0.50519935]. \t  -41.38767607132942 \t -6.514049603237977\n",
            "66     \t [-1.39531822  2.4391062 ]. \t  -45.086885754394146 \t -6.514049603237977\n",
            "67     \t [0.30426578 3.70034975]. \t  -40.19836654229631 \t -6.514049603237977\n",
            "68     \t [-0.44395705  0.87695304]. \t  -23.195248271812 \t -6.514049603237977\n",
            "69     \t [-1.98756076 -0.28168466]. \t  -16.03795377639564 \t -6.514049603237977\n",
            "70     \t [-4.31312367 -0.88532756]. \t  -35.735143795085925 \t -6.514049603237977\n",
            "71     \t [-0.79010043  1.50811363]. \t  -30.392663146147598 \t -6.514049603237977\n",
            "72     \t [-1.53400488 -2.9418091 ]. \t  -31.441020332000136 \t -6.514049603237977\n",
            "73     \t [0.49767931 2.49322548]. \t  -46.45373717686851 \t -6.514049603237977\n",
            "74     \t [2.7313472  3.01008383]. \t  -27.71023594850628 \t -6.514049603237977\n",
            "75     \t [-2.26477051  3.10676017]. \t  -27.874586865645806 \t -6.514049603237977\n",
            "76     \t [-4.59029259  2.76767153]. \t  -56.05615098402836 \t -6.514049603237977\n",
            "77     \t [1.90836395 3.45876062]. \t  -36.883296680509744 \t -6.514049603237977\n",
            "78     \t [-0.36165079  3.14485905]. \t  -30.338678819682002 \t -6.514049603237977\n",
            "79     \t [-2.02197054  4.40249241]. \t  -41.74665825442201 \t -6.514049603237977\n",
            "80     \t [-1.92782857  2.65448189]. \t  -27.421186389235395 \t -6.514049603237977\n",
            "81     \t [0.81460938 4.45287844]. \t  -46.10766765752004 \t -6.514049603237977\n",
            "82     \t [-2.34820913 -4.38360352]. \t  -57.95931168308461 \t -6.514049603237977\n",
            "83     \t [-3.74560249  4.56981912]. \t  -64.24215724368503 \t -6.514049603237977\n",
            "84     \t [-4.05341591 -0.70657827]. \t  -30.18192423449405 \t -6.514049603237977\n",
            "85     \t [-2.67511491  4.38194642]. \t  -58.26402239960815 \t -6.514049603237977\n",
            "86     \t [-1.40244017 -4.27521731]. \t  -50.00148539991395 \t -6.514049603237977\n",
            "87     \t [-1.15496942  3.79906654]. \t  -27.11010144355324 \t -6.514049603237977\n",
            "88     \t [0.62186693 1.41723148]. \t  -38.28209449486792 \t -6.514049603237977\n",
            "89     \t [0.97147524 3.52843281]. \t  -33.394630509132426 \t -6.514049603237977\n",
            "90     \t [1.40058794 0.56877832]. \t  -39.477670231483124 \t -6.514049603237977\n",
            "91     \t [2.35130469 1.78582251]. \t  -32.4299317937473 \t -6.514049603237977\n",
            "92     \t [ 1.65153436 -1.21731727]. \t  -27.969898364573783 \t -6.514049603237977\n",
            "93     \t [ 3.67133472 -5.10761775]. \t  -56.51066049892342 \t -6.514049603237977\n",
            "94     \t [-1.74045266  1.21758794]. \t  -23.088751067077986 \t -6.514049603237977\n",
            "95     \t [ 1.31570102 -0.61555025]. \t  -33.60003721068201 \t -6.514049603237977\n",
            "96     \t [1.8987751  1.41996257]. \t  -26.338880906412147 \t -6.514049603237977\n",
            "97     \t [ 2.89363155 -0.23454342]. \t  -19.60991490737298 \t -6.514049603237977\n",
            "98     \t [ 1.94827252 -0.23214877]. \t  -13.2539188195518 \t -6.514049603237977\n",
            "99     \t [-4.47820036 -2.77352412]. \t  -56.180367679219074 \t -6.514049603237977\n",
            "100    \t [-4.96247016  3.1183738 ]. \t  -37.2678542019581 \t -6.514049603237977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c91883-5649-4102-ab33-ac10f77aae15"
      },
      "source": [
        "end_win = time.time()\n",
        "end_win\n",
        "\n",
        "time_win = end_win - start_win\n",
        "time_win"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4007.168317556381"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "640a77b1-c05c-4979-a606-4669b06f89c5"
      },
      "source": [
        "### Training regret minimization: run number = 1\n",
        "\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\n",
        "\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\n",
        "\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\n",
        "\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.1677287289204734, 0.6590273257335292)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0164e3c5-f4fa-4423-8c16-43e2dcb739ad"
      },
      "source": [
        "### Training regret minimization: run number = 2\n",
        "\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\n",
        "\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\n",
        "\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\n",
        "\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.07421050873085763, 1.3059695060437821)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b4b740-be61-4e7f-9fd5-4c9cb5acebe8"
      },
      "source": [
        "### Training regret minimization: run number = 3\n",
        "\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\n",
        "\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\n",
        "\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\n",
        "\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.3216638947317279, 1.3216638947317279)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3027f723-0d70-4c0d-fb93-7c4d94006968"
      },
      "source": [
        "### Training regret minimization: run number = 4\n",
        "\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\n",
        "\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\n",
        "\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\n",
        "\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.2299679288216367, 1.5525716055482397)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3c1efb-75ab-4021-bf64-d7078c66eac4"
      },
      "source": [
        "### Training regret minimization: run number = 5\n",
        "\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\n",
        "\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\n",
        "\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\n",
        "\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.6545617990251684, 2.0237762127263155)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89ec2aa-4258-4de2-d32f-b1edd98a02d7"
      },
      "source": [
        "### Training regret minimization: run number = 6\n",
        "\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\n",
        "\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\n",
        "\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\n",
        "\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.5509901375216157, 1.8286939100495485)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92b3e31-cd09-4d4f-86f0-6c625ba73a31"
      },
      "source": [
        "### Training regret minimization: run number = 7\n",
        "\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\n",
        "\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\n",
        "\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\n",
        "\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0435891733992444, 0.8059160278654809)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f725dc-2516-41bb-a7ee-4c2ba6978907"
      },
      "source": [
        "### Training regret minimization: run number = 8\n",
        "\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\n",
        "\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\n",
        "\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\n",
        "\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.4878937408233297, 1.1695298252999689)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0f0f76-e697-4601-86ec-3ff31c94103c"
      },
      "source": [
        "### Training regret minimization: run number = 9\n",
        "\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\n",
        "\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\n",
        "\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\n",
        "\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0961463543053815, 1.7658615393980437)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62e2b59-9443-4012-9fdc-3b4db469334f"
      },
      "source": [
        "### Training regret minimization: run number = 10\n",
        "\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\n",
        "\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\n",
        "\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\n",
        "\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.039483155954458594, 1.7646372975051001)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6bab0c-147d-4176-ca8b-4cabf02b301b"
      },
      "source": [
        "### Training regret minimization: run number = 11\n",
        "\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\n",
        "\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\n",
        "\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\n",
        "\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.629878266306832, 1.1371064298816924)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1012973-a847-44ce-fd3b-525ef66507c2"
      },
      "source": [
        "### Training regret minimization: run number = 12\n",
        "\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\n",
        "\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\n",
        "\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\n",
        "\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.47388400195018, 1.0243435704429162)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72c9774-2c9b-41e0-e622-2d2b912ff2a3"
      },
      "source": [
        "### Training regret minimization: run number = 13\n",
        "\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\n",
        "\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\n",
        "\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\n",
        "\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7101678319753499, 1.4496006380834179)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a462ae-7b6a-4660-aa2a-22ab5f5a0b6a"
      },
      "source": [
        "### Training regret minimization: run number = 14\n",
        "\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\n",
        "\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\n",
        "\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\n",
        "\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.389856959164257, 1.7897507635310257)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa67e4d2-9b15-4a1d-f5e8-1662f9856a97"
      },
      "source": [
        "### Training regret minimization: run number = 15\n",
        "\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\n",
        "\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\n",
        "\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\n",
        "\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.328980148958704, 1.470650314349906)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee0cc98-e2be-450c-8cd7-9b995a638b0f"
      },
      "source": [
        "### Training regret minimization: run number = 16\n",
        "\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\n",
        "\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\n",
        "\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\n",
        "\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.5436132344954814, 1.970182466479029)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90afc719-43dd-4f51-fd5a-ca9bdb513624"
      },
      "source": [
        "### Training regret minimization: run number = 17\n",
        "\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\n",
        "\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\n",
        "\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\n",
        "\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.28751547559518825, -1.4804447763133115)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1f1ed1-07e1-4f2a-e524-ddfdfd265a56"
      },
      "source": [
        "### Training regret minimization: run number = 18\n",
        "\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\n",
        "\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\n",
        "\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\n",
        "\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.228881137226914, 1.2740237857366554)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55810f89-755a-44b6-9ec7-792fd32b0999"
      },
      "source": [
        "### Training regret minimization: run number = 19\n",
        "\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\n",
        "\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\n",
        "\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\n",
        "\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.055085277026383, 1.627136986241222)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd10448c-f486-47af-b99e-60d8d78cc24a"
      },
      "source": [
        "### Training regret minimization: run number = 20\n",
        "\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \n",
        "\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\n",
        "\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\n",
        "\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\n",
        "\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.6461659605950376, 1.8739613216911797)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\n",
        "\n",
        "slice1 = 0\n",
        "\n",
        "loser1 = [train_regret_loser_1[slice1],\n",
        "       train_regret_loser_2[slice1],\n",
        "       train_regret_loser_3[slice1],\n",
        "       train_regret_loser_4[slice1],\n",
        "       train_regret_loser_5[slice1],\n",
        "       train_regret_loser_6[slice1],\n",
        "       train_regret_loser_7[slice1],\n",
        "       train_regret_loser_8[slice1],\n",
        "       train_regret_loser_9[slice1],\n",
        "       train_regret_loser_10[slice1],\n",
        "       train_regret_loser_11[slice1],\n",
        "       train_regret_loser_12[slice1],\n",
        "       train_regret_loser_13[slice1],\n",
        "       train_regret_loser_14[slice1],\n",
        "       train_regret_loser_15[slice1],\n",
        "       train_regret_loser_16[slice1],\n",
        "       train_regret_loser_17[slice1],\n",
        "       train_regret_loser_18[slice1],\n",
        "       train_regret_loser_19[slice1],\n",
        "       train_regret_loser_20[slice1]]\n",
        "\n",
        "winner1 = [train_regret_winner_1[slice1],\n",
        "       train_regret_winner_2[slice1],\n",
        "       train_regret_winner_3[slice1],\n",
        "       train_regret_winner_4[slice1],\n",
        "       train_regret_winner_5[slice1],\n",
        "       train_regret_winner_6[slice1],\n",
        "       train_regret_winner_7[slice1],\n",
        "       train_regret_winner_8[slice1],\n",
        "       train_regret_winner_9[slice1],\n",
        "       train_regret_winner_10[slice1],\n",
        "       train_regret_winner_11[slice1],\n",
        "       train_regret_winner_12[slice1],\n",
        "       train_regret_winner_13[slice1],\n",
        "       train_regret_winner_14[slice1],\n",
        "       train_regret_winner_15[slice1],\n",
        "       train_regret_winner_16[slice1],\n",
        "       train_regret_winner_17[slice1],\n",
        "       train_regret_winner_18[slice1],\n",
        "       train_regret_winner_19[slice1],\n",
        "       train_regret_winner_20[slice1]]\n",
        "\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\n",
        "\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\n",
        "\n",
        "slice11 = 10\n",
        "\n",
        "loser11 = [train_regret_loser_1[slice11],\n",
        "       train_regret_loser_2[slice11],\n",
        "       train_regret_loser_3[slice11],\n",
        "       train_regret_loser_4[slice11],\n",
        "       train_regret_loser_5[slice11],\n",
        "       train_regret_loser_6[slice11],\n",
        "       train_regret_loser_7[slice11],\n",
        "       train_regret_loser_8[slice11],\n",
        "       train_regret_loser_9[slice11],\n",
        "       train_regret_loser_10[slice11],\n",
        "       train_regret_loser_11[slice11],\n",
        "       train_regret_loser_12[slice11],\n",
        "       train_regret_loser_13[slice11],\n",
        "       train_regret_loser_14[slice11],\n",
        "       train_regret_loser_15[slice11],\n",
        "       train_regret_loser_16[slice11],\n",
        "       train_regret_loser_17[slice11],\n",
        "       train_regret_loser_18[slice11],\n",
        "       train_regret_loser_19[slice11],\n",
        "       train_regret_loser_20[slice11]]\n",
        "\n",
        "winner11 = [train_regret_winner_1[slice11],\n",
        "       train_regret_winner_2[slice11],\n",
        "       train_regret_winner_3[slice11],\n",
        "       train_regret_winner_4[slice11],\n",
        "       train_regret_winner_5[slice11],\n",
        "       train_regret_winner_6[slice11],\n",
        "       train_regret_winner_7[slice11],\n",
        "       train_regret_winner_8[slice11],\n",
        "       train_regret_winner_9[slice11],\n",
        "       train_regret_winner_10[slice11],\n",
        "       train_regret_winner_11[slice11],\n",
        "       train_regret_winner_12[slice11],\n",
        "       train_regret_winner_13[slice11],\n",
        "       train_regret_winner_14[slice11],\n",
        "       train_regret_winner_15[slice11],\n",
        "       train_regret_winner_16[slice11],\n",
        "       train_regret_winner_17[slice11],\n",
        "       train_regret_winner_18[slice11],\n",
        "       train_regret_winner_19[slice11],\n",
        "       train_regret_winner_20[slice11]]\n",
        "\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\n",
        "\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\n",
        "\n",
        "slice21 = 20\n",
        "\n",
        "loser21 = [train_regret_loser_1[slice21],\n",
        "       train_regret_loser_2[slice21],\n",
        "       train_regret_loser_3[slice21],\n",
        "       train_regret_loser_4[slice21],\n",
        "       train_regret_loser_5[slice21],\n",
        "       train_regret_loser_6[slice21],\n",
        "       train_regret_loser_7[slice21],\n",
        "       train_regret_loser_8[slice21],\n",
        "       train_regret_loser_9[slice21],\n",
        "       train_regret_loser_10[slice21],\n",
        "       train_regret_loser_11[slice21],\n",
        "       train_regret_loser_12[slice21],\n",
        "       train_regret_loser_13[slice21],\n",
        "       train_regret_loser_14[slice21],\n",
        "       train_regret_loser_15[slice21],\n",
        "       train_regret_loser_16[slice21],\n",
        "       train_regret_loser_17[slice21],\n",
        "       train_regret_loser_18[slice21],\n",
        "       train_regret_loser_19[slice21],\n",
        "       train_regret_loser_20[slice21]]\n",
        "\n",
        "winner21 = [train_regret_winner_1[slice21],\n",
        "       train_regret_winner_2[slice21],\n",
        "       train_regret_winner_3[slice21],\n",
        "       train_regret_winner_4[slice21],\n",
        "       train_regret_winner_5[slice21],\n",
        "       train_regret_winner_6[slice21],\n",
        "       train_regret_winner_7[slice21],\n",
        "       train_regret_winner_8[slice21],\n",
        "       train_regret_winner_9[slice21],\n",
        "       train_regret_winner_10[slice21],\n",
        "       train_regret_winner_11[slice21],\n",
        "       train_regret_winner_12[slice21],\n",
        "       train_regret_winner_13[slice21],\n",
        "       train_regret_winner_14[slice21],\n",
        "       train_regret_winner_15[slice21],\n",
        "       train_regret_winner_16[slice21],\n",
        "       train_regret_winner_17[slice21],\n",
        "       train_regret_winner_18[slice21],\n",
        "       train_regret_winner_19[slice21],\n",
        "       train_regret_winner_20[slice21]]\n",
        "\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\n",
        "\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\n",
        "\n",
        "slice31 = 30\n",
        "\n",
        "loser31 = [train_regret_loser_1[slice31],\n",
        "       train_regret_loser_2[slice31],\n",
        "       train_regret_loser_3[slice31],\n",
        "       train_regret_loser_4[slice31],\n",
        "       train_regret_loser_5[slice31],\n",
        "       train_regret_loser_6[slice31],\n",
        "       train_regret_loser_7[slice31],\n",
        "       train_regret_loser_8[slice31],\n",
        "       train_regret_loser_9[slice31],\n",
        "       train_regret_loser_10[slice31],\n",
        "       train_regret_loser_11[slice31],\n",
        "       train_regret_loser_12[slice31],\n",
        "       train_regret_loser_13[slice31],\n",
        "       train_regret_loser_14[slice31],\n",
        "       train_regret_loser_15[slice31],\n",
        "       train_regret_loser_16[slice31],\n",
        "       train_regret_loser_17[slice31],\n",
        "       train_regret_loser_18[slice31],\n",
        "       train_regret_loser_19[slice31],\n",
        "       train_regret_loser_20[slice31]]\n",
        "\n",
        "winner31 = [train_regret_winner_1[slice31],\n",
        "       train_regret_winner_2[slice31],\n",
        "       train_regret_winner_3[slice31],\n",
        "       train_regret_winner_4[slice31],\n",
        "       train_regret_winner_5[slice31],\n",
        "       train_regret_winner_6[slice31],\n",
        "       train_regret_winner_7[slice31],\n",
        "       train_regret_winner_8[slice31],\n",
        "       train_regret_winner_9[slice31],\n",
        "       train_regret_winner_10[slice31],\n",
        "       train_regret_winner_11[slice31],\n",
        "       train_regret_winner_12[slice31],\n",
        "       train_regret_winner_13[slice31],\n",
        "       train_regret_winner_14[slice31],\n",
        "       train_regret_winner_15[slice31],\n",
        "       train_regret_winner_16[slice31],\n",
        "       train_regret_winner_17[slice31],\n",
        "       train_regret_winner_18[slice31],\n",
        "       train_regret_winner_19[slice31],\n",
        "       train_regret_winner_20[slice31]]\n",
        "\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\n",
        "\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\n",
        "\n",
        "slice41 = 40\n",
        "\n",
        "loser41 = [train_regret_loser_1[slice41],\n",
        "       train_regret_loser_2[slice41],\n",
        "       train_regret_loser_3[slice41],\n",
        "       train_regret_loser_4[slice41],\n",
        "       train_regret_loser_5[slice41],\n",
        "       train_regret_loser_6[slice41],\n",
        "       train_regret_loser_7[slice41],\n",
        "       train_regret_loser_8[slice41],\n",
        "       train_regret_loser_9[slice41],\n",
        "       train_regret_loser_10[slice41],\n",
        "       train_regret_loser_11[slice41],\n",
        "       train_regret_loser_12[slice41],\n",
        "       train_regret_loser_13[slice41],\n",
        "       train_regret_loser_14[slice41],\n",
        "       train_regret_loser_15[slice41],\n",
        "       train_regret_loser_16[slice41],\n",
        "       train_regret_loser_17[slice41],\n",
        "       train_regret_loser_18[slice41],\n",
        "       train_regret_loser_19[slice41],\n",
        "       train_regret_loser_20[slice41]]\n",
        "\n",
        "winner41 = [train_regret_winner_1[slice41],\n",
        "       train_regret_winner_2[slice41],\n",
        "       train_regret_winner_3[slice41],\n",
        "       train_regret_winner_4[slice41],\n",
        "       train_regret_winner_5[slice41],\n",
        "       train_regret_winner_6[slice41],\n",
        "       train_regret_winner_7[slice41],\n",
        "       train_regret_winner_8[slice41],\n",
        "       train_regret_winner_9[slice41],\n",
        "       train_regret_winner_10[slice41],\n",
        "       train_regret_winner_11[slice41],\n",
        "       train_regret_winner_12[slice41],\n",
        "       train_regret_winner_13[slice41],\n",
        "       train_regret_winner_14[slice41],\n",
        "       train_regret_winner_15[slice41],\n",
        "       train_regret_winner_16[slice41],\n",
        "       train_regret_winner_17[slice41],\n",
        "       train_regret_winner_18[slice41],\n",
        "       train_regret_winner_19[slice41],\n",
        "       train_regret_winner_20[slice41]]\n",
        "\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\n",
        "\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\n",
        "\n",
        "slice51 = 50\n",
        "\n",
        "loser51 = [train_regret_loser_1[slice51],\n",
        "       train_regret_loser_2[slice51],\n",
        "       train_regret_loser_3[slice51],\n",
        "       train_regret_loser_4[slice51],\n",
        "       train_regret_loser_5[slice51],\n",
        "       train_regret_loser_6[slice51],\n",
        "       train_regret_loser_7[slice51],\n",
        "       train_regret_loser_8[slice51],\n",
        "       train_regret_loser_9[slice51],\n",
        "       train_regret_loser_10[slice51],\n",
        "       train_regret_loser_11[slice51],\n",
        "       train_regret_loser_12[slice51],\n",
        "       train_regret_loser_13[slice51],\n",
        "       train_regret_loser_14[slice51],\n",
        "       train_regret_loser_15[slice51],\n",
        "       train_regret_loser_16[slice51],\n",
        "       train_regret_loser_17[slice51],\n",
        "       train_regret_loser_18[slice51],\n",
        "       train_regret_loser_19[slice51],\n",
        "       train_regret_loser_20[slice51]]\n",
        "\n",
        "winner51 = [train_regret_winner_1[slice51],\n",
        "       train_regret_winner_2[slice51],\n",
        "       train_regret_winner_3[slice51],\n",
        "       train_regret_winner_4[slice51],\n",
        "       train_regret_winner_5[slice51],\n",
        "       train_regret_winner_6[slice51],\n",
        "       train_regret_winner_7[slice51],\n",
        "       train_regret_winner_8[slice51],\n",
        "       train_regret_winner_9[slice51],\n",
        "       train_regret_winner_10[slice51],\n",
        "       train_regret_winner_11[slice51],\n",
        "       train_regret_winner_12[slice51],\n",
        "       train_regret_winner_13[slice51],\n",
        "       train_regret_winner_14[slice51],\n",
        "       train_regret_winner_15[slice51],\n",
        "       train_regret_winner_16[slice51],\n",
        "       train_regret_winner_17[slice51],\n",
        "       train_regret_winner_18[slice51],\n",
        "       train_regret_winner_19[slice51],\n",
        "       train_regret_winner_20[slice51]]\n",
        "\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\n",
        "\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\n",
        "\n",
        "slice61 = 60\n",
        "\n",
        "loser61 = [train_regret_loser_1[slice61],\n",
        "       train_regret_loser_2[slice61],\n",
        "       train_regret_loser_3[slice61],\n",
        "       train_regret_loser_4[slice61],\n",
        "       train_regret_loser_5[slice61],\n",
        "       train_regret_loser_6[slice61],\n",
        "       train_regret_loser_7[slice61],\n",
        "       train_regret_loser_8[slice61],\n",
        "       train_regret_loser_9[slice61],\n",
        "       train_regret_loser_10[slice61],\n",
        "       train_regret_loser_11[slice61],\n",
        "       train_regret_loser_12[slice61],\n",
        "       train_regret_loser_13[slice61],\n",
        "       train_regret_loser_14[slice61],\n",
        "       train_regret_loser_15[slice61],\n",
        "       train_regret_loser_16[slice61],\n",
        "       train_regret_loser_17[slice61],\n",
        "       train_regret_loser_18[slice61],\n",
        "       train_regret_loser_19[slice61],\n",
        "       train_regret_loser_20[slice61]]\n",
        "\n",
        "winner61 = [train_regret_winner_1[slice61],\n",
        "       train_regret_winner_2[slice61],\n",
        "       train_regret_winner_3[slice61],\n",
        "       train_regret_winner_4[slice61],\n",
        "       train_regret_winner_5[slice61],\n",
        "       train_regret_winner_6[slice61],\n",
        "       train_regret_winner_7[slice61],\n",
        "       train_regret_winner_8[slice61],\n",
        "       train_regret_winner_9[slice61],\n",
        "       train_regret_winner_10[slice61],\n",
        "       train_regret_winner_11[slice61],\n",
        "       train_regret_winner_12[slice61],\n",
        "       train_regret_winner_13[slice61],\n",
        "       train_regret_winner_14[slice61],\n",
        "       train_regret_winner_15[slice61],\n",
        "       train_regret_winner_16[slice61],\n",
        "       train_regret_winner_17[slice61],\n",
        "       train_regret_winner_18[slice61],\n",
        "       train_regret_winner_19[slice61],\n",
        "       train_regret_winner_20[slice61]]\n",
        "\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\n",
        "\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\n",
        "\n",
        "slice71 = 70\n",
        "\n",
        "loser71 = [train_regret_loser_1[slice71],\n",
        "       train_regret_loser_2[slice71],\n",
        "       train_regret_loser_3[slice71],\n",
        "       train_regret_loser_4[slice71],\n",
        "       train_regret_loser_5[slice71],\n",
        "       train_regret_loser_6[slice71],\n",
        "       train_regret_loser_7[slice71],\n",
        "       train_regret_loser_8[slice71],\n",
        "       train_regret_loser_9[slice71],\n",
        "       train_regret_loser_10[slice71],\n",
        "       train_regret_loser_11[slice71],\n",
        "       train_regret_loser_12[slice71],\n",
        "       train_regret_loser_13[slice71],\n",
        "       train_regret_loser_14[slice71],\n",
        "       train_regret_loser_15[slice71],\n",
        "       train_regret_loser_16[slice71],\n",
        "       train_regret_loser_17[slice71],\n",
        "       train_regret_loser_18[slice71],\n",
        "       train_regret_loser_19[slice71],\n",
        "       train_regret_loser_20[slice71]]\n",
        "\n",
        "winner71 = [train_regret_winner_1[slice71],\n",
        "       train_regret_winner_2[slice71],\n",
        "       train_regret_winner_3[slice71],\n",
        "       train_regret_winner_4[slice71],\n",
        "       train_regret_winner_5[slice71],\n",
        "       train_regret_winner_6[slice71],\n",
        "       train_regret_winner_7[slice71],\n",
        "       train_regret_winner_8[slice71],\n",
        "       train_regret_winner_9[slice71],\n",
        "       train_regret_winner_10[slice71],\n",
        "       train_regret_winner_11[slice71],\n",
        "       train_regret_winner_12[slice71],\n",
        "       train_regret_winner_13[slice71],\n",
        "       train_regret_winner_14[slice71],\n",
        "       train_regret_winner_15[slice71],\n",
        "       train_regret_winner_16[slice71],\n",
        "       train_regret_winner_17[slice71],\n",
        "       train_regret_winner_18[slice71],\n",
        "       train_regret_winner_19[slice71],\n",
        "       train_regret_winner_20[slice71]]\n",
        "\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\n",
        "\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\n",
        "\n",
        "slice81 = 80\n",
        "\n",
        "loser81 = [train_regret_loser_1[slice81],\n",
        "       train_regret_loser_2[slice81],\n",
        "       train_regret_loser_3[slice81],\n",
        "       train_regret_loser_4[slice81],\n",
        "       train_regret_loser_5[slice81],\n",
        "       train_regret_loser_6[slice81],\n",
        "       train_regret_loser_7[slice81],\n",
        "       train_regret_loser_8[slice81],\n",
        "       train_regret_loser_9[slice81],\n",
        "       train_regret_loser_10[slice81],\n",
        "       train_regret_loser_11[slice81],\n",
        "       train_regret_loser_12[slice81],\n",
        "       train_regret_loser_13[slice81],\n",
        "       train_regret_loser_14[slice81],\n",
        "       train_regret_loser_15[slice81],\n",
        "       train_regret_loser_16[slice81],\n",
        "       train_regret_loser_17[slice81],\n",
        "       train_regret_loser_18[slice81],\n",
        "       train_regret_loser_19[slice81],\n",
        "       train_regret_loser_20[slice81]]\n",
        "\n",
        "winner81 = [train_regret_winner_1[slice81],\n",
        "       train_regret_winner_2[slice81],\n",
        "       train_regret_winner_3[slice81],\n",
        "       train_regret_winner_4[slice81],\n",
        "       train_regret_winner_5[slice81],\n",
        "       train_regret_winner_6[slice81],\n",
        "       train_regret_winner_7[slice81],\n",
        "       train_regret_winner_8[slice81],\n",
        "       train_regret_winner_9[slice81],\n",
        "       train_regret_winner_10[slice81],\n",
        "       train_regret_winner_11[slice81],\n",
        "       train_regret_winner_12[slice81],\n",
        "       train_regret_winner_13[slice81],\n",
        "       train_regret_winner_14[slice81],\n",
        "       train_regret_winner_15[slice81],\n",
        "       train_regret_winner_16[slice81],\n",
        "       train_regret_winner_17[slice81],\n",
        "       train_regret_winner_18[slice81],\n",
        "       train_regret_winner_19[slice81],\n",
        "       train_regret_winner_20[slice81]]\n",
        "\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\n",
        "\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\n",
        "\n",
        "slice91 = 90\n",
        "\n",
        "loser91 = [train_regret_loser_1[slice91],\n",
        "       train_regret_loser_2[slice91],\n",
        "       train_regret_loser_3[slice91],\n",
        "       train_regret_loser_4[slice91],\n",
        "       train_regret_loser_5[slice91],\n",
        "       train_regret_loser_6[slice91],\n",
        "       train_regret_loser_7[slice91],\n",
        "       train_regret_loser_8[slice91],\n",
        "       train_regret_loser_9[slice91],\n",
        "       train_regret_loser_10[slice91],\n",
        "       train_regret_loser_11[slice91],\n",
        "       train_regret_loser_12[slice91],\n",
        "       train_regret_loser_13[slice91],\n",
        "       train_regret_loser_14[slice91],\n",
        "       train_regret_loser_15[slice91],\n",
        "       train_regret_loser_16[slice91],\n",
        "       train_regret_loser_17[slice91],\n",
        "       train_regret_loser_18[slice91],\n",
        "       train_regret_loser_19[slice91],\n",
        "       train_regret_loser_20[slice91]]\n",
        "\n",
        "winner91 = [train_regret_winner_1[slice91],\n",
        "       train_regret_winner_2[slice91],\n",
        "       train_regret_winner_3[slice91],\n",
        "       train_regret_winner_4[slice91],\n",
        "       train_regret_winner_5[slice91],\n",
        "       train_regret_winner_6[slice91],\n",
        "       train_regret_winner_7[slice91],\n",
        "       train_regret_winner_8[slice91],\n",
        "       train_regret_winner_9[slice91],\n",
        "       train_regret_winner_10[slice91],\n",
        "       train_regret_winner_11[slice91],\n",
        "       train_regret_winner_12[slice91],\n",
        "       train_regret_winner_13[slice91],\n",
        "       train_regret_winner_14[slice91],\n",
        "       train_regret_winner_15[slice91],\n",
        "       train_regret_winner_16[slice91],\n",
        "       train_regret_winner_17[slice91],\n",
        "       train_regret_winner_18[slice91],\n",
        "       train_regret_winner_19[slice91],\n",
        "       train_regret_winner_20[slice91]]\n",
        "\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\n",
        "\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\n",
        "\n",
        "slice101 = 100\n",
        "\n",
        "loser101 = [train_regret_loser_1[slice101],\n",
        "       train_regret_loser_2[slice101],\n",
        "       train_regret_loser_3[slice101],\n",
        "       train_regret_loser_4[slice101],\n",
        "       train_regret_loser_5[slice101],\n",
        "       train_regret_loser_6[slice101],\n",
        "       train_regret_loser_7[slice101],\n",
        "       train_regret_loser_8[slice101],\n",
        "       train_regret_loser_9[slice101],\n",
        "       train_regret_loser_10[slice101],\n",
        "       train_regret_loser_11[slice101],\n",
        "       train_regret_loser_12[slice101],\n",
        "       train_regret_loser_13[slice101],\n",
        "       train_regret_loser_14[slice101],\n",
        "       train_regret_loser_15[slice101],\n",
        "       train_regret_loser_16[slice101],\n",
        "       train_regret_loser_17[slice101],\n",
        "       train_regret_loser_18[slice101],\n",
        "       train_regret_loser_19[slice101],\n",
        "       train_regret_loser_20[slice101]]\n",
        "\n",
        "winner101 = [train_regret_winner_1[slice101],\n",
        "       train_regret_winner_2[slice101],\n",
        "       train_regret_winner_3[slice101],\n",
        "       train_regret_winner_4[slice101],\n",
        "       train_regret_winner_5[slice101],\n",
        "       train_regret_winner_6[slice101],\n",
        "       train_regret_winner_7[slice101],\n",
        "       train_regret_winner_8[slice101],\n",
        "       train_regret_winner_9[slice101],\n",
        "       train_regret_winner_10[slice101],\n",
        "       train_regret_winner_11[slice101],\n",
        "       train_regret_winner_12[slice101],\n",
        "       train_regret_winner_13[slice101],\n",
        "       train_regret_winner_14[slice101],\n",
        "       train_regret_winner_15[slice101],\n",
        "       train_regret_winner_16[slice101],\n",
        "       train_regret_winner_17[slice101],\n",
        "       train_regret_winner_18[slice101],\n",
        "       train_regret_winner_19[slice101],\n",
        "       train_regret_winner_20[slice101]]\n",
        "\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\n",
        "\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\n",
        "\n",
        "slice2 = 1\n",
        "\n",
        "loser2 = [train_regret_loser_1[slice2],\n",
        "       train_regret_loser_2[slice2],\n",
        "       train_regret_loser_3[slice2],\n",
        "       train_regret_loser_4[slice2],\n",
        "       train_regret_loser_5[slice2],\n",
        "       train_regret_loser_6[slice2],\n",
        "       train_regret_loser_7[slice2],\n",
        "       train_regret_loser_8[slice2],\n",
        "       train_regret_loser_9[slice2],\n",
        "       train_regret_loser_10[slice2],\n",
        "       train_regret_loser_11[slice2],\n",
        "       train_regret_loser_12[slice2],\n",
        "       train_regret_loser_13[slice2],\n",
        "       train_regret_loser_14[slice2],\n",
        "       train_regret_loser_15[slice2],\n",
        "       train_regret_loser_16[slice2],\n",
        "       train_regret_loser_17[slice2],\n",
        "       train_regret_loser_18[slice2],\n",
        "       train_regret_loser_19[slice2],\n",
        "       train_regret_loser_20[slice2]]\n",
        "\n",
        "winner2 = [train_regret_winner_1[slice2],\n",
        "       train_regret_winner_2[slice2],\n",
        "       train_regret_winner_3[slice2],\n",
        "       train_regret_winner_4[slice2],\n",
        "       train_regret_winner_5[slice2],\n",
        "       train_regret_winner_6[slice2],\n",
        "       train_regret_winner_7[slice2],\n",
        "       train_regret_winner_8[slice2],\n",
        "       train_regret_winner_9[slice2],\n",
        "       train_regret_winner_10[slice2],\n",
        "       train_regret_winner_11[slice2],\n",
        "       train_regret_winner_12[slice2],\n",
        "       train_regret_winner_13[slice2],\n",
        "       train_regret_winner_14[slice2],\n",
        "       train_regret_winner_15[slice2],\n",
        "       train_regret_winner_16[slice2],\n",
        "       train_regret_winner_17[slice2],\n",
        "       train_regret_winner_18[slice2],\n",
        "       train_regret_winner_19[slice2],\n",
        "       train_regret_winner_20[slice2]]\n",
        "\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\n",
        "\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\n",
        "\n",
        "slice12 = 11\n",
        "\n",
        "loser12 = [train_regret_loser_1[slice12],\n",
        "       train_regret_loser_2[slice12],\n",
        "       train_regret_loser_3[slice12],\n",
        "       train_regret_loser_4[slice12],\n",
        "       train_regret_loser_5[slice12],\n",
        "       train_regret_loser_6[slice12],\n",
        "       train_regret_loser_7[slice12],\n",
        "       train_regret_loser_8[slice12],\n",
        "       train_regret_loser_9[slice12],\n",
        "       train_regret_loser_10[slice12],\n",
        "       train_regret_loser_11[slice12],\n",
        "       train_regret_loser_12[slice12],\n",
        "       train_regret_loser_13[slice12],\n",
        "       train_regret_loser_14[slice12],\n",
        "       train_regret_loser_15[slice12],\n",
        "       train_regret_loser_16[slice12],\n",
        "       train_regret_loser_17[slice12],\n",
        "       train_regret_loser_18[slice12],\n",
        "       train_regret_loser_19[slice12],\n",
        "       train_regret_loser_20[slice12]]\n",
        "\n",
        "winner12 = [train_regret_winner_1[slice12],\n",
        "       train_regret_winner_2[slice12],\n",
        "       train_regret_winner_3[slice12],\n",
        "       train_regret_winner_4[slice12],\n",
        "       train_regret_winner_5[slice12],\n",
        "       train_regret_winner_6[slice12],\n",
        "       train_regret_winner_7[slice12],\n",
        "       train_regret_winner_8[slice12],\n",
        "       train_regret_winner_9[slice12],\n",
        "       train_regret_winner_10[slice12],\n",
        "       train_regret_winner_11[slice12],\n",
        "       train_regret_winner_12[slice12],\n",
        "       train_regret_winner_13[slice12],\n",
        "       train_regret_winner_14[slice12],\n",
        "       train_regret_winner_15[slice12],\n",
        "       train_regret_winner_16[slice12],\n",
        "       train_regret_winner_17[slice12],\n",
        "       train_regret_winner_18[slice12],\n",
        "       train_regret_winner_19[slice12],\n",
        "       train_regret_winner_20[slice12]]\n",
        "\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\n",
        "\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\n",
        "\n",
        "slice22 = 21\n",
        "\n",
        "loser22 = [train_regret_loser_1[slice22],\n",
        "       train_regret_loser_2[slice22],\n",
        "       train_regret_loser_3[slice22],\n",
        "       train_regret_loser_4[slice22],\n",
        "       train_regret_loser_5[slice22],\n",
        "       train_regret_loser_6[slice22],\n",
        "       train_regret_loser_7[slice22],\n",
        "       train_regret_loser_8[slice22],\n",
        "       train_regret_loser_9[slice22],\n",
        "       train_regret_loser_10[slice22],\n",
        "       train_regret_loser_11[slice22],\n",
        "       train_regret_loser_12[slice22],\n",
        "       train_regret_loser_13[slice22],\n",
        "       train_regret_loser_14[slice22],\n",
        "       train_regret_loser_15[slice22],\n",
        "       train_regret_loser_16[slice22],\n",
        "       train_regret_loser_17[slice22],\n",
        "       train_regret_loser_18[slice22],\n",
        "       train_regret_loser_19[slice22],\n",
        "       train_regret_loser_20[slice22]]\n",
        "\n",
        "winner22 = [train_regret_winner_1[slice22],\n",
        "       train_regret_winner_2[slice22],\n",
        "       train_regret_winner_3[slice22],\n",
        "       train_regret_winner_4[slice22],\n",
        "       train_regret_winner_5[slice22],\n",
        "       train_regret_winner_6[slice22],\n",
        "       train_regret_winner_7[slice22],\n",
        "       train_regret_winner_8[slice22],\n",
        "       train_regret_winner_9[slice22],\n",
        "       train_regret_winner_10[slice22],\n",
        "       train_regret_winner_11[slice22],\n",
        "       train_regret_winner_12[slice22],\n",
        "       train_regret_winner_13[slice22],\n",
        "       train_regret_winner_14[slice22],\n",
        "       train_regret_winner_15[slice22],\n",
        "       train_regret_winner_16[slice22],\n",
        "       train_regret_winner_17[slice22],\n",
        "       train_regret_winner_18[slice22],\n",
        "       train_regret_winner_19[slice22],\n",
        "       train_regret_winner_20[slice22]]\n",
        "\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\n",
        "\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\n",
        "\n",
        "slice32 = 31\n",
        "\n",
        "loser32 = [train_regret_loser_1[slice32],\n",
        "       train_regret_loser_2[slice32],\n",
        "       train_regret_loser_3[slice32],\n",
        "       train_regret_loser_4[slice32],\n",
        "       train_regret_loser_5[slice32],\n",
        "       train_regret_loser_6[slice32],\n",
        "       train_regret_loser_7[slice32],\n",
        "       train_regret_loser_8[slice32],\n",
        "       train_regret_loser_9[slice32],\n",
        "       train_regret_loser_10[slice32],\n",
        "       train_regret_loser_11[slice32],\n",
        "       train_regret_loser_12[slice32],\n",
        "       train_regret_loser_13[slice32],\n",
        "       train_regret_loser_14[slice32],\n",
        "       train_regret_loser_15[slice32],\n",
        "       train_regret_loser_16[slice32],\n",
        "       train_regret_loser_17[slice32],\n",
        "       train_regret_loser_18[slice32],\n",
        "       train_regret_loser_19[slice32],\n",
        "       train_regret_loser_20[slice32]]\n",
        "\n",
        "winner32 = [train_regret_winner_1[slice32],\n",
        "       train_regret_winner_2[slice32],\n",
        "       train_regret_winner_3[slice32],\n",
        "       train_regret_winner_4[slice32],\n",
        "       train_regret_winner_5[slice32],\n",
        "       train_regret_winner_6[slice32],\n",
        "       train_regret_winner_7[slice32],\n",
        "       train_regret_winner_8[slice32],\n",
        "       train_regret_winner_9[slice32],\n",
        "       train_regret_winner_10[slice32],\n",
        "       train_regret_winner_11[slice32],\n",
        "       train_regret_winner_12[slice32],\n",
        "       train_regret_winner_13[slice32],\n",
        "       train_regret_winner_14[slice32],\n",
        "       train_regret_winner_15[slice32],\n",
        "       train_regret_winner_16[slice32],\n",
        "       train_regret_winner_17[slice32],\n",
        "       train_regret_winner_18[slice32],\n",
        "       train_regret_winner_19[slice32],\n",
        "       train_regret_winner_20[slice32]]\n",
        "\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\n",
        "\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\n",
        "\n",
        "slice42 = 41\n",
        "\n",
        "loser42 = [train_regret_loser_1[slice42],\n",
        "       train_regret_loser_2[slice42],\n",
        "       train_regret_loser_3[slice42],\n",
        "       train_regret_loser_4[slice42],\n",
        "       train_regret_loser_5[slice42],\n",
        "       train_regret_loser_6[slice42],\n",
        "       train_regret_loser_7[slice42],\n",
        "       train_regret_loser_8[slice42],\n",
        "       train_regret_loser_9[slice42],\n",
        "       train_regret_loser_10[slice42],\n",
        "       train_regret_loser_11[slice42],\n",
        "       train_regret_loser_12[slice42],\n",
        "       train_regret_loser_13[slice42],\n",
        "       train_regret_loser_14[slice42],\n",
        "       train_regret_loser_15[slice42],\n",
        "       train_regret_loser_16[slice42],\n",
        "       train_regret_loser_17[slice42],\n",
        "       train_regret_loser_18[slice42],\n",
        "       train_regret_loser_19[slice42],\n",
        "       train_regret_loser_20[slice42]]\n",
        "\n",
        "winner42 = [train_regret_winner_1[slice42],\n",
        "       train_regret_winner_2[slice42],\n",
        "       train_regret_winner_3[slice42],\n",
        "       train_regret_winner_4[slice42],\n",
        "       train_regret_winner_5[slice42],\n",
        "       train_regret_winner_6[slice42],\n",
        "       train_regret_winner_7[slice42],\n",
        "       train_regret_winner_8[slice42],\n",
        "       train_regret_winner_9[slice42],\n",
        "       train_regret_winner_10[slice42],\n",
        "       train_regret_winner_11[slice42],\n",
        "       train_regret_winner_12[slice42],\n",
        "       train_regret_winner_13[slice42],\n",
        "       train_regret_winner_14[slice42],\n",
        "       train_regret_winner_15[slice42],\n",
        "       train_regret_winner_16[slice42],\n",
        "       train_regret_winner_17[slice42],\n",
        "       train_regret_winner_18[slice42],\n",
        "       train_regret_winner_19[slice42],\n",
        "       train_regret_winner_20[slice42]]\n",
        "\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\n",
        "\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\n",
        "\n",
        "slice52 = 51\n",
        "\n",
        "loser52 = [train_regret_loser_1[slice52],\n",
        "       train_regret_loser_2[slice52],\n",
        "       train_regret_loser_3[slice52],\n",
        "       train_regret_loser_4[slice52],\n",
        "       train_regret_loser_5[slice52],\n",
        "       train_regret_loser_6[slice52],\n",
        "       train_regret_loser_7[slice52],\n",
        "       train_regret_loser_8[slice52],\n",
        "       train_regret_loser_9[slice52],\n",
        "       train_regret_loser_10[slice52],\n",
        "       train_regret_loser_11[slice52],\n",
        "       train_regret_loser_12[slice52],\n",
        "       train_regret_loser_13[slice52],\n",
        "       train_regret_loser_14[slice52],\n",
        "       train_regret_loser_15[slice52],\n",
        "       train_regret_loser_16[slice52],\n",
        "       train_regret_loser_17[slice52],\n",
        "       train_regret_loser_18[slice52],\n",
        "       train_regret_loser_19[slice52],\n",
        "       train_regret_loser_20[slice52]]\n",
        "\n",
        "winner52 = [train_regret_winner_1[slice52],\n",
        "       train_regret_winner_2[slice52],\n",
        "       train_regret_winner_3[slice52],\n",
        "       train_regret_winner_4[slice52],\n",
        "       train_regret_winner_5[slice52],\n",
        "       train_regret_winner_6[slice52],\n",
        "       train_regret_winner_7[slice52],\n",
        "       train_regret_winner_8[slice52],\n",
        "       train_regret_winner_9[slice52],\n",
        "       train_regret_winner_10[slice52],\n",
        "       train_regret_winner_11[slice52],\n",
        "       train_regret_winner_12[slice52],\n",
        "       train_regret_winner_13[slice52],\n",
        "       train_regret_winner_14[slice52],\n",
        "       train_regret_winner_15[slice52],\n",
        "       train_regret_winner_16[slice52],\n",
        "       train_regret_winner_17[slice52],\n",
        "       train_regret_winner_18[slice52],\n",
        "       train_regret_winner_19[slice52],\n",
        "       train_regret_winner_20[slice52]]\n",
        "\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\n",
        "\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\n",
        "\n",
        "slice62 = 61\n",
        "\n",
        "loser62 = [train_regret_loser_1[slice62],\n",
        "       train_regret_loser_2[slice62],\n",
        "       train_regret_loser_3[slice62],\n",
        "       train_regret_loser_4[slice62],\n",
        "       train_regret_loser_5[slice62],\n",
        "       train_regret_loser_6[slice62],\n",
        "       train_regret_loser_7[slice62],\n",
        "       train_regret_loser_8[slice62],\n",
        "       train_regret_loser_9[slice62],\n",
        "       train_regret_loser_10[slice62],\n",
        "       train_regret_loser_11[slice62],\n",
        "       train_regret_loser_12[slice62],\n",
        "       train_regret_loser_13[slice62],\n",
        "       train_regret_loser_14[slice62],\n",
        "       train_regret_loser_15[slice62],\n",
        "       train_regret_loser_16[slice62],\n",
        "       train_regret_loser_17[slice62],\n",
        "       train_regret_loser_18[slice62],\n",
        "       train_regret_loser_19[slice62],\n",
        "       train_regret_loser_20[slice62]]\n",
        "\n",
        "winner62 = [train_regret_winner_1[slice62],\n",
        "       train_regret_winner_2[slice62],\n",
        "       train_regret_winner_3[slice62],\n",
        "       train_regret_winner_4[slice62],\n",
        "       train_regret_winner_5[slice62],\n",
        "       train_regret_winner_6[slice62],\n",
        "       train_regret_winner_7[slice62],\n",
        "       train_regret_winner_8[slice62],\n",
        "       train_regret_winner_9[slice62],\n",
        "       train_regret_winner_10[slice62],\n",
        "       train_regret_winner_11[slice62],\n",
        "       train_regret_winner_12[slice62],\n",
        "       train_regret_winner_13[slice62],\n",
        "       train_regret_winner_14[slice62],\n",
        "       train_regret_winner_15[slice62],\n",
        "       train_regret_winner_16[slice62],\n",
        "       train_regret_winner_17[slice62],\n",
        "       train_regret_winner_18[slice62],\n",
        "       train_regret_winner_19[slice62],\n",
        "       train_regret_winner_20[slice62]]\n",
        "\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\n",
        "\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\n",
        "\n",
        "slice72 = 71\n",
        "\n",
        "loser72 = [train_regret_loser_1[slice72],\n",
        "       train_regret_loser_2[slice72],\n",
        "       train_regret_loser_3[slice72],\n",
        "       train_regret_loser_4[slice72],\n",
        "       train_regret_loser_5[slice72],\n",
        "       train_regret_loser_6[slice72],\n",
        "       train_regret_loser_7[slice72],\n",
        "       train_regret_loser_8[slice72],\n",
        "       train_regret_loser_9[slice72],\n",
        "       train_regret_loser_10[slice72],\n",
        "       train_regret_loser_11[slice72],\n",
        "       train_regret_loser_12[slice72],\n",
        "       train_regret_loser_13[slice72],\n",
        "       train_regret_loser_14[slice72],\n",
        "       train_regret_loser_15[slice72],\n",
        "       train_regret_loser_16[slice72],\n",
        "       train_regret_loser_17[slice72],\n",
        "       train_regret_loser_18[slice72],\n",
        "       train_regret_loser_19[slice72],\n",
        "       train_regret_loser_20[slice72]]\n",
        "\n",
        "winner72 = [train_regret_winner_1[slice72],\n",
        "       train_regret_winner_2[slice72],\n",
        "       train_regret_winner_3[slice72],\n",
        "       train_regret_winner_4[slice72],\n",
        "       train_regret_winner_5[slice72],\n",
        "       train_regret_winner_6[slice72],\n",
        "       train_regret_winner_7[slice72],\n",
        "       train_regret_winner_8[slice72],\n",
        "       train_regret_winner_9[slice72],\n",
        "       train_regret_winner_10[slice72],\n",
        "       train_regret_winner_11[slice72],\n",
        "       train_regret_winner_12[slice72],\n",
        "       train_regret_winner_13[slice72],\n",
        "       train_regret_winner_14[slice72],\n",
        "       train_regret_winner_15[slice72],\n",
        "       train_regret_winner_16[slice72],\n",
        "       train_regret_winner_17[slice72],\n",
        "       train_regret_winner_18[slice72],\n",
        "       train_regret_winner_19[slice72],\n",
        "       train_regret_winner_20[slice72]]\n",
        "\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\n",
        "\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\n",
        "\n",
        "slice82 = 81\n",
        "\n",
        "loser82 = [train_regret_loser_1[slice82],\n",
        "       train_regret_loser_2[slice82],\n",
        "       train_regret_loser_3[slice82],\n",
        "       train_regret_loser_4[slice82],\n",
        "       train_regret_loser_5[slice82],\n",
        "       train_regret_loser_6[slice82],\n",
        "       train_regret_loser_7[slice82],\n",
        "       train_regret_loser_8[slice82],\n",
        "       train_regret_loser_9[slice82],\n",
        "       train_regret_loser_10[slice82],\n",
        "       train_regret_loser_11[slice82],\n",
        "       train_regret_loser_12[slice82],\n",
        "       train_regret_loser_13[slice82],\n",
        "       train_regret_loser_14[slice82],\n",
        "       train_regret_loser_15[slice82],\n",
        "       train_regret_loser_16[slice82],\n",
        "       train_regret_loser_17[slice82],\n",
        "       train_regret_loser_18[slice82],\n",
        "       train_regret_loser_19[slice82],\n",
        "       train_regret_loser_20[slice82]]\n",
        "\n",
        "winner82 = [train_regret_winner_1[slice82],\n",
        "       train_regret_winner_2[slice82],\n",
        "       train_regret_winner_3[slice82],\n",
        "       train_regret_winner_4[slice82],\n",
        "       train_regret_winner_5[slice82],\n",
        "       train_regret_winner_6[slice82],\n",
        "       train_regret_winner_7[slice82],\n",
        "       train_regret_winner_8[slice82],\n",
        "       train_regret_winner_9[slice82],\n",
        "       train_regret_winner_10[slice82],\n",
        "       train_regret_winner_11[slice82],\n",
        "       train_regret_winner_12[slice82],\n",
        "       train_regret_winner_13[slice82],\n",
        "       train_regret_winner_14[slice82],\n",
        "       train_regret_winner_15[slice82],\n",
        "       train_regret_winner_16[slice82],\n",
        "       train_regret_winner_17[slice82],\n",
        "       train_regret_winner_18[slice82],\n",
        "       train_regret_winner_19[slice82],\n",
        "       train_regret_winner_20[slice82]]\n",
        "\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\n",
        "\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\n",
        "\n",
        "slice92 = 91\n",
        "\n",
        "loser92 = [train_regret_loser_1[slice92],\n",
        "       train_regret_loser_2[slice92],\n",
        "       train_regret_loser_3[slice92],\n",
        "       train_regret_loser_4[slice92],\n",
        "       train_regret_loser_5[slice92],\n",
        "       train_regret_loser_6[slice92],\n",
        "       train_regret_loser_7[slice92],\n",
        "       train_regret_loser_8[slice92],\n",
        "       train_regret_loser_9[slice92],\n",
        "       train_regret_loser_10[slice92],\n",
        "       train_regret_loser_11[slice92],\n",
        "       train_regret_loser_12[slice92],\n",
        "       train_regret_loser_13[slice92],\n",
        "       train_regret_loser_14[slice92],\n",
        "       train_regret_loser_15[slice92],\n",
        "       train_regret_loser_16[slice92],\n",
        "       train_regret_loser_17[slice92],\n",
        "       train_regret_loser_18[slice92],\n",
        "       train_regret_loser_19[slice92],\n",
        "       train_regret_loser_20[slice92]]\n",
        "\n",
        "winner92 = [train_regret_winner_1[slice92],\n",
        "       train_regret_winner_2[slice92],\n",
        "       train_regret_winner_3[slice92],\n",
        "       train_regret_winner_4[slice92],\n",
        "       train_regret_winner_5[slice92],\n",
        "       train_regret_winner_6[slice92],\n",
        "       train_regret_winner_7[slice92],\n",
        "       train_regret_winner_8[slice92],\n",
        "       train_regret_winner_9[slice92],\n",
        "       train_regret_winner_10[slice92],\n",
        "       train_regret_winner_11[slice92],\n",
        "       train_regret_winner_12[slice92],\n",
        "       train_regret_winner_13[slice92],\n",
        "       train_regret_winner_14[slice92],\n",
        "       train_regret_winner_15[slice92],\n",
        "       train_regret_winner_16[slice92],\n",
        "       train_regret_winner_17[slice92],\n",
        "       train_regret_winner_18[slice92],\n",
        "       train_regret_winner_19[slice92],\n",
        "       train_regret_winner_20[slice92]]\n",
        "\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\n",
        "\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\n",
        "\n",
        "slice3 = 2\n",
        "\n",
        "loser3 = [train_regret_loser_1[slice3],\n",
        "       train_regret_loser_2[slice3],\n",
        "       train_regret_loser_3[slice3],\n",
        "       train_regret_loser_4[slice3],\n",
        "       train_regret_loser_5[slice3],\n",
        "       train_regret_loser_6[slice3],\n",
        "       train_regret_loser_7[slice3],\n",
        "       train_regret_loser_8[slice3],\n",
        "       train_regret_loser_9[slice3],\n",
        "       train_regret_loser_10[slice3],\n",
        "       train_regret_loser_11[slice3],\n",
        "       train_regret_loser_12[slice3],\n",
        "       train_regret_loser_13[slice3],\n",
        "       train_regret_loser_14[slice3],\n",
        "       train_regret_loser_15[slice3],\n",
        "       train_regret_loser_16[slice3],\n",
        "       train_regret_loser_17[slice3],\n",
        "       train_regret_loser_18[slice3],\n",
        "       train_regret_loser_19[slice3],\n",
        "       train_regret_loser_20[slice3]]\n",
        "\n",
        "winner3 = [train_regret_winner_1[slice3],\n",
        "       train_regret_winner_2[slice3],\n",
        "       train_regret_winner_3[slice3],\n",
        "       train_regret_winner_4[slice3],\n",
        "       train_regret_winner_5[slice3],\n",
        "       train_regret_winner_6[slice3],\n",
        "       train_regret_winner_7[slice3],\n",
        "       train_regret_winner_8[slice3],\n",
        "       train_regret_winner_9[slice3],\n",
        "       train_regret_winner_10[slice3],\n",
        "       train_regret_winner_11[slice3],\n",
        "       train_regret_winner_12[slice3],\n",
        "       train_regret_winner_13[slice3],\n",
        "       train_regret_winner_14[slice3],\n",
        "       train_regret_winner_15[slice3],\n",
        "       train_regret_winner_16[slice3],\n",
        "       train_regret_winner_17[slice3],\n",
        "       train_regret_winner_18[slice3],\n",
        "       train_regret_winner_19[slice3],\n",
        "       train_regret_winner_20[slice3]]\n",
        "\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\n",
        "\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\n",
        "\n",
        "slice13 = 12\n",
        "\n",
        "loser13 = [train_regret_loser_1[slice13],\n",
        "       train_regret_loser_2[slice13],\n",
        "       train_regret_loser_3[slice13],\n",
        "       train_regret_loser_4[slice13],\n",
        "       train_regret_loser_5[slice13],\n",
        "       train_regret_loser_6[slice13],\n",
        "       train_regret_loser_7[slice13],\n",
        "       train_regret_loser_8[slice13],\n",
        "       train_regret_loser_9[slice13],\n",
        "       train_regret_loser_10[slice13],\n",
        "       train_regret_loser_11[slice13],\n",
        "       train_regret_loser_12[slice13],\n",
        "       train_regret_loser_13[slice13],\n",
        "       train_regret_loser_14[slice13],\n",
        "       train_regret_loser_15[slice13],\n",
        "       train_regret_loser_16[slice13],\n",
        "       train_regret_loser_17[slice13],\n",
        "       train_regret_loser_18[slice13],\n",
        "       train_regret_loser_19[slice13],\n",
        "       train_regret_loser_20[slice13]]\n",
        "\n",
        "winner13 = [train_regret_winner_1[slice13],\n",
        "       train_regret_winner_2[slice13],\n",
        "       train_regret_winner_3[slice13],\n",
        "       train_regret_winner_4[slice13],\n",
        "       train_regret_winner_5[slice13],\n",
        "       train_regret_winner_6[slice13],\n",
        "       train_regret_winner_7[slice13],\n",
        "       train_regret_winner_8[slice13],\n",
        "       train_regret_winner_9[slice13],\n",
        "       train_regret_winner_10[slice13],\n",
        "       train_regret_winner_11[slice13],\n",
        "       train_regret_winner_12[slice13],\n",
        "       train_regret_winner_13[slice13],\n",
        "       train_regret_winner_14[slice13],\n",
        "       train_regret_winner_15[slice13],\n",
        "       train_regret_winner_16[slice13],\n",
        "       train_regret_winner_17[slice13],\n",
        "       train_regret_winner_18[slice13],\n",
        "       train_regret_winner_19[slice13],\n",
        "       train_regret_winner_20[slice13]]\n",
        "\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\n",
        "\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\n",
        "\n",
        "slice23 = 22\n",
        "\n",
        "loser23 = [train_regret_loser_1[slice23],\n",
        "       train_regret_loser_2[slice23],\n",
        "       train_regret_loser_3[slice23],\n",
        "       train_regret_loser_4[slice23],\n",
        "       train_regret_loser_5[slice23],\n",
        "       train_regret_loser_6[slice23],\n",
        "       train_regret_loser_7[slice23],\n",
        "       train_regret_loser_8[slice23],\n",
        "       train_regret_loser_9[slice23],\n",
        "       train_regret_loser_10[slice23],\n",
        "       train_regret_loser_11[slice23],\n",
        "       train_regret_loser_12[slice23],\n",
        "       train_regret_loser_13[slice23],\n",
        "       train_regret_loser_14[slice23],\n",
        "       train_regret_loser_15[slice23],\n",
        "       train_regret_loser_16[slice23],\n",
        "       train_regret_loser_17[slice23],\n",
        "       train_regret_loser_18[slice23],\n",
        "       train_regret_loser_19[slice23],\n",
        "       train_regret_loser_20[slice23]]\n",
        "\n",
        "winner23 = [train_regret_winner_1[slice23],\n",
        "       train_regret_winner_2[slice23],\n",
        "       train_regret_winner_3[slice23],\n",
        "       train_regret_winner_4[slice23],\n",
        "       train_regret_winner_5[slice23],\n",
        "       train_regret_winner_6[slice23],\n",
        "       train_regret_winner_7[slice23],\n",
        "       train_regret_winner_8[slice23],\n",
        "       train_regret_winner_9[slice23],\n",
        "       train_regret_winner_10[slice23],\n",
        "       train_regret_winner_11[slice23],\n",
        "       train_regret_winner_12[slice23],\n",
        "       train_regret_winner_13[slice23],\n",
        "       train_regret_winner_14[slice23],\n",
        "       train_regret_winner_15[slice23],\n",
        "       train_regret_winner_16[slice23],\n",
        "       train_regret_winner_17[slice23],\n",
        "       train_regret_winner_18[slice23],\n",
        "       train_regret_winner_19[slice23],\n",
        "       train_regret_winner_20[slice23]]\n",
        "\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\n",
        "\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\n",
        "\n",
        "slice33 = 32\n",
        "\n",
        "loser33 = [train_regret_loser_1[slice33],\n",
        "       train_regret_loser_2[slice33],\n",
        "       train_regret_loser_3[slice33],\n",
        "       train_regret_loser_4[slice33],\n",
        "       train_regret_loser_5[slice33],\n",
        "       train_regret_loser_6[slice33],\n",
        "       train_regret_loser_7[slice33],\n",
        "       train_regret_loser_8[slice33],\n",
        "       train_regret_loser_9[slice33],\n",
        "       train_regret_loser_10[slice33],\n",
        "       train_regret_loser_11[slice33],\n",
        "       train_regret_loser_12[slice33],\n",
        "       train_regret_loser_13[slice33],\n",
        "       train_regret_loser_14[slice33],\n",
        "       train_regret_loser_15[slice33],\n",
        "       train_regret_loser_16[slice33],\n",
        "       train_regret_loser_17[slice33],\n",
        "       train_regret_loser_18[slice33],\n",
        "       train_regret_loser_19[slice33],\n",
        "       train_regret_loser_20[slice33]]\n",
        "\n",
        "winner33 = [train_regret_winner_1[slice33],\n",
        "       train_regret_winner_2[slice33],\n",
        "       train_regret_winner_3[slice33],\n",
        "       train_regret_winner_4[slice33],\n",
        "       train_regret_winner_5[slice33],\n",
        "       train_regret_winner_6[slice33],\n",
        "       train_regret_winner_7[slice33],\n",
        "       train_regret_winner_8[slice33],\n",
        "       train_regret_winner_9[slice33],\n",
        "       train_regret_winner_10[slice33],\n",
        "       train_regret_winner_11[slice33],\n",
        "       train_regret_winner_12[slice33],\n",
        "       train_regret_winner_13[slice33],\n",
        "       train_regret_winner_14[slice33],\n",
        "       train_regret_winner_15[slice33],\n",
        "       train_regret_winner_16[slice33],\n",
        "       train_regret_winner_17[slice33],\n",
        "       train_regret_winner_18[slice33],\n",
        "       train_regret_winner_19[slice33],\n",
        "       train_regret_winner_20[slice33]]\n",
        "\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\n",
        "\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\n",
        "\n",
        "slice43 = 42\n",
        "\n",
        "loser43 = [train_regret_loser_1[slice43],\n",
        "       train_regret_loser_2[slice43],\n",
        "       train_regret_loser_3[slice43],\n",
        "       train_regret_loser_4[slice43],\n",
        "       train_regret_loser_5[slice43],\n",
        "       train_regret_loser_6[slice43],\n",
        "       train_regret_loser_7[slice43],\n",
        "       train_regret_loser_8[slice43],\n",
        "       train_regret_loser_9[slice43],\n",
        "       train_regret_loser_10[slice43],\n",
        "       train_regret_loser_11[slice43],\n",
        "       train_regret_loser_12[slice43],\n",
        "       train_regret_loser_13[slice43],\n",
        "       train_regret_loser_14[slice43],\n",
        "       train_regret_loser_15[slice43],\n",
        "       train_regret_loser_16[slice43],\n",
        "       train_regret_loser_17[slice43],\n",
        "       train_regret_loser_18[slice43],\n",
        "       train_regret_loser_19[slice43],\n",
        "       train_regret_loser_20[slice43]]\n",
        "\n",
        "winner43 = [train_regret_winner_1[slice43],\n",
        "       train_regret_winner_2[slice43],\n",
        "       train_regret_winner_3[slice43],\n",
        "       train_regret_winner_4[slice43],\n",
        "       train_regret_winner_5[slice43],\n",
        "       train_regret_winner_6[slice43],\n",
        "       train_regret_winner_7[slice43],\n",
        "       train_regret_winner_8[slice43],\n",
        "       train_regret_winner_9[slice43],\n",
        "       train_regret_winner_10[slice43],\n",
        "       train_regret_winner_11[slice43],\n",
        "       train_regret_winner_12[slice43],\n",
        "       train_regret_winner_13[slice43],\n",
        "       train_regret_winner_14[slice43],\n",
        "       train_regret_winner_15[slice43],\n",
        "       train_regret_winner_16[slice43],\n",
        "       train_regret_winner_17[slice43],\n",
        "       train_regret_winner_18[slice43],\n",
        "       train_regret_winner_19[slice43],\n",
        "       train_regret_winner_20[slice43]]\n",
        "\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\n",
        "\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\n",
        "\n",
        "slice53 = 52\n",
        "\n",
        "loser53 = [train_regret_loser_1[slice53],\n",
        "       train_regret_loser_2[slice53],\n",
        "       train_regret_loser_3[slice53],\n",
        "       train_regret_loser_4[slice53],\n",
        "       train_regret_loser_5[slice53],\n",
        "       train_regret_loser_6[slice53],\n",
        "       train_regret_loser_7[slice53],\n",
        "       train_regret_loser_8[slice53],\n",
        "       train_regret_loser_9[slice53],\n",
        "       train_regret_loser_10[slice53],\n",
        "       train_regret_loser_11[slice53],\n",
        "       train_regret_loser_12[slice53],\n",
        "       train_regret_loser_13[slice53],\n",
        "       train_regret_loser_14[slice53],\n",
        "       train_regret_loser_15[slice53],\n",
        "       train_regret_loser_16[slice53],\n",
        "       train_regret_loser_17[slice53],\n",
        "       train_regret_loser_18[slice53],\n",
        "       train_regret_loser_19[slice53],\n",
        "       train_regret_loser_20[slice53]]\n",
        "\n",
        "winner53 = [train_regret_winner_1[slice53],\n",
        "       train_regret_winner_2[slice53],\n",
        "       train_regret_winner_3[slice53],\n",
        "       train_regret_winner_4[slice53],\n",
        "       train_regret_winner_5[slice53],\n",
        "       train_regret_winner_6[slice53],\n",
        "       train_regret_winner_7[slice53],\n",
        "       train_regret_winner_8[slice53],\n",
        "       train_regret_winner_9[slice53],\n",
        "       train_regret_winner_10[slice53],\n",
        "       train_regret_winner_11[slice53],\n",
        "       train_regret_winner_12[slice53],\n",
        "       train_regret_winner_13[slice53],\n",
        "       train_regret_winner_14[slice53],\n",
        "       train_regret_winner_15[slice53],\n",
        "       train_regret_winner_16[slice53],\n",
        "       train_regret_winner_17[slice53],\n",
        "       train_regret_winner_18[slice53],\n",
        "       train_regret_winner_19[slice53],\n",
        "       train_regret_winner_20[slice53]]\n",
        "\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\n",
        "\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\n",
        "\n",
        "slice63 = 62\n",
        "\n",
        "loser63 = [train_regret_loser_1[slice63],\n",
        "       train_regret_loser_2[slice63],\n",
        "       train_regret_loser_3[slice63],\n",
        "       train_regret_loser_4[slice63],\n",
        "       train_regret_loser_5[slice63],\n",
        "       train_regret_loser_6[slice63],\n",
        "       train_regret_loser_7[slice63],\n",
        "       train_regret_loser_8[slice63],\n",
        "       train_regret_loser_9[slice63],\n",
        "       train_regret_loser_10[slice63],\n",
        "       train_regret_loser_11[slice63],\n",
        "       train_regret_loser_12[slice63],\n",
        "       train_regret_loser_13[slice63],\n",
        "       train_regret_loser_14[slice63],\n",
        "       train_regret_loser_15[slice63],\n",
        "       train_regret_loser_16[slice63],\n",
        "       train_regret_loser_17[slice63],\n",
        "       train_regret_loser_18[slice63],\n",
        "       train_regret_loser_19[slice63],\n",
        "       train_regret_loser_20[slice63]]\n",
        "\n",
        "winner63 = [train_regret_winner_1[slice63],\n",
        "       train_regret_winner_2[slice63],\n",
        "       train_regret_winner_3[slice63],\n",
        "       train_regret_winner_4[slice63],\n",
        "       train_regret_winner_5[slice63],\n",
        "       train_regret_winner_6[slice63],\n",
        "       train_regret_winner_7[slice63],\n",
        "       train_regret_winner_8[slice63],\n",
        "       train_regret_winner_9[slice63],\n",
        "       train_regret_winner_10[slice63],\n",
        "       train_regret_winner_11[slice63],\n",
        "       train_regret_winner_12[slice63],\n",
        "       train_regret_winner_13[slice63],\n",
        "       train_regret_winner_14[slice63],\n",
        "       train_regret_winner_15[slice63],\n",
        "       train_regret_winner_16[slice63],\n",
        "       train_regret_winner_17[slice63],\n",
        "       train_regret_winner_18[slice63],\n",
        "       train_regret_winner_19[slice63],\n",
        "       train_regret_winner_20[slice63]]\n",
        "\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\n",
        "\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\n",
        "\n",
        "slice73 = 72\n",
        "\n",
        "loser73 = [train_regret_loser_1[slice73],\n",
        "       train_regret_loser_2[slice73],\n",
        "       train_regret_loser_3[slice73],\n",
        "       train_regret_loser_4[slice73],\n",
        "       train_regret_loser_5[slice73],\n",
        "       train_regret_loser_6[slice73],\n",
        "       train_regret_loser_7[slice73],\n",
        "       train_regret_loser_8[slice73],\n",
        "       train_regret_loser_9[slice73],\n",
        "       train_regret_loser_10[slice73],\n",
        "       train_regret_loser_11[slice73],\n",
        "       train_regret_loser_12[slice73],\n",
        "       train_regret_loser_13[slice73],\n",
        "       train_regret_loser_14[slice73],\n",
        "       train_regret_loser_15[slice73],\n",
        "       train_regret_loser_16[slice73],\n",
        "       train_regret_loser_17[slice73],\n",
        "       train_regret_loser_18[slice73],\n",
        "       train_regret_loser_19[slice73],\n",
        "       train_regret_loser_20[slice73]]\n",
        "\n",
        "winner73 = [train_regret_winner_1[slice73],\n",
        "       train_regret_winner_2[slice73],\n",
        "       train_regret_winner_3[slice73],\n",
        "       train_regret_winner_4[slice73],\n",
        "       train_regret_winner_5[slice73],\n",
        "       train_regret_winner_6[slice73],\n",
        "       train_regret_winner_7[slice73],\n",
        "       train_regret_winner_8[slice73],\n",
        "       train_regret_winner_9[slice73],\n",
        "       train_regret_winner_10[slice73],\n",
        "       train_regret_winner_11[slice73],\n",
        "       train_regret_winner_12[slice73],\n",
        "       train_regret_winner_13[slice73],\n",
        "       train_regret_winner_14[slice73],\n",
        "       train_regret_winner_15[slice73],\n",
        "       train_regret_winner_16[slice73],\n",
        "       train_regret_winner_17[slice73],\n",
        "       train_regret_winner_18[slice73],\n",
        "       train_regret_winner_19[slice73],\n",
        "       train_regret_winner_20[slice73]]\n",
        "\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\n",
        "\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\n",
        "\n",
        "slice83 = 82\n",
        "\n",
        "loser83 = [train_regret_loser_1[slice83],\n",
        "       train_regret_loser_2[slice83],\n",
        "       train_regret_loser_3[slice83],\n",
        "       train_regret_loser_4[slice83],\n",
        "       train_regret_loser_5[slice83],\n",
        "       train_regret_loser_6[slice83],\n",
        "       train_regret_loser_7[slice83],\n",
        "       train_regret_loser_8[slice83],\n",
        "       train_regret_loser_9[slice83],\n",
        "       train_regret_loser_10[slice83],\n",
        "       train_regret_loser_11[slice83],\n",
        "       train_regret_loser_12[slice83],\n",
        "       train_regret_loser_13[slice83],\n",
        "       train_regret_loser_14[slice83],\n",
        "       train_regret_loser_15[slice83],\n",
        "       train_regret_loser_16[slice83],\n",
        "       train_regret_loser_17[slice83],\n",
        "       train_regret_loser_18[slice83],\n",
        "       train_regret_loser_19[slice83],\n",
        "       train_regret_loser_20[slice83]]\n",
        "\n",
        "winner83 = [train_regret_winner_1[slice83],\n",
        "       train_regret_winner_2[slice83],\n",
        "       train_regret_winner_3[slice83],\n",
        "       train_regret_winner_4[slice83],\n",
        "       train_regret_winner_5[slice83],\n",
        "       train_regret_winner_6[slice83],\n",
        "       train_regret_winner_7[slice83],\n",
        "       train_regret_winner_8[slice83],\n",
        "       train_regret_winner_9[slice83],\n",
        "       train_regret_winner_10[slice83],\n",
        "       train_regret_winner_11[slice83],\n",
        "       train_regret_winner_12[slice83],\n",
        "       train_regret_winner_13[slice83],\n",
        "       train_regret_winner_14[slice83],\n",
        "       train_regret_winner_15[slice83],\n",
        "       train_regret_winner_16[slice83],\n",
        "       train_regret_winner_17[slice83],\n",
        "       train_regret_winner_18[slice83],\n",
        "       train_regret_winner_19[slice83],\n",
        "       train_regret_winner_20[slice83]]\n",
        "\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\n",
        "\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\n",
        "\n",
        "slice93 = 92\n",
        "\n",
        "loser93 = [train_regret_loser_1[slice93],\n",
        "       train_regret_loser_2[slice93],\n",
        "       train_regret_loser_3[slice93],\n",
        "       train_regret_loser_4[slice93],\n",
        "       train_regret_loser_5[slice93],\n",
        "       train_regret_loser_6[slice93],\n",
        "       train_regret_loser_7[slice93],\n",
        "       train_regret_loser_8[slice93],\n",
        "       train_regret_loser_9[slice93],\n",
        "       train_regret_loser_10[slice93],\n",
        "       train_regret_loser_11[slice93],\n",
        "       train_regret_loser_12[slice93],\n",
        "       train_regret_loser_13[slice93],\n",
        "       train_regret_loser_14[slice93],\n",
        "       train_regret_loser_15[slice93],\n",
        "       train_regret_loser_16[slice93],\n",
        "       train_regret_loser_17[slice93],\n",
        "       train_regret_loser_18[slice93],\n",
        "       train_regret_loser_19[slice93],\n",
        "       train_regret_loser_20[slice93]]\n",
        "\n",
        "winner93 = [train_regret_winner_1[slice93],\n",
        "       train_regret_winner_2[slice93],\n",
        "       train_regret_winner_3[slice93],\n",
        "       train_regret_winner_4[slice93],\n",
        "       train_regret_winner_5[slice93],\n",
        "       train_regret_winner_6[slice93],\n",
        "       train_regret_winner_7[slice93],\n",
        "       train_regret_winner_8[slice93],\n",
        "       train_regret_winner_9[slice93],\n",
        "       train_regret_winner_10[slice93],\n",
        "       train_regret_winner_11[slice93],\n",
        "       train_regret_winner_12[slice93],\n",
        "       train_regret_winner_13[slice93],\n",
        "       train_regret_winner_14[slice93],\n",
        "       train_regret_winner_15[slice93],\n",
        "       train_regret_winner_16[slice93],\n",
        "       train_regret_winner_17[slice93],\n",
        "       train_regret_winner_18[slice93],\n",
        "       train_regret_winner_19[slice93],\n",
        "       train_regret_winner_20[slice93]]\n",
        "\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\n",
        "\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\n",
        "\n",
        "slice4 = 3\n",
        "\n",
        "loser4 = [train_regret_loser_1[slice4],\n",
        "       train_regret_loser_2[slice4],\n",
        "       train_regret_loser_3[slice4],\n",
        "       train_regret_loser_4[slice4],\n",
        "       train_regret_loser_5[slice4],\n",
        "       train_regret_loser_6[slice4],\n",
        "       train_regret_loser_7[slice4],\n",
        "       train_regret_loser_8[slice4],\n",
        "       train_regret_loser_9[slice4],\n",
        "       train_regret_loser_10[slice4],\n",
        "       train_regret_loser_11[slice4],\n",
        "       train_regret_loser_12[slice4],\n",
        "       train_regret_loser_13[slice4],\n",
        "       train_regret_loser_14[slice4],\n",
        "       train_regret_loser_15[slice4],\n",
        "       train_regret_loser_16[slice4],\n",
        "       train_regret_loser_17[slice4],\n",
        "       train_regret_loser_18[slice4],\n",
        "       train_regret_loser_19[slice4],\n",
        "       train_regret_loser_20[slice4]]\n",
        "\n",
        "winner4 = [train_regret_winner_1[slice4],\n",
        "       train_regret_winner_2[slice4],\n",
        "       train_regret_winner_3[slice4],\n",
        "       train_regret_winner_4[slice4],\n",
        "       train_regret_winner_5[slice4],\n",
        "       train_regret_winner_6[slice4],\n",
        "       train_regret_winner_7[slice4],\n",
        "       train_regret_winner_8[slice4],\n",
        "       train_regret_winner_9[slice4],\n",
        "       train_regret_winner_10[slice4],\n",
        "       train_regret_winner_11[slice4],\n",
        "       train_regret_winner_12[slice4],\n",
        "       train_regret_winner_13[slice4],\n",
        "       train_regret_winner_14[slice4],\n",
        "       train_regret_winner_15[slice4],\n",
        "       train_regret_winner_16[slice4],\n",
        "       train_regret_winner_17[slice4],\n",
        "       train_regret_winner_18[slice4],\n",
        "       train_regret_winner_19[slice4],\n",
        "       train_regret_winner_20[slice4]]\n",
        "\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\n",
        "\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\n",
        "\n",
        "slice14 = 13\n",
        "\n",
        "loser14 = [train_regret_loser_1[slice14],\n",
        "       train_regret_loser_2[slice14],\n",
        "       train_regret_loser_3[slice14],\n",
        "       train_regret_loser_4[slice14],\n",
        "       train_regret_loser_5[slice14],\n",
        "       train_regret_loser_6[slice14],\n",
        "       train_regret_loser_7[slice14],\n",
        "       train_regret_loser_8[slice14],\n",
        "       train_regret_loser_9[slice14],\n",
        "       train_regret_loser_10[slice14],\n",
        "       train_regret_loser_11[slice14],\n",
        "       train_regret_loser_12[slice14],\n",
        "       train_regret_loser_13[slice14],\n",
        "       train_regret_loser_14[slice14],\n",
        "       train_regret_loser_15[slice14],\n",
        "       train_regret_loser_16[slice14],\n",
        "       train_regret_loser_17[slice14],\n",
        "       train_regret_loser_18[slice14],\n",
        "       train_regret_loser_19[slice14],\n",
        "       train_regret_loser_20[slice14]]\n",
        "\n",
        "winner14 = [train_regret_winner_1[slice14],\n",
        "       train_regret_winner_2[slice14],\n",
        "       train_regret_winner_3[slice14],\n",
        "       train_regret_winner_4[slice14],\n",
        "       train_regret_winner_5[slice14],\n",
        "       train_regret_winner_6[slice14],\n",
        "       train_regret_winner_7[slice14],\n",
        "       train_regret_winner_8[slice14],\n",
        "       train_regret_winner_9[slice14],\n",
        "       train_regret_winner_10[slice14],\n",
        "       train_regret_winner_11[slice14],\n",
        "       train_regret_winner_12[slice14],\n",
        "       train_regret_winner_13[slice14],\n",
        "       train_regret_winner_14[slice14],\n",
        "       train_regret_winner_15[slice14],\n",
        "       train_regret_winner_16[slice14],\n",
        "       train_regret_winner_17[slice14],\n",
        "       train_regret_winner_18[slice14],\n",
        "       train_regret_winner_19[slice14],\n",
        "       train_regret_winner_20[slice14]]\n",
        "\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\n",
        "\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\n",
        "\n",
        "slice24 = 23\n",
        "\n",
        "loser24 = [train_regret_loser_1[slice24],\n",
        "       train_regret_loser_2[slice24],\n",
        "       train_regret_loser_3[slice24],\n",
        "       train_regret_loser_4[slice24],\n",
        "       train_regret_loser_5[slice24],\n",
        "       train_regret_loser_6[slice24],\n",
        "       train_regret_loser_7[slice24],\n",
        "       train_regret_loser_8[slice24],\n",
        "       train_regret_loser_9[slice24],\n",
        "       train_regret_loser_10[slice24],\n",
        "       train_regret_loser_11[slice24],\n",
        "       train_regret_loser_12[slice24],\n",
        "       train_regret_loser_13[slice24],\n",
        "       train_regret_loser_14[slice24],\n",
        "       train_regret_loser_15[slice24],\n",
        "       train_regret_loser_16[slice24],\n",
        "       train_regret_loser_17[slice24],\n",
        "       train_regret_loser_18[slice24],\n",
        "       train_regret_loser_19[slice24],\n",
        "       train_regret_loser_20[slice24]]\n",
        "\n",
        "winner24 = [train_regret_winner_1[slice24],\n",
        "       train_regret_winner_2[slice24],\n",
        "       train_regret_winner_3[slice24],\n",
        "       train_regret_winner_4[slice24],\n",
        "       train_regret_winner_5[slice24],\n",
        "       train_regret_winner_6[slice24],\n",
        "       train_regret_winner_7[slice24],\n",
        "       train_regret_winner_8[slice24],\n",
        "       train_regret_winner_9[slice24],\n",
        "       train_regret_winner_10[slice24],\n",
        "       train_regret_winner_11[slice24],\n",
        "       train_regret_winner_12[slice24],\n",
        "       train_regret_winner_13[slice24],\n",
        "       train_regret_winner_14[slice24],\n",
        "       train_regret_winner_15[slice24],\n",
        "       train_regret_winner_16[slice24],\n",
        "       train_regret_winner_17[slice24],\n",
        "       train_regret_winner_18[slice24],\n",
        "       train_regret_winner_19[slice24],\n",
        "       train_regret_winner_20[slice24]]\n",
        "\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\n",
        "\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\n",
        "\n",
        "slice34 = 33\n",
        "\n",
        "loser34 = [train_regret_loser_1[slice34],\n",
        "       train_regret_loser_2[slice34],\n",
        "       train_regret_loser_3[slice34],\n",
        "       train_regret_loser_4[slice34],\n",
        "       train_regret_loser_5[slice34],\n",
        "       train_regret_loser_6[slice34],\n",
        "       train_regret_loser_7[slice34],\n",
        "       train_regret_loser_8[slice34],\n",
        "       train_regret_loser_9[slice34],\n",
        "       train_regret_loser_10[slice34],\n",
        "       train_regret_loser_11[slice34],\n",
        "       train_regret_loser_12[slice34],\n",
        "       train_regret_loser_13[slice34],\n",
        "       train_regret_loser_14[slice34],\n",
        "       train_regret_loser_15[slice34],\n",
        "       train_regret_loser_16[slice34],\n",
        "       train_regret_loser_17[slice34],\n",
        "       train_regret_loser_18[slice34],\n",
        "       train_regret_loser_19[slice34],\n",
        "       train_regret_loser_20[slice34]]\n",
        "\n",
        "winner34 = [train_regret_winner_1[slice34],\n",
        "       train_regret_winner_2[slice34],\n",
        "       train_regret_winner_3[slice34],\n",
        "       train_regret_winner_4[slice34],\n",
        "       train_regret_winner_5[slice34],\n",
        "       train_regret_winner_6[slice34],\n",
        "       train_regret_winner_7[slice34],\n",
        "       train_regret_winner_8[slice34],\n",
        "       train_regret_winner_9[slice34],\n",
        "       train_regret_winner_10[slice34],\n",
        "       train_regret_winner_11[slice34],\n",
        "       train_regret_winner_12[slice34],\n",
        "       train_regret_winner_13[slice34],\n",
        "       train_regret_winner_14[slice34],\n",
        "       train_regret_winner_15[slice34],\n",
        "       train_regret_winner_16[slice34],\n",
        "       train_regret_winner_17[slice34],\n",
        "       train_regret_winner_18[slice34],\n",
        "       train_regret_winner_19[slice34],\n",
        "       train_regret_winner_20[slice34]]\n",
        "\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\n",
        "\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\n",
        "\n",
        "slice44 = 43\n",
        "\n",
        "loser44 = [train_regret_loser_1[slice44],\n",
        "       train_regret_loser_2[slice44],\n",
        "       train_regret_loser_3[slice44],\n",
        "       train_regret_loser_4[slice44],\n",
        "       train_regret_loser_5[slice44],\n",
        "       train_regret_loser_6[slice44],\n",
        "       train_regret_loser_7[slice44],\n",
        "       train_regret_loser_8[slice44],\n",
        "       train_regret_loser_9[slice44],\n",
        "       train_regret_loser_10[slice44],\n",
        "       train_regret_loser_11[slice44],\n",
        "       train_regret_loser_12[slice44],\n",
        "       train_regret_loser_13[slice44],\n",
        "       train_regret_loser_14[slice44],\n",
        "       train_regret_loser_15[slice44],\n",
        "       train_regret_loser_16[slice44],\n",
        "       train_regret_loser_17[slice44],\n",
        "       train_regret_loser_18[slice44],\n",
        "       train_regret_loser_19[slice44],\n",
        "       train_regret_loser_20[slice44]]\n",
        "\n",
        "winner44 = [train_regret_winner_1[slice44],\n",
        "       train_regret_winner_2[slice44],\n",
        "       train_regret_winner_3[slice44],\n",
        "       train_regret_winner_4[slice44],\n",
        "       train_regret_winner_5[slice44],\n",
        "       train_regret_winner_6[slice44],\n",
        "       train_regret_winner_7[slice44],\n",
        "       train_regret_winner_8[slice44],\n",
        "       train_regret_winner_9[slice44],\n",
        "       train_regret_winner_10[slice44],\n",
        "       train_regret_winner_11[slice44],\n",
        "       train_regret_winner_12[slice44],\n",
        "       train_regret_winner_13[slice44],\n",
        "       train_regret_winner_14[slice44],\n",
        "       train_regret_winner_15[slice44],\n",
        "       train_regret_winner_16[slice44],\n",
        "       train_regret_winner_17[slice44],\n",
        "       train_regret_winner_18[slice44],\n",
        "       train_regret_winner_19[slice44],\n",
        "       train_regret_winner_20[slice44]]\n",
        "\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\n",
        "\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\n",
        "\n",
        "slice54 = 53\n",
        "\n",
        "loser54 = [train_regret_loser_1[slice54],\n",
        "       train_regret_loser_2[slice54],\n",
        "       train_regret_loser_3[slice54],\n",
        "       train_regret_loser_4[slice54],\n",
        "       train_regret_loser_5[slice54],\n",
        "       train_regret_loser_6[slice54],\n",
        "       train_regret_loser_7[slice54],\n",
        "       train_regret_loser_8[slice54],\n",
        "       train_regret_loser_9[slice54],\n",
        "       train_regret_loser_10[slice54],\n",
        "       train_regret_loser_11[slice54],\n",
        "       train_regret_loser_12[slice54],\n",
        "       train_regret_loser_13[slice54],\n",
        "       train_regret_loser_14[slice54],\n",
        "       train_regret_loser_15[slice54],\n",
        "       train_regret_loser_16[slice54],\n",
        "       train_regret_loser_17[slice54],\n",
        "       train_regret_loser_18[slice54],\n",
        "       train_regret_loser_19[slice54],\n",
        "       train_regret_loser_20[slice54]]\n",
        "\n",
        "winner54 = [train_regret_winner_1[slice54],\n",
        "       train_regret_winner_2[slice54],\n",
        "       train_regret_winner_3[slice54],\n",
        "       train_regret_winner_4[slice54],\n",
        "       train_regret_winner_5[slice54],\n",
        "       train_regret_winner_6[slice54],\n",
        "       train_regret_winner_7[slice54],\n",
        "       train_regret_winner_8[slice54],\n",
        "       train_regret_winner_9[slice54],\n",
        "       train_regret_winner_10[slice54],\n",
        "       train_regret_winner_11[slice54],\n",
        "       train_regret_winner_12[slice54],\n",
        "       train_regret_winner_13[slice54],\n",
        "       train_regret_winner_14[slice54],\n",
        "       train_regret_winner_15[slice54],\n",
        "       train_regret_winner_16[slice54],\n",
        "       train_regret_winner_17[slice54],\n",
        "       train_regret_winner_18[slice54],\n",
        "       train_regret_winner_19[slice54],\n",
        "       train_regret_winner_20[slice54]]\n",
        "\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\n",
        "\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\n",
        "\n",
        "slice64 = 63\n",
        "\n",
        "loser64 = [train_regret_loser_1[slice64],\n",
        "       train_regret_loser_2[slice64],\n",
        "       train_regret_loser_3[slice64],\n",
        "       train_regret_loser_4[slice64],\n",
        "       train_regret_loser_5[slice64],\n",
        "       train_regret_loser_6[slice64],\n",
        "       train_regret_loser_7[slice64],\n",
        "       train_regret_loser_8[slice64],\n",
        "       train_regret_loser_9[slice64],\n",
        "       train_regret_loser_10[slice64],\n",
        "       train_regret_loser_11[slice64],\n",
        "       train_regret_loser_12[slice64],\n",
        "       train_regret_loser_13[slice64],\n",
        "       train_regret_loser_14[slice64],\n",
        "       train_regret_loser_15[slice64],\n",
        "       train_regret_loser_16[slice64],\n",
        "       train_regret_loser_17[slice64],\n",
        "       train_regret_loser_18[slice64],\n",
        "       train_regret_loser_19[slice64],\n",
        "       train_regret_loser_20[slice64]]\n",
        "\n",
        "winner64 = [train_regret_winner_1[slice64],\n",
        "       train_regret_winner_2[slice64],\n",
        "       train_regret_winner_3[slice64],\n",
        "       train_regret_winner_4[slice64],\n",
        "       train_regret_winner_5[slice64],\n",
        "       train_regret_winner_6[slice64],\n",
        "       train_regret_winner_7[slice64],\n",
        "       train_regret_winner_8[slice64],\n",
        "       train_regret_winner_9[slice64],\n",
        "       train_regret_winner_10[slice64],\n",
        "       train_regret_winner_11[slice64],\n",
        "       train_regret_winner_12[slice64],\n",
        "       train_regret_winner_13[slice64],\n",
        "       train_regret_winner_14[slice64],\n",
        "       train_regret_winner_15[slice64],\n",
        "       train_regret_winner_16[slice64],\n",
        "       train_regret_winner_17[slice64],\n",
        "       train_regret_winner_18[slice64],\n",
        "       train_regret_winner_19[slice64],\n",
        "       train_regret_winner_20[slice64]]\n",
        "\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\n",
        "\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\n",
        "\n",
        "slice74 = 73\n",
        "\n",
        "loser74 = [train_regret_loser_1[slice74],\n",
        "       train_regret_loser_2[slice74],\n",
        "       train_regret_loser_3[slice74],\n",
        "       train_regret_loser_4[slice74],\n",
        "       train_regret_loser_5[slice74],\n",
        "       train_regret_loser_6[slice74],\n",
        "       train_regret_loser_7[slice74],\n",
        "       train_regret_loser_8[slice74],\n",
        "       train_regret_loser_9[slice74],\n",
        "       train_regret_loser_10[slice74],\n",
        "       train_regret_loser_11[slice74],\n",
        "       train_regret_loser_12[slice74],\n",
        "       train_regret_loser_13[slice74],\n",
        "       train_regret_loser_14[slice74],\n",
        "       train_regret_loser_15[slice74],\n",
        "       train_regret_loser_16[slice74],\n",
        "       train_regret_loser_17[slice74],\n",
        "       train_regret_loser_18[slice74],\n",
        "       train_regret_loser_19[slice74],\n",
        "       train_regret_loser_20[slice74]]\n",
        "\n",
        "winner74 = [train_regret_winner_1[slice74],\n",
        "       train_regret_winner_2[slice74],\n",
        "       train_regret_winner_3[slice74],\n",
        "       train_regret_winner_4[slice74],\n",
        "       train_regret_winner_5[slice74],\n",
        "       train_regret_winner_6[slice74],\n",
        "       train_regret_winner_7[slice74],\n",
        "       train_regret_winner_8[slice74],\n",
        "       train_regret_winner_9[slice74],\n",
        "       train_regret_winner_10[slice74],\n",
        "       train_regret_winner_11[slice74],\n",
        "       train_regret_winner_12[slice74],\n",
        "       train_regret_winner_13[slice74],\n",
        "       train_regret_winner_14[slice74],\n",
        "       train_regret_winner_15[slice74],\n",
        "       train_regret_winner_16[slice74],\n",
        "       train_regret_winner_17[slice74],\n",
        "       train_regret_winner_18[slice74],\n",
        "       train_regret_winner_19[slice74],\n",
        "       train_regret_winner_20[slice74]]\n",
        "\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\n",
        "\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\n",
        "\n",
        "slice84 = 83\n",
        "\n",
        "loser84 = [train_regret_loser_1[slice84],\n",
        "       train_regret_loser_2[slice84],\n",
        "       train_regret_loser_3[slice84],\n",
        "       train_regret_loser_4[slice84],\n",
        "       train_regret_loser_5[slice84],\n",
        "       train_regret_loser_6[slice84],\n",
        "       train_regret_loser_7[slice84],\n",
        "       train_regret_loser_8[slice84],\n",
        "       train_regret_loser_9[slice84],\n",
        "       train_regret_loser_10[slice84],\n",
        "       train_regret_loser_11[slice84],\n",
        "       train_regret_loser_12[slice84],\n",
        "       train_regret_loser_13[slice84],\n",
        "       train_regret_loser_14[slice84],\n",
        "       train_regret_loser_15[slice84],\n",
        "       train_regret_loser_16[slice84],\n",
        "       train_regret_loser_17[slice84],\n",
        "       train_regret_loser_18[slice84],\n",
        "       train_regret_loser_19[slice84],\n",
        "       train_regret_loser_20[slice84]]\n",
        "\n",
        "winner84 = [train_regret_winner_1[slice84],\n",
        "       train_regret_winner_2[slice84],\n",
        "       train_regret_winner_3[slice84],\n",
        "       train_regret_winner_4[slice84],\n",
        "       train_regret_winner_5[slice84],\n",
        "       train_regret_winner_6[slice84],\n",
        "       train_regret_winner_7[slice84],\n",
        "       train_regret_winner_8[slice84],\n",
        "       train_regret_winner_9[slice84],\n",
        "       train_regret_winner_10[slice84],\n",
        "       train_regret_winner_11[slice84],\n",
        "       train_regret_winner_12[slice84],\n",
        "       train_regret_winner_13[slice84],\n",
        "       train_regret_winner_14[slice84],\n",
        "       train_regret_winner_15[slice84],\n",
        "       train_regret_winner_16[slice84],\n",
        "       train_regret_winner_17[slice84],\n",
        "       train_regret_winner_18[slice84],\n",
        "       train_regret_winner_19[slice84],\n",
        "       train_regret_winner_20[slice84]]\n",
        "\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\n",
        "\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\n",
        "\n",
        "slice94 = 93\n",
        "\n",
        "loser94 = [train_regret_loser_1[slice94],\n",
        "       train_regret_loser_2[slice94],\n",
        "       train_regret_loser_3[slice94],\n",
        "       train_regret_loser_4[slice94],\n",
        "       train_regret_loser_5[slice94],\n",
        "       train_regret_loser_6[slice94],\n",
        "       train_regret_loser_7[slice94],\n",
        "       train_regret_loser_8[slice94],\n",
        "       train_regret_loser_9[slice94],\n",
        "       train_regret_loser_10[slice94],\n",
        "       train_regret_loser_11[slice94],\n",
        "       train_regret_loser_12[slice94],\n",
        "       train_regret_loser_13[slice94],\n",
        "       train_regret_loser_14[slice94],\n",
        "       train_regret_loser_15[slice94],\n",
        "       train_regret_loser_16[slice94],\n",
        "       train_regret_loser_17[slice94],\n",
        "       train_regret_loser_18[slice94],\n",
        "       train_regret_loser_19[slice94],\n",
        "       train_regret_loser_20[slice94]]\n",
        "\n",
        "winner94 = [train_regret_winner_1[slice94],\n",
        "       train_regret_winner_2[slice94],\n",
        "       train_regret_winner_3[slice94],\n",
        "       train_regret_winner_4[slice94],\n",
        "       train_regret_winner_5[slice94],\n",
        "       train_regret_winner_6[slice94],\n",
        "       train_regret_winner_7[slice94],\n",
        "       train_regret_winner_8[slice94],\n",
        "       train_regret_winner_9[slice94],\n",
        "       train_regret_winner_10[slice94],\n",
        "       train_regret_winner_11[slice94],\n",
        "       train_regret_winner_12[slice94],\n",
        "       train_regret_winner_13[slice94],\n",
        "       train_regret_winner_14[slice94],\n",
        "       train_regret_winner_15[slice94],\n",
        "       train_regret_winner_16[slice94],\n",
        "       train_regret_winner_17[slice94],\n",
        "       train_regret_winner_18[slice94],\n",
        "       train_regret_winner_19[slice94],\n",
        "       train_regret_winner_20[slice94]]\n",
        "\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\n",
        "\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\n",
        "\n",
        "slice5 = 4\n",
        "\n",
        "loser5 = [train_regret_loser_1[slice5],\n",
        "       train_regret_loser_2[slice5],\n",
        "       train_regret_loser_3[slice5],\n",
        "       train_regret_loser_4[slice5],\n",
        "       train_regret_loser_5[slice5],\n",
        "       train_regret_loser_6[slice5],\n",
        "       train_regret_loser_7[slice5],\n",
        "       train_regret_loser_8[slice5],\n",
        "       train_regret_loser_9[slice5],\n",
        "       train_regret_loser_10[slice5],\n",
        "       train_regret_loser_11[slice5],\n",
        "       train_regret_loser_12[slice5],\n",
        "       train_regret_loser_13[slice5],\n",
        "       train_regret_loser_14[slice5],\n",
        "       train_regret_loser_15[slice5],\n",
        "       train_regret_loser_16[slice5],\n",
        "       train_regret_loser_17[slice5],\n",
        "       train_regret_loser_18[slice5],\n",
        "       train_regret_loser_19[slice5],\n",
        "       train_regret_loser_20[slice5]]\n",
        "\n",
        "winner5 = [train_regret_winner_1[slice5],\n",
        "       train_regret_winner_2[slice5],\n",
        "       train_regret_winner_3[slice5],\n",
        "       train_regret_winner_4[slice5],\n",
        "       train_regret_winner_5[slice5],\n",
        "       train_regret_winner_6[slice5],\n",
        "       train_regret_winner_7[slice5],\n",
        "       train_regret_winner_8[slice5],\n",
        "       train_regret_winner_9[slice5],\n",
        "       train_regret_winner_10[slice5],\n",
        "       train_regret_winner_11[slice5],\n",
        "       train_regret_winner_12[slice5],\n",
        "       train_regret_winner_13[slice5],\n",
        "       train_regret_winner_14[slice5],\n",
        "       train_regret_winner_15[slice5],\n",
        "       train_regret_winner_16[slice5],\n",
        "       train_regret_winner_17[slice5],\n",
        "       train_regret_winner_18[slice5],\n",
        "       train_regret_winner_19[slice5],\n",
        "       train_regret_winner_20[slice5]]\n",
        "\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\n",
        "\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\n",
        "\n",
        "slice15 = 14\n",
        "\n",
        "loser15 = [train_regret_loser_1[slice15],\n",
        "       train_regret_loser_2[slice15],\n",
        "       train_regret_loser_3[slice15],\n",
        "       train_regret_loser_4[slice15],\n",
        "       train_regret_loser_5[slice15],\n",
        "       train_regret_loser_6[slice15],\n",
        "       train_regret_loser_7[slice15],\n",
        "       train_regret_loser_8[slice15],\n",
        "       train_regret_loser_9[slice15],\n",
        "       train_regret_loser_10[slice15],\n",
        "       train_regret_loser_11[slice15],\n",
        "       train_regret_loser_12[slice15],\n",
        "       train_regret_loser_13[slice15],\n",
        "       train_regret_loser_14[slice15],\n",
        "       train_regret_loser_15[slice15],\n",
        "       train_regret_loser_16[slice15],\n",
        "       train_regret_loser_17[slice15],\n",
        "       train_regret_loser_18[slice15],\n",
        "       train_regret_loser_19[slice15],\n",
        "       train_regret_loser_20[slice15]]\n",
        "\n",
        "winner15 = [train_regret_winner_1[slice15],\n",
        "       train_regret_winner_2[slice15],\n",
        "       train_regret_winner_3[slice15],\n",
        "       train_regret_winner_4[slice15],\n",
        "       train_regret_winner_5[slice15],\n",
        "       train_regret_winner_6[slice15],\n",
        "       train_regret_winner_7[slice15],\n",
        "       train_regret_winner_8[slice15],\n",
        "       train_regret_winner_9[slice15],\n",
        "       train_regret_winner_10[slice15],\n",
        "       train_regret_winner_11[slice15],\n",
        "       train_regret_winner_12[slice15],\n",
        "       train_regret_winner_13[slice15],\n",
        "       train_regret_winner_14[slice15],\n",
        "       train_regret_winner_15[slice15],\n",
        "       train_regret_winner_16[slice15],\n",
        "       train_regret_winner_17[slice15],\n",
        "       train_regret_winner_18[slice15],\n",
        "       train_regret_winner_19[slice15],\n",
        "       train_regret_winner_20[slice15]]\n",
        "\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\n",
        "\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\n",
        "\n",
        "slice25 = 24\n",
        "\n",
        "loser25 = [train_regret_loser_1[slice25],\n",
        "       train_regret_loser_2[slice25],\n",
        "       train_regret_loser_3[slice25],\n",
        "       train_regret_loser_4[slice25],\n",
        "       train_regret_loser_5[slice25],\n",
        "       train_regret_loser_6[slice25],\n",
        "       train_regret_loser_7[slice25],\n",
        "       train_regret_loser_8[slice25],\n",
        "       train_regret_loser_9[slice25],\n",
        "       train_regret_loser_10[slice25],\n",
        "       train_regret_loser_11[slice25],\n",
        "       train_regret_loser_12[slice25],\n",
        "       train_regret_loser_13[slice25],\n",
        "       train_regret_loser_14[slice25],\n",
        "       train_regret_loser_15[slice25],\n",
        "       train_regret_loser_16[slice25],\n",
        "       train_regret_loser_17[slice25],\n",
        "       train_regret_loser_18[slice25],\n",
        "       train_regret_loser_19[slice25],\n",
        "       train_regret_loser_20[slice25]]\n",
        "\n",
        "winner25 = [train_regret_winner_1[slice25],\n",
        "       train_regret_winner_2[slice25],\n",
        "       train_regret_winner_3[slice25],\n",
        "       train_regret_winner_4[slice25],\n",
        "       train_regret_winner_5[slice25],\n",
        "       train_regret_winner_6[slice25],\n",
        "       train_regret_winner_7[slice25],\n",
        "       train_regret_winner_8[slice25],\n",
        "       train_regret_winner_9[slice25],\n",
        "       train_regret_winner_10[slice25],\n",
        "       train_regret_winner_11[slice25],\n",
        "       train_regret_winner_12[slice25],\n",
        "       train_regret_winner_13[slice25],\n",
        "       train_regret_winner_14[slice25],\n",
        "       train_regret_winner_15[slice25],\n",
        "       train_regret_winner_16[slice25],\n",
        "       train_regret_winner_17[slice25],\n",
        "       train_regret_winner_18[slice25],\n",
        "       train_regret_winner_19[slice25],\n",
        "       train_regret_winner_20[slice25]]\n",
        "\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\n",
        "\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\n",
        "\n",
        "slice35 = 34\n",
        "\n",
        "loser35 = [train_regret_loser_1[slice35],\n",
        "       train_regret_loser_2[slice35],\n",
        "       train_regret_loser_3[slice35],\n",
        "       train_regret_loser_4[slice35],\n",
        "       train_regret_loser_5[slice35],\n",
        "       train_regret_loser_6[slice35],\n",
        "       train_regret_loser_7[slice35],\n",
        "       train_regret_loser_8[slice35],\n",
        "       train_regret_loser_9[slice35],\n",
        "       train_regret_loser_10[slice35],\n",
        "       train_regret_loser_11[slice35],\n",
        "       train_regret_loser_12[slice35],\n",
        "       train_regret_loser_13[slice35],\n",
        "       train_regret_loser_14[slice35],\n",
        "       train_regret_loser_15[slice35],\n",
        "       train_regret_loser_16[slice35],\n",
        "       train_regret_loser_17[slice35],\n",
        "       train_regret_loser_18[slice35],\n",
        "       train_regret_loser_19[slice35],\n",
        "       train_regret_loser_20[slice35]]\n",
        "\n",
        "winner35 = [train_regret_winner_1[slice35],\n",
        "       train_regret_winner_2[slice35],\n",
        "       train_regret_winner_3[slice35],\n",
        "       train_regret_winner_4[slice35],\n",
        "       train_regret_winner_5[slice35],\n",
        "       train_regret_winner_6[slice35],\n",
        "       train_regret_winner_7[slice35],\n",
        "       train_regret_winner_8[slice35],\n",
        "       train_regret_winner_9[slice35],\n",
        "       train_regret_winner_10[slice35],\n",
        "       train_regret_winner_11[slice35],\n",
        "       train_regret_winner_12[slice35],\n",
        "       train_regret_winner_13[slice35],\n",
        "       train_regret_winner_14[slice35],\n",
        "       train_regret_winner_15[slice35],\n",
        "       train_regret_winner_16[slice35],\n",
        "       train_regret_winner_17[slice35],\n",
        "       train_regret_winner_18[slice35],\n",
        "       train_regret_winner_19[slice35],\n",
        "       train_regret_winner_20[slice35]]\n",
        "\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\n",
        "\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\n",
        "\n",
        "slice45 = 44\n",
        "\n",
        "loser45 = [train_regret_loser_1[slice45],\n",
        "       train_regret_loser_2[slice45],\n",
        "       train_regret_loser_3[slice45],\n",
        "       train_regret_loser_4[slice45],\n",
        "       train_regret_loser_5[slice45],\n",
        "       train_regret_loser_6[slice45],\n",
        "       train_regret_loser_7[slice45],\n",
        "       train_regret_loser_8[slice45],\n",
        "       train_regret_loser_9[slice45],\n",
        "       train_regret_loser_10[slice45],\n",
        "       train_regret_loser_11[slice45],\n",
        "       train_regret_loser_12[slice45],\n",
        "       train_regret_loser_13[slice45],\n",
        "       train_regret_loser_14[slice45],\n",
        "       train_regret_loser_15[slice45],\n",
        "       train_regret_loser_16[slice45],\n",
        "       train_regret_loser_17[slice45],\n",
        "       train_regret_loser_18[slice45],\n",
        "       train_regret_loser_19[slice45],\n",
        "       train_regret_loser_20[slice45]]\n",
        "\n",
        "winner45 = [train_regret_winner_1[slice45],\n",
        "       train_regret_winner_2[slice45],\n",
        "       train_regret_winner_3[slice45],\n",
        "       train_regret_winner_4[slice45],\n",
        "       train_regret_winner_5[slice45],\n",
        "       train_regret_winner_6[slice45],\n",
        "       train_regret_winner_7[slice45],\n",
        "       train_regret_winner_8[slice45],\n",
        "       train_regret_winner_9[slice45],\n",
        "       train_regret_winner_10[slice45],\n",
        "       train_regret_winner_11[slice45],\n",
        "       train_regret_winner_12[slice45],\n",
        "       train_regret_winner_13[slice45],\n",
        "       train_regret_winner_14[slice45],\n",
        "       train_regret_winner_15[slice45],\n",
        "       train_regret_winner_16[slice45],\n",
        "       train_regret_winner_17[slice45],\n",
        "       train_regret_winner_18[slice45],\n",
        "       train_regret_winner_19[slice45],\n",
        "       train_regret_winner_20[slice45]]\n",
        "\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\n",
        "\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\n",
        "\n",
        "slice55 = 54\n",
        "\n",
        "loser55 = [train_regret_loser_1[slice55],\n",
        "       train_regret_loser_2[slice55],\n",
        "       train_regret_loser_3[slice55],\n",
        "       train_regret_loser_4[slice55],\n",
        "       train_regret_loser_5[slice55],\n",
        "       train_regret_loser_6[slice55],\n",
        "       train_regret_loser_7[slice55],\n",
        "       train_regret_loser_8[slice55],\n",
        "       train_regret_loser_9[slice55],\n",
        "       train_regret_loser_10[slice55],\n",
        "       train_regret_loser_11[slice55],\n",
        "       train_regret_loser_12[slice55],\n",
        "       train_regret_loser_13[slice55],\n",
        "       train_regret_loser_14[slice55],\n",
        "       train_regret_loser_15[slice55],\n",
        "       train_regret_loser_16[slice55],\n",
        "       train_regret_loser_17[slice55],\n",
        "       train_regret_loser_18[slice55],\n",
        "       train_regret_loser_19[slice55],\n",
        "       train_regret_loser_20[slice55]]\n",
        "\n",
        "winner55 = [train_regret_winner_1[slice55],\n",
        "       train_regret_winner_2[slice55],\n",
        "       train_regret_winner_3[slice55],\n",
        "       train_regret_winner_4[slice55],\n",
        "       train_regret_winner_5[slice55],\n",
        "       train_regret_winner_6[slice55],\n",
        "       train_regret_winner_7[slice55],\n",
        "       train_regret_winner_8[slice55],\n",
        "       train_regret_winner_9[slice55],\n",
        "       train_regret_winner_10[slice55],\n",
        "       train_regret_winner_11[slice55],\n",
        "       train_regret_winner_12[slice55],\n",
        "       train_regret_winner_13[slice55],\n",
        "       train_regret_winner_14[slice55],\n",
        "       train_regret_winner_15[slice55],\n",
        "       train_regret_winner_16[slice55],\n",
        "       train_regret_winner_17[slice55],\n",
        "       train_regret_winner_18[slice55],\n",
        "       train_regret_winner_19[slice55],\n",
        "       train_regret_winner_20[slice55]]\n",
        "\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\n",
        "\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\n",
        "\n",
        "slice65 = 64\n",
        "\n",
        "loser65 = [train_regret_loser_1[slice65],\n",
        "       train_regret_loser_2[slice65],\n",
        "       train_regret_loser_3[slice65],\n",
        "       train_regret_loser_4[slice65],\n",
        "       train_regret_loser_5[slice65],\n",
        "       train_regret_loser_6[slice65],\n",
        "       train_regret_loser_7[slice65],\n",
        "       train_regret_loser_8[slice65],\n",
        "       train_regret_loser_9[slice65],\n",
        "       train_regret_loser_10[slice65],\n",
        "       train_regret_loser_11[slice65],\n",
        "       train_regret_loser_12[slice65],\n",
        "       train_regret_loser_13[slice65],\n",
        "       train_regret_loser_14[slice65],\n",
        "       train_regret_loser_15[slice65],\n",
        "       train_regret_loser_16[slice65],\n",
        "       train_regret_loser_17[slice65],\n",
        "       train_regret_loser_18[slice65],\n",
        "       train_regret_loser_19[slice65],\n",
        "       train_regret_loser_20[slice65]]\n",
        "\n",
        "winner65 = [train_regret_winner_1[slice65],\n",
        "       train_regret_winner_2[slice65],\n",
        "       train_regret_winner_3[slice65],\n",
        "       train_regret_winner_4[slice65],\n",
        "       train_regret_winner_5[slice65],\n",
        "       train_regret_winner_6[slice65],\n",
        "       train_regret_winner_7[slice65],\n",
        "       train_regret_winner_8[slice65],\n",
        "       train_regret_winner_9[slice65],\n",
        "       train_regret_winner_10[slice65],\n",
        "       train_regret_winner_11[slice65],\n",
        "       train_regret_winner_12[slice65],\n",
        "       train_regret_winner_13[slice65],\n",
        "       train_regret_winner_14[slice65],\n",
        "       train_regret_winner_15[slice65],\n",
        "       train_regret_winner_16[slice65],\n",
        "       train_regret_winner_17[slice65],\n",
        "       train_regret_winner_18[slice65],\n",
        "       train_regret_winner_19[slice65],\n",
        "       train_regret_winner_20[slice65]]\n",
        "\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\n",
        "\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\n",
        "\n",
        "slice75 = 74\n",
        "\n",
        "loser75 = [train_regret_loser_1[slice75],\n",
        "       train_regret_loser_2[slice75],\n",
        "       train_regret_loser_3[slice75],\n",
        "       train_regret_loser_4[slice75],\n",
        "       train_regret_loser_5[slice75],\n",
        "       train_regret_loser_6[slice75],\n",
        "       train_regret_loser_7[slice75],\n",
        "       train_regret_loser_8[slice75],\n",
        "       train_regret_loser_9[slice75],\n",
        "       train_regret_loser_10[slice75],\n",
        "       train_regret_loser_11[slice75],\n",
        "       train_regret_loser_12[slice75],\n",
        "       train_regret_loser_13[slice75],\n",
        "       train_regret_loser_14[slice75],\n",
        "       train_regret_loser_15[slice75],\n",
        "       train_regret_loser_16[slice75],\n",
        "       train_regret_loser_17[slice75],\n",
        "       train_regret_loser_18[slice75],\n",
        "       train_regret_loser_19[slice75],\n",
        "       train_regret_loser_20[slice75]]\n",
        "\n",
        "winner75 = [train_regret_winner_1[slice75],\n",
        "       train_regret_winner_2[slice75],\n",
        "       train_regret_winner_3[slice75],\n",
        "       train_regret_winner_4[slice75],\n",
        "       train_regret_winner_5[slice75],\n",
        "       train_regret_winner_6[slice75],\n",
        "       train_regret_winner_7[slice75],\n",
        "       train_regret_winner_8[slice75],\n",
        "       train_regret_winner_9[slice75],\n",
        "       train_regret_winner_10[slice75],\n",
        "       train_regret_winner_11[slice75],\n",
        "       train_regret_winner_12[slice75],\n",
        "       train_regret_winner_13[slice75],\n",
        "       train_regret_winner_14[slice75],\n",
        "       train_regret_winner_15[slice75],\n",
        "       train_regret_winner_16[slice75],\n",
        "       train_regret_winner_17[slice75],\n",
        "       train_regret_winner_18[slice75],\n",
        "       train_regret_winner_19[slice75],\n",
        "       train_regret_winner_20[slice75]]\n",
        "\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\n",
        "\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\n",
        "\n",
        "slice85 = 84\n",
        "\n",
        "loser85 = [train_regret_loser_1[slice85],\n",
        "       train_regret_loser_2[slice85],\n",
        "       train_regret_loser_3[slice85],\n",
        "       train_regret_loser_4[slice85],\n",
        "       train_regret_loser_5[slice85],\n",
        "       train_regret_loser_6[slice85],\n",
        "       train_regret_loser_7[slice85],\n",
        "       train_regret_loser_8[slice85],\n",
        "       train_regret_loser_9[slice85],\n",
        "       train_regret_loser_10[slice85],\n",
        "       train_regret_loser_11[slice85],\n",
        "       train_regret_loser_12[slice85],\n",
        "       train_regret_loser_13[slice85],\n",
        "       train_regret_loser_14[slice85],\n",
        "       train_regret_loser_15[slice85],\n",
        "       train_regret_loser_16[slice85],\n",
        "       train_regret_loser_17[slice85],\n",
        "       train_regret_loser_18[slice85],\n",
        "       train_regret_loser_19[slice85],\n",
        "       train_regret_loser_20[slice85]]\n",
        "\n",
        "winner85 = [train_regret_winner_1[slice85],\n",
        "       train_regret_winner_2[slice85],\n",
        "       train_regret_winner_3[slice85],\n",
        "       train_regret_winner_4[slice85],\n",
        "       train_regret_winner_5[slice85],\n",
        "       train_regret_winner_6[slice85],\n",
        "       train_regret_winner_7[slice85],\n",
        "       train_regret_winner_8[slice85],\n",
        "       train_regret_winner_9[slice85],\n",
        "       train_regret_winner_10[slice85],\n",
        "       train_regret_winner_11[slice85],\n",
        "       train_regret_winner_12[slice85],\n",
        "       train_regret_winner_13[slice85],\n",
        "       train_regret_winner_14[slice85],\n",
        "       train_regret_winner_15[slice85],\n",
        "       train_regret_winner_16[slice85],\n",
        "       train_regret_winner_17[slice85],\n",
        "       train_regret_winner_18[slice85],\n",
        "       train_regret_winner_19[slice85],\n",
        "       train_regret_winner_20[slice85]]\n",
        "\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\n",
        "\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\n",
        "\n",
        "slice95 = 94\n",
        "\n",
        "loser95 = [train_regret_loser_1[slice95],\n",
        "       train_regret_loser_2[slice95],\n",
        "       train_regret_loser_3[slice95],\n",
        "       train_regret_loser_4[slice95],\n",
        "       train_regret_loser_5[slice95],\n",
        "       train_regret_loser_6[slice95],\n",
        "       train_regret_loser_7[slice95],\n",
        "       train_regret_loser_8[slice95],\n",
        "       train_regret_loser_9[slice95],\n",
        "       train_regret_loser_10[slice95],\n",
        "       train_regret_loser_11[slice95],\n",
        "       train_regret_loser_12[slice95],\n",
        "       train_regret_loser_13[slice95],\n",
        "       train_regret_loser_14[slice95],\n",
        "       train_regret_loser_15[slice95],\n",
        "       train_regret_loser_16[slice95],\n",
        "       train_regret_loser_17[slice95],\n",
        "       train_regret_loser_18[slice95],\n",
        "       train_regret_loser_19[slice95],\n",
        "       train_regret_loser_20[slice95]]\n",
        "\n",
        "winner95 = [train_regret_winner_1[slice95],\n",
        "       train_regret_winner_2[slice95],\n",
        "       train_regret_winner_3[slice95],\n",
        "       train_regret_winner_4[slice95],\n",
        "       train_regret_winner_5[slice95],\n",
        "       train_regret_winner_6[slice95],\n",
        "       train_regret_winner_7[slice95],\n",
        "       train_regret_winner_8[slice95],\n",
        "       train_regret_winner_9[slice95],\n",
        "       train_regret_winner_10[slice95],\n",
        "       train_regret_winner_11[slice95],\n",
        "       train_regret_winner_12[slice95],\n",
        "       train_regret_winner_13[slice95],\n",
        "       train_regret_winner_14[slice95],\n",
        "       train_regret_winner_15[slice95],\n",
        "       train_regret_winner_16[slice95],\n",
        "       train_regret_winner_17[slice95],\n",
        "       train_regret_winner_18[slice95],\n",
        "       train_regret_winner_19[slice95],\n",
        "       train_regret_winner_20[slice95]]\n",
        "\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\n",
        "\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\n",
        "\n",
        "slice6 = 5\n",
        "\n",
        "loser6 = [train_regret_loser_1[slice6],\n",
        "       train_regret_loser_2[slice6],\n",
        "       train_regret_loser_3[slice6],\n",
        "       train_regret_loser_4[slice6],\n",
        "       train_regret_loser_5[slice6],\n",
        "       train_regret_loser_6[slice6],\n",
        "       train_regret_loser_7[slice6],\n",
        "       train_regret_loser_8[slice6],\n",
        "       train_regret_loser_9[slice6],\n",
        "       train_regret_loser_10[slice6],\n",
        "       train_regret_loser_11[slice6],\n",
        "       train_regret_loser_12[slice6],\n",
        "       train_regret_loser_13[slice6],\n",
        "       train_regret_loser_14[slice6],\n",
        "       train_regret_loser_15[slice6],\n",
        "       train_regret_loser_16[slice6],\n",
        "       train_regret_loser_17[slice6],\n",
        "       train_regret_loser_18[slice6],\n",
        "       train_regret_loser_19[slice6],\n",
        "       train_regret_loser_20[slice6]]\n",
        "\n",
        "winner6 = [train_regret_winner_1[slice6],\n",
        "       train_regret_winner_2[slice6],\n",
        "       train_regret_winner_3[slice6],\n",
        "       train_regret_winner_4[slice6],\n",
        "       train_regret_winner_5[slice6],\n",
        "       train_regret_winner_6[slice6],\n",
        "       train_regret_winner_7[slice6],\n",
        "       train_regret_winner_8[slice6],\n",
        "       train_regret_winner_9[slice6],\n",
        "       train_regret_winner_10[slice6],\n",
        "       train_regret_winner_11[slice6],\n",
        "       train_regret_winner_12[slice6],\n",
        "       train_regret_winner_13[slice6],\n",
        "       train_regret_winner_14[slice6],\n",
        "       train_regret_winner_15[slice6],\n",
        "       train_regret_winner_16[slice6],\n",
        "       train_regret_winner_17[slice6],\n",
        "       train_regret_winner_18[slice6],\n",
        "       train_regret_winner_19[slice6],\n",
        "       train_regret_winner_20[slice6]]\n",
        "\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\n",
        "\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\n",
        "\n",
        "slice16 = 15\n",
        "\n",
        "loser16 = [train_regret_loser_1[slice16],\n",
        "       train_regret_loser_2[slice16],\n",
        "       train_regret_loser_3[slice16],\n",
        "       train_regret_loser_4[slice16],\n",
        "       train_regret_loser_5[slice16],\n",
        "       train_regret_loser_6[slice16],\n",
        "       train_regret_loser_7[slice16],\n",
        "       train_regret_loser_8[slice16],\n",
        "       train_regret_loser_9[slice16],\n",
        "       train_regret_loser_10[slice16],\n",
        "       train_regret_loser_11[slice16],\n",
        "       train_regret_loser_12[slice16],\n",
        "       train_regret_loser_13[slice16],\n",
        "       train_regret_loser_14[slice16],\n",
        "       train_regret_loser_15[slice16],\n",
        "       train_regret_loser_16[slice16],\n",
        "       train_regret_loser_17[slice16],\n",
        "       train_regret_loser_18[slice16],\n",
        "       train_regret_loser_19[slice16],\n",
        "       train_regret_loser_20[slice16]]\n",
        "\n",
        "winner16 = [train_regret_winner_1[slice16],\n",
        "       train_regret_winner_2[slice16],\n",
        "       train_regret_winner_3[slice16],\n",
        "       train_regret_winner_4[slice16],\n",
        "       train_regret_winner_5[slice16],\n",
        "       train_regret_winner_6[slice16],\n",
        "       train_regret_winner_7[slice16],\n",
        "       train_regret_winner_8[slice16],\n",
        "       train_regret_winner_9[slice16],\n",
        "       train_regret_winner_10[slice16],\n",
        "       train_regret_winner_11[slice16],\n",
        "       train_regret_winner_12[slice16],\n",
        "       train_regret_winner_13[slice16],\n",
        "       train_regret_winner_14[slice16],\n",
        "       train_regret_winner_15[slice16],\n",
        "       train_regret_winner_16[slice16],\n",
        "       train_regret_winner_17[slice16],\n",
        "       train_regret_winner_18[slice16],\n",
        "       train_regret_winner_19[slice16],\n",
        "       train_regret_winner_20[slice16]]\n",
        "\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\n",
        "\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\n",
        "\n",
        "slice26 = 25\n",
        "\n",
        "loser26 = [train_regret_loser_1[slice26],\n",
        "       train_regret_loser_2[slice26],\n",
        "       train_regret_loser_3[slice26],\n",
        "       train_regret_loser_4[slice26],\n",
        "       train_regret_loser_5[slice26],\n",
        "       train_regret_loser_6[slice26],\n",
        "       train_regret_loser_7[slice26],\n",
        "       train_regret_loser_8[slice26],\n",
        "       train_regret_loser_9[slice26],\n",
        "       train_regret_loser_10[slice26],\n",
        "       train_regret_loser_11[slice26],\n",
        "       train_regret_loser_12[slice26],\n",
        "       train_regret_loser_13[slice26],\n",
        "       train_regret_loser_14[slice26],\n",
        "       train_regret_loser_15[slice26],\n",
        "       train_regret_loser_16[slice26],\n",
        "       train_regret_loser_17[slice26],\n",
        "       train_regret_loser_18[slice26],\n",
        "       train_regret_loser_19[slice26],\n",
        "       train_regret_loser_20[slice26]]\n",
        "\n",
        "winner26 = [train_regret_winner_1[slice26],\n",
        "       train_regret_winner_2[slice26],\n",
        "       train_regret_winner_3[slice26],\n",
        "       train_regret_winner_4[slice26],\n",
        "       train_regret_winner_5[slice26],\n",
        "       train_regret_winner_6[slice26],\n",
        "       train_regret_winner_7[slice26],\n",
        "       train_regret_winner_8[slice26],\n",
        "       train_regret_winner_9[slice26],\n",
        "       train_regret_winner_10[slice26],\n",
        "       train_regret_winner_11[slice26],\n",
        "       train_regret_winner_12[slice26],\n",
        "       train_regret_winner_13[slice26],\n",
        "       train_regret_winner_14[slice26],\n",
        "       train_regret_winner_15[slice26],\n",
        "       train_regret_winner_16[slice26],\n",
        "       train_regret_winner_17[slice26],\n",
        "       train_regret_winner_18[slice26],\n",
        "       train_regret_winner_19[slice26],\n",
        "       train_regret_winner_20[slice26]]\n",
        "\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\n",
        "\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\n",
        "\n",
        "slice36 = 35\n",
        "\n",
        "loser36 = [train_regret_loser_1[slice36],\n",
        "       train_regret_loser_2[slice36],\n",
        "       train_regret_loser_3[slice36],\n",
        "       train_regret_loser_4[slice36],\n",
        "       train_regret_loser_5[slice36],\n",
        "       train_regret_loser_6[slice36],\n",
        "       train_regret_loser_7[slice36],\n",
        "       train_regret_loser_8[slice36],\n",
        "       train_regret_loser_9[slice36],\n",
        "       train_regret_loser_10[slice36],\n",
        "       train_regret_loser_11[slice36],\n",
        "       train_regret_loser_12[slice36],\n",
        "       train_regret_loser_13[slice36],\n",
        "       train_regret_loser_14[slice36],\n",
        "       train_regret_loser_15[slice36],\n",
        "       train_regret_loser_16[slice36],\n",
        "       train_regret_loser_17[slice36],\n",
        "       train_regret_loser_18[slice36],\n",
        "       train_regret_loser_19[slice36],\n",
        "       train_regret_loser_20[slice36]]\n",
        "\n",
        "winner36 = [train_regret_winner_1[slice36],\n",
        "       train_regret_winner_2[slice36],\n",
        "       train_regret_winner_3[slice36],\n",
        "       train_regret_winner_4[slice36],\n",
        "       train_regret_winner_5[slice36],\n",
        "       train_regret_winner_6[slice36],\n",
        "       train_regret_winner_7[slice36],\n",
        "       train_regret_winner_8[slice36],\n",
        "       train_regret_winner_9[slice36],\n",
        "       train_regret_winner_10[slice36],\n",
        "       train_regret_winner_11[slice36],\n",
        "       train_regret_winner_12[slice36],\n",
        "       train_regret_winner_13[slice36],\n",
        "       train_regret_winner_14[slice36],\n",
        "       train_regret_winner_15[slice36],\n",
        "       train_regret_winner_16[slice36],\n",
        "       train_regret_winner_17[slice36],\n",
        "       train_regret_winner_18[slice36],\n",
        "       train_regret_winner_19[slice36],\n",
        "       train_regret_winner_20[slice36]]\n",
        "\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\n",
        "\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\n",
        "\n",
        "slice46 = 45\n",
        "\n",
        "loser46 = [train_regret_loser_1[slice46],\n",
        "       train_regret_loser_2[slice46],\n",
        "       train_regret_loser_3[slice46],\n",
        "       train_regret_loser_4[slice46],\n",
        "       train_regret_loser_5[slice46],\n",
        "       train_regret_loser_6[slice46],\n",
        "       train_regret_loser_7[slice46],\n",
        "       train_regret_loser_8[slice46],\n",
        "       train_regret_loser_9[slice46],\n",
        "       train_regret_loser_10[slice46],\n",
        "       train_regret_loser_11[slice46],\n",
        "       train_regret_loser_12[slice46],\n",
        "       train_regret_loser_13[slice46],\n",
        "       train_regret_loser_14[slice46],\n",
        "       train_regret_loser_15[slice46],\n",
        "       train_regret_loser_16[slice46],\n",
        "       train_regret_loser_17[slice46],\n",
        "       train_regret_loser_18[slice46],\n",
        "       train_regret_loser_19[slice46],\n",
        "       train_regret_loser_20[slice46]]\n",
        "\n",
        "winner46 = [train_regret_winner_1[slice46],\n",
        "       train_regret_winner_2[slice46],\n",
        "       train_regret_winner_3[slice46],\n",
        "       train_regret_winner_4[slice46],\n",
        "       train_regret_winner_5[slice46],\n",
        "       train_regret_winner_6[slice46],\n",
        "       train_regret_winner_7[slice46],\n",
        "       train_regret_winner_8[slice46],\n",
        "       train_regret_winner_9[slice46],\n",
        "       train_regret_winner_10[slice46],\n",
        "       train_regret_winner_11[slice46],\n",
        "       train_regret_winner_12[slice46],\n",
        "       train_regret_winner_13[slice46],\n",
        "       train_regret_winner_14[slice46],\n",
        "       train_regret_winner_15[slice46],\n",
        "       train_regret_winner_16[slice46],\n",
        "       train_regret_winner_17[slice46],\n",
        "       train_regret_winner_18[slice46],\n",
        "       train_regret_winner_19[slice46],\n",
        "       train_regret_winner_20[slice46]]\n",
        "\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\n",
        "\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\n",
        "\n",
        "slice56 = 55\n",
        "\n",
        "loser56 = [train_regret_loser_1[slice56],\n",
        "       train_regret_loser_2[slice56],\n",
        "       train_regret_loser_3[slice56],\n",
        "       train_regret_loser_4[slice56],\n",
        "       train_regret_loser_5[slice56],\n",
        "       train_regret_loser_6[slice56],\n",
        "       train_regret_loser_7[slice56],\n",
        "       train_regret_loser_8[slice56],\n",
        "       train_regret_loser_9[slice56],\n",
        "       train_regret_loser_10[slice56],\n",
        "       train_regret_loser_11[slice56],\n",
        "       train_regret_loser_12[slice56],\n",
        "       train_regret_loser_13[slice56],\n",
        "       train_regret_loser_14[slice56],\n",
        "       train_regret_loser_15[slice56],\n",
        "       train_regret_loser_16[slice56],\n",
        "       train_regret_loser_17[slice56],\n",
        "       train_regret_loser_18[slice56],\n",
        "       train_regret_loser_19[slice56],\n",
        "       train_regret_loser_20[slice56]]\n",
        "\n",
        "winner56 = [train_regret_winner_1[slice56],\n",
        "       train_regret_winner_2[slice56],\n",
        "       train_regret_winner_3[slice56],\n",
        "       train_regret_winner_4[slice56],\n",
        "       train_regret_winner_5[slice56],\n",
        "       train_regret_winner_6[slice56],\n",
        "       train_regret_winner_7[slice56],\n",
        "       train_regret_winner_8[slice56],\n",
        "       train_regret_winner_9[slice56],\n",
        "       train_regret_winner_10[slice56],\n",
        "       train_regret_winner_11[slice56],\n",
        "       train_regret_winner_12[slice56],\n",
        "       train_regret_winner_13[slice56],\n",
        "       train_regret_winner_14[slice56],\n",
        "       train_regret_winner_15[slice56],\n",
        "       train_regret_winner_16[slice56],\n",
        "       train_regret_winner_17[slice56],\n",
        "       train_regret_winner_18[slice56],\n",
        "       train_regret_winner_19[slice56],\n",
        "       train_regret_winner_20[slice56]]\n",
        "\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\n",
        "\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\n",
        "\n",
        "slice66 = 65\n",
        "\n",
        "loser66 = [train_regret_loser_1[slice66],\n",
        "       train_regret_loser_2[slice66],\n",
        "       train_regret_loser_3[slice66],\n",
        "       train_regret_loser_4[slice66],\n",
        "       train_regret_loser_5[slice66],\n",
        "       train_regret_loser_6[slice66],\n",
        "       train_regret_loser_7[slice66],\n",
        "       train_regret_loser_8[slice66],\n",
        "       train_regret_loser_9[slice66],\n",
        "       train_regret_loser_10[slice66],\n",
        "       train_regret_loser_11[slice66],\n",
        "       train_regret_loser_12[slice66],\n",
        "       train_regret_loser_13[slice66],\n",
        "       train_regret_loser_14[slice66],\n",
        "       train_regret_loser_15[slice66],\n",
        "       train_regret_loser_16[slice66],\n",
        "       train_regret_loser_17[slice66],\n",
        "       train_regret_loser_18[slice66],\n",
        "       train_regret_loser_19[slice66],\n",
        "       train_regret_loser_20[slice66]]\n",
        "\n",
        "winner66 = [train_regret_winner_1[slice66],\n",
        "       train_regret_winner_2[slice66],\n",
        "       train_regret_winner_3[slice66],\n",
        "       train_regret_winner_4[slice66],\n",
        "       train_regret_winner_5[slice66],\n",
        "       train_regret_winner_6[slice66],\n",
        "       train_regret_winner_7[slice66],\n",
        "       train_regret_winner_8[slice66],\n",
        "       train_regret_winner_9[slice66],\n",
        "       train_regret_winner_10[slice66],\n",
        "       train_regret_winner_11[slice66],\n",
        "       train_regret_winner_12[slice66],\n",
        "       train_regret_winner_13[slice66],\n",
        "       train_regret_winner_14[slice66],\n",
        "       train_regret_winner_15[slice66],\n",
        "       train_regret_winner_16[slice66],\n",
        "       train_regret_winner_17[slice66],\n",
        "       train_regret_winner_18[slice66],\n",
        "       train_regret_winner_19[slice66],\n",
        "       train_regret_winner_20[slice66]]\n",
        "\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\n",
        "\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\n",
        "\n",
        "slice76 = 75\n",
        "\n",
        "loser76 = [train_regret_loser_1[slice76],\n",
        "       train_regret_loser_2[slice76],\n",
        "       train_regret_loser_3[slice76],\n",
        "       train_regret_loser_4[slice76],\n",
        "       train_regret_loser_5[slice76],\n",
        "       train_regret_loser_6[slice76],\n",
        "       train_regret_loser_7[slice76],\n",
        "       train_regret_loser_8[slice76],\n",
        "       train_regret_loser_9[slice76],\n",
        "       train_regret_loser_10[slice76],\n",
        "       train_regret_loser_11[slice76],\n",
        "       train_regret_loser_12[slice76],\n",
        "       train_regret_loser_13[slice76],\n",
        "       train_regret_loser_14[slice76],\n",
        "       train_regret_loser_15[slice76],\n",
        "       train_regret_loser_16[slice76],\n",
        "       train_regret_loser_17[slice76],\n",
        "       train_regret_loser_18[slice76],\n",
        "       train_regret_loser_19[slice76],\n",
        "       train_regret_loser_20[slice76]]\n",
        "\n",
        "winner76 = [train_regret_winner_1[slice76],\n",
        "       train_regret_winner_2[slice76],\n",
        "       train_regret_winner_3[slice76],\n",
        "       train_regret_winner_4[slice76],\n",
        "       train_regret_winner_5[slice76],\n",
        "       train_regret_winner_6[slice76],\n",
        "       train_regret_winner_7[slice76],\n",
        "       train_regret_winner_8[slice76],\n",
        "       train_regret_winner_9[slice76],\n",
        "       train_regret_winner_10[slice76],\n",
        "       train_regret_winner_11[slice76],\n",
        "       train_regret_winner_12[slice76],\n",
        "       train_regret_winner_13[slice76],\n",
        "       train_regret_winner_14[slice76],\n",
        "       train_regret_winner_15[slice76],\n",
        "       train_regret_winner_16[slice76],\n",
        "       train_regret_winner_17[slice76],\n",
        "       train_regret_winner_18[slice76],\n",
        "       train_regret_winner_19[slice76],\n",
        "       train_regret_winner_20[slice76]]\n",
        "\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\n",
        "\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\n",
        "\n",
        "slice86 = 85\n",
        "\n",
        "loser86 = [train_regret_loser_1[slice86],\n",
        "       train_regret_loser_2[slice86],\n",
        "       train_regret_loser_3[slice86],\n",
        "       train_regret_loser_4[slice86],\n",
        "       train_regret_loser_5[slice86],\n",
        "       train_regret_loser_6[slice86],\n",
        "       train_regret_loser_7[slice86],\n",
        "       train_regret_loser_8[slice86],\n",
        "       train_regret_loser_9[slice86],\n",
        "       train_regret_loser_10[slice86],\n",
        "       train_regret_loser_11[slice86],\n",
        "       train_regret_loser_12[slice86],\n",
        "       train_regret_loser_13[slice86],\n",
        "       train_regret_loser_14[slice86],\n",
        "       train_regret_loser_15[slice86],\n",
        "       train_regret_loser_16[slice86],\n",
        "       train_regret_loser_17[slice86],\n",
        "       train_regret_loser_18[slice86],\n",
        "       train_regret_loser_19[slice86],\n",
        "       train_regret_loser_20[slice86]]\n",
        "\n",
        "winner86 = [train_regret_winner_1[slice86],\n",
        "       train_regret_winner_2[slice86],\n",
        "       train_regret_winner_3[slice86],\n",
        "       train_regret_winner_4[slice86],\n",
        "       train_regret_winner_5[slice86],\n",
        "       train_regret_winner_6[slice86],\n",
        "       train_regret_winner_7[slice86],\n",
        "       train_regret_winner_8[slice86],\n",
        "       train_regret_winner_9[slice86],\n",
        "       train_regret_winner_10[slice86],\n",
        "       train_regret_winner_11[slice86],\n",
        "       train_regret_winner_12[slice86],\n",
        "       train_regret_winner_13[slice86],\n",
        "       train_regret_winner_14[slice86],\n",
        "       train_regret_winner_15[slice86],\n",
        "       train_regret_winner_16[slice86],\n",
        "       train_regret_winner_17[slice86],\n",
        "       train_regret_winner_18[slice86],\n",
        "       train_regret_winner_19[slice86],\n",
        "       train_regret_winner_20[slice86]]\n",
        "\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\n",
        "\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\n",
        "\n",
        "slice96 = 95\n",
        "\n",
        "loser96 = [train_regret_loser_1[slice96],\n",
        "       train_regret_loser_2[slice96],\n",
        "       train_regret_loser_3[slice96],\n",
        "       train_regret_loser_4[slice96],\n",
        "       train_regret_loser_5[slice96],\n",
        "       train_regret_loser_6[slice96],\n",
        "       train_regret_loser_7[slice96],\n",
        "       train_regret_loser_8[slice96],\n",
        "       train_regret_loser_9[slice96],\n",
        "       train_regret_loser_10[slice96],\n",
        "       train_regret_loser_11[slice96],\n",
        "       train_regret_loser_12[slice96],\n",
        "       train_regret_loser_13[slice96],\n",
        "       train_regret_loser_14[slice96],\n",
        "       train_regret_loser_15[slice96],\n",
        "       train_regret_loser_16[slice96],\n",
        "       train_regret_loser_17[slice96],\n",
        "       train_regret_loser_18[slice96],\n",
        "       train_regret_loser_19[slice96],\n",
        "       train_regret_loser_20[slice96]]\n",
        "\n",
        "winner96 = [train_regret_winner_1[slice96],\n",
        "       train_regret_winner_2[slice96],\n",
        "       train_regret_winner_3[slice96],\n",
        "       train_regret_winner_4[slice96],\n",
        "       train_regret_winner_5[slice96],\n",
        "       train_regret_winner_6[slice96],\n",
        "       train_regret_winner_7[slice96],\n",
        "       train_regret_winner_8[slice96],\n",
        "       train_regret_winner_9[slice96],\n",
        "       train_regret_winner_10[slice96],\n",
        "       train_regret_winner_11[slice96],\n",
        "       train_regret_winner_12[slice96],\n",
        "       train_regret_winner_13[slice96],\n",
        "       train_regret_winner_14[slice96],\n",
        "       train_regret_winner_15[slice96],\n",
        "       train_regret_winner_16[slice96],\n",
        "       train_regret_winner_17[slice96],\n",
        "       train_regret_winner_18[slice96],\n",
        "       train_regret_winner_19[slice96],\n",
        "       train_regret_winner_20[slice96]]\n",
        "\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\n",
        "\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\n",
        "\n",
        "slice7 = 6\n",
        "\n",
        "loser7 = [train_regret_loser_1[slice7],\n",
        "       train_regret_loser_2[slice7],\n",
        "       train_regret_loser_3[slice7],\n",
        "       train_regret_loser_4[slice7],\n",
        "       train_regret_loser_5[slice7],\n",
        "       train_regret_loser_6[slice7],\n",
        "       train_regret_loser_7[slice7],\n",
        "       train_regret_loser_8[slice7],\n",
        "       train_regret_loser_9[slice7],\n",
        "       train_regret_loser_10[slice7],\n",
        "       train_regret_loser_11[slice7],\n",
        "       train_regret_loser_12[slice7],\n",
        "       train_regret_loser_13[slice7],\n",
        "       train_regret_loser_14[slice7],\n",
        "       train_regret_loser_15[slice7],\n",
        "       train_regret_loser_16[slice7],\n",
        "       train_regret_loser_17[slice7],\n",
        "       train_regret_loser_18[slice7],\n",
        "       train_regret_loser_19[slice7],\n",
        "       train_regret_loser_20[slice7]]\n",
        "\n",
        "winner7 = [train_regret_winner_1[slice7],\n",
        "       train_regret_winner_2[slice7],\n",
        "       train_regret_winner_3[slice7],\n",
        "       train_regret_winner_4[slice7],\n",
        "       train_regret_winner_5[slice7],\n",
        "       train_regret_winner_6[slice7],\n",
        "       train_regret_winner_7[slice7],\n",
        "       train_regret_winner_8[slice7],\n",
        "       train_regret_winner_9[slice7],\n",
        "       train_regret_winner_10[slice7],\n",
        "       train_regret_winner_11[slice7],\n",
        "       train_regret_winner_12[slice7],\n",
        "       train_regret_winner_13[slice7],\n",
        "       train_regret_winner_14[slice7],\n",
        "       train_regret_winner_15[slice7],\n",
        "       train_regret_winner_16[slice7],\n",
        "       train_regret_winner_17[slice7],\n",
        "       train_regret_winner_18[slice7],\n",
        "       train_regret_winner_19[slice7],\n",
        "       train_regret_winner_20[slice7]]\n",
        "\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\n",
        "\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\n",
        "\n",
        "slice17 = 16\n",
        "\n",
        "loser17 = [train_regret_loser_1[slice17],\n",
        "       train_regret_loser_2[slice17],\n",
        "       train_regret_loser_3[slice17],\n",
        "       train_regret_loser_4[slice17],\n",
        "       train_regret_loser_5[slice17],\n",
        "       train_regret_loser_6[slice17],\n",
        "       train_regret_loser_7[slice17],\n",
        "       train_regret_loser_8[slice17],\n",
        "       train_regret_loser_9[slice17],\n",
        "       train_regret_loser_10[slice17],\n",
        "       train_regret_loser_11[slice17],\n",
        "       train_regret_loser_12[slice17],\n",
        "       train_regret_loser_13[slice17],\n",
        "       train_regret_loser_14[slice17],\n",
        "       train_regret_loser_15[slice17],\n",
        "       train_regret_loser_16[slice17],\n",
        "       train_regret_loser_17[slice17],\n",
        "       train_regret_loser_18[slice17],\n",
        "       train_regret_loser_19[slice17],\n",
        "       train_regret_loser_20[slice17]]\n",
        "\n",
        "winner17 = [train_regret_winner_1[slice17],\n",
        "       train_regret_winner_2[slice17],\n",
        "       train_regret_winner_3[slice17],\n",
        "       train_regret_winner_4[slice17],\n",
        "       train_regret_winner_5[slice17],\n",
        "       train_regret_winner_6[slice17],\n",
        "       train_regret_winner_7[slice17],\n",
        "       train_regret_winner_8[slice17],\n",
        "       train_regret_winner_9[slice17],\n",
        "       train_regret_winner_10[slice17],\n",
        "       train_regret_winner_11[slice17],\n",
        "       train_regret_winner_12[slice17],\n",
        "       train_regret_winner_13[slice17],\n",
        "       train_regret_winner_14[slice17],\n",
        "       train_regret_winner_15[slice17],\n",
        "       train_regret_winner_16[slice17],\n",
        "       train_regret_winner_17[slice17],\n",
        "       train_regret_winner_18[slice17],\n",
        "       train_regret_winner_19[slice17],\n",
        "       train_regret_winner_20[slice17]]\n",
        "\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\n",
        "\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\n",
        "\n",
        "slice27 = 26\n",
        "\n",
        "loser27 = [train_regret_loser_1[slice27],\n",
        "       train_regret_loser_2[slice27],\n",
        "       train_regret_loser_3[slice27],\n",
        "       train_regret_loser_4[slice27],\n",
        "       train_regret_loser_5[slice27],\n",
        "       train_regret_loser_6[slice27],\n",
        "       train_regret_loser_7[slice27],\n",
        "       train_regret_loser_8[slice27],\n",
        "       train_regret_loser_9[slice27],\n",
        "       train_regret_loser_10[slice27],\n",
        "       train_regret_loser_11[slice27],\n",
        "       train_regret_loser_12[slice27],\n",
        "       train_regret_loser_13[slice27],\n",
        "       train_regret_loser_14[slice27],\n",
        "       train_regret_loser_15[slice27],\n",
        "       train_regret_loser_16[slice27],\n",
        "       train_regret_loser_17[slice27],\n",
        "       train_regret_loser_18[slice27],\n",
        "       train_regret_loser_19[slice27],\n",
        "       train_regret_loser_20[slice27]]\n",
        "\n",
        "winner27 = [train_regret_winner_1[slice27],\n",
        "       train_regret_winner_2[slice27],\n",
        "       train_regret_winner_3[slice27],\n",
        "       train_regret_winner_4[slice27],\n",
        "       train_regret_winner_5[slice27],\n",
        "       train_regret_winner_6[slice27],\n",
        "       train_regret_winner_7[slice27],\n",
        "       train_regret_winner_8[slice27],\n",
        "       train_regret_winner_9[slice27],\n",
        "       train_regret_winner_10[slice27],\n",
        "       train_regret_winner_11[slice27],\n",
        "       train_regret_winner_12[slice27],\n",
        "       train_regret_winner_13[slice27],\n",
        "       train_regret_winner_14[slice27],\n",
        "       train_regret_winner_15[slice27],\n",
        "       train_regret_winner_16[slice27],\n",
        "       train_regret_winner_17[slice27],\n",
        "       train_regret_winner_18[slice27],\n",
        "       train_regret_winner_19[slice27],\n",
        "       train_regret_winner_20[slice27]]\n",
        "\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\n",
        "\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\n",
        "\n",
        "slice37 = 36\n",
        "\n",
        "loser37 = [train_regret_loser_1[slice37],\n",
        "       train_regret_loser_2[slice37],\n",
        "       train_regret_loser_3[slice37],\n",
        "       train_regret_loser_4[slice37],\n",
        "       train_regret_loser_5[slice37],\n",
        "       train_regret_loser_6[slice37],\n",
        "       train_regret_loser_7[slice37],\n",
        "       train_regret_loser_8[slice37],\n",
        "       train_regret_loser_9[slice37],\n",
        "       train_regret_loser_10[slice37],\n",
        "       train_regret_loser_11[slice37],\n",
        "       train_regret_loser_12[slice37],\n",
        "       train_regret_loser_13[slice37],\n",
        "       train_regret_loser_14[slice37],\n",
        "       train_regret_loser_15[slice37],\n",
        "       train_regret_loser_16[slice37],\n",
        "       train_regret_loser_17[slice37],\n",
        "       train_regret_loser_18[slice37],\n",
        "       train_regret_loser_19[slice37],\n",
        "       train_regret_loser_20[slice37]]\n",
        "\n",
        "winner37 = [train_regret_winner_1[slice37],\n",
        "       train_regret_winner_2[slice37],\n",
        "       train_regret_winner_3[slice37],\n",
        "       train_regret_winner_4[slice37],\n",
        "       train_regret_winner_5[slice37],\n",
        "       train_regret_winner_6[slice37],\n",
        "       train_regret_winner_7[slice37],\n",
        "       train_regret_winner_8[slice37],\n",
        "       train_regret_winner_9[slice37],\n",
        "       train_regret_winner_10[slice37],\n",
        "       train_regret_winner_11[slice37],\n",
        "       train_regret_winner_12[slice37],\n",
        "       train_regret_winner_13[slice37],\n",
        "       train_regret_winner_14[slice37],\n",
        "       train_regret_winner_15[slice37],\n",
        "       train_regret_winner_16[slice37],\n",
        "       train_regret_winner_17[slice37],\n",
        "       train_regret_winner_18[slice37],\n",
        "       train_regret_winner_19[slice37],\n",
        "       train_regret_winner_20[slice37]]\n",
        "\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\n",
        "\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\n",
        "\n",
        "slice47 = 46\n",
        "\n",
        "loser47 = [train_regret_loser_1[slice47],\n",
        "       train_regret_loser_2[slice47],\n",
        "       train_regret_loser_3[slice47],\n",
        "       train_regret_loser_4[slice47],\n",
        "       train_regret_loser_5[slice47],\n",
        "       train_regret_loser_6[slice47],\n",
        "       train_regret_loser_7[slice47],\n",
        "       train_regret_loser_8[slice47],\n",
        "       train_regret_loser_9[slice47],\n",
        "       train_regret_loser_10[slice47],\n",
        "       train_regret_loser_11[slice47],\n",
        "       train_regret_loser_12[slice47],\n",
        "       train_regret_loser_13[slice47],\n",
        "       train_regret_loser_14[slice47],\n",
        "       train_regret_loser_15[slice47],\n",
        "       train_regret_loser_16[slice47],\n",
        "       train_regret_loser_17[slice47],\n",
        "       train_regret_loser_18[slice47],\n",
        "       train_regret_loser_19[slice47],\n",
        "       train_regret_loser_20[slice47]]\n",
        "\n",
        "winner47 = [train_regret_winner_1[slice47],\n",
        "       train_regret_winner_2[slice47],\n",
        "       train_regret_winner_3[slice47],\n",
        "       train_regret_winner_4[slice47],\n",
        "       train_regret_winner_5[slice47],\n",
        "       train_regret_winner_6[slice47],\n",
        "       train_regret_winner_7[slice47],\n",
        "       train_regret_winner_8[slice47],\n",
        "       train_regret_winner_9[slice47],\n",
        "       train_regret_winner_10[slice47],\n",
        "       train_regret_winner_11[slice47],\n",
        "       train_regret_winner_12[slice47],\n",
        "       train_regret_winner_13[slice47],\n",
        "       train_regret_winner_14[slice47],\n",
        "       train_regret_winner_15[slice47],\n",
        "       train_regret_winner_16[slice47],\n",
        "       train_regret_winner_17[slice47],\n",
        "       train_regret_winner_18[slice47],\n",
        "       train_regret_winner_19[slice47],\n",
        "       train_regret_winner_20[slice47]]\n",
        "\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\n",
        "\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\n",
        "\n",
        "slice57 = 56\n",
        "\n",
        "loser57 = [train_regret_loser_1[slice57],\n",
        "       train_regret_loser_2[slice57],\n",
        "       train_regret_loser_3[slice57],\n",
        "       train_regret_loser_4[slice57],\n",
        "       train_regret_loser_5[slice57],\n",
        "       train_regret_loser_6[slice57],\n",
        "       train_regret_loser_7[slice57],\n",
        "       train_regret_loser_8[slice57],\n",
        "       train_regret_loser_9[slice57],\n",
        "       train_regret_loser_10[slice57],\n",
        "       train_regret_loser_11[slice57],\n",
        "       train_regret_loser_12[slice57],\n",
        "       train_regret_loser_13[slice57],\n",
        "       train_regret_loser_14[slice57],\n",
        "       train_regret_loser_15[slice57],\n",
        "       train_regret_loser_16[slice57],\n",
        "       train_regret_loser_17[slice57],\n",
        "       train_regret_loser_18[slice57],\n",
        "       train_regret_loser_19[slice57],\n",
        "       train_regret_loser_20[slice57]]\n",
        "\n",
        "winner57 = [train_regret_winner_1[slice57],\n",
        "       train_regret_winner_2[slice57],\n",
        "       train_regret_winner_3[slice57],\n",
        "       train_regret_winner_4[slice57],\n",
        "       train_regret_winner_5[slice57],\n",
        "       train_regret_winner_6[slice57],\n",
        "       train_regret_winner_7[slice57],\n",
        "       train_regret_winner_8[slice57],\n",
        "       train_regret_winner_9[slice57],\n",
        "       train_regret_winner_10[slice57],\n",
        "       train_regret_winner_11[slice57],\n",
        "       train_regret_winner_12[slice57],\n",
        "       train_regret_winner_13[slice57],\n",
        "       train_regret_winner_14[slice57],\n",
        "       train_regret_winner_15[slice57],\n",
        "       train_regret_winner_16[slice57],\n",
        "       train_regret_winner_17[slice57],\n",
        "       train_regret_winner_18[slice57],\n",
        "       train_regret_winner_19[slice57],\n",
        "       train_regret_winner_20[slice57]]\n",
        "\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\n",
        "\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\n",
        "\n",
        "slice67 = 66\n",
        "\n",
        "loser67 = [train_regret_loser_1[slice67],\n",
        "       train_regret_loser_2[slice67],\n",
        "       train_regret_loser_3[slice67],\n",
        "       train_regret_loser_4[slice67],\n",
        "       train_regret_loser_5[slice67],\n",
        "       train_regret_loser_6[slice67],\n",
        "       train_regret_loser_7[slice67],\n",
        "       train_regret_loser_8[slice67],\n",
        "       train_regret_loser_9[slice67],\n",
        "       train_regret_loser_10[slice67],\n",
        "       train_regret_loser_11[slice67],\n",
        "       train_regret_loser_12[slice67],\n",
        "       train_regret_loser_13[slice67],\n",
        "       train_regret_loser_14[slice67],\n",
        "       train_regret_loser_15[slice67],\n",
        "       train_regret_loser_16[slice67],\n",
        "       train_regret_loser_17[slice67],\n",
        "       train_regret_loser_18[slice67],\n",
        "       train_regret_loser_19[slice67],\n",
        "       train_regret_loser_20[slice67]]\n",
        "\n",
        "winner67 = [train_regret_winner_1[slice67],\n",
        "       train_regret_winner_2[slice67],\n",
        "       train_regret_winner_3[slice67],\n",
        "       train_regret_winner_4[slice67],\n",
        "       train_regret_winner_5[slice67],\n",
        "       train_regret_winner_6[slice67],\n",
        "       train_regret_winner_7[slice67],\n",
        "       train_regret_winner_8[slice67],\n",
        "       train_regret_winner_9[slice67],\n",
        "       train_regret_winner_10[slice67],\n",
        "       train_regret_winner_11[slice67],\n",
        "       train_regret_winner_12[slice67],\n",
        "       train_regret_winner_13[slice67],\n",
        "       train_regret_winner_14[slice67],\n",
        "       train_regret_winner_15[slice67],\n",
        "       train_regret_winner_16[slice67],\n",
        "       train_regret_winner_17[slice67],\n",
        "       train_regret_winner_18[slice67],\n",
        "       train_regret_winner_19[slice67],\n",
        "       train_regret_winner_20[slice67]]\n",
        "\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\n",
        "\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\n",
        "\n",
        "slice77 = 76\n",
        "\n",
        "loser77 = [train_regret_loser_1[slice77],\n",
        "       train_regret_loser_2[slice77],\n",
        "       train_regret_loser_3[slice77],\n",
        "       train_regret_loser_4[slice77],\n",
        "       train_regret_loser_5[slice77],\n",
        "       train_regret_loser_6[slice77],\n",
        "       train_regret_loser_7[slice77],\n",
        "       train_regret_loser_8[slice77],\n",
        "       train_regret_loser_9[slice77],\n",
        "       train_regret_loser_10[slice77],\n",
        "       train_regret_loser_11[slice77],\n",
        "       train_regret_loser_12[slice77],\n",
        "       train_regret_loser_13[slice77],\n",
        "       train_regret_loser_14[slice77],\n",
        "       train_regret_loser_15[slice77],\n",
        "       train_regret_loser_16[slice77],\n",
        "       train_regret_loser_17[slice77],\n",
        "       train_regret_loser_18[slice77],\n",
        "       train_regret_loser_19[slice77],\n",
        "       train_regret_loser_20[slice77]]\n",
        "\n",
        "winner77 = [train_regret_winner_1[slice77],\n",
        "       train_regret_winner_2[slice77],\n",
        "       train_regret_winner_3[slice77],\n",
        "       train_regret_winner_4[slice77],\n",
        "       train_regret_winner_5[slice77],\n",
        "       train_regret_winner_6[slice77],\n",
        "       train_regret_winner_7[slice77],\n",
        "       train_regret_winner_8[slice77],\n",
        "       train_regret_winner_9[slice77],\n",
        "       train_regret_winner_10[slice77],\n",
        "       train_regret_winner_11[slice77],\n",
        "       train_regret_winner_12[slice77],\n",
        "       train_regret_winner_13[slice77],\n",
        "       train_regret_winner_14[slice77],\n",
        "       train_regret_winner_15[slice77],\n",
        "       train_regret_winner_16[slice77],\n",
        "       train_regret_winner_17[slice77],\n",
        "       train_regret_winner_18[slice77],\n",
        "       train_regret_winner_19[slice77],\n",
        "       train_regret_winner_20[slice77]]\n",
        "\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\n",
        "\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\n",
        "\n",
        "slice87 = 86\n",
        "\n",
        "loser87 = [train_regret_loser_1[slice87],\n",
        "       train_regret_loser_2[slice87],\n",
        "       train_regret_loser_3[slice87],\n",
        "       train_regret_loser_4[slice87],\n",
        "       train_regret_loser_5[slice87],\n",
        "       train_regret_loser_6[slice87],\n",
        "       train_regret_loser_7[slice87],\n",
        "       train_regret_loser_8[slice87],\n",
        "       train_regret_loser_9[slice87],\n",
        "       train_regret_loser_10[slice87],\n",
        "       train_regret_loser_11[slice87],\n",
        "       train_regret_loser_12[slice87],\n",
        "       train_regret_loser_13[slice87],\n",
        "       train_regret_loser_14[slice87],\n",
        "       train_regret_loser_15[slice87],\n",
        "       train_regret_loser_16[slice87],\n",
        "       train_regret_loser_17[slice87],\n",
        "       train_regret_loser_18[slice87],\n",
        "       train_regret_loser_19[slice87],\n",
        "       train_regret_loser_20[slice87]]\n",
        "\n",
        "winner87 = [train_regret_winner_1[slice87],\n",
        "       train_regret_winner_2[slice87],\n",
        "       train_regret_winner_3[slice87],\n",
        "       train_regret_winner_4[slice87],\n",
        "       train_regret_winner_5[slice87],\n",
        "       train_regret_winner_6[slice87],\n",
        "       train_regret_winner_7[slice87],\n",
        "       train_regret_winner_8[slice87],\n",
        "       train_regret_winner_9[slice87],\n",
        "       train_regret_winner_10[slice87],\n",
        "       train_regret_winner_11[slice87],\n",
        "       train_regret_winner_12[slice87],\n",
        "       train_regret_winner_13[slice87],\n",
        "       train_regret_winner_14[slice87],\n",
        "       train_regret_winner_15[slice87],\n",
        "       train_regret_winner_16[slice87],\n",
        "       train_regret_winner_17[slice87],\n",
        "       train_regret_winner_18[slice87],\n",
        "       train_regret_winner_19[slice87],\n",
        "       train_regret_winner_20[slice87]]\n",
        "\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\n",
        "\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\n",
        "\n",
        "slice97 = 96\n",
        "\n",
        "loser97 = [train_regret_loser_1[slice97],\n",
        "       train_regret_loser_2[slice97],\n",
        "       train_regret_loser_3[slice97],\n",
        "       train_regret_loser_4[slice97],\n",
        "       train_regret_loser_5[slice97],\n",
        "       train_regret_loser_6[slice97],\n",
        "       train_regret_loser_7[slice97],\n",
        "       train_regret_loser_8[slice97],\n",
        "       train_regret_loser_9[slice97],\n",
        "       train_regret_loser_10[slice97],\n",
        "       train_regret_loser_11[slice97],\n",
        "       train_regret_loser_12[slice97],\n",
        "       train_regret_loser_13[slice97],\n",
        "       train_regret_loser_14[slice97],\n",
        "       train_regret_loser_15[slice97],\n",
        "       train_regret_loser_16[slice97],\n",
        "       train_regret_loser_17[slice97],\n",
        "       train_regret_loser_18[slice97],\n",
        "       train_regret_loser_19[slice97],\n",
        "       train_regret_loser_20[slice97]]\n",
        "\n",
        "winner97 = [train_regret_winner_1[slice97],\n",
        "       train_regret_winner_2[slice97],\n",
        "       train_regret_winner_3[slice97],\n",
        "       train_regret_winner_4[slice97],\n",
        "       train_regret_winner_5[slice97],\n",
        "       train_regret_winner_6[slice97],\n",
        "       train_regret_winner_7[slice97],\n",
        "       train_regret_winner_8[slice97],\n",
        "       train_regret_winner_9[slice97],\n",
        "       train_regret_winner_10[slice97],\n",
        "       train_regret_winner_11[slice97],\n",
        "       train_regret_winner_12[slice97],\n",
        "       train_regret_winner_13[slice97],\n",
        "       train_regret_winner_14[slice97],\n",
        "       train_regret_winner_15[slice97],\n",
        "       train_regret_winner_16[slice97],\n",
        "       train_regret_winner_17[slice97],\n",
        "       train_regret_winner_18[slice97],\n",
        "       train_regret_winner_19[slice97],\n",
        "       train_regret_winner_20[slice97]]\n",
        "\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\n",
        "\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\n",
        "\n",
        "slice8 = 7\n",
        "\n",
        "loser8 = [train_regret_loser_1[slice8],\n",
        "       train_regret_loser_2[slice8],\n",
        "       train_regret_loser_3[slice8],\n",
        "       train_regret_loser_4[slice8],\n",
        "       train_regret_loser_5[slice8],\n",
        "       train_regret_loser_6[slice8],\n",
        "       train_regret_loser_7[slice8],\n",
        "       train_regret_loser_8[slice8],\n",
        "       train_regret_loser_9[slice8],\n",
        "       train_regret_loser_10[slice8],\n",
        "       train_regret_loser_11[slice8],\n",
        "       train_regret_loser_12[slice8],\n",
        "       train_regret_loser_13[slice8],\n",
        "       train_regret_loser_14[slice8],\n",
        "       train_regret_loser_15[slice8],\n",
        "       train_regret_loser_16[slice8],\n",
        "       train_regret_loser_17[slice8],\n",
        "       train_regret_loser_18[slice8],\n",
        "       train_regret_loser_19[slice8],\n",
        "       train_regret_loser_20[slice8]]\n",
        "\n",
        "winner8 = [train_regret_winner_1[slice8],\n",
        "       train_regret_winner_2[slice8],\n",
        "       train_regret_winner_3[slice8],\n",
        "       train_regret_winner_4[slice8],\n",
        "       train_regret_winner_5[slice8],\n",
        "       train_regret_winner_6[slice8],\n",
        "       train_regret_winner_7[slice8],\n",
        "       train_regret_winner_8[slice8],\n",
        "       train_regret_winner_9[slice8],\n",
        "       train_regret_winner_10[slice8],\n",
        "       train_regret_winner_11[slice8],\n",
        "       train_regret_winner_12[slice8],\n",
        "       train_regret_winner_13[slice8],\n",
        "       train_regret_winner_14[slice8],\n",
        "       train_regret_winner_15[slice8],\n",
        "       train_regret_winner_16[slice8],\n",
        "       train_regret_winner_17[slice8],\n",
        "       train_regret_winner_18[slice8],\n",
        "       train_regret_winner_19[slice8],\n",
        "       train_regret_winner_20[slice8]]\n",
        "\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\n",
        "\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\n",
        "\n",
        "slice18 = 17\n",
        "\n",
        "loser18 = [train_regret_loser_1[slice18],\n",
        "       train_regret_loser_2[slice18],\n",
        "       train_regret_loser_3[slice18],\n",
        "       train_regret_loser_4[slice18],\n",
        "       train_regret_loser_5[slice18],\n",
        "       train_regret_loser_6[slice18],\n",
        "       train_regret_loser_7[slice18],\n",
        "       train_regret_loser_8[slice18],\n",
        "       train_regret_loser_9[slice18],\n",
        "       train_regret_loser_10[slice18],\n",
        "       train_regret_loser_11[slice18],\n",
        "       train_regret_loser_12[slice18],\n",
        "       train_regret_loser_13[slice18],\n",
        "       train_regret_loser_14[slice18],\n",
        "       train_regret_loser_15[slice18],\n",
        "       train_regret_loser_16[slice18],\n",
        "       train_regret_loser_17[slice18],\n",
        "       train_regret_loser_18[slice18],\n",
        "       train_regret_loser_19[slice18],\n",
        "       train_regret_loser_20[slice18]]\n",
        "\n",
        "winner18 = [train_regret_winner_1[slice18],\n",
        "       train_regret_winner_2[slice18],\n",
        "       train_regret_winner_3[slice18],\n",
        "       train_regret_winner_4[slice18],\n",
        "       train_regret_winner_5[slice18],\n",
        "       train_regret_winner_6[slice18],\n",
        "       train_regret_winner_7[slice18],\n",
        "       train_regret_winner_8[slice18],\n",
        "       train_regret_winner_9[slice18],\n",
        "       train_regret_winner_10[slice18],\n",
        "       train_regret_winner_11[slice18],\n",
        "       train_regret_winner_12[slice18],\n",
        "       train_regret_winner_13[slice18],\n",
        "       train_regret_winner_14[slice18],\n",
        "       train_regret_winner_15[slice18],\n",
        "       train_regret_winner_16[slice18],\n",
        "       train_regret_winner_17[slice18],\n",
        "       train_regret_winner_18[slice18],\n",
        "       train_regret_winner_19[slice18],\n",
        "       train_regret_winner_20[slice18]]\n",
        "\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\n",
        "\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\n",
        "\n",
        "slice28 = 27\n",
        "\n",
        "loser28 = [train_regret_loser_1[slice28],\n",
        "       train_regret_loser_2[slice28],\n",
        "       train_regret_loser_3[slice28],\n",
        "       train_regret_loser_4[slice28],\n",
        "       train_regret_loser_5[slice28],\n",
        "       train_regret_loser_6[slice28],\n",
        "       train_regret_loser_7[slice28],\n",
        "       train_regret_loser_8[slice28],\n",
        "       train_regret_loser_9[slice28],\n",
        "       train_regret_loser_10[slice28],\n",
        "       train_regret_loser_11[slice28],\n",
        "       train_regret_loser_12[slice28],\n",
        "       train_regret_loser_13[slice28],\n",
        "       train_regret_loser_14[slice28],\n",
        "       train_regret_loser_15[slice28],\n",
        "       train_regret_loser_16[slice28],\n",
        "       train_regret_loser_17[slice28],\n",
        "       train_regret_loser_18[slice28],\n",
        "       train_regret_loser_19[slice28],\n",
        "       train_regret_loser_20[slice28]]\n",
        "\n",
        "winner28 = [train_regret_winner_1[slice28],\n",
        "       train_regret_winner_2[slice28],\n",
        "       train_regret_winner_3[slice28],\n",
        "       train_regret_winner_4[slice28],\n",
        "       train_regret_winner_5[slice28],\n",
        "       train_regret_winner_6[slice28],\n",
        "       train_regret_winner_7[slice28],\n",
        "       train_regret_winner_8[slice28],\n",
        "       train_regret_winner_9[slice28],\n",
        "       train_regret_winner_10[slice28],\n",
        "       train_regret_winner_11[slice28],\n",
        "       train_regret_winner_12[slice28],\n",
        "       train_regret_winner_13[slice28],\n",
        "       train_regret_winner_14[slice28],\n",
        "       train_regret_winner_15[slice28],\n",
        "       train_regret_winner_16[slice28],\n",
        "       train_regret_winner_17[slice28],\n",
        "       train_regret_winner_18[slice28],\n",
        "       train_regret_winner_19[slice28],\n",
        "       train_regret_winner_20[slice28]]\n",
        "\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\n",
        "\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\n",
        "\n",
        "slice38 = 37\n",
        "\n",
        "loser38 = [train_regret_loser_1[slice38],\n",
        "       train_regret_loser_2[slice38],\n",
        "       train_regret_loser_3[slice38],\n",
        "       train_regret_loser_4[slice38],\n",
        "       train_regret_loser_5[slice38],\n",
        "       train_regret_loser_6[slice38],\n",
        "       train_regret_loser_7[slice38],\n",
        "       train_regret_loser_8[slice38],\n",
        "       train_regret_loser_9[slice38],\n",
        "       train_regret_loser_10[slice38],\n",
        "       train_regret_loser_11[slice38],\n",
        "       train_regret_loser_12[slice38],\n",
        "       train_regret_loser_13[slice38],\n",
        "       train_regret_loser_14[slice38],\n",
        "       train_regret_loser_15[slice38],\n",
        "       train_regret_loser_16[slice38],\n",
        "       train_regret_loser_17[slice38],\n",
        "       train_regret_loser_18[slice38],\n",
        "       train_regret_loser_19[slice38],\n",
        "       train_regret_loser_20[slice38]]\n",
        "\n",
        "winner38 = [train_regret_winner_1[slice38],\n",
        "       train_regret_winner_2[slice38],\n",
        "       train_regret_winner_3[slice38],\n",
        "       train_regret_winner_4[slice38],\n",
        "       train_regret_winner_5[slice38],\n",
        "       train_regret_winner_6[slice38],\n",
        "       train_regret_winner_7[slice38],\n",
        "       train_regret_winner_8[slice38],\n",
        "       train_regret_winner_9[slice38],\n",
        "       train_regret_winner_10[slice38],\n",
        "       train_regret_winner_11[slice38],\n",
        "       train_regret_winner_12[slice38],\n",
        "       train_regret_winner_13[slice38],\n",
        "       train_regret_winner_14[slice38],\n",
        "       train_regret_winner_15[slice38],\n",
        "       train_regret_winner_16[slice38],\n",
        "       train_regret_winner_17[slice38],\n",
        "       train_regret_winner_18[slice38],\n",
        "       train_regret_winner_19[slice38],\n",
        "       train_regret_winner_20[slice38]]\n",
        "\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\n",
        "\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\n",
        "\n",
        "slice48 = 47\n",
        "\n",
        "loser48 = [train_regret_loser_1[slice48],\n",
        "       train_regret_loser_2[slice48],\n",
        "       train_regret_loser_3[slice48],\n",
        "       train_regret_loser_4[slice48],\n",
        "       train_regret_loser_5[slice48],\n",
        "       train_regret_loser_6[slice48],\n",
        "       train_regret_loser_7[slice48],\n",
        "       train_regret_loser_8[slice48],\n",
        "       train_regret_loser_9[slice48],\n",
        "       train_regret_loser_10[slice48],\n",
        "       train_regret_loser_11[slice48],\n",
        "       train_regret_loser_12[slice48],\n",
        "       train_regret_loser_13[slice48],\n",
        "       train_regret_loser_14[slice48],\n",
        "       train_regret_loser_15[slice48],\n",
        "       train_regret_loser_16[slice48],\n",
        "       train_regret_loser_17[slice48],\n",
        "       train_regret_loser_18[slice48],\n",
        "       train_regret_loser_19[slice48],\n",
        "       train_regret_loser_20[slice48]]\n",
        "\n",
        "winner48 = [train_regret_winner_1[slice48],\n",
        "       train_regret_winner_2[slice48],\n",
        "       train_regret_winner_3[slice48],\n",
        "       train_regret_winner_4[slice48],\n",
        "       train_regret_winner_5[slice48],\n",
        "       train_regret_winner_6[slice48],\n",
        "       train_regret_winner_7[slice48],\n",
        "       train_regret_winner_8[slice48],\n",
        "       train_regret_winner_9[slice48],\n",
        "       train_regret_winner_10[slice48],\n",
        "       train_regret_winner_11[slice48],\n",
        "       train_regret_winner_12[slice48],\n",
        "       train_regret_winner_13[slice48],\n",
        "       train_regret_winner_14[slice48],\n",
        "       train_regret_winner_15[slice48],\n",
        "       train_regret_winner_16[slice48],\n",
        "       train_regret_winner_17[slice48],\n",
        "       train_regret_winner_18[slice48],\n",
        "       train_regret_winner_19[slice48],\n",
        "       train_regret_winner_20[slice48]]\n",
        "\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\n",
        "\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\n",
        "\n",
        "slice58 = 57\n",
        "\n",
        "loser58 = [train_regret_loser_1[slice58],\n",
        "       train_regret_loser_2[slice58],\n",
        "       train_regret_loser_3[slice58],\n",
        "       train_regret_loser_4[slice58],\n",
        "       train_regret_loser_5[slice58],\n",
        "       train_regret_loser_6[slice58],\n",
        "       train_regret_loser_7[slice58],\n",
        "       train_regret_loser_8[slice58],\n",
        "       train_regret_loser_9[slice58],\n",
        "       train_regret_loser_10[slice58],\n",
        "       train_regret_loser_11[slice58],\n",
        "       train_regret_loser_12[slice58],\n",
        "       train_regret_loser_13[slice58],\n",
        "       train_regret_loser_14[slice58],\n",
        "       train_regret_loser_15[slice58],\n",
        "       train_regret_loser_16[slice58],\n",
        "       train_regret_loser_17[slice58],\n",
        "       train_regret_loser_18[slice58],\n",
        "       train_regret_loser_19[slice58],\n",
        "       train_regret_loser_20[slice58]]\n",
        "\n",
        "winner58 = [train_regret_winner_1[slice58],\n",
        "       train_regret_winner_2[slice58],\n",
        "       train_regret_winner_3[slice58],\n",
        "       train_regret_winner_4[slice58],\n",
        "       train_regret_winner_5[slice58],\n",
        "       train_regret_winner_6[slice58],\n",
        "       train_regret_winner_7[slice58],\n",
        "       train_regret_winner_8[slice58],\n",
        "       train_regret_winner_9[slice58],\n",
        "       train_regret_winner_10[slice58],\n",
        "       train_regret_winner_11[slice58],\n",
        "       train_regret_winner_12[slice58],\n",
        "       train_regret_winner_13[slice58],\n",
        "       train_regret_winner_14[slice58],\n",
        "       train_regret_winner_15[slice58],\n",
        "       train_regret_winner_16[slice58],\n",
        "       train_regret_winner_17[slice58],\n",
        "       train_regret_winner_18[slice58],\n",
        "       train_regret_winner_19[slice58],\n",
        "       train_regret_winner_20[slice58]]\n",
        "\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\n",
        "\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\n",
        "\n",
        "slice68 = 67\n",
        "\n",
        "loser68 = [train_regret_loser_1[slice68],\n",
        "       train_regret_loser_2[slice68],\n",
        "       train_regret_loser_3[slice68],\n",
        "       train_regret_loser_4[slice68],\n",
        "       train_regret_loser_5[slice68],\n",
        "       train_regret_loser_6[slice68],\n",
        "       train_regret_loser_7[slice68],\n",
        "       train_regret_loser_8[slice68],\n",
        "       train_regret_loser_9[slice68],\n",
        "       train_regret_loser_10[slice68],\n",
        "       train_regret_loser_11[slice68],\n",
        "       train_regret_loser_12[slice68],\n",
        "       train_regret_loser_13[slice68],\n",
        "       train_regret_loser_14[slice68],\n",
        "       train_regret_loser_15[slice68],\n",
        "       train_regret_loser_16[slice68],\n",
        "       train_regret_loser_17[slice68],\n",
        "       train_regret_loser_18[slice68],\n",
        "       train_regret_loser_19[slice68],\n",
        "       train_regret_loser_20[slice68]]\n",
        "\n",
        "winner68 = [train_regret_winner_1[slice68],\n",
        "       train_regret_winner_2[slice68],\n",
        "       train_regret_winner_3[slice68],\n",
        "       train_regret_winner_4[slice68],\n",
        "       train_regret_winner_5[slice68],\n",
        "       train_regret_winner_6[slice68],\n",
        "       train_regret_winner_7[slice68],\n",
        "       train_regret_winner_8[slice68],\n",
        "       train_regret_winner_9[slice68],\n",
        "       train_regret_winner_10[slice68],\n",
        "       train_regret_winner_11[slice68],\n",
        "       train_regret_winner_12[slice68],\n",
        "       train_regret_winner_13[slice68],\n",
        "       train_regret_winner_14[slice68],\n",
        "       train_regret_winner_15[slice68],\n",
        "       train_regret_winner_16[slice68],\n",
        "       train_regret_winner_17[slice68],\n",
        "       train_regret_winner_18[slice68],\n",
        "       train_regret_winner_19[slice68],\n",
        "       train_regret_winner_20[slice68]]\n",
        "\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\n",
        "\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\n",
        "\n",
        "slice78 = 77\n",
        "\n",
        "loser78 = [train_regret_loser_1[slice78],\n",
        "       train_regret_loser_2[slice78],\n",
        "       train_regret_loser_3[slice78],\n",
        "       train_regret_loser_4[slice78],\n",
        "       train_regret_loser_5[slice78],\n",
        "       train_regret_loser_6[slice78],\n",
        "       train_regret_loser_7[slice78],\n",
        "       train_regret_loser_8[slice78],\n",
        "       train_regret_loser_9[slice78],\n",
        "       train_regret_loser_10[slice78],\n",
        "       train_regret_loser_11[slice78],\n",
        "       train_regret_loser_12[slice78],\n",
        "       train_regret_loser_13[slice78],\n",
        "       train_regret_loser_14[slice78],\n",
        "       train_regret_loser_15[slice78],\n",
        "       train_regret_loser_16[slice78],\n",
        "       train_regret_loser_17[slice78],\n",
        "       train_regret_loser_18[slice78],\n",
        "       train_regret_loser_19[slice78],\n",
        "       train_regret_loser_20[slice78]]\n",
        "\n",
        "winner78 = [train_regret_winner_1[slice78],\n",
        "       train_regret_winner_2[slice78],\n",
        "       train_regret_winner_3[slice78],\n",
        "       train_regret_winner_4[slice78],\n",
        "       train_regret_winner_5[slice78],\n",
        "       train_regret_winner_6[slice78],\n",
        "       train_regret_winner_7[slice78],\n",
        "       train_regret_winner_8[slice78],\n",
        "       train_regret_winner_9[slice78],\n",
        "       train_regret_winner_10[slice78],\n",
        "       train_regret_winner_11[slice78],\n",
        "       train_regret_winner_12[slice78],\n",
        "       train_regret_winner_13[slice78],\n",
        "       train_regret_winner_14[slice78],\n",
        "       train_regret_winner_15[slice78],\n",
        "       train_regret_winner_16[slice78],\n",
        "       train_regret_winner_17[slice78],\n",
        "       train_regret_winner_18[slice78],\n",
        "       train_regret_winner_19[slice78],\n",
        "       train_regret_winner_20[slice78]]\n",
        "\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\n",
        "\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\n",
        "\n",
        "slice88 = 87\n",
        "\n",
        "loser88 = [train_regret_loser_1[slice88],\n",
        "       train_regret_loser_2[slice88],\n",
        "       train_regret_loser_3[slice88],\n",
        "       train_regret_loser_4[slice88],\n",
        "       train_regret_loser_5[slice88],\n",
        "       train_regret_loser_6[slice88],\n",
        "       train_regret_loser_7[slice88],\n",
        "       train_regret_loser_8[slice88],\n",
        "       train_regret_loser_9[slice88],\n",
        "       train_regret_loser_10[slice88],\n",
        "       train_regret_loser_11[slice88],\n",
        "       train_regret_loser_12[slice88],\n",
        "       train_regret_loser_13[slice88],\n",
        "       train_regret_loser_14[slice88],\n",
        "       train_regret_loser_15[slice88],\n",
        "       train_regret_loser_16[slice88],\n",
        "       train_regret_loser_17[slice88],\n",
        "       train_regret_loser_18[slice88],\n",
        "       train_regret_loser_19[slice88],\n",
        "       train_regret_loser_20[slice88]]\n",
        "\n",
        "winner88 = [train_regret_winner_1[slice88],\n",
        "       train_regret_winner_2[slice88],\n",
        "       train_regret_winner_3[slice88],\n",
        "       train_regret_winner_4[slice88],\n",
        "       train_regret_winner_5[slice88],\n",
        "       train_regret_winner_6[slice88],\n",
        "       train_regret_winner_7[slice88],\n",
        "       train_regret_winner_8[slice88],\n",
        "       train_regret_winner_9[slice88],\n",
        "       train_regret_winner_10[slice88],\n",
        "       train_regret_winner_11[slice88],\n",
        "       train_regret_winner_12[slice88],\n",
        "       train_regret_winner_13[slice88],\n",
        "       train_regret_winner_14[slice88],\n",
        "       train_regret_winner_15[slice88],\n",
        "       train_regret_winner_16[slice88],\n",
        "       train_regret_winner_17[slice88],\n",
        "       train_regret_winner_18[slice88],\n",
        "       train_regret_winner_19[slice88],\n",
        "       train_regret_winner_20[slice88]]\n",
        "\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\n",
        "\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\n",
        "\n",
        "slice98 = 97\n",
        "\n",
        "loser98 = [train_regret_loser_1[slice98],\n",
        "       train_regret_loser_2[slice98],\n",
        "       train_regret_loser_3[slice98],\n",
        "       train_regret_loser_4[slice98],\n",
        "       train_regret_loser_5[slice98],\n",
        "       train_regret_loser_6[slice98],\n",
        "       train_regret_loser_7[slice98],\n",
        "       train_regret_loser_8[slice98],\n",
        "       train_regret_loser_9[slice98],\n",
        "       train_regret_loser_10[slice98],\n",
        "       train_regret_loser_11[slice98],\n",
        "       train_regret_loser_12[slice98],\n",
        "       train_regret_loser_13[slice98],\n",
        "       train_regret_loser_14[slice98],\n",
        "       train_regret_loser_15[slice98],\n",
        "       train_regret_loser_16[slice98],\n",
        "       train_regret_loser_17[slice98],\n",
        "       train_regret_loser_18[slice98],\n",
        "       train_regret_loser_19[slice98],\n",
        "       train_regret_loser_20[slice98]]\n",
        "\n",
        "winner98 = [train_regret_winner_1[slice98],\n",
        "       train_regret_winner_2[slice98],\n",
        "       train_regret_winner_3[slice98],\n",
        "       train_regret_winner_4[slice98],\n",
        "       train_regret_winner_5[slice98],\n",
        "       train_regret_winner_6[slice98],\n",
        "       train_regret_winner_7[slice98],\n",
        "       train_regret_winner_8[slice98],\n",
        "       train_regret_winner_9[slice98],\n",
        "       train_regret_winner_10[slice98],\n",
        "       train_regret_winner_11[slice98],\n",
        "       train_regret_winner_12[slice98],\n",
        "       train_regret_winner_13[slice98],\n",
        "       train_regret_winner_14[slice98],\n",
        "       train_regret_winner_15[slice98],\n",
        "       train_regret_winner_16[slice98],\n",
        "       train_regret_winner_17[slice98],\n",
        "       train_regret_winner_18[slice98],\n",
        "       train_regret_winner_19[slice98],\n",
        "       train_regret_winner_20[slice98]]\n",
        "\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\n",
        "\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\n",
        "\n",
        "slice9 = 8\n",
        "\n",
        "loser9 = [train_regret_loser_1[slice9],\n",
        "       train_regret_loser_2[slice9],\n",
        "       train_regret_loser_3[slice9],\n",
        "       train_regret_loser_4[slice9],\n",
        "       train_regret_loser_5[slice9],\n",
        "       train_regret_loser_6[slice9],\n",
        "       train_regret_loser_7[slice9],\n",
        "       train_regret_loser_8[slice9],\n",
        "       train_regret_loser_9[slice9],\n",
        "       train_regret_loser_10[slice9],\n",
        "       train_regret_loser_11[slice9],\n",
        "       train_regret_loser_12[slice9],\n",
        "       train_regret_loser_13[slice9],\n",
        "       train_regret_loser_14[slice9],\n",
        "       train_regret_loser_15[slice9],\n",
        "       train_regret_loser_16[slice9],\n",
        "       train_regret_loser_17[slice9],\n",
        "       train_regret_loser_18[slice9],\n",
        "       train_regret_loser_19[slice9],\n",
        "       train_regret_loser_20[slice9]]\n",
        "\n",
        "winner9 = [train_regret_winner_1[slice9],\n",
        "       train_regret_winner_2[slice9],\n",
        "       train_regret_winner_3[slice9],\n",
        "       train_regret_winner_4[slice9],\n",
        "       train_regret_winner_5[slice9],\n",
        "       train_regret_winner_6[slice9],\n",
        "       train_regret_winner_7[slice9],\n",
        "       train_regret_winner_8[slice9],\n",
        "       train_regret_winner_9[slice9],\n",
        "       train_regret_winner_10[slice9],\n",
        "       train_regret_winner_11[slice9],\n",
        "       train_regret_winner_12[slice9],\n",
        "       train_regret_winner_13[slice9],\n",
        "       train_regret_winner_14[slice9],\n",
        "       train_regret_winner_15[slice9],\n",
        "       train_regret_winner_16[slice9],\n",
        "       train_regret_winner_17[slice9],\n",
        "       train_regret_winner_18[slice9],\n",
        "       train_regret_winner_19[slice9],\n",
        "       train_regret_winner_20[slice9]]\n",
        "\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\n",
        "\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\n",
        "\n",
        "slice19 = 18\n",
        "\n",
        "loser19 = [train_regret_loser_1[slice19],\n",
        "       train_regret_loser_2[slice19],\n",
        "       train_regret_loser_3[slice19],\n",
        "       train_regret_loser_4[slice19],\n",
        "       train_regret_loser_5[slice19],\n",
        "       train_regret_loser_6[slice19],\n",
        "       train_regret_loser_7[slice19],\n",
        "       train_regret_loser_8[slice19],\n",
        "       train_regret_loser_9[slice19],\n",
        "       train_regret_loser_10[slice19],\n",
        "       train_regret_loser_11[slice19],\n",
        "       train_regret_loser_12[slice19],\n",
        "       train_regret_loser_13[slice19],\n",
        "       train_regret_loser_14[slice19],\n",
        "       train_regret_loser_15[slice19],\n",
        "       train_regret_loser_16[slice19],\n",
        "       train_regret_loser_17[slice19],\n",
        "       train_regret_loser_18[slice19],\n",
        "       train_regret_loser_19[slice19],\n",
        "       train_regret_loser_20[slice19]]\n",
        "\n",
        "winner19 = [train_regret_winner_1[slice19],\n",
        "       train_regret_winner_2[slice19],\n",
        "       train_regret_winner_3[slice19],\n",
        "       train_regret_winner_4[slice19],\n",
        "       train_regret_winner_5[slice19],\n",
        "       train_regret_winner_6[slice19],\n",
        "       train_regret_winner_7[slice19],\n",
        "       train_regret_winner_8[slice19],\n",
        "       train_regret_winner_9[slice19],\n",
        "       train_regret_winner_10[slice19],\n",
        "       train_regret_winner_11[slice19],\n",
        "       train_regret_winner_12[slice19],\n",
        "       train_regret_winner_13[slice19],\n",
        "       train_regret_winner_14[slice19],\n",
        "       train_regret_winner_15[slice19],\n",
        "       train_regret_winner_16[slice19],\n",
        "       train_regret_winner_17[slice19],\n",
        "       train_regret_winner_18[slice19],\n",
        "       train_regret_winner_19[slice19],\n",
        "       train_regret_winner_20[slice19]]\n",
        "\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\n",
        "\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\n",
        "\n",
        "slice29 = 28\n",
        "\n",
        "loser29 = [train_regret_loser_1[slice29],\n",
        "       train_regret_loser_2[slice29],\n",
        "       train_regret_loser_3[slice29],\n",
        "       train_regret_loser_4[slice29],\n",
        "       train_regret_loser_5[slice29],\n",
        "       train_regret_loser_6[slice29],\n",
        "       train_regret_loser_7[slice29],\n",
        "       train_regret_loser_8[slice29],\n",
        "       train_regret_loser_9[slice29],\n",
        "       train_regret_loser_10[slice29],\n",
        "       train_regret_loser_11[slice29],\n",
        "       train_regret_loser_12[slice29],\n",
        "       train_regret_loser_13[slice29],\n",
        "       train_regret_loser_14[slice29],\n",
        "       train_regret_loser_15[slice29],\n",
        "       train_regret_loser_16[slice29],\n",
        "       train_regret_loser_17[slice29],\n",
        "       train_regret_loser_18[slice29],\n",
        "       train_regret_loser_19[slice29],\n",
        "       train_regret_loser_20[slice29]]\n",
        "\n",
        "winner29 = [train_regret_winner_1[slice29],\n",
        "       train_regret_winner_2[slice29],\n",
        "       train_regret_winner_3[slice29],\n",
        "       train_regret_winner_4[slice29],\n",
        "       train_regret_winner_5[slice29],\n",
        "       train_regret_winner_6[slice29],\n",
        "       train_regret_winner_7[slice29],\n",
        "       train_regret_winner_8[slice29],\n",
        "       train_regret_winner_9[slice29],\n",
        "       train_regret_winner_10[slice29],\n",
        "       train_regret_winner_11[slice29],\n",
        "       train_regret_winner_12[slice29],\n",
        "       train_regret_winner_13[slice29],\n",
        "       train_regret_winner_14[slice29],\n",
        "       train_regret_winner_15[slice29],\n",
        "       train_regret_winner_16[slice29],\n",
        "       train_regret_winner_17[slice29],\n",
        "       train_regret_winner_18[slice29],\n",
        "       train_regret_winner_19[slice29],\n",
        "       train_regret_winner_20[slice29]]\n",
        "\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\n",
        "\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\n",
        "\n",
        "slice39 = 38\n",
        "\n",
        "loser39 = [train_regret_loser_1[slice39],\n",
        "       train_regret_loser_2[slice39],\n",
        "       train_regret_loser_3[slice39],\n",
        "       train_regret_loser_4[slice39],\n",
        "       train_regret_loser_5[slice39],\n",
        "       train_regret_loser_6[slice39],\n",
        "       train_regret_loser_7[slice39],\n",
        "       train_regret_loser_8[slice39],\n",
        "       train_regret_loser_9[slice39],\n",
        "       train_regret_loser_10[slice39],\n",
        "       train_regret_loser_11[slice39],\n",
        "       train_regret_loser_12[slice39],\n",
        "       train_regret_loser_13[slice39],\n",
        "       train_regret_loser_14[slice39],\n",
        "       train_regret_loser_15[slice39],\n",
        "       train_regret_loser_16[slice39],\n",
        "       train_regret_loser_17[slice39],\n",
        "       train_regret_loser_18[slice39],\n",
        "       train_regret_loser_19[slice39],\n",
        "       train_regret_loser_20[slice39]]\n",
        "\n",
        "winner39 = [train_regret_winner_1[slice39],\n",
        "       train_regret_winner_2[slice39],\n",
        "       train_regret_winner_3[slice39],\n",
        "       train_regret_winner_4[slice39],\n",
        "       train_regret_winner_5[slice39],\n",
        "       train_regret_winner_6[slice39],\n",
        "       train_regret_winner_7[slice39],\n",
        "       train_regret_winner_8[slice39],\n",
        "       train_regret_winner_9[slice39],\n",
        "       train_regret_winner_10[slice39],\n",
        "       train_regret_winner_11[slice39],\n",
        "       train_regret_winner_12[slice39],\n",
        "       train_regret_winner_13[slice39],\n",
        "       train_regret_winner_14[slice39],\n",
        "       train_regret_winner_15[slice39],\n",
        "       train_regret_winner_16[slice39],\n",
        "       train_regret_winner_17[slice39],\n",
        "       train_regret_winner_18[slice39],\n",
        "       train_regret_winner_19[slice39],\n",
        "       train_regret_winner_20[slice39]]\n",
        "\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\n",
        "\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\n",
        "\n",
        "slice49 = 48\n",
        "\n",
        "loser49 = [train_regret_loser_1[slice49],\n",
        "       train_regret_loser_2[slice49],\n",
        "       train_regret_loser_3[slice49],\n",
        "       train_regret_loser_4[slice49],\n",
        "       train_regret_loser_5[slice49],\n",
        "       train_regret_loser_6[slice49],\n",
        "       train_regret_loser_7[slice49],\n",
        "       train_regret_loser_8[slice49],\n",
        "       train_regret_loser_9[slice49],\n",
        "       train_regret_loser_10[slice49],\n",
        "       train_regret_loser_11[slice49],\n",
        "       train_regret_loser_12[slice49],\n",
        "       train_regret_loser_13[slice49],\n",
        "       train_regret_loser_14[slice49],\n",
        "       train_regret_loser_15[slice49],\n",
        "       train_regret_loser_16[slice49],\n",
        "       train_regret_loser_17[slice49],\n",
        "       train_regret_loser_18[slice49],\n",
        "       train_regret_loser_19[slice49],\n",
        "       train_regret_loser_20[slice49]]\n",
        "\n",
        "winner49 = [train_regret_winner_1[slice49],\n",
        "       train_regret_winner_2[slice49],\n",
        "       train_regret_winner_3[slice49],\n",
        "       train_regret_winner_4[slice49],\n",
        "       train_regret_winner_5[slice49],\n",
        "       train_regret_winner_6[slice49],\n",
        "       train_regret_winner_7[slice49],\n",
        "       train_regret_winner_8[slice49],\n",
        "       train_regret_winner_9[slice49],\n",
        "       train_regret_winner_10[slice49],\n",
        "       train_regret_winner_11[slice49],\n",
        "       train_regret_winner_12[slice49],\n",
        "       train_regret_winner_13[slice49],\n",
        "       train_regret_winner_14[slice49],\n",
        "       train_regret_winner_15[slice49],\n",
        "       train_regret_winner_16[slice49],\n",
        "       train_regret_winner_17[slice49],\n",
        "       train_regret_winner_18[slice49],\n",
        "       train_regret_winner_19[slice49],\n",
        "       train_regret_winner_20[slice49]]\n",
        "\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\n",
        "\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\n",
        "\n",
        "slice59 = 58\n",
        "\n",
        "loser59 = [train_regret_loser_1[slice59],\n",
        "       train_regret_loser_2[slice59],\n",
        "       train_regret_loser_3[slice59],\n",
        "       train_regret_loser_4[slice59],\n",
        "       train_regret_loser_5[slice59],\n",
        "       train_regret_loser_6[slice59],\n",
        "       train_regret_loser_7[slice59],\n",
        "       train_regret_loser_8[slice59],\n",
        "       train_regret_loser_9[slice59],\n",
        "       train_regret_loser_10[slice59],\n",
        "       train_regret_loser_11[slice59],\n",
        "       train_regret_loser_12[slice59],\n",
        "       train_regret_loser_13[slice59],\n",
        "       train_regret_loser_14[slice59],\n",
        "       train_regret_loser_15[slice59],\n",
        "       train_regret_loser_16[slice59],\n",
        "       train_regret_loser_17[slice59],\n",
        "       train_regret_loser_18[slice59],\n",
        "       train_regret_loser_19[slice59],\n",
        "       train_regret_loser_20[slice59]]\n",
        "\n",
        "winner59 = [train_regret_winner_1[slice59],\n",
        "       train_regret_winner_2[slice59],\n",
        "       train_regret_winner_3[slice59],\n",
        "       train_regret_winner_4[slice59],\n",
        "       train_regret_winner_5[slice59],\n",
        "       train_regret_winner_6[slice59],\n",
        "       train_regret_winner_7[slice59],\n",
        "       train_regret_winner_8[slice59],\n",
        "       train_regret_winner_9[slice59],\n",
        "       train_regret_winner_10[slice59],\n",
        "       train_regret_winner_11[slice59],\n",
        "       train_regret_winner_12[slice59],\n",
        "       train_regret_winner_13[slice59],\n",
        "       train_regret_winner_14[slice59],\n",
        "       train_regret_winner_15[slice59],\n",
        "       train_regret_winner_16[slice59],\n",
        "       train_regret_winner_17[slice59],\n",
        "       train_regret_winner_18[slice59],\n",
        "       train_regret_winner_19[slice59],\n",
        "       train_regret_winner_20[slice59]]\n",
        "\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\n",
        "\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\n",
        "\n",
        "slice69 = 68\n",
        "\n",
        "loser69 = [train_regret_loser_1[slice69],\n",
        "       train_regret_loser_2[slice69],\n",
        "       train_regret_loser_3[slice69],\n",
        "       train_regret_loser_4[slice69],\n",
        "       train_regret_loser_5[slice69],\n",
        "       train_regret_loser_6[slice69],\n",
        "       train_regret_loser_7[slice69],\n",
        "       train_regret_loser_8[slice69],\n",
        "       train_regret_loser_9[slice69],\n",
        "       train_regret_loser_10[slice69],\n",
        "       train_regret_loser_11[slice69],\n",
        "       train_regret_loser_12[slice69],\n",
        "       train_regret_loser_13[slice69],\n",
        "       train_regret_loser_14[slice69],\n",
        "       train_regret_loser_15[slice69],\n",
        "       train_regret_loser_16[slice69],\n",
        "       train_regret_loser_17[slice69],\n",
        "       train_regret_loser_18[slice69],\n",
        "       train_regret_loser_19[slice69],\n",
        "       train_regret_loser_20[slice69]]\n",
        "\n",
        "winner69 = [train_regret_winner_1[slice69],\n",
        "       train_regret_winner_2[slice69],\n",
        "       train_regret_winner_3[slice69],\n",
        "       train_regret_winner_4[slice69],\n",
        "       train_regret_winner_5[slice69],\n",
        "       train_regret_winner_6[slice69],\n",
        "       train_regret_winner_7[slice69],\n",
        "       train_regret_winner_8[slice69],\n",
        "       train_regret_winner_9[slice69],\n",
        "       train_regret_winner_10[slice69],\n",
        "       train_regret_winner_11[slice69],\n",
        "       train_regret_winner_12[slice69],\n",
        "       train_regret_winner_13[slice69],\n",
        "       train_regret_winner_14[slice69],\n",
        "       train_regret_winner_15[slice69],\n",
        "       train_regret_winner_16[slice69],\n",
        "       train_regret_winner_17[slice69],\n",
        "       train_regret_winner_18[slice69],\n",
        "       train_regret_winner_19[slice69],\n",
        "       train_regret_winner_20[slice69]]\n",
        "\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\n",
        "\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\n",
        "\n",
        "slice79 = 78\n",
        "\n",
        "loser79 = [train_regret_loser_1[slice79],\n",
        "       train_regret_loser_2[slice79],\n",
        "       train_regret_loser_3[slice79],\n",
        "       train_regret_loser_4[slice79],\n",
        "       train_regret_loser_5[slice79],\n",
        "       train_regret_loser_6[slice79],\n",
        "       train_regret_loser_7[slice79],\n",
        "       train_regret_loser_8[slice79],\n",
        "       train_regret_loser_9[slice79],\n",
        "       train_regret_loser_10[slice79],\n",
        "       train_regret_loser_11[slice79],\n",
        "       train_regret_loser_12[slice79],\n",
        "       train_regret_loser_13[slice79],\n",
        "       train_regret_loser_14[slice79],\n",
        "       train_regret_loser_15[slice79],\n",
        "       train_regret_loser_16[slice79],\n",
        "       train_regret_loser_17[slice79],\n",
        "       train_regret_loser_18[slice79],\n",
        "       train_regret_loser_19[slice79],\n",
        "       train_regret_loser_20[slice79]]\n",
        "\n",
        "winner79 = [train_regret_winner_1[slice79],\n",
        "       train_regret_winner_2[slice79],\n",
        "       train_regret_winner_3[slice79],\n",
        "       train_regret_winner_4[slice79],\n",
        "       train_regret_winner_5[slice79],\n",
        "       train_regret_winner_6[slice79],\n",
        "       train_regret_winner_7[slice79],\n",
        "       train_regret_winner_8[slice79],\n",
        "       train_regret_winner_9[slice79],\n",
        "       train_regret_winner_10[slice79],\n",
        "       train_regret_winner_11[slice79],\n",
        "       train_regret_winner_12[slice79],\n",
        "       train_regret_winner_13[slice79],\n",
        "       train_regret_winner_14[slice79],\n",
        "       train_regret_winner_15[slice79],\n",
        "       train_regret_winner_16[slice79],\n",
        "       train_regret_winner_17[slice79],\n",
        "       train_regret_winner_18[slice79],\n",
        "       train_regret_winner_19[slice79],\n",
        "       train_regret_winner_20[slice79]]\n",
        "\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\n",
        "\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\n",
        "\n",
        "slice89 = 88\n",
        "\n",
        "loser89 = [train_regret_loser_1[slice89],\n",
        "       train_regret_loser_2[slice89],\n",
        "       train_regret_loser_3[slice89],\n",
        "       train_regret_loser_4[slice89],\n",
        "       train_regret_loser_5[slice89],\n",
        "       train_regret_loser_6[slice89],\n",
        "       train_regret_loser_7[slice89],\n",
        "       train_regret_loser_8[slice89],\n",
        "       train_regret_loser_9[slice89],\n",
        "       train_regret_loser_10[slice89],\n",
        "       train_regret_loser_11[slice89],\n",
        "       train_regret_loser_12[slice89],\n",
        "       train_regret_loser_13[slice89],\n",
        "       train_regret_loser_14[slice89],\n",
        "       train_regret_loser_15[slice89],\n",
        "       train_regret_loser_16[slice89],\n",
        "       train_regret_loser_17[slice89],\n",
        "       train_regret_loser_18[slice89],\n",
        "       train_regret_loser_19[slice89],\n",
        "       train_regret_loser_20[slice89]]\n",
        "\n",
        "winner89 = [train_regret_winner_1[slice89],\n",
        "       train_regret_winner_2[slice89],\n",
        "       train_regret_winner_3[slice89],\n",
        "       train_regret_winner_4[slice89],\n",
        "       train_regret_winner_5[slice89],\n",
        "       train_regret_winner_6[slice89],\n",
        "       train_regret_winner_7[slice89],\n",
        "       train_regret_winner_8[slice89],\n",
        "       train_regret_winner_9[slice89],\n",
        "       train_regret_winner_10[slice89],\n",
        "       train_regret_winner_11[slice89],\n",
        "       train_regret_winner_12[slice89],\n",
        "       train_regret_winner_13[slice89],\n",
        "       train_regret_winner_14[slice89],\n",
        "       train_regret_winner_15[slice89],\n",
        "       train_regret_winner_16[slice89],\n",
        "       train_regret_winner_17[slice89],\n",
        "       train_regret_winner_18[slice89],\n",
        "       train_regret_winner_19[slice89],\n",
        "       train_regret_winner_20[slice89]]\n",
        "\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\n",
        "\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\n",
        "\n",
        "slice99 = 98\n",
        "\n",
        "loser99 = [train_regret_loser_1[slice99],\n",
        "       train_regret_loser_2[slice99],\n",
        "       train_regret_loser_3[slice99],\n",
        "       train_regret_loser_4[slice99],\n",
        "       train_regret_loser_5[slice99],\n",
        "       train_regret_loser_6[slice99],\n",
        "       train_regret_loser_7[slice99],\n",
        "       train_regret_loser_8[slice99],\n",
        "       train_regret_loser_9[slice99],\n",
        "       train_regret_loser_10[slice99],\n",
        "       train_regret_loser_11[slice99],\n",
        "       train_regret_loser_12[slice99],\n",
        "       train_regret_loser_13[slice99],\n",
        "       train_regret_loser_14[slice99],\n",
        "       train_regret_loser_15[slice99],\n",
        "       train_regret_loser_16[slice99],\n",
        "       train_regret_loser_17[slice99],\n",
        "       train_regret_loser_18[slice99],\n",
        "       train_regret_loser_19[slice99],\n",
        "       train_regret_loser_20[slice99]]\n",
        "\n",
        "winner99 = [train_regret_winner_1[slice99],\n",
        "       train_regret_winner_2[slice99],\n",
        "       train_regret_winner_3[slice99],\n",
        "       train_regret_winner_4[slice99],\n",
        "       train_regret_winner_5[slice99],\n",
        "       train_regret_winner_6[slice99],\n",
        "       train_regret_winner_7[slice99],\n",
        "       train_regret_winner_8[slice99],\n",
        "       train_regret_winner_9[slice99],\n",
        "       train_regret_winner_10[slice99],\n",
        "       train_regret_winner_11[slice99],\n",
        "       train_regret_winner_12[slice99],\n",
        "       train_regret_winner_13[slice99],\n",
        "       train_regret_winner_14[slice99],\n",
        "       train_regret_winner_15[slice99],\n",
        "       train_regret_winner_16[slice99],\n",
        "       train_regret_winner_17[slice99],\n",
        "       train_regret_winner_18[slice99],\n",
        "       train_regret_winner_19[slice99],\n",
        "       train_regret_winner_20[slice99]]\n",
        "\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\n",
        "\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\n",
        "\n",
        "slice10 = 9\n",
        "\n",
        "loser10 = [train_regret_loser_1[slice10],\n",
        "       train_regret_loser_2[slice10],\n",
        "       train_regret_loser_3[slice10],\n",
        "       train_regret_loser_4[slice10],\n",
        "       train_regret_loser_5[slice10],\n",
        "       train_regret_loser_6[slice10],\n",
        "       train_regret_loser_7[slice10],\n",
        "       train_regret_loser_8[slice10],\n",
        "       train_regret_loser_9[slice10],\n",
        "       train_regret_loser_10[slice10],\n",
        "       train_regret_loser_11[slice10],\n",
        "       train_regret_loser_12[slice10],\n",
        "       train_regret_loser_13[slice10],\n",
        "       train_regret_loser_14[slice10],\n",
        "       train_regret_loser_15[slice10],\n",
        "       train_regret_loser_16[slice10],\n",
        "       train_regret_loser_17[slice10],\n",
        "       train_regret_loser_18[slice10],\n",
        "       train_regret_loser_19[slice10],\n",
        "       train_regret_loser_20[slice10]]\n",
        "\n",
        "winner10 = [train_regret_winner_1[slice10],\n",
        "       train_regret_winner_2[slice10],\n",
        "       train_regret_winner_3[slice10],\n",
        "       train_regret_winner_4[slice10],\n",
        "       train_regret_winner_5[slice10],\n",
        "       train_regret_winner_6[slice10],\n",
        "       train_regret_winner_7[slice10],\n",
        "       train_regret_winner_8[slice10],\n",
        "       train_regret_winner_9[slice10],\n",
        "       train_regret_winner_10[slice10],\n",
        "       train_regret_winner_11[slice10],\n",
        "       train_regret_winner_12[slice10],\n",
        "       train_regret_winner_13[slice10],\n",
        "       train_regret_winner_14[slice10],\n",
        "       train_regret_winner_15[slice10],\n",
        "       train_regret_winner_16[slice10],\n",
        "       train_regret_winner_17[slice10],\n",
        "       train_regret_winner_18[slice10],\n",
        "       train_regret_winner_19[slice10],\n",
        "       train_regret_winner_20[slice10]]\n",
        "\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\n",
        "\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\n",
        "\n",
        "slice20 = 19\n",
        "\n",
        "loser20 = [train_regret_loser_1[slice20],\n",
        "       train_regret_loser_2[slice20],\n",
        "       train_regret_loser_3[slice20],\n",
        "       train_regret_loser_4[slice20],\n",
        "       train_regret_loser_5[slice20],\n",
        "       train_regret_loser_6[slice20],\n",
        "       train_regret_loser_7[slice20],\n",
        "       train_regret_loser_8[slice20],\n",
        "       train_regret_loser_9[slice20],\n",
        "       train_regret_loser_10[slice20],\n",
        "       train_regret_loser_11[slice20],\n",
        "       train_regret_loser_12[slice20],\n",
        "       train_regret_loser_13[slice20],\n",
        "       train_regret_loser_14[slice20],\n",
        "       train_regret_loser_15[slice20],\n",
        "       train_regret_loser_16[slice20],\n",
        "       train_regret_loser_17[slice20],\n",
        "       train_regret_loser_18[slice20],\n",
        "       train_regret_loser_19[slice20],\n",
        "       train_regret_loser_20[slice20]]\n",
        "\n",
        "winner20 = [train_regret_winner_1[slice20],\n",
        "       train_regret_winner_2[slice20],\n",
        "       train_regret_winner_3[slice20],\n",
        "       train_regret_winner_4[slice20],\n",
        "       train_regret_winner_5[slice20],\n",
        "       train_regret_winner_6[slice20],\n",
        "       train_regret_winner_7[slice20],\n",
        "       train_regret_winner_8[slice20],\n",
        "       train_regret_winner_9[slice20],\n",
        "       train_regret_winner_10[slice20],\n",
        "       train_regret_winner_11[slice20],\n",
        "       train_regret_winner_12[slice20],\n",
        "       train_regret_winner_13[slice20],\n",
        "       train_regret_winner_14[slice20],\n",
        "       train_regret_winner_15[slice20],\n",
        "       train_regret_winner_16[slice20],\n",
        "       train_regret_winner_17[slice20],\n",
        "       train_regret_winner_18[slice20],\n",
        "       train_regret_winner_19[slice20],\n",
        "       train_regret_winner_20[slice20]]\n",
        "\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\n",
        "\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\n",
        "\n",
        "slice30 = 29\n",
        "\n",
        "loser30 = [train_regret_loser_1[slice30],\n",
        "       train_regret_loser_2[slice30],\n",
        "       train_regret_loser_3[slice30],\n",
        "       train_regret_loser_4[slice30],\n",
        "       train_regret_loser_5[slice30],\n",
        "       train_regret_loser_6[slice30],\n",
        "       train_regret_loser_7[slice30],\n",
        "       train_regret_loser_8[slice30],\n",
        "       train_regret_loser_9[slice30],\n",
        "       train_regret_loser_10[slice30],\n",
        "       train_regret_loser_11[slice30],\n",
        "       train_regret_loser_12[slice30],\n",
        "       train_regret_loser_13[slice30],\n",
        "       train_regret_loser_14[slice30],\n",
        "       train_regret_loser_15[slice30],\n",
        "       train_regret_loser_16[slice30],\n",
        "       train_regret_loser_17[slice30],\n",
        "       train_regret_loser_18[slice30],\n",
        "       train_regret_loser_19[slice30],\n",
        "       train_regret_loser_20[slice30]]\n",
        "\n",
        "winner30 = [train_regret_winner_1[slice30],\n",
        "       train_regret_winner_2[slice30],\n",
        "       train_regret_winner_3[slice30],\n",
        "       train_regret_winner_4[slice30],\n",
        "       train_regret_winner_5[slice30],\n",
        "       train_regret_winner_6[slice30],\n",
        "       train_regret_winner_7[slice30],\n",
        "       train_regret_winner_8[slice30],\n",
        "       train_regret_winner_9[slice30],\n",
        "       train_regret_winner_10[slice30],\n",
        "       train_regret_winner_11[slice30],\n",
        "       train_regret_winner_12[slice30],\n",
        "       train_regret_winner_13[slice30],\n",
        "       train_regret_winner_14[slice30],\n",
        "       train_regret_winner_15[slice30],\n",
        "       train_regret_winner_16[slice30],\n",
        "       train_regret_winner_17[slice30],\n",
        "       train_regret_winner_18[slice30],\n",
        "       train_regret_winner_19[slice30],\n",
        "       train_regret_winner_20[slice30]]\n",
        "\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\n",
        "\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\n",
        "\n",
        "slice40 = 39\n",
        "\n",
        "loser40 = [train_regret_loser_1[slice40],\n",
        "       train_regret_loser_2[slice40],\n",
        "       train_regret_loser_3[slice40],\n",
        "       train_regret_loser_4[slice40],\n",
        "       train_regret_loser_5[slice40],\n",
        "       train_regret_loser_6[slice40],\n",
        "       train_regret_loser_7[slice40],\n",
        "       train_regret_loser_8[slice40],\n",
        "       train_regret_loser_9[slice40],\n",
        "       train_regret_loser_10[slice40],\n",
        "       train_regret_loser_11[slice40],\n",
        "       train_regret_loser_12[slice40],\n",
        "       train_regret_loser_13[slice40],\n",
        "       train_regret_loser_14[slice40],\n",
        "       train_regret_loser_15[slice40],\n",
        "       train_regret_loser_16[slice40],\n",
        "       train_regret_loser_17[slice40],\n",
        "       train_regret_loser_18[slice40],\n",
        "       train_regret_loser_19[slice40],\n",
        "       train_regret_loser_20[slice40]]\n",
        "\n",
        "winner40 = [train_regret_winner_1[slice40],\n",
        "       train_regret_winner_2[slice40],\n",
        "       train_regret_winner_3[slice40],\n",
        "       train_regret_winner_4[slice40],\n",
        "       train_regret_winner_5[slice40],\n",
        "       train_regret_winner_6[slice40],\n",
        "       train_regret_winner_7[slice40],\n",
        "       train_regret_winner_8[slice40],\n",
        "       train_regret_winner_9[slice40],\n",
        "       train_regret_winner_10[slice40],\n",
        "       train_regret_winner_11[slice40],\n",
        "       train_regret_winner_12[slice40],\n",
        "       train_regret_winner_13[slice40],\n",
        "       train_regret_winner_14[slice40],\n",
        "       train_regret_winner_15[slice40],\n",
        "       train_regret_winner_16[slice40],\n",
        "       train_regret_winner_17[slice40],\n",
        "       train_regret_winner_18[slice40],\n",
        "       train_regret_winner_19[slice40],\n",
        "       train_regret_winner_20[slice40]]\n",
        "\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\n",
        "\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\n",
        "\n",
        "slice50 = 49\n",
        "\n",
        "loser50 = [train_regret_loser_1[slice50],\n",
        "       train_regret_loser_2[slice50],\n",
        "       train_regret_loser_3[slice50],\n",
        "       train_regret_loser_4[slice50],\n",
        "       train_regret_loser_5[slice50],\n",
        "       train_regret_loser_6[slice50],\n",
        "       train_regret_loser_7[slice50],\n",
        "       train_regret_loser_8[slice50],\n",
        "       train_regret_loser_9[slice50],\n",
        "       train_regret_loser_10[slice50],\n",
        "       train_regret_loser_11[slice50],\n",
        "       train_regret_loser_12[slice50],\n",
        "       train_regret_loser_13[slice50],\n",
        "       train_regret_loser_14[slice50],\n",
        "       train_regret_loser_15[slice50],\n",
        "       train_regret_loser_16[slice50],\n",
        "       train_regret_loser_17[slice50],\n",
        "       train_regret_loser_18[slice50],\n",
        "       train_regret_loser_19[slice50],\n",
        "       train_regret_loser_20[slice50]]\n",
        "\n",
        "winner50 = [train_regret_winner_1[slice50],\n",
        "       train_regret_winner_2[slice50],\n",
        "       train_regret_winner_3[slice50],\n",
        "       train_regret_winner_4[slice50],\n",
        "       train_regret_winner_5[slice50],\n",
        "       train_regret_winner_6[slice50],\n",
        "       train_regret_winner_7[slice50],\n",
        "       train_regret_winner_8[slice50],\n",
        "       train_regret_winner_9[slice50],\n",
        "       train_regret_winner_10[slice50],\n",
        "       train_regret_winner_11[slice50],\n",
        "       train_regret_winner_12[slice50],\n",
        "       train_regret_winner_13[slice50],\n",
        "       train_regret_winner_14[slice50],\n",
        "       train_regret_winner_15[slice50],\n",
        "       train_regret_winner_16[slice50],\n",
        "       train_regret_winner_17[slice50],\n",
        "       train_regret_winner_18[slice50],\n",
        "       train_regret_winner_19[slice50],\n",
        "       train_regret_winner_20[slice50]]\n",
        "\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\n",
        "\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\n",
        "\n",
        "slice60 = 59\n",
        "\n",
        "loser60 = [train_regret_loser_1[slice60],\n",
        "       train_regret_loser_2[slice60],\n",
        "       train_regret_loser_3[slice60],\n",
        "       train_regret_loser_4[slice60],\n",
        "       train_regret_loser_5[slice60],\n",
        "       train_regret_loser_6[slice60],\n",
        "       train_regret_loser_7[slice60],\n",
        "       train_regret_loser_8[slice60],\n",
        "       train_regret_loser_9[slice60],\n",
        "       train_regret_loser_10[slice60],\n",
        "       train_regret_loser_11[slice60],\n",
        "       train_regret_loser_12[slice60],\n",
        "       train_regret_loser_13[slice60],\n",
        "       train_regret_loser_14[slice60],\n",
        "       train_regret_loser_15[slice60],\n",
        "       train_regret_loser_16[slice60],\n",
        "       train_regret_loser_17[slice60],\n",
        "       train_regret_loser_18[slice60],\n",
        "       train_regret_loser_19[slice60],\n",
        "       train_regret_loser_20[slice60]]\n",
        "\n",
        "winner60 = [train_regret_winner_1[slice60],\n",
        "       train_regret_winner_2[slice60],\n",
        "       train_regret_winner_3[slice60],\n",
        "       train_regret_winner_4[slice60],\n",
        "       train_regret_winner_5[slice60],\n",
        "       train_regret_winner_6[slice60],\n",
        "       train_regret_winner_7[slice60],\n",
        "       train_regret_winner_8[slice60],\n",
        "       train_regret_winner_9[slice60],\n",
        "       train_regret_winner_10[slice60],\n",
        "       train_regret_winner_11[slice60],\n",
        "       train_regret_winner_12[slice60],\n",
        "       train_regret_winner_13[slice60],\n",
        "       train_regret_winner_14[slice60],\n",
        "       train_regret_winner_15[slice60],\n",
        "       train_regret_winner_16[slice60],\n",
        "       train_regret_winner_17[slice60],\n",
        "       train_regret_winner_18[slice60],\n",
        "       train_regret_winner_19[slice60],\n",
        "       train_regret_winner_20[slice60]]\n",
        "\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\n",
        "\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\n",
        "\n",
        "slice70 = 69\n",
        "\n",
        "loser70 = [train_regret_loser_1[slice70],\n",
        "       train_regret_loser_2[slice70],\n",
        "       train_regret_loser_3[slice70],\n",
        "       train_regret_loser_4[slice70],\n",
        "       train_regret_loser_5[slice70],\n",
        "       train_regret_loser_6[slice70],\n",
        "       train_regret_loser_7[slice70],\n",
        "       train_regret_loser_8[slice70],\n",
        "       train_regret_loser_9[slice70],\n",
        "       train_regret_loser_10[slice70],\n",
        "       train_regret_loser_11[slice70],\n",
        "       train_regret_loser_12[slice70],\n",
        "       train_regret_loser_13[slice70],\n",
        "       train_regret_loser_14[slice70],\n",
        "       train_regret_loser_15[slice70],\n",
        "       train_regret_loser_16[slice70],\n",
        "       train_regret_loser_17[slice70],\n",
        "       train_regret_loser_18[slice70],\n",
        "       train_regret_loser_19[slice70],\n",
        "       train_regret_loser_20[slice70]]\n",
        "\n",
        "winner70 = [train_regret_winner_1[slice70],\n",
        "       train_regret_winner_2[slice70],\n",
        "       train_regret_winner_3[slice70],\n",
        "       train_regret_winner_4[slice70],\n",
        "       train_regret_winner_5[slice70],\n",
        "       train_regret_winner_6[slice70],\n",
        "       train_regret_winner_7[slice70],\n",
        "       train_regret_winner_8[slice70],\n",
        "       train_regret_winner_9[slice70],\n",
        "       train_regret_winner_10[slice70],\n",
        "       train_regret_winner_11[slice70],\n",
        "       train_regret_winner_12[slice70],\n",
        "       train_regret_winner_13[slice70],\n",
        "       train_regret_winner_14[slice70],\n",
        "       train_regret_winner_15[slice70],\n",
        "       train_regret_winner_16[slice70],\n",
        "       train_regret_winner_17[slice70],\n",
        "       train_regret_winner_18[slice70],\n",
        "       train_regret_winner_19[slice70],\n",
        "       train_regret_winner_20[slice70]]\n",
        "\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\n",
        "\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\n",
        "\n",
        "slice80 = 79\n",
        "\n",
        "loser80 = [train_regret_loser_1[slice80],\n",
        "       train_regret_loser_2[slice80],\n",
        "       train_regret_loser_3[slice80],\n",
        "       train_regret_loser_4[slice80],\n",
        "       train_regret_loser_5[slice80],\n",
        "       train_regret_loser_6[slice80],\n",
        "       train_regret_loser_7[slice80],\n",
        "       train_regret_loser_8[slice80],\n",
        "       train_regret_loser_9[slice80],\n",
        "       train_regret_loser_10[slice80],\n",
        "       train_regret_loser_11[slice80],\n",
        "       train_regret_loser_12[slice80],\n",
        "       train_regret_loser_13[slice80],\n",
        "       train_regret_loser_14[slice80],\n",
        "       train_regret_loser_15[slice80],\n",
        "       train_regret_loser_16[slice80],\n",
        "       train_regret_loser_17[slice80],\n",
        "       train_regret_loser_18[slice80],\n",
        "       train_regret_loser_19[slice80],\n",
        "       train_regret_loser_20[slice80]]\n",
        "\n",
        "winner80 = [train_regret_winner_1[slice80],\n",
        "       train_regret_winner_2[slice80],\n",
        "       train_regret_winner_3[slice80],\n",
        "       train_regret_winner_4[slice80],\n",
        "       train_regret_winner_5[slice80],\n",
        "       train_regret_winner_6[slice80],\n",
        "       train_regret_winner_7[slice80],\n",
        "       train_regret_winner_8[slice80],\n",
        "       train_regret_winner_9[slice80],\n",
        "       train_regret_winner_10[slice80],\n",
        "       train_regret_winner_11[slice80],\n",
        "       train_regret_winner_12[slice80],\n",
        "       train_regret_winner_13[slice80],\n",
        "       train_regret_winner_14[slice80],\n",
        "       train_regret_winner_15[slice80],\n",
        "       train_regret_winner_16[slice80],\n",
        "       train_regret_winner_17[slice80],\n",
        "       train_regret_winner_18[slice80],\n",
        "       train_regret_winner_19[slice80],\n",
        "       train_regret_winner_20[slice80]]\n",
        "\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\n",
        "\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\n",
        "\n",
        "slice90 = 89\n",
        "\n",
        "loser90 = [train_regret_loser_1[slice90],\n",
        "       train_regret_loser_2[slice90],\n",
        "       train_regret_loser_3[slice90],\n",
        "       train_regret_loser_4[slice90],\n",
        "       train_regret_loser_5[slice90],\n",
        "       train_regret_loser_6[slice90],\n",
        "       train_regret_loser_7[slice90],\n",
        "       train_regret_loser_8[slice90],\n",
        "       train_regret_loser_9[slice90],\n",
        "       train_regret_loser_10[slice90],\n",
        "       train_regret_loser_11[slice90],\n",
        "       train_regret_loser_12[slice90],\n",
        "       train_regret_loser_13[slice90],\n",
        "       train_regret_loser_14[slice90],\n",
        "       train_regret_loser_15[slice90],\n",
        "       train_regret_loser_16[slice90],\n",
        "       train_regret_loser_17[slice90],\n",
        "       train_regret_loser_18[slice90],\n",
        "       train_regret_loser_19[slice90],\n",
        "       train_regret_loser_20[slice90]]\n",
        "\n",
        "winner90 = [train_regret_winner_1[slice90],\n",
        "       train_regret_winner_2[slice90],\n",
        "       train_regret_winner_3[slice90],\n",
        "       train_regret_winner_4[slice90],\n",
        "       train_regret_winner_5[slice90],\n",
        "       train_regret_winner_6[slice90],\n",
        "       train_regret_winner_7[slice90],\n",
        "       train_regret_winner_8[slice90],\n",
        "       train_regret_winner_9[slice90],\n",
        "       train_regret_winner_10[slice90],\n",
        "       train_regret_winner_11[slice90],\n",
        "       train_regret_winner_12[slice90],\n",
        "       train_regret_winner_13[slice90],\n",
        "       train_regret_winner_14[slice90],\n",
        "       train_regret_winner_15[slice90],\n",
        "       train_regret_winner_16[slice90],\n",
        "       train_regret_winner_17[slice90],\n",
        "       train_regret_winner_18[slice90],\n",
        "       train_regret_winner_19[slice90],\n",
        "       train_regret_winner_20[slice90]]\n",
        "\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\n",
        "\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\n",
        "\n",
        "slice100 = 99\n",
        "\n",
        "loser100 = [train_regret_loser_1[slice100],\n",
        "       train_regret_loser_2[slice100],\n",
        "       train_regret_loser_3[slice100],\n",
        "       train_regret_loser_4[slice100],\n",
        "       train_regret_loser_5[slice100],\n",
        "       train_regret_loser_6[slice100],\n",
        "       train_regret_loser_7[slice100],\n",
        "       train_regret_loser_8[slice100],\n",
        "       train_regret_loser_9[slice100],\n",
        "       train_regret_loser_10[slice100],\n",
        "       train_regret_loser_11[slice100],\n",
        "       train_regret_loser_12[slice100],\n",
        "       train_regret_loser_13[slice100],\n",
        "       train_regret_loser_14[slice100],\n",
        "       train_regret_loser_15[slice100],\n",
        "       train_regret_loser_16[slice100],\n",
        "       train_regret_loser_17[slice100],\n",
        "       train_regret_loser_18[slice100],\n",
        "       train_regret_loser_19[slice100],\n",
        "       train_regret_loser_20[slice100]]\n",
        "\n",
        "winner100 = [train_regret_winner_1[slice100],\n",
        "       train_regret_winner_2[slice100],\n",
        "       train_regret_winner_3[slice100],\n",
        "       train_regret_winner_4[slice100],\n",
        "       train_regret_winner_5[slice100],\n",
        "       train_regret_winner_6[slice100],\n",
        "       train_regret_winner_7[slice100],\n",
        "       train_regret_winner_8[slice100],\n",
        "       train_regret_winner_9[slice100],\n",
        "       train_regret_winner_10[slice100],\n",
        "       train_regret_winner_11[slice100],\n",
        "       train_regret_winner_12[slice100],\n",
        "       train_regret_winner_13[slice100],\n",
        "       train_regret_winner_14[slice100],\n",
        "       train_regret_winner_15[slice100],\n",
        "       train_regret_winner_16[slice100],\n",
        "       train_regret_winner_17[slice100],\n",
        "       train_regret_winner_18[slice100],\n",
        "       train_regret_winner_19[slice100],\n",
        "       train_regret_winner_20[slice100]]\n",
        "\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\n",
        "\n",
        "### Best training regret minimization IQR - loser:\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\n",
        "\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\n",
        "\n",
        "lower_loser = [lower_loser1,\n",
        "            lower_loser2,\n",
        "            lower_loser3,\n",
        "            lower_loser4,\n",
        "            lower_loser5,\n",
        "            lower_loser6,\n",
        "            lower_loser7,\n",
        "            lower_loser8,\n",
        "            lower_loser9,\n",
        "            lower_loser10,\n",
        "            lower_loser11,\n",
        "            lower_loser12,\n",
        "            lower_loser13,\n",
        "            lower_loser14,\n",
        "            lower_loser15,\n",
        "            lower_loser16,\n",
        "            lower_loser17,\n",
        "            lower_loser18,\n",
        "            lower_loser19,\n",
        "            lower_loser20,\n",
        "            lower_loser21,\n",
        "            lower_loser22,\n",
        "            lower_loser23,\n",
        "            lower_loser24,\n",
        "            lower_loser25,\n",
        "            lower_loser26,\n",
        "            lower_loser27,\n",
        "            lower_loser28,\n",
        "            lower_loser29,\n",
        "            lower_loser30,\n",
        "            lower_loser31,\n",
        "            lower_loser32,\n",
        "            lower_loser33,\n",
        "            lower_loser34,\n",
        "            lower_loser35,\n",
        "            lower_loser36,\n",
        "            lower_loser37,\n",
        "            lower_loser38,\n",
        "            lower_loser39,\n",
        "            lower_loser40,\n",
        "            lower_loser41,\n",
        "            lower_loser42,\n",
        "            lower_loser43,\n",
        "            lower_loser44,\n",
        "            lower_loser45,\n",
        "            lower_loser46,\n",
        "            lower_loser47,\n",
        "            lower_loser48,\n",
        "            lower_loser49,\n",
        "            lower_loser50,\n",
        "            lower_loser51,\n",
        "            lower_loser52,\n",
        "            lower_loser53,\n",
        "            lower_loser54,\n",
        "            lower_loser55,\n",
        "            lower_loser56,\n",
        "            lower_loser57,\n",
        "            lower_loser58,\n",
        "            lower_loser59,\n",
        "            lower_loser60,\n",
        "            lower_loser61,\n",
        "            lower_loser62,\n",
        "            lower_loser63,\n",
        "            lower_loser64,\n",
        "            lower_loser65,\n",
        "            lower_loser66,\n",
        "            lower_loser67,\n",
        "            lower_loser68,\n",
        "            lower_loser69,\n",
        "            lower_loser70,\n",
        "            lower_loser71,\n",
        "            lower_loser72,\n",
        "            lower_loser73,\n",
        "            lower_loser74,\n",
        "            lower_loser75,\n",
        "            lower_loser76,\n",
        "            lower_loser77,\n",
        "            lower_loser78,\n",
        "            lower_loser79,\n",
        "            lower_loser80,\n",
        "            lower_loser81,\n",
        "            lower_loser82,\n",
        "            lower_loser83,\n",
        "            lower_loser84,\n",
        "            lower_loser85,\n",
        "            lower_loser86,\n",
        "            lower_loser87,\n",
        "            lower_loser88,\n",
        "            lower_loser89,\n",
        "            lower_loser90,\n",
        "            lower_loser91,\n",
        "            lower_loser92,\n",
        "            lower_loser93,\n",
        "            lower_loser94,\n",
        "            lower_loser95,\n",
        "            lower_loser96,\n",
        "            lower_loser97,\n",
        "            lower_loser98,\n",
        "            lower_loser99,\n",
        "            lower_loser100,\n",
        "            lower_loser101]\n",
        "\n",
        "median_loser = [median_loser1,\n",
        "            median_loser2,\n",
        "            median_loser3,\n",
        "            median_loser4,\n",
        "            median_loser5,\n",
        "            median_loser6,\n",
        "            median_loser7,\n",
        "            median_loser8,\n",
        "            median_loser9,\n",
        "            median_loser10,\n",
        "            median_loser11,\n",
        "            median_loser12,\n",
        "            median_loser13,\n",
        "            median_loser14,\n",
        "            median_loser15,\n",
        "            median_loser16,\n",
        "            median_loser17,\n",
        "            median_loser18,\n",
        "            median_loser19,\n",
        "            median_loser20,\n",
        "            median_loser21,\n",
        "            median_loser22,\n",
        "            median_loser23,\n",
        "            median_loser24,\n",
        "            median_loser25,\n",
        "            median_loser26,\n",
        "            median_loser27,\n",
        "            median_loser28,\n",
        "            median_loser29,\n",
        "            median_loser30,\n",
        "            median_loser31,\n",
        "            median_loser32,\n",
        "            median_loser33,\n",
        "            median_loser34,\n",
        "            median_loser35,\n",
        "            median_loser36,\n",
        "            median_loser37,\n",
        "            median_loser38,\n",
        "            median_loser39,\n",
        "            median_loser40,\n",
        "            median_loser41,\n",
        "            median_loser42,\n",
        "            median_loser43,\n",
        "            median_loser44,\n",
        "            median_loser45,\n",
        "            median_loser46,\n",
        "            median_loser47,\n",
        "            median_loser48,\n",
        "            median_loser49,\n",
        "            median_loser50,\n",
        "            median_loser51,\n",
        "            median_loser52,\n",
        "            median_loser53,\n",
        "            median_loser54,\n",
        "            median_loser55,\n",
        "            median_loser56,\n",
        "            median_loser57,\n",
        "            median_loser58,\n",
        "            median_loser59,\n",
        "            median_loser60,\n",
        "            median_loser61,\n",
        "            median_loser62,\n",
        "            median_loser63,\n",
        "            median_loser64,\n",
        "            median_loser65,\n",
        "            median_loser66,\n",
        "            median_loser67,\n",
        "            median_loser68,\n",
        "            median_loser69,\n",
        "            median_loser70,\n",
        "            median_loser71,\n",
        "            median_loser72,\n",
        "            median_loser73,\n",
        "            median_loser74,\n",
        "            median_loser75,\n",
        "            median_loser76,\n",
        "            median_loser77,\n",
        "            median_loser78,\n",
        "            median_loser79,\n",
        "            median_loser80,\n",
        "            median_loser81,\n",
        "            median_loser82,\n",
        "            median_loser83,\n",
        "            median_loser84,\n",
        "            median_loser85,\n",
        "            median_loser86,\n",
        "            median_loser87,\n",
        "            median_loser88,\n",
        "            median_loser89,\n",
        "            median_loser90,\n",
        "            median_loser91,\n",
        "            median_loser92,\n",
        "            median_loser93,\n",
        "            median_loser94,\n",
        "            median_loser95,\n",
        "            median_loser96,\n",
        "            median_loser97,\n",
        "            median_loser98,\n",
        "            median_loser99,\n",
        "            median_loser100,\n",
        "            median_loser101]\n",
        "\n",
        "upper_loser = [upper_loser1,\n",
        "            upper_loser2,\n",
        "            upper_loser3,\n",
        "            upper_loser4,\n",
        "            upper_loser5,\n",
        "            upper_loser6,\n",
        "            upper_loser7,\n",
        "            upper_loser8,\n",
        "            upper_loser9,\n",
        "            upper_loser10,\n",
        "            upper_loser11,\n",
        "            upper_loser12,\n",
        "            upper_loser13,\n",
        "            upper_loser14,\n",
        "            upper_loser15,\n",
        "            upper_loser16,\n",
        "            upper_loser17,\n",
        "            upper_loser18,\n",
        "            upper_loser19,\n",
        "            upper_loser20,\n",
        "            upper_loser21,\n",
        "            upper_loser22,\n",
        "            upper_loser23,\n",
        "            upper_loser24,\n",
        "            upper_loser25,\n",
        "            upper_loser26,\n",
        "            upper_loser27,\n",
        "            upper_loser28,\n",
        "            upper_loser29,\n",
        "            upper_loser30,\n",
        "            upper_loser31,\n",
        "            upper_loser32,\n",
        "            upper_loser33,\n",
        "            upper_loser34,\n",
        "            upper_loser35,\n",
        "            upper_loser36,\n",
        "            upper_loser37,\n",
        "            upper_loser38,\n",
        "            upper_loser39,\n",
        "            upper_loser40,\n",
        "            upper_loser41,\n",
        "            upper_loser42,\n",
        "            upper_loser43,\n",
        "            upper_loser44,\n",
        "            upper_loser45,\n",
        "            upper_loser46,\n",
        "            upper_loser47,\n",
        "            upper_loser48,\n",
        "            upper_loser49,\n",
        "            upper_loser50,\n",
        "            upper_loser51,\n",
        "            upper_loser52,\n",
        "            upper_loser53,\n",
        "            upper_loser54,\n",
        "            upper_loser55,\n",
        "            upper_loser56,\n",
        "            upper_loser57,\n",
        "            upper_loser58,\n",
        "            upper_loser59,\n",
        "            upper_loser60,\n",
        "            upper_loser61,\n",
        "            upper_loser62,\n",
        "            upper_loser63,\n",
        "            upper_loser64,\n",
        "            upper_loser65,\n",
        "            upper_loser66,\n",
        "            upper_loser67,\n",
        "            upper_loser68,\n",
        "            upper_loser69,\n",
        "            upper_loser70,\n",
        "            upper_loser71,\n",
        "            upper_loser72,\n",
        "            upper_loser73,\n",
        "            upper_loser74,\n",
        "            upper_loser75,\n",
        "            upper_loser76,\n",
        "            upper_loser77,\n",
        "            upper_loser78,\n",
        "            upper_loser79,\n",
        "            upper_loser80,\n",
        "            upper_loser81,\n",
        "            upper_loser82,\n",
        "            upper_loser83,\n",
        "            upper_loser84,\n",
        "            upper_loser85,\n",
        "            upper_loser86,\n",
        "            upper_loser87,\n",
        "            upper_loser88,\n",
        "            upper_loser89,\n",
        "            upper_loser90,\n",
        "            upper_loser91,\n",
        "            upper_loser92,\n",
        "            upper_loser93,\n",
        "            upper_loser94,\n",
        "            upper_loser95,\n",
        "            upper_loser96,\n",
        "            upper_loser97,\n",
        "            upper_loser98,\n",
        "            upper_loser99,\n",
        "            upper_loser100,\n",
        "            upper_loser101]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\n",
        "\n",
        "lower_winner = [lower_winner1,\n",
        "            lower_winner2,\n",
        "            lower_winner3,\n",
        "            lower_winner4,\n",
        "            lower_winner5,\n",
        "            lower_winner6,\n",
        "            lower_winner7,\n",
        "            lower_winner8,\n",
        "            lower_winner9,\n",
        "            lower_winner10,\n",
        "            lower_winner11,\n",
        "            lower_winner12,\n",
        "            lower_winner13,\n",
        "            lower_winner14,\n",
        "            lower_winner15,\n",
        "            lower_winner16,\n",
        "            lower_winner17,\n",
        "            lower_winner18,\n",
        "            lower_winner19,\n",
        "            lower_winner20,\n",
        "            lower_winner21,\n",
        "            lower_winner22,\n",
        "            lower_winner23,\n",
        "            lower_winner24,\n",
        "            lower_winner25,\n",
        "            lower_winner26,\n",
        "            lower_winner27,\n",
        "            lower_winner28,\n",
        "            lower_winner29,\n",
        "            lower_winner30,\n",
        "            lower_winner31,\n",
        "            lower_winner32,\n",
        "            lower_winner33,\n",
        "            lower_winner34,\n",
        "            lower_winner35,\n",
        "            lower_winner36,\n",
        "            lower_winner37,\n",
        "            lower_winner38,\n",
        "            lower_winner39,\n",
        "            lower_winner40,\n",
        "            lower_winner41,\n",
        "            lower_winner42,\n",
        "            lower_winner43,\n",
        "            lower_winner44,\n",
        "            lower_winner45,\n",
        "            lower_winner46,\n",
        "            lower_winner47,\n",
        "            lower_winner48,\n",
        "            lower_winner49,\n",
        "            lower_winner50,\n",
        "            lower_winner51,\n",
        "            lower_winner52,\n",
        "            lower_winner53,\n",
        "            lower_winner54,\n",
        "            lower_winner55,\n",
        "            lower_winner56,\n",
        "            lower_winner57,\n",
        "            lower_winner58,\n",
        "            lower_winner59,\n",
        "            lower_winner60,\n",
        "            lower_winner61,\n",
        "            lower_winner62,\n",
        "            lower_winner63,\n",
        "            lower_winner64,\n",
        "            lower_winner65,\n",
        "            lower_winner66,\n",
        "            lower_winner67,\n",
        "            lower_winner68,\n",
        "            lower_winner69,\n",
        "            lower_winner70,\n",
        "            lower_winner71,\n",
        "            lower_winner72,\n",
        "            lower_winner73,\n",
        "            lower_winner74,\n",
        "            lower_winner75,\n",
        "            lower_winner76,\n",
        "            lower_winner77,\n",
        "            lower_winner78,\n",
        "            lower_winner79,\n",
        "            lower_winner80,\n",
        "            lower_winner81,\n",
        "            lower_winner82,\n",
        "            lower_winner83,\n",
        "            lower_winner84,\n",
        "            lower_winner85,\n",
        "            lower_winner86,\n",
        "            lower_winner87,\n",
        "            lower_winner88,\n",
        "            lower_winner89,\n",
        "            lower_winner90,\n",
        "            lower_winner91,\n",
        "            lower_winner92,\n",
        "            lower_winner93,\n",
        "            lower_winner94,\n",
        "            lower_winner95,\n",
        "            lower_winner96,\n",
        "            lower_winner97,\n",
        "            lower_winner98,\n",
        "            lower_winner99,\n",
        "            lower_winner100,\n",
        "            lower_winner101]\n",
        "\n",
        "median_winner = [median_winner1,\n",
        "            median_winner2,\n",
        "            median_winner3,\n",
        "            median_winner4,\n",
        "            median_winner5,\n",
        "            median_winner6,\n",
        "            median_winner7,\n",
        "            median_winner8,\n",
        "            median_winner9,\n",
        "            median_winner10,\n",
        "            median_winner11,\n",
        "            median_winner12,\n",
        "            median_winner13,\n",
        "            median_winner14,\n",
        "            median_winner15,\n",
        "            median_winner16,\n",
        "            median_winner17,\n",
        "            median_winner18,\n",
        "            median_winner19,\n",
        "            median_winner20,\n",
        "            median_winner21,\n",
        "            median_winner22,\n",
        "            median_winner23,\n",
        "            median_winner24,\n",
        "            median_winner25,\n",
        "            median_winner26,\n",
        "            median_winner27,\n",
        "            median_winner28,\n",
        "            median_winner29,\n",
        "            median_winner30,\n",
        "            median_winner31,\n",
        "            median_winner32,\n",
        "            median_winner33,\n",
        "            median_winner34,\n",
        "            median_winner35,\n",
        "            median_winner36,\n",
        "            median_winner37,\n",
        "            median_winner38,\n",
        "            median_winner39,\n",
        "            median_winner40,\n",
        "            median_winner41,\n",
        "            median_winner42,\n",
        "            median_winner43,\n",
        "            median_winner44,\n",
        "            median_winner45,\n",
        "            median_winner46,\n",
        "            median_winner47,\n",
        "            median_winner48,\n",
        "            median_winner49,\n",
        "            median_winner50,\n",
        "            median_winner51,\n",
        "            median_winner52,\n",
        "            median_winner53,\n",
        "            median_winner54,\n",
        "            median_winner55,\n",
        "            median_winner56,\n",
        "            median_winner57,\n",
        "            median_winner58,\n",
        "            median_winner59,\n",
        "            median_winner60,\n",
        "            median_winner61,\n",
        "            median_winner62,\n",
        "            median_winner63,\n",
        "            median_winner64,\n",
        "            median_winner65,\n",
        "            median_winner66,\n",
        "            median_winner67,\n",
        "            median_winner68,\n",
        "            median_winner69,\n",
        "            median_winner70,\n",
        "            median_winner71,\n",
        "            median_winner72,\n",
        "            median_winner73,\n",
        "            median_winner74,\n",
        "            median_winner75,\n",
        "            median_winner76,\n",
        "            median_winner77,\n",
        "            median_winner78,\n",
        "            median_winner79,\n",
        "            median_winner80,\n",
        "            median_winner81,\n",
        "            median_winner82,\n",
        "            median_winner83,\n",
        "            median_winner84,\n",
        "            median_winner85,\n",
        "            median_winner86,\n",
        "            median_winner87,\n",
        "            median_winner88,\n",
        "            median_winner89,\n",
        "            median_winner90,\n",
        "            median_winner91,\n",
        "            median_winner92,\n",
        "            median_winner93,\n",
        "            median_winner94,\n",
        "            median_winner95,\n",
        "            median_winner96,\n",
        "            median_winner97,\n",
        "            median_winner98,\n",
        "            median_winner99,\n",
        "            median_winner100,\n",
        "            median_winner101]\n",
        "\n",
        "upper_winner = [upper_winner1,\n",
        "            upper_winner2,\n",
        "            upper_winner3,\n",
        "            upper_winner4,\n",
        "            upper_winner5,\n",
        "            upper_winner6,\n",
        "            upper_winner7,\n",
        "            upper_winner8,\n",
        "            upper_winner9,\n",
        "            upper_winner10,\n",
        "            upper_winner11,\n",
        "            upper_winner12,\n",
        "            upper_winner13,\n",
        "            upper_winner14,\n",
        "            upper_winner15,\n",
        "            upper_winner16,\n",
        "            upper_winner17,\n",
        "            upper_winner18,\n",
        "            upper_winner19,\n",
        "            upper_winner20,\n",
        "            upper_winner21,\n",
        "            upper_winner22,\n",
        "            upper_winner23,\n",
        "            upper_winner24,\n",
        "            upper_winner25,\n",
        "            upper_winner26,\n",
        "            upper_winner27,\n",
        "            upper_winner28,\n",
        "            upper_winner29,\n",
        "            upper_winner30,\n",
        "            upper_winner31,\n",
        "            upper_winner32,\n",
        "            upper_winner33,\n",
        "            upper_winner34,\n",
        "            upper_winner35,\n",
        "            upper_winner36,\n",
        "            upper_winner37,\n",
        "            upper_winner38,\n",
        "            upper_winner39,\n",
        "            upper_winner40,\n",
        "            upper_winner41,\n",
        "            upper_winner42,\n",
        "            upper_winner43,\n",
        "            upper_winner44,\n",
        "            upper_winner45,\n",
        "            upper_winner46,\n",
        "            upper_winner47,\n",
        "            upper_winner48,\n",
        "            upper_winner49,\n",
        "            upper_winner50,\n",
        "            upper_winner51,\n",
        "            upper_winner52,\n",
        "            upper_winner53,\n",
        "            upper_winner54,\n",
        "            upper_winner55,\n",
        "            upper_winner56,\n",
        "            upper_winner57,\n",
        "            upper_winner58,\n",
        "            upper_winner59,\n",
        "            upper_winner60,\n",
        "            upper_winner61,\n",
        "            upper_winner62,\n",
        "            upper_winner63,\n",
        "            upper_winner64,\n",
        "            upper_winner65,\n",
        "            upper_winner66,\n",
        "            upper_winner67,\n",
        "            upper_winner68,\n",
        "            upper_winner69,\n",
        "            upper_winner70,\n",
        "            upper_winner71,\n",
        "            upper_winner72,\n",
        "            upper_winner73,\n",
        "            upper_winner74,\n",
        "            upper_winner75,\n",
        "            upper_winner76,\n",
        "            upper_winner77,\n",
        "            upper_winner78,\n",
        "            upper_winner79,\n",
        "            upper_winner80,\n",
        "            upper_winner81,\n",
        "            upper_winner82,\n",
        "            upper_winner83,\n",
        "            upper_winner84,\n",
        "            upper_winner85,\n",
        "            upper_winner86,\n",
        "            upper_winner87,\n",
        "            upper_winner88,\n",
        "            upper_winner89,\n",
        "            upper_winner90,\n",
        "            upper_winner91,\n",
        "            upper_winner92,\n",
        "            upper_winner93,\n",
        "            upper_winner94,\n",
        "            upper_winner95,\n",
        "            upper_winner96,\n",
        "            upper_winner97,\n",
        "            upper_winner98,\n",
        "            upper_winner99,\n",
        "            upper_winner100,\n",
        "            upper_winner101]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "90bb6569-2bf2-4119-f9d1-78956c7d4799"
      },
      "source": [
        "### Visualize!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(median_loser, color = 'Red')\n",
        "plt.plot(median_winner, color = 'Yellow')\n",
        "\n",
        "xstar = np.arange(0, max_iter+1, step=1)\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Yellow', alpha=0.4, label='GP EI Regret IQR: L-BFGS-B')\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP EI Regret IQR: Newton-CG with GP d$^{2}$EI')\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \n",
        "plt.legend(loc=1) # add plot legend\n",
        "\n",
        "plt.show() "
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEaCAYAAAAcz1CnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZgU1dW43zP7sMMw7MsAImGRdUDBgKABxai4EBWJotGQRBRFY6JJPrfol0XjgqL+9NOoEZdEBZWgMSgqiKiMLLIom+z7zjAMMDP398etHntmuqfX6p7uPu/z1NNTVbdunerqqVP3nHPPEWMMiqIoSuqSFm8BFEVRlPiiikBRFCXFUUWgKIqS4qgiUBRFSXFUESiKoqQ4qggURVFSHFUEiuIyIrJBRIyIDA+yfYHTXmO7lZigikBJSrwevkZEykVku4hMF5GWUej7aqffj4I85DngUWBLkO0POe0fDUM8RQmZjHgLoCguMwvYDFwEXIF9+RkXixOLSKYx5oQx5t5QjjPG7ANudkksRamBjgiUZOdZY8z1wO3Oeh8AEfm1iKwRkSMickxElorIWM9BIjJSRIqc/QdF5CsRuVhErgb+7jQ7wxkZbHCO8YxCfi8iK4DSatuHO+ttROR9p+9PReQeZ/8SZ38N05DX6OYGEVktIodF5CURyXLzy1NSA1UEStLjPCwHOKvLnM9OwNfA88BbQE/gJREpcPb/Has03nCWCqAXsBL4r9NmK9Z881y1U97j9P2mH5FeBkZiTUXr+V5JBcM9wAIgHRgPXBnCsYriEzUNKcnODK+/PwFucP7+DXAJ0BU4DuwGWgFDgA1AJvaN/m3sQ30NIMaYchHxPMjXGmN8mXD+1xhzpy9hRKQdcIazOsoYs1FE9gGTg7yeXxpj/iUiAlwF9AvyOEXxiyoCJdmZBZQBFwKDgG4icghYiH3Dr06+8/kL4AHgX876XqwSeTWIc35ay762zudRY8xG5++VQfTpYbHzecD5bBDCsYriEzUNKcnOs8aYi7CmnhxgKtADqwTKgC7Y/wPPw1icz3eNMV2B5sBYIA+439lX7nz6+/85Vos8W53PXGd0APCDoK/GygygoaVK1NARgZIq3IO1p/fH2v4rsL//vwENsSYibxY7TuBNQHtnm+ctfLPzOUBEngAWG2OeCUYIY8wWEfkYax56X0QWAZeFdUWKEiV0RKCkBI4Z5h/O6tXAjcBO4EygCOuA9WYO0A2YAPwQ+Ai4ztn3CdbhWw78ChgTojjjsQ7njtgRycPO9tpGEoriGqKFaRQltohIY2PMQa/1/wdMBF4yxmgUkBJz1DSkKLHnGhEZA3yMDWP9KdZUNS2uUikpiyoCRYk932JDVX8LHAHmAX80xiyMq1RKyqKmIUVRlBRHncWKoigpTsKZhpo3b24KCgriLYaiKEpCUVRUtMcYk+9rX8IpgoKCAhYtWhRvMRRFURIKEdnob5+ahhRFUVIcVQSKoigpjioCRVGUFCfhfARK5Jw4cYItW7ZQWloab1EURYkyOTk5tGvXjszMzKCPUUWQgmzZsoWGDRtSUFCATWuvKEoyYIxh7969bNmyhU6dOgV9nJqGUpDS0lLy8vJUCShKkiEi5OXlhTzaV0WQoqgSUJTkJJz/bVUEiqIoKU5q+Qi+fgO++8rHjnygc6ylqcnZZ0N2dhxO/HSU+5sYsMXOnTuZMmUKCxcupGnTpmRlZfGb3/yGiy66iI8++ogxY8bQqVMnjh07xuWXX85dd91V5fgNGzbQvXt3unXrVrntlltu4aqrrqqcdNi8efMqxxQUFNCwYUNEhKZNm/Liiy/SsWPH6FyyDw4cOMDLL7/M9ddf73N/gwYNKC4uBmDFihXceOONbN26lbKyMn76059y1113kZaWxvPPP89tt91G27ZtKS0t5Re/+AVTpkyp9dzPP/88ixYt4vHHH/fbZvjw4Wzfvp3c3FyOHTvGlClTmDjR3jvPd5Weng7AE088wZAhQ1izZg1Tpkxh1apVNGnShEaNGnHPPfcwbNgwdu7cybXXXsvmzZs5ceIEBQUFzJ49u8Z5vfsuLy/nvvvuY8yYUEs6KNEktRTBwZ2ww9fkul1AvVhLU5MtW6BLl3hL4TrGGC688EImTJjAyy+/DMDGjRt5++23K9sMHTqUWbNmceTIEfr27cv5559P//79q/TTpUsXlixZEtK5586dS/Pmzbnrrru47777eOaZoAqL1XotxhjS0moOrg8cOMATTzzhVxF4OHr0KBdccAFPPvkko0aNoqSkhEsuuYRHH3208oF/2WWX8fjjj7N37166devG2LFjad++fa39BsP06dMpLCxk3759dOnShauvvpqsrCzg++/KQ2lpKT/+8Y958MEHueCCCwBYvnw5ixYtYtiwYdx5552MHDmSm266CYBly5b5Pa+n72+//ZZRo0apIogzahoC4ChQEm8hYNOmeEsQEz788EOysrL45S9/WbmtY8eO3HjjjTXa1q9fnwEDBrB27dqoyjB48GC2brXlg3fv3s0ll1zCwIEDGThwIJ9++mnl9pEjR9KzZ0+uu+46OnbsyJ49e9iwYQPdunXjqquuolevXmzevJkHHniAgQMH0rt378rRy+233866devo27cvt912m19ZXn75ZU4//XRGjRoFQL169Xj88cd54IEHarTNy8vjpJNOYvv27VH9PoqLi6lfv37lCMAX06dPZ/DgwZVKAKBXr15cffXVAGzfvp127dpV7uvdu3fA8x46dIimTZuGL7gSFVQRVLI/3gLA5s2QAmnBV6xYUePt3h979+5l4cKF9OzZs8Y+z0PWs8ybNy9oGd577z0uvPBCAG666SamTJnCl19+yRtvvMF119mKlPfccw9nnnkmK1asYOzYsWzyUtRr1qzh+uuvZ8WKFXz77besWbOGL774giVLllBUVMQnn3zCn//858pRi6+Huvf3MWDAgCrbunTpwtGjRzlw4ECV7Zs2baK0tLTyIXvnnXdWGUmFyvjx4+nduzfdunXjf/7nf6ooghEjRtC3b19OPfXUSjlru2+TJk3i2muvZcSIEdx///1s27bNb9sRI0bQq1cvzjjjDO67776w5VeiQ2qZhmplH9A2viKUlsKePZDvM0Fg0jJp0iTmz59PVlYWX375JQDz5s2jX79+pKWlcfvtt/tUBOGYhkaMGMG+ffto0KABf/zjHwGYM2cOK1eurGxz6NAhiouLmT9/PjNmzADgnHPOqfLm2rFjR0477TQA3n//fd5//3369esH2LfrNWvW0KFDh5Bkq43XXnuNTz75hG+++YbHH3+cnJwcAO69996I+vWYhnbv3s2QIUM455xzKv0m1U1D1bnoootYs2YNJ598Mm+++SZnn30269ev57333uPdd9+lX79+LF++nHwfv2dP3+vWreOss85i+PDhNGjQIKJrUcJHRwSVHMTWIo8zmzfHWwLX6dmzJ1999b3Tftq0aXzwwQfs3r27ctvQoUNZvHgxRUVFVUxIkTJ37lw2btxI3759K004FRUVLFy4kCVLlrBkyRK2bt0a8KFUv379yr+NMdxxxx2Vx69du5Zrr702aJl69OhBUVFRlW3r168nLy+PJk2aANZHsGzZMhYsWMDtt9/Ojh07gu7fw7Rp0ypHT9Xf1vPz8+nfvz+ff/653+Or37cZM2bw/PPPs2/fvsptzZo144orruAf//gHAwcO5JNPPuH3v/995Xmr06VLF1q2bFlFESuxRxVBJRXAgYCtXCcF/ARnnnkmpaWlPPnkk5XbSkpi56PJyMjgkUce4cUXX2Tfvn2MGjWKxx57rHK/Z5Rx+umn889//hOwb/379/s2H5599tk899xzlRFAW7duZdeuXTRs2JDDhw8HlGf8+PHMnz+fOXPmANZ5PHnyZO65554abQsLC7nyyit59NFHQ7to7MjLo6zatGlTZV9JSQmLFy+mSy3BCldccQWffvppFVOU93378MMPK9cPHz7MunXr6NChA/fff3/leauza9cuvvvuO1ejt5TAqGmoCvuBvPiKsGuXNRE5Q//YEDjcM5qICDNnzmTKlCn89a9/JT8/n/r16/OXv/wlpH48PgIPP/vZz5g8eXJQx7Zu3Zpx48Yxbdo0pk6dyqRJk+jduzdlZWUMGzaMp556irvuuotx48bxj3/8g8GDB9OqVSsaNmxY+cD3MGrUKFatWsXgwYMBGxb60ksv0aVLF04//XR69erF6NGj/foJcnNzefvtt7nxxhu5/vrr2bp1K3/4wx8YP368z/a//e1v6d+/P7/73e944IEHKCwsrOLA9fD8888zc+bMyvWFCxdWceaCVUKe8NGrr766hq+iupyzZs3illtu4eabb6Zly5Y0bNiQP/zhDwAUFRVxww03kJGRQUVFBddddx0DBw702deIESNIT0/nxIkT/PnPf6Zly5Z+z6u4T8LVLC4sLDRhF6aZ/wSsXFBLgxxgUHh9R5Mzz4STTnKt+1WrVtG9e3fX+k8Wjh07Rnp6OhkZGXz22Wf86le/CtknEQ4zZ87klltuYe7cufqmrISFr/9xESkyxhT6aq8jgiqUYsNI4zynYPNmVxWBEhybNm3i0ksvpaKigqysrIjnHATLhRdeWBnRpCixQBVBDXYBvoap6UBWbETYvBkOHoxOX7m5kBUjuZOMrl27snjx4niLoSiuo4qgBpucpTr1Af/206hSWgqvvRadvurVg7POgtato9OfoihJh0YNBc0R7AzkBKOkBGbNgiVLUmKymqIooaMjgpCoA5POwsEY+OILWLkSMjKgc2fwiv0mPR0aNgQf+XIURUl+9D8/JPbGW4DIKC6GAwegogLKy79fjh+328vrwIQ6RVFijo4IQuIgcAIIvhZowlBebpVBo0ahjQxEdCShKAmOKoKQMNhJZy3iLYg7VFRYZRAqjRtrZJKiJDCuvcqJSI6IfCEiS0VkhYjUmC8vItki8pqIrBWRz0WkwC15okeCm4fc4OBBa15SFCUiZs6cyc9//nMuu+wy3n///Zid180x/THgTGNMH6AvcI6InFatzbXAfmPMScDDQGg5BuLCfmxeIqUKhw6FrAx27tzJFVdcQefOnRkwYACDBw+uzPaZnp5O37596dWrFz/5yU985iLytPEsf/7znyv3+Usa593v+eefXyPNsxt4CtT4w1vWLVu2MGbMGLp27Urnzp254YYbOHbsWOX+cOQXEW699dbK9QcffJC77747vItxCHRN4bBjxw4uv/xyunTpwoABAzj33HNZvXo1UPtvJVSGDBkC1LyGDRs20KtXr4DHB5IlmN+uN3fffTcPPvggYCcTPvPMMzz11FO85oSQh/M7DxXXTEPG5q7wJGXJdJbq8YtjgLudv18HHhcRMXU670UZcAhoEm9BoseLL0a3v6uuqrouAnl59tMhUJWy3NzcynQO48eP56mnnuKWW26p0q13m2DxPmbChAlMmzaN3//+9yH1UZ3aqpRB8JXKjDFcfPHF/OpXv+Ktt96ivLyciRMn8pvf/KYyyVw48mdnZ/Pmm29yxx131JpWOhSCvaZgMcZw0UUXMWHCBF599VUAli5dys6dO+natWvAinahsGDBgrCvIZjqesH8dgNx3333MWnSpBr9uYWrPgIRSQeKgJOAacaY6jlu2wKbAYwxZSJyEJv1bU/UhfnyHmh/L7TzoWOOCozPgc3BDpBmUfWrE38NAzN0KJxzTvjHJwLGwLFjVRLphVKlbOjQobWWPQyXwYMHV+n3pZdeYurUqRw/fpxTTz2VJ554gvT0dP74xz/y0ksvkZ+fT/v27RkwYABjx47l7LPP5tRTT6WoqIjZs2czb948n8d7VyobOXKk3+RzH374ITk5OVxzzTWAfRN8+OGH6dixI/fff3+Nt7/q8vsjIyODiRMn8vDDD3P//ffX2O/ruh966CGys7OZPHkyU6ZMYenSpXz44Yd8+OGHPPvss5SXl9e4poceeojnnnsOgOuuu46bb76ZDRs2MHr0aH74wx+yYMEC2rZty1tvvUVubm4VGebOnUtmZmaV30OfPn0A+OCDD4L+rTzwwAO1yj19+vTKWtHV78ukSZMoLy/n5z//uV9ZQ/ndgv/f7v33388LL7xAixYtKn9TYBXN7bffzujRo4Mu3hQNXA33MMaUG2P6Au2AQSISeNzlAxGZKCKLRGSRd876kKjXAdY0hVX1qy7f1oPuFTAhB07KD3JpBic1cpYmcFIXmxso1CUzE95/PzXCNktLq6wGW6WsrKyMd999l1NOOaXGvqNHj1YZMr8Wwmzs8vJyPvjgg8qsnatWreK1117j008/ZcmSJaSnpzN9+vTKqmVLly7l3XffxTvhoXeVspKSEp/HAxFVKmvUqBEFBQU1SnVWl//cc8+ttSLYpEmTmD59OgerpS7xd91Dhw6trPi2aNEiiouLOXHiBPPmzWPYsGE1rqmoqIi///3vfP755yxcuJBnnnmmMj3HmjVrmDRpEitWrKBJkya88cYbNeRbvny538ynoVS0CyS3N77uSyBZQ5HF32+3qKiIV199lSVLljB79uzKYkwAjz32GHPmzOH111/nqaeeAiL7nQdLTKKGjDEHRGQucA6w3GvXVqA9sEVEMoDG+PDGGmOeBp4Gm300LCF6XgP7j8J6H9lH978DF2VD7pCwuobehGUq+uor+H//D1avhmTPBnriBJSV2QltPqhepczz4wf7z+2r0Es4Q2ZPv1u3bqV79+6MHDkSsG+dRUVFlWmTjx49SosWLdi3bx9jxowhJyeHnJwczj///Mq+vKuU+Ts+2viTf/bs2bUe16hRI6666iqmTp1a5Q3Xn9zjxo2jqKiIQ4cOkZ2dTf/+/Vm0aFHlqKc68+fP56KLLqos2HPxxRczb948LrjgAjp16lR5LwcMGMCGDRsi+g58VbTzMGDAgJDkrk6osvqSJdBvd968eVx00UXUq2eTW3qnEJ88eXKNVOoJbRoSkXzghKMEcoGR1HQGvw1MAD4DxgIfxsU/sLUVdFsHaeVQ4b94t3/2EJYi6NXLhl0uXpz8igDsqMAxb/Ts2bPK29a0adPYs2cPhYU2S65bP35PvyUlJZx99tlMmzaNyZMnY4xhwoQJ/OlPf6rS/pFHHvHbV/UqZb6OD4UePXrw+uuvV9l26NAhduzYQbdu3WqVPxhuvvlm+vfvX2l6CiR3p06deP755xkyZAi9e/dm7ty5rF27lu7du7Nx48agrys7O7vy7/T0dI4ePcq0adMqs7nOnj2bnj171rh2D4F+K95kZmbWKnc4soYqSywe3NHGTdNQa2CuiCwDvgT+a4yZJSL3iohHBT4L5InIWuAW4HYX5fHP1taQWQ4tw3VN7KGmHzwIsrKsMli82MbwJzvHjlXmO4p3lbJ69eoxdepU/va3v1FWVsZZZ53F66+/zq5duwDYt28fGzdu5PTTT+edd96htLSU4uJiZs2a5bM/f8cDQVcqO+ussygpKeFFx3lfXl7Orbfeyg033FDDpl5d/mBo1qwZl156Kc8++2xQcg8dOpQHH3yQYcOGMXToUJ566in69euHiNS4pqFDhzJz5kxKSko4cuQIM2bMYOjQoX5lqV4t7cwzz+TYsWM8/fTTlW2WLVvGvHnzQv6t1Ca3N8HeF2+i8bsdNmwYM2fO5OjRoxw+fJh33nknpOPdwDVFYIxZZozpZ4zpbYzpZYy519l+pzHmbefvUmPMT4wxJxljBhlj1rslT61sawEVAm23h9nBcWwkURj0729DL6vZgJOSiorKEFNPlbKPP/6YTp06MWjQICZMmBBSlbLqttPbbw/tPaJfv3707t2bV155hR49enDfffcxatQoevfuzciRI9m+fTsDBw7kggsuoHfv3owePZpTTjmFxo0b1+jL3/EAeXl5lZXKbrvtNr/yiAgzZszg9ddfp2vXruTl5ZGWluY3Kshb/kA+Ag+33nore/Z8/8JTm9xDhw5l+/btDB48mJYtW5KTk1P5cK9+Tf379+fqq69m0KBBnHrqqVx33XX069cvoDzVr33OnDl06dKFnj17cscdd9CqVauQfyu1ye1NsPelupyR/m779+/PZZddRp8+fRg9erTfKm4eIv2dB4NWKPNwwX8gzcDMcCN42gL+6736pbQUfv1rGz102WVhnjs0VvXtS/dOnWJyrhpkZtrU2B4SYEZycXExDRo0oKSkhGHDhvH000/HJKJjwYIFjBs3jhkzZsQ0gkRJfLRCWbhsbQ39lkPWMTieHbh9DfYSliLIyYEePazj+Cc/Sf68PSdOVC26U68eeNna6yITJ05k5cqVlJaWMmHChJg9lIcMGRKSLV5RwkUVgYetrWDA19BmJ2zoEEYHpVjzUKPQD+3fH5Yuhe++gy5hKJNEpqTEKr9qNvC6hGfikKIkK0n++hkCO5vD8QxouyOCTsLMQ9Snj60JsGBBahaPKS62jmRFUeKCjgg8mDTY3iJCRbADaAiEOI0/NxcGDYL5823BmPHjIUqpABKGQ4f8zjEgO9t+RxLBDG5FUfyiisCbra2h4zYYuATK0qEiDVZ1hePBOjRPACuBllh/QQhf71VXQceOMGMG3HsvFBbaUUKkiNgHbFaW/RSBbt2q2ukjpUGD6MjqLwyyrMyOGBo0sM5mRVGiiioCbza2hYFLod+K77fVOwqf+XS018JObBGbAUCQD8i0NBgxwpqJXn0VopVbp6LCPkhPnPh+rsIvfoE5cCCSDElVKSuzSeXcpKzM1koIVuGI1BxBZGRY53SyO+SVlCacSFBVBN4cbgh/vxTE+SKHfgE91sCy7nAk1MiWUmAbNoNGCDRrBlHK6FgDRxHk1KvH3qZNyWvQoMYkm5DZvx8OH7aVzWLxth5JXqYTJ2y4br16ampSkhJjDHv37iXHK8FjMKgiqIGAcR4QRadA1++g/9cwr3ophWDYgp1gXUe+ZudNuN369WwBdnvH84eLMfZN/dChyvQRCUFmZpVsqIqSLOTk5NCuXbuQjqkjT6g6ypH6sLIr9FwNy3rAwVBDQ09glUFB9GWLgMyyMjo5BT+iwsyZ8O678Ic/QPsQR0DxZPhwOPnkeEuhKHFHjaWBWNITytNgQLg2+61YhZDEjBplzS1vvRVvSUJj/vzwajQrSpKhiiAQR3Nh+Q/gpI2Qty+MDspxau8kL/Xqwdlnw9dfw5o18ZYmeMrK4IMPUqMehKLUgiqCYFjaHY5mw+mLCCvLKFuxmbZ9Lcuc/Uf9Hp0QjBhhHd1//zscORJvaYJn716b/VVRUhj1EQTD8Wz4vB8MXwjd1sO3oaaBMPg3Dx1wlnVEVPKSNoSV6yhaZGfDxInwwANWGVx/feKEaS5bZvM9RcN5rigJSIL8p9YBVneG7flw6mLIdisdgolg2QqEWcYzWnTqZBPnff21LcGZKJSVQVFRvKVQlLihiiBoBOYPhKzjVhnUSdYQdxPT8OEwcKCNJEqkGgvffKOOYyVlUUUQCvubwtc/gB+sg3PmwtkfwaiPIT/cymbRpgxYBcSx2pkI/PSndk7BBx/ET45QMQY+/zzeUihKXFAfQagUnQINi6Gh4xBtfBhyS+GtUURm448WxVgndCDygB+4I0JODgwYAJ9+amfyJsrErY0b4dtv7Szp2qhfP3AbRUkgVBGESlkmzBn2/Xr31TD0S1vHYFur+MlVhWDCIXcBLYBm7ohQWAgffWQdsYMGuXMON/j448BtmjWDsWPdl0VRYoSahiJldRc4kgv9l8dbkjBYS3BKIwy6dIEmTSDcsqJ1mX37YHu49a0Vpe6hiiBSytNhaQ87Imi5K97ShEgpsMGdrtPSrHlo+XJbhSzZWLEicBtFSRBUEUSDVSfZCWcJOSrYBuzHKoVSopoOY+BAO2t3yZLo9VlX+O67xJo4pyi1oIogGpRn2FTV7bdDq53xliZEDPA18IWzLCVqUUcFBbbS2pdfRqe/uoQxsGpVvKVQlKiQWs7i7iOhXW8fO7YA6yPrW7pA2To4/wMoLoQDZ0J5iJElm/fDNzviXLe4BPt9dIi8KxHrNH7/fVuXOJHSVAfDqlXQr190qrMpShyRcKrZxJPCwkKzKOoOyDXA3Cj0cxiYDXyMrUw2HPgR0Dj4Lg6UwBcbYJOfBHcVsZgjkIatrpYbeVebN8N990GrVrYYTHX69YORIxMnHUV1hg+3jvFwEEnc61YSDhEpMsb4LLeoigCA7cA7UexvN/AWsAirEIYAXfE9zyAXG8LZDAgi3n7mEth1KFqC1kJT4JTIuzEGXnsNdvowmZWUwIYNcMopcM01Nj4/lcjIgMsv1xxHSkxQRRCQw8ArUe4TbKz++9gJXn4Ks1dBCDgpzRjfCVCL68GiPrC2IHAfQdMFCGTOyQTCfJAZY+ca/Otf0LQpnHXW9+Uju3e3o4hkp2dPOP30eEuhpACqCAJSAfxflPv05ghW2VTHYG3y+4C92KidAOw5Apv21tzebju02Ac782BJLzju1A8uyYGDIZimQqYR0DeyLtatg2eesfWPPdSrB7fdBm3aRNZ3XSctDS67DBo2jLckSpKjiiAoXsI+lOs4e4vhja987DC2vvKgJVDfK/FchcC/zgujzGaweExfEY5Cysu/n29w+DA8/LB1wt5+u52YlsycfLL1NSiKi9SmCNRTVUmCRLQ0refHwSiwpjO8dgG88yN45yyYPcJOeAu7zGYwlBPUSCYQ6en2rbhhQzsKmDzZKobHHoOjCV60JxBr1lQdDSlKjHFNEYhIexGZKyIrRWSFiNzko81wETkoIkuc5U635AlMgiiCtDTIq8WpWpYB21vC9lawpQ0s72bLbDZz80HjwsSq9u3hl7+Ebdvg5Zej339dwpP5dMeOmosvJ7uiRBk35xGUAbcaY74SkYZAkYj81xizslq7ecaY81yUI0gSRBEANG8Au335HHywtDv0WA0Dl8J/hrskUDHQPPrd9ugBZ5xhi8wnUhbTcNi0yS6+GDvWJrpTFJdwbURgjNlujPnK+fswNlF+W7fOFzkJpAjyQ3AsHs+GZT2g41Zo4VbdBBd9K/36wYkTqZ3bJ5EK/CgJSUx8BCJSAPQDfFX+GCwiS0XkXRpey0IAACAASURBVBHp6ef4iSKySEQW7d7tVjnGBIphbx6i0vq6m82FVLjUHXkodqlf4KST7IzkZMxXFCxr18Z5trmS7LiuCESkAfAGcLMxpvpMqK+AjsaYPsBjwExffRhjnjbGFBpjCvPz812SNIFGBH4dxn4oy7S5kNrtgAZuPLRLCW6eRBikp0OfPrauQZlL56jrFBdr2mvFVVxVBCKSiVUC040xb1bfb4w5ZIwpdv6eDWSKiAvG5mBIIEWQngbNQpzE5Smak+8ndUXEuJiJs18/6yP45hv3zlHXUfOQ4iJuRg0J8CywyhjzkJ82rZx2iMggRx4fs6ViQS42Jj5BCNU8tK+JnVPQ3K2v10VF8IMfQHY2LF7s3jnqOuvX27kWiuICbo4ITgeuBM70Cg89V0R+KSK/dNqMBZaLyFJgKnC5iesMtwTyE4TiMAY7n2Bfk8QcEWRmQq9esHRpjJLu1UGOH7c1lRXFBVwLHzXGzCfAdFNjzOPA427JEDoNgFgkdIsCoY4IAHY3g06bsaktopWPyIPLRVr69YOiIpuOomtXd89VV1m7Fjp3jrcUShKSWvUIApJAfoJm9a3DOJQ35D3NoPs6aHAEiqN9rUdwR8E49Opls3X+5z+wZYvdVlAAnTq5c766yKZNMNMrnuLccyErK37yKEmDKoIqJJAiSE+DUT2gLIDduLwCjhyH4mOQUQZ8CT1O2OR0vjhQAgfDSengSTURhRoGvsjNtdFDRUXw9dd2W36+rXWQKlRUwC6vuth79iR/Uj4lJqgiqEICKQKADqHONu0I/Av6VgA+p2zAqu0wb02YAhXjmiIAuO46GDfO/j1nDrz3Hhw7Zh3JqYgqAiVKaNK5KiSYIgiZTOzkbj+pDMCanMLGZT9BWtr3iek6drTbUjm+3rXJlUqqoSOCKiS7IgBbi3gJfu35TSOplrWHwIXv07Ajkwh9CW2dbCXbtllfQSqyx62UIUqqEVARiEh94DxgKFDgbN6ILcz7b2OMy6+BsSSBwkfDpiPwKbYYjg8/QVYGNMiB4nBSS5cQXN6h+kCEM8Tz863zeNu2yPpJZA4etGGl6jBWIqRW05CIPATswNZxnIitaF4I/Bx4FdguIn9zW8jYkQkku725g/NZS0x6qLOWQ2YTvutthkBaGrRundqKAHRUoESFQD6CS4FHgNOA+saY1saYVlgbymDsJLDL3BUx1iS7eagd9ra75ScIhiNEZQK5KgJVBEpUCGQa6miMqRGfaIw5js0k+rmI3OWKZHGjPnHLchETMoE21D4iiIWJbBMR1zBo2xa++MJWMMt1MVqpLqMOYyUK1Doi8CgBEVkvIj/2bBeRM0Tkfe82yUOyjwjA+glqMc/ERBEUE7HCbd3afqbyqEAVgRIFah0RiEgjoCnWSdxRRDwG5jOAs9wVLV60I7SUyiXAFpdkcQuPw/grrNunGo1zQSQGOfA349NhHSzekUNdukRFooTj0CF1GCsRE8g0NAW4E/vq+JizeKjFyJzIFPB9cFQwHARec0US9zgVa9n7P+ygsF/V3elp0KQe7Hc7IOwwNtw0zOkszZrZB2AqjwhAJ5YpERPoP3A18C426HsJMBv4N/ASMN5d0RKFhriWX8c1coAbsSODp4HFwDFncUZDEc0nCBZDRGUu09LsA3Dr1qhJlJCoeUiJkFpHBMaYV4BXHIfwv3wUnldIAxphRwaJRC5wEzYo7Cmv7RnAndZPsD4WD5hiIvLLtG6d2vWMQRWBEjHBjskfAK4RkcUicrqITBWRS90ULLFoHG8BwiQXuBm4HLgYO2+wDPgmBnMJPERofmrb1trJi12sm1zX0RBSJUKCVQQPYf0FvbEzrtKB29wSKvFIVEUAVhmMAM7GKoKGwPoYRQ5BxIrAYxtPZT/BoUO2lKeihEmwiuAS7KjAQxHQLfriJCqJrAi8EaAzsB4a5kBGLEp3RkkRpLqfwDs9taKESLCKoIKqHtE+WOOuAkCTeAsQRToDu0CKY+QwPgEcD//wJk3sZLJUzkIKqgiUiAg2++i/gVucv/8BtMLGHipA8owIwCoCqDQP7T4cg3MWA6HWVnAQsaOCFStgxoyoShUUGRnwox/Ff2bzzp3xPb+S0ASrCG7Gjgh+jM1R8ALwa7eESjzqY7/KUCai1VUKsAPF9TDwx9ArQHz6nmL4eHWE5zxC2IoA4JRTYNYsW6wm1pSVQf36cOaZsT+3N7t22QmAkmihzEpdIJg01OnAXcCLxphr3BcpUWlMcuQoysLOrv4O6mXZpTaa1oPP1sPxSJRghH6C0aPtEg/uvhuWLIm/IjhxAvbvt5PsFCVEAvoInFxCFwIpOoc/WJLNPLQBW4c4AGlpYZTMrE4Cl7To0wfWrIEjdeAa1DykhEmwzuKPgDtFZJKIXOxZXJQrAUkmh3EX7CzjIEMyO0aQLwiAowSubFZH6dvXFpX/+ut4S6KKQAmbYH0EHpPQVOdTsPkBYhFfmCAk24gAYD3QPnDz9k3tyKAi3Id5BTbVRAJmfu3YERo3hqVL4bTT4iuLKgIlTIJVBPcScUmpZCeZFEEeNm3Gemyi2QBkZUCbxrBlfwTnPEJCKoK0NGse+vxza6fPzIyfLAcP2ollOTnxk0FJSIJSBMaYu12WIwlIJkXgNbEsWAryoqAIEpQ+feCTT+Cbb2wEUzzZtQs6dAjcTlG8CEoRiMiHPjYfAP5rjHkyuiIlKtnYrJ7JMtW/Mzbh7KfYn0k2cAp+rYEd82D+2gjOl8CKoFs3+xa+ZEn8FcHOnaoIlJAJ1jQ03M/2MSLS3BjzxyjJk+A0JnkUwQ+czxe9tg0BrsJn2u362ZDfMIIJaIeBdWEeG2cygZ4dYdliqBgMabXF8qcDHQi7BkMg1E+ghEGwiuB+bMK5W7FPgQeBtUALYAKgigCwiiBZ/hE7An/BRg8BLADeA9oCP/JzSF4EiqAMSOB8QX3zoOhbeOgFyKw2akoTuLAPtPeE2R4HTnZHjt27dWKZEjLBKoJJwJ+MMWsBRGQeVin8FBjrkmwJSDKFkELV6xmDVXKvYzOM9KrZvF976NMuvFOVlcMLn4V3bF2gdzs7C/vIcSirFj21cS+0auSlCHZgs74GEZEVKidOwLPPRr9fJTCFhTacOAEJVhFsBe4XkfOx0UODgVXY8BKf02lFpD3WrtDSOeZpY8yj1doI8ChwLjZ+8GpjzFdhXEcdIZkcxtVJw0YR/xV4Bvgd9tZ6IQLpYb6JpqfZWcwlESSgiyc5mXDjCN/7/vIf2FD93+Q7oB4R1Wz2R9hhvEpE1IVJhWESrKHyCmA58ENgKPA1djSwE5js55gy4FZjTA/gNGCSiPSo1mY00NVZJgIJ7nhug3UYJyvZ2MGhAK8S9Yjihkn63RXkwcZ9UF79Ab0SWAgswpYLXeIsS0keX1MKURJB2dU4E5QiMMZ8bYzpj7UVNDHGDHC2fWyMedPPMds9b/fGmMPYEUTbas3GYHMYGWPMQqCJiLQO+2riTjZQGG8hXKYZcAH2IbY0ul03inMGT7folAcnymFb9XKmBusvKME6yw85y0HgGxJ2tnWqkuyKQERyReQB4GPglFBLVYpIAdAP+LzarrbAZq/1LdRUFojIRBFZJCKLdtf5+qw/IKJMmgnBGdjRzz+JqJZAdRol64iguf2sYR6qjUPARjekUdwi2RUBtsJ5WKUqRaQB8AZwszHmUDhCGmOeNsYUGmMK8/Pzw+kihqRhwyyTmXRsneO9wH+i122ymobyG1j/x4ZQawtvBiKZpKfElARWBME6iy/Glqr8jbNeBFwZ6CARycQqgel+TEhbqRo60Y6EjiH00AbohHUIJivdgIFYRTAYaB55l8k6IhCxfoKQRgQeviGyEWZ7rFNacZ3ycjh+HLICpG6vgwSrCEIuVelEBD0LrDLGPOSn2dvADSLyKnAqcNAYkyQ1B08ncObuCmAuiZvG6RKsk/M/wPjIu0tWHwFYP8HsFXCsDLKD/bcDW8ozkrkp6cBJERyvhERJSVIrgnBKVZ6OHTV8LSJLnG2/w06rxBjzFDAbGzq6FusxS6LCN/X4PotnbSwB9rksi1s0xZrBFgDnYxPVRUC9LBtGWiO6JgkoyLMTvTbtg64tYnjindjRqSYKjgklJbaOdoLhWqlKY8x8fOYiqNLGYOMRU5gWJK4iADvLeB7wIbZ+UYQ0yoX9iRuP7ZcCZ77Ahr0xVgTlwC4ggYPxEokE9RMEGz56yBhzjTGmhbP8DJuDQImYuu78DkRLbEDYx0Ql9r1hduR91EUa5UJeffguVIdxNEgSa2sikKyKQEQuEZHbROQMZ/0UEZmBtWkoEZPoigBgFNayNz/yrpLZTxC2wzhSirHhqIrrJKgiqNU0JCKPAjfgVCQTkUewppwsbOSQEjHNsPbbIOoD11k6YSeHzwFGEJE9Olkjh8AqgqJNcKg0Dte5nYh9OEpgklERAJdh58BPw/6HT8FWNb/JGPOOu6KlCmnY0MtEz1p6NvA49v1gUPjdJPuIAGDBOlveUwS65IcYRRQuu7FhzS6lvw6LNGzyvSQiSRVBPnCLMeZlEZkDXAv8VpVAtMkn8RVBT6xCm09EiiBZfQQAHZrZFNUzvKyqo3vChbHIWFmBDfWtSwj2d5NEM/GTVBEIcIuIXI6NFjLAFBG5Ehv0M8ZtAVODWEaRuIVnRvXbwB7CnmCWrLOLwWYovfs8OHjUrr/0eZx8BnUFg01B1oeErFfti6NH4y1BWAQzJu3vLB5Ocz4TdRZUHSQZHMZgZxi/A3yGnVcQBhnpiZ2OOhDNG9gFoFNzWLYlxQvJlGMTG/clKTL3HjtmZxinJ9a8jUCKoFNMpEh5GmP974n+8GuGTbq3EDvlJEx7dKPc5FUE3rRrCp+usyOEJqmcBuI4sAxo6Gd/JxJKSZSUQEN/11I3CaQIDhpjDtTWQESaBGqjBEMLbPLVRGcw8BywBpuPKAwa5cCO6imbk5D2Te3nlv0prgjAzkHxNw/lMDbfZYIogwRUBIFe2baKyAsiMlZEOopIpohkiUiBs+1FkuPpVQdIFvNQP+w/7ILwu0jmyCFv2jmpCDZrhtHaKcWOGBLE/p6AfoJAI4I7sDmGrqSmT0CwCdPvcEGuFCRZFEEWNivp58A4wnqLS+bIIW9ys6y/QBVBEJRiCyElQDnYkgzcSbzQCPu/FX1qVQTGmKnAVBEZii1T6UkZvQmY7+QTUqJCC+ztKIu3IFFgCDb/0J3YYLMQ6VQBLeuwj8AAR3OgpB6U5EBFBLH5DwPFO+A0l+ZnGoHyNChPj0zOYCjLgFUnQYVbjtLj2PkQdZyStbjzf9yCuCgCD8aYedj/bMU16gE/w775HMKdmrXl2DKIB7B2V1+BX0ecfZHQCessDjc0shx2xCMnT5CkVUBuKTQ9AG1LQSLIlnpSBZRVQM7a6MnnTZqx8qbFKMjvWBasTfEYkwQMdAhKEYjIcz42HwDmGGNmR1ekVCeH+DrFSoDXiUwRCbaucZhkAPM/hbJETrsRJEs2w5OfwG/Pgs5RKO7jD6mwCsG1/g2Mewvab1NFkKyKALga+/roCXb2/H2TiExyagsoSUE9YCjw3/iK0SgH9iVhOurqeEcOuakITJo1EbnJltbQfrtVOqYupbKIMQmoCIK9Ww9iZwmNwiaV+Qx4Avu0mOyOaEr86ETYoZ/RIlUih5rVtxPoksFhvKkN5ByD5olcXyMKJLEiuAp4xRgzxxjzX+Bl7NTRh4ACl2RT4soQ4pqtMpmzkHojYsNIk0ERbGljbQUdtsVbkvhy9ISdLZ5ABGsaKgH+V0Q82cTGYD2BuQSoXawkKpnYmsQnnPUybOLZb4EYPLRSRRHA9zOMKyogLYFNKseyYVdz6yco6h1vaeKHMVYZ1Euc2sXBKoLrgOnY+QQAO5xtDYF7XZBLqRNkUjX8s7ez7MN3Oow9RDSRzJtUMQ2B9RMcK4PdxdAywWsGbG4DA5ZBTimUppAyr07J8eRTBMaYD0WkIzaRDMA3xpjEM4QpUcJf2uDGRE0RNE4hRdDOcRhv3p8ciqBwGbTbntrRQ0cT6/EYbPhoJvA7YLSz6d8i8idjzIlaDlNSjlxs1FEUcrLXz7JmkgoXQx7rCq0bQ5rA85/Z1NTVaVIPOuXZwjbN6n+/vX3TupejaHczOJqtYaQJ5jAO1jT0V+AmbHULgEKgCTb9hKJ4kUdUFEFaGjTIhkOJl7clZDLT4cpT/TuMdx2GZVthwfqq2+tlwQ3DbZWzOoPYUUGHbakdRrp6JxyI8m+3Qamt4+MCwSqCS4G/A7/Czh94AlvGUhWBUo08YHN0umqUkxqKAGBIl9r3GwN7iqH4mF0/XgYvfQEPfwC/GAqntHVfxmDZ3AZO/g7G/jv4tBZG4Ms+sLkOXUckbD9ol2jSgrgrglzgW49fQERWAxe5I5KS2ORFr6vGuXailWLDTPMb2sXDbaPgsbnwxMcw/OTvax+f3BJ6tI6PnAAb28K3nSErBPNI/j44fRH8s5WLuYoUfwSrCD4B7heR87GRwqcBs1yTSklgoqgIUilyKBwa5cAtP4Jn58NHq+02Y+A/K+F3o7+ftRxryjLh48GhHdN+K4z+CLqtg1UnuyKW4p9gFcENQFNs7gGAj4EbXZFISXAaE7Usqqk0lyBccjPhhhHfrxeXwt3/hhc+gzvOgfQEsdFvbgM78qH/cljdGcqDfTQp0aDWX4mIvC0ib2N9AgeBOc5y2NmmKNUQ/IeXhogqgtBpkANXDLSO5/+sjLc0ISDwRV+ofxR6fRtvYVKOQGr3vFr2JdYcaiWG5AG7Iu+moSqCsOjfAQo7wqyvoU87aNsk3hIFx44WNl9Rn5V2VHAijFoWiYQRWyeiDqDF6xUXiJKfICMd6mfDkWPR6S+VuLwQvt0Bf3oPsuJsZslOh6sHQ7dWgdt+2QcueReufNN9ueKNAd4fBhvbB2zqNoEqlG2MlSBKMhHFdMqNclQRhEPDHJg0HBZ+F29JYPk2ePFzuOvHgZXS3mb24dgo0uJICUD3NXYW9sZ2fJ/hPz649qrgFLM5D9hljOnlY/9w4C3A80t90xijeYuSgij5CMBGDkU7HjtV6NTcLvFm9U742xz499dwUb/A7TfE/w05JhzNgRGfQYetsKldXEVxM6TgeeCcAG3mGWP6OosqgaQhg6gVGVeHceJzcksY0hneXwVbD8RbmrrD2gI4VB/6LSfeLlfXRgTGmE9EpMCt/pW6Th420CxCdC5BcnBJP1i6FaZ/DjedFW9LSOikp0U/FNekwdIeMPRLaLMTtgXhQ3GJeAfrDhaRpcA24NfGmBW+GonIRGAiQIcOHWIonhI+ecD6gK0CoiOC5KBBDvykv02sN/m1eEsTOpnp0LstnNoJera2gQzRYHUXO3ei3/KUVQRfAR2NMcUici4wE+jqq6Ex5mngaYDCwkINW00IOgBRyBPU6DhRCUWtlf1EJVGeUjundbJv1fsTsBb13iNQtMkuGX5GBzmZcNvIqmlAAlGeDsu6w+CvYPSHUFHLUCk7F1tC9uoQhQ9M3BSBMeaQ19+zReQJEWlujNkTL5mUaJKHLXcZIdlA451w0E2H8SZs9TXFVURgUEG8pQifywph5Xbr/K7+OmqMTfPx/ioYP8jn4X5Z1RXa7oDc0trbZVRg5/JGn7gpAhFpBew0xhinBGYatvylolSlbVuXFUEDF/tWkob0NJvl1V+m12NlsGAdnH9KaL6tsgx4b0Tgdi3aw4XuZPZxLWpIRF4BPgO6icgWEblWRH4pIr90mowFljs+gqnA5cYkWMVnJTa0aePyCVQRKFFgZHcor4APEi9FhptRQ+MC7H8ceNyt8ytJhOuKIMtZEquqlFLHaNnIpvf4aDWc0wNyE6dmcYKkJlRSmpwcyItiemuf6KhAiQJn94DSE/DJmnhLEhLxDh9VlOBo2xb2uulCagDsc7F/JSXomAfdW8Gcb2y51WjSYBs0+wSGDYtuv6giUBKFNm1g2TIXT6AjAiVK/PgUeGgOvP5V9PtuOUgVgZLCtG5tww9diydQRaBEia4tYOpl1nEcTfLbwsX3RLdPB1UESmKQmQktWsDOnS6dIAfIBE641L+SUmSm2yWa5GZDdnZ0+3RQZ7GSOLT1E78dNeq73L+i1E1UESiJg84nUBRXUNOQkji0bAk9erh4gkZAlO26icy2A3BAczClAqoIlMQhPR1++EMXT3AAOBSwVcpQtNEuStKjpiFFqaQx+m7kRTP1maQKqggUpRLBZk1VAGhaL94SKDFCX38UpQpD0doEDo0NpJdBeXkMTnYcSLxkbcmCKgJFqUIzZ1EQoGln2BOrEiFbgeIYnUvxRk1DiqL4p1kslWLLGJ5L8UYVgaIo/ompImiBPpLig37riqL4J6aKIBM1y8UHVQSKovinadMYn1DNQ/FAFYGiKP6pX9+1RGe+aYqtFqfEElUEiqLUTkxHBWlYX4ESSzR8VFGU2mnWDHbsiOEJ26IJAH3RyrWeVREoilI7MXUYA2SjowJfNHetZzUNKYpSOzFXBEqsUUWgKErtqCJIelQRKIpSO1lZ0EBt9smMKgJFUQIT8/kESixRZ7GiKIEZNQqMCe/YI0dgxgw4fjy6MilRQ0cEiqIEJj0dMjLCWxo3huHD430FSi2oIlAUxX0KCuCUU+ItheIHVQSKosSGU0+FFjo/oC7imo9ARJ4DzgN2GWN6+dgvwKPAudiSUFcbY75ySx5FUeJMWhqcdx4cOxZvSdxh6VJYvjzeUoSFm87i54HHgRf97B8NdHWWU4EnnU9FUZIVj98gGWnSJN4ShI1rpiFjzCfAvlqajAFeNJaFQBMRae2WPIqiKK7SsGG8JQibePoI2gKbvda3ONtqICITRWSRiCzavXt3TIRTFEUJiQSedJcQzmJjzNPGmEJjTGF+fn68xVEURalJ/frxliBs4qkItgLtvdbbOdsURVESj6wsuyQg8VQEbwNXieU04KAxZnsc5VEURYmMBDUPuRk++gowHGguIluAu7DVqTHGPAXMxoaOrsWGj17jliyKoigxoWFD2FdbjEzdxDVFYIwZF2C/ASa5dX5FUZSYk6AjgoRwFiuKoiQEqggURVFSHFUEiqIoKY4qAkVRlBRHFYGiKEqKU6+eTa6XYCSexIqiKHUVkYScYayKQFEUJZokoHlIFYGiKEo0UUWgKIqS4qgiUBRFSXFUESiKoqQ4qggURVFSHFUEiqIoKY4qAkVRlBQnMxOys+MtRUioIlAURYk2CTYqUEWgKIoSbRJMEbhWmEZRFCVladoUdu6Mbp8u1kNWRaAoihJtBg2yS4KgpiFFUZQURxWBoihKiqOKQFEUJcVRRaAoipLiqCJQFEVJcVQRKIqipDiqCBRFUVIcVQSKoigpjioCRVGUFEeMMfGWISREZDewMczDmwN7oihOIqDXnBroNacGkVxzR2NMvq8dCacIIkFEFhljCuMtRyzRa04N9JpTA7euWU1DiqIoKY4qAkVRlBQn1RTB0/EWIA7oNacGes2pgSvXnFI+AkVRFKUmqTYiUBRFUaqhikBRFCXFSRlFICLniMi3IrJWRG6PtzxuICLtRWSuiKwUkRUicpOzvZmI/FdE1jifTeMtazQRkXQRWSwis5z1TiLyuXOvXxMR92r8xQERaSIir4vINyKySkQGp8A9nuL8ppeLyCsikpNs91lEnhORXSKy3Gubz/sqlqnOtS8Tkf6RnDslFIGIpAPTgNFAD2CciPSIr1SuUAbcaozpAZwGTHKu83bgA2NMV+ADZz2ZuAlY5bX+F+BhY8xJwH7g2rhI5R6PAu8ZY34A9MFee9LeYxFpC0wGCo0xvYB04HKS7z4/D5xTbZu/+zoa6OosE4EnIzlxSigCYBCw1hiz3hhzHHgVGBNnmaKOMWa7MeYr5+/D2AdEW+y1vuA0ewG4MD4SRh8RaQf8GPg/Z12AM4HXnSbJdr2NgWHAswDGmOPGmAMk8T12yAByRSQDqAdsJ8nuszHmE2Bftc3+7usY4EVjWQg0EZHW4Z47VRRBW2Cz1/oWZ1vSIiIFQD/gc6ClMWa7s2sH0DJOYrnBI8BvgApnPQ84YIwpc9aT7V53AnYDf3fMYf8nIvVJ4ntsjNkKPAhswiqAg0ARyX2fPfi7r1F9pqWKIkgpRKQB8AZwszHmkPc+Y+OFkyJmWETOA3YZY4riLUsMyQD6A08aY/oBR6hmBkqmewzg2MXHYJVgG6A+NU0oSY+b9zVVFMFWoL3XejtnW9IhIplYJTDdGPOms3mnZ9jofO6Kl3xR5nTgAhHZgDX3nYm1nzdxTAiQfPd6C7DFGPO5s/46VjEk6z0G+BHwnTFmtzHmBPAm9t4n83324O++RvWZliqK4EugqxNlkIV1NL0dZ5mijmMffxZYZYx5yGvX28AE5+8JwFuxls0NjDF3GGPaGWMKsPf0Q2PMeGAuMNZpljTXC2CM2QFsFpFuzqazgJUk6T122AScJiL1nN+455qT9j574e++vg1c5UQPnQYc9DIhhY4xJiUW4FxgNbAO+H285XHpGn+IHTouA5Y4y7lYu/kHwBpgDtAs3rK6cO3DgVnO352BL4C1wL+A7HjLF+Vr7Qsscu7zTKBpst9j4B7gG2A58A8gO9nuM/AK1gdyAjvyu9bffQUEGwm5DvgaG1EV9rk1xYSiKEqKkyqmIUVRFMUPqggURVFSHFUEiqIoKY4qAkVRlBRHFYGiKEqKo4pAURQlxVFFoCiKkuKoIkhhnJzu20TkLyJSICLGa9knIq+KSF6YfdcTkbtF5Opa2njOOSuI/irb+uo72L6qtwtFBj/9VZEl0v68+s0TkaMicrOf/bV+H9Eiku86jHOdJSL/iGafSpDEezadLvFbsDMXDXASUOD8/RUwDpuvyADPhtl3c+f4j2ppUx+bGmJoEP1VtvXVd7B9eV3nrFBlCOY6I+2vQy61UgAABC1JREFUWt8vARtwaouH8n2EeJ6MUO5jNK+x2rluAW6JZp+6BPndx1sAXeJ48+3U9ZXO39UfkN2d9eXO+s+x09yPYKf1/9DZ3sLppxg4hE17ne88wIzXcreP81c/p2d9AfCu09/L2On0lW199V1tfz6w2JGpGJgH9AxwzlnA1dX6Nc622vqrLsvz3v0H+O78Xq+z/zJn/+Davjt/3zXwM+Bb57wLgP4+zjsH2OnvGgN915FeY7VregEYgU0f8Tzwv77a6RL9RU1DKYpTte00bEI+bzJFJJ/vC2BsEpEzgaexefBvAToAbztmo/HYrJ9/A27F5jdKB37nHL8KO8J43TEzNHeWBrWIdyrwCfYhNg6bQ8mbGn1X21+BzVB5E/BnbBWvR2o5n4ePnf6uAvYAx7F5XGrrr7osD3p3GOC7C3S9nnszNIDcvr7r4dgEhBuA+7A5a94RkRyv4wZj8/r/Ty3XGOi7jvQavemNza75H2COMeZ3xtEQisvEWxPpEp8FW+DCAH9y1guo+Ta8BZvg7EFnfaTT9n5n/cfAec7f87EPkDOdNr5MCndT9c3Zc84aIwJn/XZn/UqqvgH76tt7fxvgU+zDzXO+HdXb+Vp3tj3nbBvvrNfWX3XTUPX+a/vu/F6vs57jrD/h4/4F+j4e8HE/DTZltefYr7za+7zGQN91pNfo1WcmtuDMMnyMgHRxd9ERgSLV1j/H5n/vD3Qxxizx2meqfWKMmYUdWbyHfcv7QER+5N3GixeBkc7y11pk8pTr81SfSq+2P9Bb4mRgCPaNdhRWoeXUeoSDiPweuAa4yxgzPYj+gn1jrfHdeeHveqvfm0B9++JWvv/Ozwa+89q3zetvf9cYyht5ONfooTt2BFQGlIdwTiUKqCJIXfYAR7FvglW2G2M+MMYsNsYcc7bNdj7vEZFfYJ3M+4GFIjIWOyrYDKxw2rXB2oIrgJNEZLyIdDS2ZvQcZ1kZgew1+vbTrim2vm+7YDoVkfOBP2Jt3atF5HIR6RSgvyqyANVl8fvdBSGS595sDNDO1/fxb2ffOKyp5lRgqjFmf4C+ql9jMN91JNfooQ/Wj3A5tgxn0pTaTARUEaQoxphy4DOgMIi2HwITsY7hh7BvixcYY/YCJcAlwFPApcBrwOvGVpJ6AGiCjX4JZOcORfZAfT+Gfbu8DFvHdXmQXQ/AvoV3xeaGfwU4o7b+AskS4LsLhOfefFJbI18yGGM+wo5sGmDz1k/EPmj94fMag7mPEV6jhz7YwITVwG+BfzrV9pQYoPUIUhgR+RnWodjVGLM23vIoVRGRl7Dmtk5G/1EVF9ERQWozHVsR6efxFkSpiog0Ay4GHlEloLiNjggURVFSHB0RKIqipDiqCBRFUVIcVQSKoigpjioCRVGUFEcVgaIoSoqjikBRFCXFUUWgKIqS4vx/PVlRVRrFOT4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c9fcd7-0332-4556-aad9-c1b70438801b"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7424.317836999893, 4007.168317556381)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    }
  ]
}