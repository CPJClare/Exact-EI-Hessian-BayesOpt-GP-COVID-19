{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "xxx3a__Rosenbrock.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Rosenbrock synthetic function:\r\n",
        "\r\n",
        "GP EI: Newton-CG (exact GP EI gradients + exact GP EI Hessian) vs. L-BFGS-B (exact GP EI gradients, without Hessian)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/rosen.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "609c7565-d031-4b7a-ad6f-95afdc3f79f4"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.6/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp36-none-any.whl size=19867 sha256=5dbde4970c0e7dd4c096c30383dabd1c1d70d361941918807d69310d9fb6bd83\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "\r\n",
        "rc('text', usetex=False)\r\n",
        "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\r\n",
        "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "n_start_AcqFunc = 100\r\n",
        "\r\n",
        "obj_func = 'Rosenbrock'\r\n",
        "n_test = n_test = n_start_AcqFunc # test points\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dEI_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Rosenbrock':\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = 0\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "# Constraints:\r\n",
        "    lb = -2.048 \r\n",
        "    ub = +2.048 \r\n",
        "    \r\n",
        "# Input array dimension(s):\r\n",
        "    dim = 2\r\n",
        "\r\n",
        "# 2-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "# Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test)\r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training):\r\n",
        "        return operator * (100 * (x2_training - x1_training ** 2) ** 2 + (x1_training - 1) ** 2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 99\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1e-4\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r **2 - 1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "### Acquisition function derivatives:\r\n",
        "\r\n",
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1e-4\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(v.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: Exact Hessian\r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    p = np.full((n_start,1),1)*0 + 1\r\n",
        "    eps = 1e-08\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "\r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        d2f = (z * norm.cdf(z) + norm.pdf(z)[0]) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx\r\n",
        "\r\n",
        "        return d2f\r\n",
        "\r\n",
        "    def hessp_nonzero1(self, xnew, p, *args):\r\n",
        "      new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "      new_std = np.sqrt(new_var + 1e-6)\r\n",
        "      ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "      df2 = np.empty((self.n_start,))\r\n",
        "      df2 = -self.dEI_GP(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0] * p\r\n",
        "      return df2\r\n",
        "\r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,                  \r\n",
        "                                                                 hessp = self.hessp_nonzero1,                      \r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "\r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):    \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "a0fb8567-d272-4f42-c1ef-695f346530f9"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1612263718.2285032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "1a587975-4941-4fea-e3a9-4e8ba429271f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_1 = d2GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.45795185 -0.30236194]. \t  \u001b[92m-28.3484050352548\u001b[0m \t -28.3484050352548\n",
            "2      \t [ 1.77122139 -1.9380348 ]. \t  -2576.421199974247 \t -28.3484050352548\n",
            "3      \t [-0.12226583 -0.75195384]. \t  -60.073467399432175 \t -28.3484050352548\n",
            "4      \t [1.88493603 1.44424789]. \t  -445.45983372037045 \t -28.3484050352548\n",
            "5      \t [-0.50551879  1.86210364]. \t  -260.36828725301234 \t -28.3484050352548\n",
            "6      \t [ 1.77169217 -0.26658693]. \t  -1160.3249553860608 \t -28.3484050352548\n",
            "7      \t [ 1.78803192 -0.60648493]. \t  -1447.3149830671293 \t -28.3484050352548\n",
            "8      \t [-0.54544849 -0.38504307]. \t  -48.97683286981722 \t -28.3484050352548\n",
            "9      \t [-0.08330601  0.31488921]. \t  \u001b[92m-10.656830259258513\u001b[0m \t -10.656830259258513\n",
            "10     \t [-1.46718824  0.91875406]. \t  -158.33479736047443 \t -10.656830259258513\n",
            "11     \t [-1.70623014  1.59082448]. \t  -181.66845577531566 \t -10.656830259258513\n",
            "12     \t [-0.76725301  0.86871125]. \t  -10.96509119669025 \t -10.656830259258513\n",
            "13     \t [-0.48162569  0.25810536]. \t  \u001b[92m-2.2635554104963815\u001b[0m \t -2.2635554104963815\n",
            "14     \t [-0.43753613  0.31418511]. \t  -3.5731987890346355 \t -2.2635554104963815\n",
            "15     \t [-0.13798151  1.2120092 ]. \t  -143.61281532895606 \t -2.2635554104963815\n",
            "16     \t [ 0.12139683 -0.09606768]. \t  \u001b[92m-1.9997155518564367\u001b[0m \t -1.9997155518564367\n",
            "17     \t [-0.2117579   0.07841845]. \t  \u001b[92m-1.5810990119982622\u001b[0m \t -1.5810990119982622\n",
            "18     \t [ 0.74775298 -0.63960254]. \t  -143.7606812354837 \t -1.5810990119982622\n",
            "19     \t [ 0.17586742 -0.17789148]. \t  -5.039808372010475 \t -1.5810990119982622\n",
            "20     \t [0.28219563 0.04960344]. \t  \u001b[92m-0.6054287969736434\u001b[0m \t -0.6054287969736434\n",
            "21     \t [-0.37843791  1.36814964]. \t  -151.94651586396787 \t -0.6054287969736434\n",
            "22     \t [-0.48004043 -1.57795417]. \t  -329.21903601179304 \t -0.6054287969736434\n",
            "23     \t [1.06089113 1.82174536]. \t  -48.480860367576014 \t -0.6054287969736434\n",
            "24     \t [-1.92372447 -0.91331464]. \t  -2137.475875853538 \t -0.6054287969736434\n",
            "25     \t [0.70604766 0.83276765]. \t  -11.25967324933536 \t -0.6054287969736434\n",
            "26     \t [0.30906968 0.13282087]. \t  -0.6164898518294839 \t -0.6054287969736434\n",
            "27     \t [ 1.160157   -0.46970718]. \t  -329.69192664755565 \t -0.6054287969736434\n",
            "28     \t [-1.02080879 -0.13940014]. \t  -143.66625118755718 \t -0.6054287969736434\n",
            "29     \t [0.60444415 0.33243615]. \t  \u001b[92m-0.26481459745132657\u001b[0m \t -0.26481459745132657\n",
            "30     \t [1.51037191 0.4337747 ]. \t  -341.56711141881 \t -0.26481459745132657\n",
            "31     \t [0.94980596 0.89421683]. \t  \u001b[92m-0.008783433047088498\u001b[0m \t -0.008783433047088498\n",
            "32     \t [0.53386496 0.02594546]. \t  -6.928818537696993 \t -0.008783433047088498\n",
            "33     \t [-1.39253331 -0.59085225]. \t  -645.8148494698042 \t -0.008783433047088498\n",
            "34     \t [-0.85840066  1.22147162]. \t  -26.939300528904813 \t -0.008783433047088498\n",
            "35     \t [ 0.81411133 -0.95859836]. \t  -262.92044163522456 \t -0.008783433047088498\n",
            "36     \t [-1.23462122  0.38938055]. \t  -133.79537733532314 \t -0.008783433047088498\n",
            "37     \t [-1.22067096  1.44149904]. \t  -5.166978555590179 \t -0.008783433047088498\n",
            "38     \t [-0.26683513 -0.66227277]. \t  -55.40324609566223 \t -0.008783433047088498\n",
            "39     \t [-1.16209919  1.86829281]. \t  -31.488249479325447 \t -0.008783433047088498\n",
            "40     \t [-1.53740048  1.2231408 ]. \t  -136.50317180697115 \t -0.008783433047088498\n",
            "41     \t [1.9426309 1.9633803]. \t  -328.65586656807943 \t -0.008783433047088498\n",
            "42     \t [0.08183052 1.23046565]. \t  -150.60419279831945 \t -0.008783433047088498\n",
            "43     \t [-1.16223016 -1.42849377]. \t  -777.1109183233424 \t -0.008783433047088498\n",
            "44     \t [-1.04316404  1.15873988]. \t  -4.672230813198803 \t -0.008783433047088498\n",
            "45     \t [-0.81179756  0.62178539]. \t  -3.421216919523331 \t -0.008783433047088498\n",
            "46     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.008783433047088498\n",
            "47     \t [-1.21082569  1.68390588]. \t  -9.631740102570523 \t -0.008783433047088498\n",
            "48     \t [-1.33437787  0.68815167]. \t  -124.78585917988336 \t -0.008783433047088498\n",
            "49     \t [-0.23094754 -1.61276938]. \t  -279.10620252253784 \t -0.008783433047088498\n",
            "50     \t [0.66014216 1.64749383]. \t  -146.93868689891644 \t -0.008783433047088498\n",
            "51     \t [-0.43226986  1.33958364]. \t  -134.92921491596502 \t -0.008783433047088498\n",
            "52     \t [1.22158747 1.08415614]. \t  -16.70527922550893 \t -0.008783433047088498\n",
            "53     \t [1.03050511 1.23641547]. \t  -3.0450722445171614 \t -0.008783433047088498\n",
            "54     \t [ 0.42173736 -1.26611057]. \t  -208.84018173094518 \t -0.008783433047088498\n",
            "55     \t [-0.5189422  -1.68782909]. \t  -385.34301020541506 \t -0.008783433047088498\n",
            "56     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.008783433047088498\n",
            "57     \t [ 0.54384183 -0.89280069]. \t  -141.4766651669655 \t -0.008783433047088498\n",
            "58     \t [0.8051323  0.49947153]. \t  -2.2511201711580875 \t -0.008783433047088498\n",
            "59     \t [0.40937437 1.18707475]. \t  -104.28428965658428 \t -0.008783433047088498\n",
            "60     \t [1.02250523 1.72285278]. \t  -45.878888676329034 \t -0.008783433047088498\n",
            "61     \t [0.65441293 0.46874256]. \t  -0.2833442488029221 \t -0.008783433047088498\n",
            "62     \t [0.98075065 1.02514641]. \t  -0.4007377904572951 \t -0.008783433047088498\n",
            "63     \t [1.05556557 1.03764811]. \t  -0.5893927390118394 \t -0.008783433047088498\n",
            "64     \t [-1.00705448  1.16548517]. \t  -6.318237027567069 \t -0.008783433047088498\n",
            "65     \t [0.46552207 0.2217355 ]. \t  -0.28819142190364433 \t -0.008783433047088498\n",
            "66     \t [-0.05750687 -0.24363503]. \t  -7.216359370591485 \t -0.008783433047088498\n",
            "67     \t [ 1.46348283 -1.48988797]. \t  -1319.1174901038878 \t -0.008783433047088498\n",
            "68     \t [0.58030806 0.31242197]. \t  -0.2353628420447037 \t -0.008783433047088498\n",
            "69     \t [0.61277162 0.34978262]. \t  -0.2160279219698361 \t -0.008783433047088498\n",
            "70     \t [-1.55333206 -1.38547729]. \t  -1449.24130480916 \t -0.008783433047088498\n",
            "71     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.008783433047088498\n",
            "72     \t [-0.28009762  1.23351318]. \t  -135.0546635396757 \t -0.008783433047088498\n",
            "73     \t [0.5994469  0.37866452]. \t  -0.19779968761968295 \t -0.008783433047088498\n",
            "74     \t [-0.27488168 -1.9321867 ]. \t  -404.7299793637241 \t -0.008783433047088498\n",
            "75     \t [-0.24921281  0.39865629]. \t  -12.887073322359871 \t -0.008783433047088498\n",
            "76     \t [-0.52180996 -0.0714736 ]. \t  -14.132946809213406 \t -0.008783433047088498\n",
            "77     \t [ 0.215878   -0.57694162]. \t  -39.495675767076335 \t -0.008783433047088498\n",
            "78     \t [1.02999374 2.00653251]. \t  -89.42542473466041 \t -0.008783433047088498\n",
            "79     \t [-0.4332399 -0.1926707]. \t  -16.522120495331716 \t -0.008783433047088498\n",
            "80     \t [1.01751313 1.02897361]. \t  \u001b[92m-0.004350858845025438\u001b[0m \t -0.004350858845025438\n",
            "81     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.004350858845025438\n",
            "82     \t [1.03758783 1.07409633]. \t  \u001b[92m-0.002033935134886275\u001b[0m \t -0.002033935134886275\n",
            "83     \t [ 0.74413541 -1.9884645 ]. \t  -646.3445759354339 \t -0.002033935134886275\n",
            "84     \t [1.18993284 1.65277456]. \t  -5.6451275735506075 \t -0.002033935134886275\n",
            "85     \t [-1.82086888  0.66929264]. \t  -708.232246310807 \t -0.002033935134886275\n",
            "86     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.002033935134886275\n",
            "87     \t [-0.39927098  1.30131624]. \t  -132.3512741366938 \t -0.002033935134886275\n",
            "88     \t [-1.08176543  0.14264375]. \t  -109.92430978318076 \t -0.002033935134886275\n",
            "89     \t [0.88647147 0.72135748]. \t  -0.42858072349503895 \t -0.002033935134886275\n",
            "90     \t [ 1.21960029 -0.11987661]. \t  -258.3900302193461 \t -0.002033935134886275\n",
            "91     \t [0.26039215 0.32972883]. \t  -7.407477600251035 \t -0.002033935134886275\n",
            "92     \t [ 1.46806628 -0.6833924 ]. \t  -805.9903261107626 \t -0.002033935134886275\n",
            "93     \t [1.10093847 1.23767273]. \t  -0.07576147281820803 \t -0.002033935134886275\n",
            "94     \t [-1.17118342  0.76949043]. \t  -40.976133562082566 \t -0.002033935134886275\n",
            "95     \t [ 1.65361628 -1.69356049]. \t  -1961.1520765663745 \t -0.002033935134886275\n",
            "96     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.002033935134886275\n",
            "97     \t [1.05498416 1.12829335]. \t  -0.026437674680498402 \t -0.002033935134886275\n",
            "98     \t [ 1.72114382 -0.85529876]. \t  -1457.9536047382583 \t -0.002033935134886275\n",
            "99     \t [0.34268958 0.38226986]. \t  -7.445746616971624 \t -0.002033935134886275\n",
            "100    \t [-1.0954881   0.33887054]. \t  -78.56168510444935 \t -0.002033935134886275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "dd520a92-59d7-4be0-a96a-2ebcf1144710"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_2 = d2GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.40493055 -0.29547146]. \t  -21.462638094675896 \t -1.3013277264983028\n",
            "2      \t [ 0.6294657  -0.02191359]. \t  -17.621456043597487 \t -1.3013277264983028\n",
            "3      \t [0.906422   1.79209788]. \t  -94.19520550936414 \t -1.3013277264983028\n",
            "4      \t [ 0.54724866 -0.48268329]. \t  -61.383096561513206 \t -1.3013277264983028\n",
            "5      \t [-0.94652994 -0.77893148]. \t  -284.3013633178235 \t -1.3013277264983028\n",
            "6      \t [0.85433046 1.0916157 ]. \t  -13.106452817727035 \t -1.3013277264983028\n",
            "7      \t [0.96123244 0.65420663]. \t  -7.278612189792711 \t -1.3013277264983028\n",
            "8      \t [ 0.96743136 -0.15673713]. \t  -119.39177174495406 \t -1.3013277264983028\n",
            "9      \t [1.79103062 2.03474241]. \t  -138.22995120200898 \t -1.3013277264983028\n",
            "10     \t [-0.28608843  0.21765342]. \t  -3.4983729268590933 \t -1.3013277264983028\n",
            "11     \t [1.09927523 1.29648606]. \t  \u001b[92m-0.7856646410865664\u001b[0m \t -0.7856646410865664\n",
            "12     \t [-0.00601554  0.13819549]. \t  -2.9208667017592775 \t -0.7856646410865664\n",
            "13     \t [0.8930342  0.78234631]. \t  \u001b[92m-0.034435655449819004\u001b[0m \t -0.034435655449819004\n",
            "14     \t [-0.98932968  1.51504328]. \t  -32.71599065448416 \t -0.034435655449819004\n",
            "15     \t [-0.36201927  0.55679108]. \t  -19.979966654397607 \t -0.034435655449819004\n",
            "16     \t [-0.0391923  -0.06182002]. \t  -1.4813195938852055 \t -0.034435655449819004\n",
            "17     \t [-0.81373056  2.01208961]. \t  -185.52130938965064 \t -0.034435655449819004\n",
            "18     \t [0.73700681 0.40973621]. \t  -1.849864314517573 \t -0.034435655449819004\n",
            "19     \t [2.00013178 1.35546904]. \t  -700.6334995177767 \t -0.034435655449819004\n",
            "20     \t [-0.3413371  -0.26077155]. \t  -16.033398685166116 \t -0.034435655449819004\n",
            "21     \t [-1.6902548   1.97448886]. \t  -85.11323081524274 \t -0.034435655449819004\n",
            "22     \t [-1.8893095  -0.46960809]. \t  -1639.7797752822478 \t -0.034435655449819004\n",
            "23     \t [-0.6844637  -1.77908245]. \t  -507.9958598563524 \t -0.034435655449819004\n",
            "24     \t [-0.96614985  0.62774566]. \t  -13.210986279483986 \t -0.034435655449819004\n",
            "25     \t [-1.51451389  1.17862824]. \t  -130.6729505759675 \t -0.034435655449819004\n",
            "26     \t [-0.55471466  0.37472925]. \t  -2.8663177035662253 \t -0.034435655449819004\n",
            "27     \t [0.90815763 0.82467626]. \t  \u001b[92m-0.008435568268275922\u001b[0m \t -0.008435568268275922\n",
            "28     \t [0.15102971 1.10510574]. \t  -117.8571634189896 \t -0.008435568268275922\n",
            "29     \t [0.78377852 0.60925593]. \t  -0.04930484126695219 \t -0.008435568268275922\n",
            "30     \t [-0.46905516 -0.16980238]. \t  -17.353706206974486 \t -0.008435568268275922\n",
            "31     \t [ 0.99179266 -0.2276715 ]. \t  -146.7306964172385 \t -0.008435568268275922\n",
            "32     \t [0.44325224 0.573012  ]. \t  -14.48816406142393 \t -0.008435568268275922\n",
            "33     \t [-0.24541195 -1.78732495]. \t  -342.8958826553927 \t -0.008435568268275922\n",
            "34     \t [-0.08853253 -0.47931179]. \t  -24.91639580189656 \t -0.008435568268275922\n",
            "35     \t [-0.68739673 -0.15970921]. \t  -42.81796004380665 \t -0.008435568268275922\n",
            "36     \t [ 1.74293882 -0.80536341]. \t  -1477.5699245440094 \t -0.008435568268275922\n",
            "37     \t [1.19934537 0.68577621]. \t  -56.6884084518539 \t -0.008435568268275922\n",
            "38     \t [1.35390183 1.87561713]. \t  -0.30644105789699677 \t -0.008435568268275922\n",
            "39     \t [-0.98073994 -2.00640186]. \t  -884.9757352175648 \t -0.008435568268275922\n",
            "40     \t [-0.80967392 -0.64047878]. \t  -171.24964427357978 \t -0.008435568268275922\n",
            "41     \t [0.3484559  0.06067072]. \t  -0.7935756286650432 \t -0.008435568268275922\n",
            "42     \t [0.51660846 0.27127043]. \t  -0.23559119232837836 \t -0.008435568268275922\n",
            "43     \t [-0.75738889  0.35083364]. \t  -8.05259127383539 \t -0.008435568268275922\n",
            "44     \t [-1.55338422  1.07956766]. \t  -184.32462558936453 \t -0.008435568268275922\n",
            "45     \t [ 1.99501367 -1.7679637 ]. \t  -3304.990185125095 \t -0.008435568268275922\n",
            "46     \t [0.98756316 1.00519882]. \t  -0.0896622955846962 \t -0.008435568268275922\n",
            "47     \t [1.4153351  2.01332853]. \t  -0.18281581734714947 \t -0.008435568268275922\n",
            "48     \t [-1.29840811  1.51852954]. \t  -8.082749096971828 \t -0.008435568268275922\n",
            "49     \t [-1.10241157  1.24311903]. \t  -4.497461530222756 \t -0.008435568268275922\n",
            "50     \t [-1.69779039 -0.43012228]. \t  -1104.6195465731287 \t -0.008435568268275922\n",
            "51     \t [2.02328375 0.22771177]. \t  -1495.6159412297106 \t -0.008435568268275922\n",
            "52     \t [-1.51848669  1.61216493]. \t  -54.45599160830821 \t -0.008435568268275922\n",
            "53     \t [2.00029358 1.68210447]. \t  -538.8091297035712 \t -0.008435568268275922\n",
            "54     \t [-0.57362638  0.37193407]. \t  -2.6602281661743064 \t -0.008435568268275922\n",
            "55     \t [-0.84109744  0.58123478]. \t  -4.982539243900186 \t -0.008435568268275922\n",
            "56     \t [1.31403595 0.46630098]. \t  -158.95678900240353 \t -0.008435568268275922\n",
            "57     \t [1.96646943 0.26738163]. \t  -1296.6607706834602 \t -0.008435568268275922\n",
            "58     \t [-1.93116689 -0.85024414]. \t  -2105.910875856383 \t -0.008435568268275922\n",
            "59     \t [-1.32427512  0.34521736]. \t  -203.78588019736992 \t -0.008435568268275922\n",
            "60     \t [0.28483277 1.61744199]. \t  -236.53700601606423 \t -0.008435568268275922\n",
            "61     \t [1.57259458 0.00595459]. \t  -608.9856696190132 \t -0.008435568268275922\n",
            "62     \t [0.75928564 0.57005929]. \t  -0.06211061277655702 \t -0.008435568268275922\n",
            "63     \t [-0.58054926 -0.49970011]. \t  -72.51110784024942 \t -0.008435568268275922\n",
            "64     \t [0.90392287 0.81702618]. \t  -0.009231068843756568 \t -0.008435568268275922\n",
            "65     \t [-0.11136909  1.32217248]. \t  -172.78473047588355 \t -0.008435568268275922\n",
            "66     \t [ 0.08406173 -2.02016692]. \t  -411.80642514590784 \t -0.008435568268275922\n",
            "67     \t [0.99058262 1.01095785]. \t  -0.08832096084938067 \t -0.008435568268275922\n",
            "68     \t [-0.04636727 -0.07615456]. \t  -1.7080436883233414 \t -0.008435568268275922\n",
            "69     \t [-1.14585149 -1.04130742]. \t  -558.8695561563712 \t -0.008435568268275922\n",
            "70     \t [ 0.27362341 -0.21059029]. \t  -8.676367778288038 \t -0.008435568268275922\n",
            "71     \t [-1.69076328  0.3014827 ]. \t  -661.1662444475091 \t -0.008435568268275922\n",
            "72     \t [ 1.57804743 -0.83791126]. \t  -1107.9890290920496 \t -0.008435568268275922\n",
            "73     \t [0.95314975 0.73193607]. \t  -3.1194807181052044 \t -0.008435568268275922\n",
            "74     \t [0.88155552 0.77067489]. \t  -0.018209025228376006 \t -0.008435568268275922\n",
            "75     \t [0.84361066 0.69447202]. \t  -0.05406546866753627 \t -0.008435568268275922\n",
            "76     \t [1.76927873 1.12586904]. \t  -402.38507314741815 \t -0.008435568268275922\n",
            "77     \t [0.84148022 0.75675366]. \t  -0.261953834007402 \t -0.008435568268275922\n",
            "78     \t [ 0.59733697 -0.66741213]. \t  -105.06553138860436 \t -0.008435568268275922\n",
            "79     \t [-1.03351079  1.23317937]. \t  -6.858815070908965 \t -0.008435568268275922\n",
            "80     \t [0.60661488 0.39143544]. \t  -0.20976009111812643 \t -0.008435568268275922\n",
            "81     \t [0.54678719 1.70335149]. \t  -197.43238918791414 \t -0.008435568268275922\n",
            "82     \t [1.40826126 2.02873192]. \t  -0.37399488228009004 \t -0.008435568268275922\n",
            "83     \t [2.010771   1.52827972]. \t  -633.504066675221 \t -0.008435568268275922\n",
            "84     \t [ 1.63095484 -0.77756562]. \t  -1182.0932557353174 \t -0.008435568268275922\n",
            "85     \t [-0.08660451 -0.70069306]. \t  -51.33449912311258 \t -0.008435568268275922\n",
            "86     \t [0.5579758  0.30972002]. \t  -0.19564685497293105 \t -0.008435568268275922\n",
            "87     \t [1.889115   0.21074836]. \t  -1128.411706159457 \t -0.008435568268275922\n",
            "88     \t [-0.34497038 -1.43593559]. \t  -243.5928354559929 \t -0.008435568268275922\n",
            "89     \t [0.74091969 0.55119627]. \t  -0.06762180313179203 \t -0.008435568268275922\n",
            "90     \t [0.87053935 0.76388366]. \t  -0.020414150695507532 \t -0.008435568268275922\n",
            "91     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.008435568268275922\n",
            "92     \t [-0.28495107 -1.45635923]. \t  -238.05904995480884 \t -0.008435568268275922\n",
            "93     \t [-0.35843956  0.40250036]. \t  -9.35413291051952 \t -0.008435568268275922\n",
            "94     \t [-1.60692968  1.75495421]. \t  -75.2334457526766 \t -0.008435568268275922\n",
            "95     \t [ 1.29687332 -1.13775409]. \t  -795.1219996279622 \t -0.008435568268275922\n",
            "96     \t [1.16166633 1.37514661]. \t  -0.09207169439339054 \t -0.008435568268275922\n",
            "97     \t [1.24606001 1.53653516]. \t  -0.08656449364615296 \t -0.008435568268275922\n",
            "98     \t [-1.20891657  0.77681768]. \t  -51.75546165606691 \t -0.008435568268275922\n",
            "99     \t [ 1.04183685 -0.83097723]. \t  -367.26112762398185 \t -0.008435568268275922\n",
            "100    \t [-0.67481668  0.98829967]. \t  -31.20560964966402 \t -0.008435568268275922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "8813689c-fdbc-45d1-c68c-ffefc17b941b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_3 = d2GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.39646336 -1.04058865]. \t  -145.41584954830765 \t -1.118465165857483\n",
            "2      \t [-0.780274    1.80164718]. \t  -145.45124892612696 \t -1.118465165857483\n",
            "3      \t [ 0.19510124 -0.39571949]. \t  -19.464716969441064 \t -1.118465165857483\n",
            "4      \t [-0.29682683  0.42104675]. \t  -12.766702899455076 \t -1.118465165857483\n",
            "5      \t [1.68125163 2.00920736]. \t  -67.2783258604902 \t -1.118465165857483\n",
            "6      \t [0.15160251 0.01246474]. \t  \u001b[92m-0.7308423449833116\u001b[0m \t -0.7308423449833116\n",
            "7      \t [1.01799814 0.68687832]. \t  -12.21128808354645 \t -0.7308423449833116\n",
            "8      \t [1.1408016  1.07110639]. \t  -5.324642340417902 \t -0.7308423449833116\n",
            "9      \t [0.23738752 0.41555916]. \t  -13.484496564086381 \t -0.7308423449833116\n",
            "10     \t [1.95990213 1.25202671]. \t  -671.3117070662171 \t -0.7308423449833116\n",
            "11     \t [0.83087944 2.38833659]. \t  -288.3408323142299 \t -0.7308423449833116\n",
            "12     \t [-1.66275924  0.75287747]. \t  -411.86075422822574 \t -0.7308423449833116\n",
            "13     \t [0.89012663 0.89586384]. \t  -1.0840925492307911 \t -0.7308423449833116\n",
            "14     \t [1.10904017 1.42359805]. \t  -3.761067948901795 \t -0.7308423449833116\n",
            "15     \t [-0.02743531  0.27978537]. \t  -8.84154647928089 \t -0.7308423449833116\n",
            "16     \t [-0.8371258  -0.20926277]. \t  -86.19274375766409 \t -0.7308423449833116\n",
            "17     \t [1.43335824 1.98257786]. \t  \u001b[92m-0.705306694804545\u001b[0m \t -0.705306694804545\n",
            "18     \t [0.66453692 0.34150974]. \t  -1.1145280478562913 \t -0.705306694804545\n",
            "19     \t [1.08267226 1.1839397 ]. \t  \u001b[92m-0.020665574744250757\u001b[0m \t -0.020665574744250757\n",
            "20     \t [-0.38415023 -0.51128198]. \t  -45.324649641757965 \t -0.020665574744250757\n",
            "21     \t [1.28837853 1.76889285]. \t  -1.2706869374915808 \t -0.020665574744250757\n",
            "22     \t [0.66998416 0.46701211]. \t  -0.1417922402519183 \t -0.020665574744250757\n",
            "23     \t [0.429955   0.22858375]. \t  -0.5161165631557405 \t -0.020665574744250757\n",
            "24     \t [-0.86774288  0.63332426]. \t  -4.920158121310172 \t -0.020665574744250757\n",
            "25     \t [-0.69547744  0.89082017]. \t  -19.45023281672756 \t -0.020665574744250757\n",
            "26     \t [0.73403791 0.52609365]. \t  -0.08691060841525322 \t -0.020665574744250757\n",
            "27     \t [0.87228122 0.72636289]. \t  -0.13541741168201976 \t -0.020665574744250757\n",
            "28     \t [0.92563577 0.87398109]. \t  -0.035043593780174784 \t -0.020665574744250757\n",
            "29     \t [1.83979201 0.74160758]. \t  -699.3701751977119 \t -0.020665574744250757\n",
            "30     \t [-1.88548814  0.78108077]. \t  -777.8251909301468 \t -0.020665574744250757\n",
            "31     \t [1.01878419 1.03000114]. \t  \u001b[92m-0.006625622098579366\u001b[0m \t -0.006625622098579366\n",
            "32     \t [-0.98261309  0.04318056]. \t  -89.00332324256519 \t -0.006625622098579366\n",
            "33     \t [-0.67499788  1.75886749]. \t  -172.6504632335947 \t -0.006625622098579366\n",
            "34     \t [-1.28509462  0.25780746]. \t  -199.45068044631728 \t -0.006625622098579366\n",
            "35     \t [-1.30914363  1.5639094 ]. \t  -7.580573497748772 \t -0.006625622098579366\n",
            "36     \t [-1.12564599  1.38405909]. \t  -5.88680745203716 \t -0.006625622098579366\n",
            "37     \t [-1.18365198  1.17967831]. \t  -9.668081766731936 \t -0.006625622098579366\n",
            "38     \t [-0.2220736  -0.01598899]. \t  -1.919947005790021 \t -0.006625622098579366\n",
            "39     \t [0.84886614 0.69730138]. \t  -0.07700164546295399 \t -0.006625622098579366\n",
            "40     \t [1.63843289 1.36021904]. \t  -175.76962581466148 \t -0.006625622098579366\n",
            "41     \t [ 0.78733669 -2.04448324]. \t  -709.9385288558885 \t -0.006625622098579366\n",
            "42     \t [-1.36308273  1.86617014]. \t  -5.590844057087686 \t -0.006625622098579366\n",
            "43     \t [-0.3711829 -0.3645665]. \t  -27.115015768712222 \t -0.006625622098579366\n",
            "44     \t [-0.54203671 -1.29930436]. \t  -256.1772363256532 \t -0.006625622098579366\n",
            "45     \t [-1.83494523 -0.90244545]. \t  -1830.873844658998 \t -0.006625622098579366\n",
            "46     \t [ 0.34057347 -0.16994831]. \t  -8.610931318744916 \t -0.006625622098579366\n",
            "47     \t [0.7946948  0.64826948]. \t  -0.07013838891524227 \t -0.006625622098579366\n",
            "48     \t [1.62675981 0.68831337]. \t  -383.7825862242523 \t -0.006625622098579366\n",
            "49     \t [-1.92677455  0.16661211]. \t  -1265.869850209115 \t -0.006625622098579366\n",
            "50     \t [-1.43196859  1.68221133]. \t  -19.48063398134607 \t -0.006625622098579366\n",
            "51     \t [-1.54677484 -1.42960587]. \t  -1467.344875545267 \t -0.006625622098579366\n",
            "52     \t [ 0.85535277 -0.34463053]. \t  -115.85424275582528 \t -0.006625622098579366\n",
            "53     \t [1.13269135 1.26759145]. \t  -0.04131755720586214 \t -0.006625622098579366\n",
            "54     \t [1.38281945 0.84828167]. \t  -113.33656656331196 \t -0.006625622098579366\n",
            "55     \t [1.26820243 1.67082223]. \t  -0.4623677211621733 \t -0.006625622098579366\n",
            "56     \t [-0.93135105 -0.28698016]. \t  -136.99288509973206 \t -0.006625622098579366\n",
            "57     \t [-1.39071243  0.62388476]. \t  -177.3769387689481 \t -0.006625622098579366\n",
            "58     \t [0.87472748 1.96633851]. \t  -144.3015198463999 \t -0.006625622098579366\n",
            "59     \t [-1.1281658   0.19057678]. \t  -121.6407261095112 \t -0.006625622098579366\n",
            "60     \t [-0.94243415  0.15022679]. \t  -58.23085882279816 \t -0.006625622098579366\n",
            "61     \t [-0.53104766 -1.92376744]. \t  -488.89022912611455 \t -0.006625622098579366\n",
            "62     \t [-0.40346979 -0.06644562]. \t  -7.22452655935794 \t -0.006625622098579366\n",
            "63     \t [-1.72920729  0.0166434 ]. \t  -891.627389019488 \t -0.006625622098579366\n",
            "64     \t [1.32731887 1.68482455]. \t  -0.6992806086671052 \t -0.006625622098579366\n",
            "65     \t [-1.21261842 -1.9313326 ]. \t  -1162.1036949545671 \t -0.006625622098579366\n",
            "66     \t [1.66604392 0.39622405]. \t  -566.6353090913572 \t -0.006625622098579366\n",
            "67     \t [1.16085906 1.35068609]. \t  -0.02683189292412615 \t -0.006625622098579366\n",
            "68     \t [ 0.19994297 -1.39459727]. \t  -206.44047973584387 \t -0.006625622098579366\n",
            "69     \t [0.97054361 0.76680745]. \t  -3.0685303410528384 \t -0.006625622098579366\n",
            "70     \t [1.76609451 0.29053984]. \t  -800.6563950055495 \t -0.006625622098579366\n",
            "71     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.006625622098579366\n",
            "72     \t [ 1.73459313 -1.33251473]. \t  -1885.252561455859 \t -0.006625622098579366\n",
            "73     \t [ 1.07843284 -1.26873019]. \t  -591.3457791951057 \t -0.006625622098579366\n",
            "74     \t [ 0.05185248 -0.61927705]. \t  -39.58312041762906 \t -0.006625622098579366\n",
            "75     \t [ 1.39075807 -1.89625902]. \t  -1467.4004652258593 \t -0.006625622098579366\n",
            "76     \t [ 0.65582879 -0.27149012]. \t  -49.342923630423144 \t -0.006625622098579366\n",
            "77     \t [1.28006811 1.59052454]. \t  -0.3093167194668013 \t -0.006625622098579366\n",
            "78     \t [-0.16329517 -0.12542853]. \t  -3.6665093913833675 \t -0.006625622098579366\n",
            "79     \t [1.12645294 0.53893632]. \t  -53.30013520835561 \t -0.006625622098579366\n",
            "80     \t [ 1.59495137 -0.17072792]. \t  -737.2580887986887 \t -0.006625622098579366\n",
            "81     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.006625622098579366\n",
            "82     \t [0.68399128 0.74996957]. \t  -8.059340982336861 \t -0.006625622098579366\n",
            "83     \t [-1.37415569  1.40566405]. \t  -28.9307325057905 \t -0.006625622098579366\n",
            "84     \t [-0.71861415  0.91425582]. \t  -18.78205873832965 \t -0.006625622098579366\n",
            "85     \t [0.78662684 0.64899484]. \t  -0.13681100995555326 \t -0.006625622098579366\n",
            "86     \t [0.37447845 0.96941186]. \t  -69.14485198697025 \t -0.006625622098579366\n",
            "87     \t [ 0.13406096 -0.30988621]. \t  -11.49897327350787 \t -0.006625622098579366\n",
            "88     \t [-1.97757266  0.61735958]. \t  -1093.5367051180658 \t -0.006625622098579366\n",
            "89     \t [-0.04372883 -0.81460824]. \t  -67.75993521587327 \t -0.006625622098579366\n",
            "90     \t [1.14099452 1.31584089]. \t  -0.039402237396146705 \t -0.006625622098579366\n",
            "91     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.006625622098579366\n",
            "92     \t [ 0.56971754 -0.26063979]. \t  -34.43313749931704 \t -0.006625622098579366\n",
            "93     \t [ 0.71186019 -0.97858099]. \t  -220.70233264226943 \t -0.006625622098579366\n",
            "94     \t [-0.68390632 -0.72930392]. \t  -146.12404581908285 \t -0.006625622098579366\n",
            "95     \t [0.64162173 0.4169937 ]. \t  -0.13126018234566295 \t -0.006625622098579366\n",
            "96     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.006625622098579366\n",
            "97     \t [-1.25274348  1.20739645]. \t  -18.17706518631196 \t -0.006625622098579366\n",
            "98     \t [-1.8188858  -2.01500327]. \t  -2841.750389796985 \t -0.006625622098579366\n",
            "99     \t [0.28598232 1.17276022]. \t  -119.53232007701105 \t -0.006625622098579366\n",
            "100    \t [ 1.94508247 -0.03896131]. \t  -1461.8963614633797 \t -0.006625622098579366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "70405c96-5fd7-483e-cf5a-62073e56078f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_4 = d2GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [ 0.0784139  -1.99165818]. \t  -399.9725697606536 \t -12.122423820878506\n",
            "2      \t [0.36848897 1.24803051]. \t  -124.10800925982912 \t -12.122423820878506\n",
            "3      \t [1.30511175 0.39567731]. \t  -171.08516983793774 \t -12.122423820878506\n",
            "4      \t [2.04167073 2.00984239]. \t  -467.03053841253114 \t -12.122423820878506\n",
            "5      \t [0.90441518 1.11027706]. \t  \u001b[92m-8.553663514823912\u001b[0m \t -8.553663514823912\n",
            "6      \t [0.99877269 0.92140453]. \t  \u001b[92m-0.5797673235093339\u001b[0m \t -0.5797673235093339\n",
            "7      \t [ 1.73111495 -0.26146777]. \t  -1062.138680699351 \t -0.5797673235093339\n",
            "8      \t [0.85840857 1.78830535]. \t  -110.5726719063626 \t -0.5797673235093339\n",
            "9      \t [ 1.56120513 -1.00487715]. \t  -1185.2156248763604 \t -0.5797673235093339\n",
            "10     \t [0.507293   0.21831102]. \t  \u001b[92m-0.3951345907402283\u001b[0m \t -0.3951345907402283\n",
            "11     \t [-0.68497996  0.16259641]. \t  -12.239583504391522 \t -0.3951345907402283\n",
            "12     \t [-1.62980095  0.77295962]. \t  -361.59455113070555 \t -0.3951345907402283\n",
            "13     \t [-0.37533707  0.35972214]. \t  -6.680831570459256 \t -0.3951345907402283\n",
            "14     \t [-0.67002381 -1.75175791]. \t  -487.092544410677 \t -0.3951345907402283\n",
            "15     \t [-0.17325487  1.62640431]. \t  -256.22169244829337 \t -0.3951345907402283\n",
            "16     \t [0.49647802 0.40052582]. \t  -2.6262245943538023 \t -0.3951345907402283\n",
            "17     \t [-0.69170413  0.71530072]. \t  -8.471471087849487 \t -0.3951345907402283\n",
            "18     \t [-0.62811217  0.50849248]. \t  -3.949610268984114 \t -0.3951345907402283\n",
            "19     \t [-1.06584612  1.83667076]. \t  -53.35775496550255 \t -0.3951345907402283\n",
            "20     \t [-0.61916606 -0.43957734]. \t  -70.34537379874358 \t -0.3951345907402283\n",
            "21     \t [ 0.04469016 -0.17838864]. \t  -4.166522367874395 \t -0.3951345907402283\n",
            "22     \t [-0.4167332   0.14111697]. \t  -2.1130804890522543 \t -0.3951345907402283\n",
            "23     \t [0.62746085 0.4133797 ]. \t  \u001b[92m-0.17748647980056836\u001b[0m \t -0.17748647980056836\n",
            "24     \t [1.83420092 1.92805264]. \t  -206.97453394502486 \t -0.17748647980056836\n",
            "25     \t [1.1921477  1.83911322]. \t  -17.500716840778388 \t -0.17748647980056836\n",
            "26     \t [-1.47557584  0.22933391]. \t  -385.5950403975736 \t -0.17748647980056836\n",
            "27     \t [0.82232225 0.65429139]. \t  \u001b[92m-0.0796289563465532\u001b[0m \t -0.0796289563465532\n",
            "28     \t [-0.55290055  1.30781461]. \t  -102.83506472840799 \t -0.0796289563465532\n",
            "29     \t [1.3452924  1.76696896]. \t  -0.30277635611878967 \t -0.0796289563465532\n",
            "30     \t [-1.92875429 -0.65237303]. \t  -1920.4236213733727 \t -0.0796289563465532\n",
            "31     \t [0.27585218 0.01876816]. \t  -0.8530201636587329 \t -0.0796289563465532\n",
            "32     \t [-0.55885019 -1.36792651]. \t  -284.7506778298577 \t -0.0796289563465532\n",
            "33     \t [0.24363327 1.74337258]. \t  -284.16287928828496 \t -0.0796289563465532\n",
            "34     \t [-0.01587209 -0.58220972]. \t  -34.958152717907716 \t -0.0796289563465532\n",
            "35     \t [1.35583035 1.90619389]. \t  -0.5879000623846063 \t -0.0796289563465532\n",
            "36     \t [1.35609206 1.88597738]. \t  -0.3476236022260277 \t -0.0796289563465532\n",
            "37     \t [1.34122372 1.78401813]. \t  -0.13852429536418476 \t -0.0796289563465532\n",
            "38     \t [0.83354712 0.71581111]. \t  \u001b[92m-0.07184983233924042\u001b[0m \t -0.07184983233924042\n",
            "39     \t [-1.12292196  1.80136345]. \t  -33.71106413908456 \t -0.07184983233924042\n",
            "40     \t [-1.39900699  1.78990789]. \t  -8.5545876016569 \t -0.07184983233924042\n",
            "41     \t [-0.25533525 -0.95263609]. \t  -105.17410174748937 \t -0.07184983233924042\n",
            "42     \t [0.80660941 0.64547466]. \t  \u001b[92m-0.04004607670756236\u001b[0m \t -0.04004607670756236\n",
            "43     \t [ 1.06414912 -0.95671635]. \t  -436.45040868224277 \t -0.04004607670756236\n",
            "44     \t [ 0.14159355 -0.00177522]. \t  -0.784490150386257 \t -0.04004607670756236\n",
            "45     \t [-1.37429839  1.73310574]. \t  -8.058127826750235 \t -0.04004607670756236\n",
            "46     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.04004607670756236\n",
            "47     \t [0.1361602  0.91973458]. \t  -81.9614576900326 \t -0.04004607670756236\n",
            "48     \t [0.26502029 1.06660612]. \t  -99.81558628550876 \t -0.04004607670756236\n",
            "49     \t [-0.85724841 -1.36641014]. \t  -444.9892310789567 \t -0.04004607670756236\n",
            "50     \t [0.88651513 0.84004552]. \t  -0.30595420640886656 \t -0.04004607670756236\n",
            "51     \t [1.80553109 0.05877185]. \t  -1025.398245789344 \t -0.04004607670756236\n",
            "52     \t [-0.21890767 -0.01206917]. \t  -1.8456127352461817 \t -0.04004607670756236\n",
            "53     \t [1.88164587 0.89465385]. \t  -700.875734542644 \t -0.04004607670756236\n",
            "54     \t [ 1.32955488 -1.4778569 ]. \t  -1053.4830680164557 \t -0.04004607670756236\n",
            "55     \t [-0.90203737  0.71729105]. \t  -4.5466638060631555 \t -0.04004607670756236\n",
            "56     \t [-1.09340419 -0.90661056]. \t  -446.28298085426337 \t -0.04004607670756236\n",
            "57     \t [0.0025451  1.09913131]. \t  -121.80245700293212 \t -0.04004607670756236\n",
            "58     \t [-1.13628881 -0.58089985]. \t  -355.02164384040896 \t -0.04004607670756236\n",
            "59     \t [-1.90671123  0.45571025]. \t  -1019.5855922763678 \t -0.04004607670756236\n",
            "60     \t [ 1.69350482 -0.70147587]. \t  -1274.5671653469997 \t -0.04004607670756236\n",
            "61     \t [ 1.58903548 -0.02424579]. \t  -650.2295841149264 \t -0.04004607670756236\n",
            "62     \t [ 1.0450378  -1.71499994]. \t  -787.9852810986221 \t -0.04004607670756236\n",
            "63     \t [-0.68900583 -0.93015724]. \t  -200.2232850113495 \t -0.04004607670756236\n",
            "64     \t [-1.9511115   1.59976625]. \t  -495.82478171662814 \t -0.04004607670756236\n",
            "65     \t [0.1433744  0.57671401]. \t  -31.66495620399632 \t -0.04004607670756236\n",
            "66     \t [-0.75099975 -0.36212237]. \t  -88.83637904884068 \t -0.04004607670756236\n",
            "67     \t [0.77813422 0.57142872]. \t  -0.16526105354640871 \t -0.04004607670756236\n",
            "68     \t [0.1155818 0.9244811]. \t  -83.79651624949977 \t -0.04004607670756236\n",
            "69     \t [0.9302396  0.89922877]. \t  -0.11967264793471892 \t -0.04004607670756236\n",
            "70     \t [-1.47252835  0.50797933]. \t  -281.7930650445806 \t -0.04004607670756236\n",
            "71     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.04004607670756236\n",
            "72     \t [0.13545758 1.00238521]. \t  -97.58020718432583 \t -0.04004607670756236\n",
            "73     \t [0.85726999 0.72052802]. \t  -0.04106127874003526 \t -0.04004607670756236\n",
            "74     \t [0.82571521 0.72114958]. \t  -0.18517002918348852 \t -0.04004607670756236\n",
            "75     \t [ 1.60969256 -1.00479884]. \t  -1293.4278517665257 \t -0.04004607670756236\n",
            "76     \t [0.33941695 0.41635123]. \t  -9.50534310900239 \t -0.04004607670756236\n",
            "77     \t [1.43984719 1.69352848]. \t  -14.605468984402297 \t -0.04004607670756236\n",
            "78     \t [1.74876814 1.68137527]. \t  -190.12253463601016 \t -0.04004607670756236\n",
            "79     \t [0.67166064 0.44099939]. \t  -0.11806563769896412 \t -0.04004607670756236\n",
            "80     \t [0.68103099 0.44398436]. \t  -0.14101994206855661 \t -0.04004607670756236\n",
            "81     \t [0.77102632 0.77151141]. \t  -3.1863844894260773 \t -0.04004607670756236\n",
            "82     \t [1.02385939 1.08043685]. \t  -0.10392376596277292 \t -0.04004607670756236\n",
            "83     \t [ 0.15937131 -1.33503194]. \t  -185.78394798982714 \t -0.04004607670756236\n",
            "84     \t [1.01586111 1.01434984]. \t  \u001b[92m-0.03131192845511335\u001b[0m \t -0.03131192845511335\n",
            "85     \t [1.78889777 0.96399803]. \t  -500.6622668173832 \t -0.03131192845511335\n",
            "86     \t [ 0.62086675 -1.99945441]. \t  -568.9328188065471 \t -0.03131192845511335\n",
            "87     \t [-1.99508571  1.16711341]. \t  -800.410112024754 \t -0.03131192845511335\n",
            "88     \t [-1.08999302  1.12840595]. \t  -4.724227218529539 \t -0.03131192845511335\n",
            "89     \t [-0.75829718 -1.78029594]. \t  -557.8403897443384 \t -0.03131192845511335\n",
            "90     \t [ 1.26053213 -0.50401093]. \t  -438.1127623760935 \t -0.03131192845511335\n",
            "91     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.03131192845511335\n",
            "92     \t [ 1.1015313  -0.52844185]. \t  -303.40158362456754 \t -0.03131192845511335\n",
            "93     \t [1.11703424 1.1815487 ]. \t  -0.4521633044595778 \t -0.03131192845511335\n",
            "94     \t [0.97817863 0.96346351]. \t  \u001b[92m-0.0048719716429606475\u001b[0m \t -0.0048719716429606475\n",
            "95     \t [-0.26471549 -0.51119417]. \t  -35.38680766315234 \t -0.0048719716429606475\n",
            "96     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.0048719716429606475\n",
            "97     \t [0.79248782 0.00551502]. \t  -38.79641589678887 \t -0.0048719716429606475\n",
            "98     \t [-1.22520577  1.49539825]. \t  -4.954825042362605 \t -0.0048719716429606475\n",
            "99     \t [-0.26276442  1.13854958]. \t  -115.97854910800217 \t -0.0048719716429606475\n",
            "100    \t [ 1.28981407 -0.97803313]. \t  -697.9172874987738 \t -0.0048719716429606475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "f47f92e1-8c53-4855-ba33-2901a42eeeb1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_5 = d2GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.26305134  0.52527889]. \t  -22.396458381172856 \t -1.9278091788796494\n",
            "2      \t [-2.0143131  -1.31251332]. \t  -2892.7444960501225 \t -1.9278091788796494\n",
            "3      \t [ 0.99390592 -1.25402384]. \t  -502.5994088777894 \t -1.9278091788796494\n",
            "4      \t [-0.07196585  0.07270944]. \t  \u001b[92m-1.6051456622453808\u001b[0m \t -1.6051456622453808\n",
            "5      \t [-0.53202063  1.9093367 ]. \t  -266.82924569916423 \t -1.6051456622453808\n",
            "6      \t [ 0.19547418 -0.20209885]. \t  -6.422103437492146 \t -1.6051456622453808\n",
            "7      \t [0.1821193  0.56549208]. \t  -29.005881434771503 \t -1.6051456622453808\n",
            "8      \t [1.90153703 1.90786623]. \t  -292.5312582941719 \t -1.6051456622453808\n",
            "9      \t [0.13222093 0.1028028 ]. \t  \u001b[92m-1.4809980230493593\u001b[0m \t -1.4809980230493593\n",
            "10     \t [0.6743064 0.3535177]. \t  \u001b[92m-1.129642156271495\u001b[0m \t -1.129642156271495\n",
            "11     \t [-0.34021369  0.21351898]. \t  -2.752140999584523 \t -1.129642156271495\n",
            "12     \t [-0.01412331  0.01296389]. \t  \u001b[92m-1.0447391334222735\u001b[0m \t -1.0447391334222735\n",
            "13     \t [-1.07397932  2.01020652]. \t  -77.70771821226292 \t -1.0447391334222735\n",
            "14     \t [0.52471136 0.37253978]. \t  -1.17102871189626 \t -1.0447391334222735\n",
            "15     \t [-2.0041177   1.87863332]. \t  -466.06688216037895 \t -1.0447391334222735\n",
            "16     \t [0.02367383 0.01051104]. \t  \u001b[92m-0.9631142138741714\u001b[0m \t -0.9631142138741714\n",
            "17     \t [-1.24497678  0.70820631]. \t  -75.89605770475478 \t -0.9631142138741714\n",
            "18     \t [0.48726809 0.25539699]. \t  \u001b[92m-0.29517461171311427\u001b[0m \t -0.29517461171311427\n",
            "19     \t [-0.17517263  0.69141474]. \t  -45.03734983140083 \t -0.29517461171311427\n",
            "20     \t [0.56258206 0.27108635]. \t  -0.3975614738708108 \t -0.29517461171311427\n",
            "21     \t [-1.08352141  1.29924697]. \t  -5.909274796479341 \t -0.29517461171311427\n",
            "22     \t [ 0.91814057 -0.34450911]. \t  -141.02024019634268 \t -0.29517461171311427\n",
            "23     \t [-0.34012153  0.38623137]. \t  -9.115586071104392 \t -0.29517461171311427\n",
            "24     \t [ 0.81500145 -0.12707556]. \t  -62.650256265456946 \t -0.29517461171311427\n",
            "25     \t [-1.33177649 -1.0812734 ]. \t  -820.4837430337147 \t -0.29517461171311427\n",
            "26     \t [-0.52411223 -1.69464399]. \t  -390.15198088998693 \t -0.29517461171311427\n",
            "27     \t [-0.48806397 -0.78651009]. \t  -107.21873123936622 \t -0.29517461171311427\n",
            "28     \t [ 0.87329433 -0.30051235]. \t  -113.04598083597291 \t -0.29517461171311427\n",
            "29     \t [-0.64761208 -0.63941521]. \t  -114.82388800391946 \t -0.29517461171311427\n",
            "30     \t [-1.47691244 -1.57602495]. \t  -1417.861890443338 \t -0.29517461171311427\n",
            "31     \t [ 0.4327339  -1.35970691]. \t  -239.6320259130426 \t -0.29517461171311427\n",
            "32     \t [0.80088738 1.00142709]. \t  -13.000112601874038 \t -0.29517461171311427\n",
            "33     \t [0.68520642 0.49761309]. \t  \u001b[92m-0.17808546563600852\u001b[0m \t -0.17808546563600852\n",
            "34     \t [1.09830144 1.56154684]. \t  -12.632106730370626 \t -0.17808546563600852\n",
            "35     \t [-1.67606772 -0.89015035]. \t  -1375.682863433497 \t -0.17808546563600852\n",
            "36     \t [-0.5033492  -1.35122097]. \t  -259.7282014742476 \t -0.17808546563600852\n",
            "37     \t [-1.67958539  0.41857656]. \t  -584.3474178954291 \t -0.17808546563600852\n",
            "38     \t [-0.92679131  1.0034377 ]. \t  -5.800421788680596 \t -0.17808546563600852\n",
            "39     \t [-0.82794695 -1.66106782]. \t  -553.9776385316743 \t -0.17808546563600852\n",
            "40     \t [0.59473711 0.31057476]. \t  -0.35032214577757537 \t -0.17808546563600852\n",
            "41     \t [-1.90053733 -1.91620189]. \t  -3064.5613349294117 \t -0.17808546563600852\n",
            "42     \t [-1.62986014  0.30449881]. \t  -560.0808238757978 \t -0.17808546563600852\n",
            "43     \t [-1.06064946  0.22065185]. \t  -86.02672492343893 \t -0.17808546563600852\n",
            "44     \t [1.03806592 1.12598124]. \t  -0.23570864091017688 \t -0.17808546563600852\n",
            "45     \t [-1.138303    1.14073548]. \t  -6.974784895939914 \t -0.17808546563600852\n",
            "46     \t [ 0.15915991 -0.78903105]. \t  -67.02570961865136 \t -0.17808546563600852\n",
            "47     \t [-1.5604199  -1.26951857]. \t  -1378.8350388257602 \t -0.17808546563600852\n",
            "48     \t [0.53315532 0.29263381]. \t  -0.22496506258828855 \t -0.17808546563600852\n",
            "49     \t [-0.12274723  1.13054738]. \t  -125.69023654816054 \t -0.17808546563600852\n",
            "50     \t [-0.20922119  0.01338527]. \t  -1.5545603511724269 \t -0.17808546563600852\n",
            "51     \t [-0.8697532   0.72259792]. \t  -3.6107129717514086 \t -0.17808546563600852\n",
            "52     \t [-1.26879899 -0.26761964]. \t  -357.63700430421267 \t -0.17808546563600852\n",
            "53     \t [-0.57842583 -1.50280606]. \t  -340.0888766860873 \t -0.17808546563600852\n",
            "54     \t [0.49963123 0.23086254]. \t  -0.2855958144241081 \t -0.17808546563600852\n",
            "55     \t [ 0.82248618 -1.04625839]. \t  -296.8154796675672 \t -0.17808546563600852\n",
            "56     \t [-0.64689927  1.63209789]. \t  -149.9994387123553 \t -0.17808546563600852\n",
            "57     \t [0.93196297 0.90382006]. \t  \u001b[92m-0.1289916174950372\u001b[0m \t -0.1289916174950372\n",
            "58     \t [-1.45316867 -0.38311089]. \t  -628.4257640568197 \t -0.1289916174950372\n",
            "59     \t [1.58170114 0.41906204]. \t  -434.1091590560216 \t -0.1289916174950372\n",
            "60     \t [ 1.54664472 -0.85143791]. \t  -1052.3590489943044 \t -0.1289916174950372\n",
            "61     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.1289916174950372\n",
            "62     \t [-0.97376053 -1.4571699 ]. \t  -582.480767503922 \t -0.1289916174950372\n",
            "63     \t [-0.61116527 -1.07885359]. \t  -213.53562625344438 \t -0.1289916174950372\n",
            "64     \t [0.82131546 0.69993234]. \t  \u001b[92m-0.09630840985964823\u001b[0m \t -0.09630840985964823\n",
            "65     \t [1.39449448 0.02403067]. \t  -369.01998822142326 \t -0.09630840985964823\n",
            "66     \t [0.99036749 0.33968709]. \t  -41.10622914377682 \t -0.09630840985964823\n",
            "67     \t [0.74876912 0.54444816]. \t  \u001b[92m-0.08938371920565814\u001b[0m \t -0.08938371920565814\n",
            "68     \t [1.18565431 0.24203744]. \t  -135.46324582697912 \t -0.08938371920565814\n",
            "69     \t [1.04541414 1.10761407]. \t  \u001b[92m-0.023740146314456253\u001b[0m \t -0.023740146314456253\n",
            "70     \t [-1.63445622 -1.84554485]. \t  -2047.2620122828723 \t -0.023740146314456253\n",
            "71     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.023740146314456253\n",
            "72     \t [ 0.51006207 -0.81553677]. \t  -115.9531049663765 \t -0.023740146314456253\n",
            "73     \t [-1.70649306  0.19121674]. \t  -747.6557790250947 \t -0.023740146314456253\n",
            "74     \t [1.13872968 1.32783194]. \t  -0.11613274787740212 \t -0.023740146314456253\n",
            "75     \t [-1.52326452 -0.02977024]. \t  -558.6662390049898 \t -0.023740146314456253\n",
            "76     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.023740146314456253\n",
            "77     \t [1.95511732 1.8879273 ]. \t  -375.16310905375974 \t -0.023740146314456253\n",
            "78     \t [-0.80366534  0.34169506]. \t  -12.505933083898942 \t -0.023740146314456253\n",
            "79     \t [-1.16481664  1.87316636]. \t  -31.35007917044794 \t -0.023740146314456253\n",
            "80     \t [1.28442412 1.84086309]. \t  -3.7334975399839903 \t -0.023740146314456253\n",
            "81     \t [1.08945152 1.21870873]. \t  -0.10915175792669088 \t -0.023740146314456253\n",
            "82     \t [-1.48490288  1.04258879]. \t  -141.27997684480601 \t -0.023740146314456253\n",
            "83     \t [0.39968827 1.86533819]. \t  -291.263238083348 \t -0.023740146314456253\n",
            "84     \t [-0.14592709  1.77647586]. \t  -309.37923450719995 \t -0.023740146314456253\n",
            "85     \t [1.05682908 1.16355347]. \t  -0.2209989883589829 \t -0.023740146314456253\n",
            "86     \t [0.525904   0.88949121]. \t  -37.79139283682128 \t -0.023740146314456253\n",
            "87     \t [-1.57565981  0.39355628]. \t  -443.08777565153105 \t -0.023740146314456253\n",
            "88     \t [ 0.32296706 -1.50958795]. \t  -260.9242971549489 \t -0.023740146314456253\n",
            "89     \t [ 1.48541484 -0.86606207]. \t  -944.273132645884 \t -0.023740146314456253\n",
            "90     \t [1.09513682 1.16603463]. \t  -0.11987352626725699 \t -0.023740146314456253\n",
            "91     \t [ 1.87252561 -0.1611803 ]. \t  -1345.8407490325253 \t -0.023740146314456253\n",
            "92     \t [1.33677796 1.90071293]. \t  -1.4070441858357547 \t -0.023740146314456253\n",
            "93     \t [1.43820795 1.95825545]. \t  -1.406135897178005 \t -0.023740146314456253\n",
            "94     \t [0.92587285 0.83305454]. \t  -0.06399105243606627 \t -0.023740146314456253\n",
            "95     \t [ 1.7290815  -1.47455581]. \t  -1993.509957799454 \t -0.023740146314456253\n",
            "96     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.023740146314456253\n",
            "97     \t [ 1.8800602  -0.87843687]. \t  -1948.2872100578213 \t -0.023740146314456253\n",
            "98     \t [-1.81468614  1.5420402 ]. \t  -314.5385193068393 \t -0.023740146314456253\n",
            "99     \t [-0.74573766  0.83727171]. \t  -10.951966768460675 \t -0.023740146314456253\n",
            "100    \t [-0.72720043 -1.80539374]. \t  -547.8388160624896 \t -0.023740146314456253\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "2ce50486-d959-46b6-d10f-36f041c230af"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_6 = d2GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.5313386  -0.45198036]. \t  -56.264804912630396 \t -3.0269049669752817\n",
            "3      \t [-0.21403066  0.06785647]. \t  \u001b[92m-1.522479008471028\u001b[0m \t -1.522479008471028\n",
            "4      \t [-0.59379378  1.76072913]. \t  -200.82546294985917 \t -1.522479008471028\n",
            "5      \t [-0.27273802  0.67394355]. \t  -37.56678420682764 \t -1.522479008471028\n",
            "6      \t [0.4693119  0.34844287]. \t  -1.9248771271053662 \t -1.522479008471028\n",
            "7      \t [0.17563781 0.01632971]. \t  \u001b[92m-0.7006529536113199\u001b[0m \t -0.7006529536113199\n",
            "8      \t [1.65232655 1.66501849]. \t  -113.88307792900925 \t -0.7006529536113199\n",
            "9      \t [0.60042579 0.50870782]. \t  -2.3558855274538892 \t -0.7006529536113199\n",
            "10     \t [-0.29199643 -1.22932525]. \t  -174.48319578277125 \t -0.7006529536113199\n",
            "11     \t [0.04661632 0.04813664]. \t  -1.1202053362700115 \t -0.7006529536113199\n",
            "12     \t [-0.62194866  0.32952288]. \t  -2.9590149731347335 \t -0.7006529536113199\n",
            "13     \t [-0.44235007 -0.34607322]. \t  -31.429332920183068 \t -0.7006529536113199\n",
            "14     \t [-0.00118993 -0.10376312]. \t  -2.079089104064461 \t -0.7006529536113199\n",
            "15     \t [-0.60187849  0.46923958]. \t  -3.7105266347053614 \t -0.7006529536113199\n",
            "16     \t [-1.1623472   1.19749284]. \t  -7.033756582304866 \t -0.7006529536113199\n",
            "17     \t [0.67850246 0.79919956]. \t  -11.584207267093849 \t -0.7006529536113199\n",
            "18     \t [-0.84207496  0.96566245]. \t  -9.976170572927838 \t -0.7006529536113199\n",
            "19     \t [-0.23267667 -1.00058447]. \t  -112.76353295205944 \t -0.7006529536113199\n",
            "20     \t [0.34600884 1.45306496]. \t  -178.20801948167232 \t -0.7006529536113199\n",
            "21     \t [-0.28573539  1.35129447]. \t  -162.85416529025727 \t -0.7006529536113199\n",
            "22     \t [-1.41740392  1.79804379]. \t  -10.29552316740765 \t -0.7006529536113199\n",
            "23     \t [0.01844167 0.44839901]. \t  -21.039136270192785 \t -0.7006529536113199\n",
            "24     \t [ 1.22131312 -1.63164916]. \t  -975.5210941657798 \t -0.7006529536113199\n",
            "25     \t [1.45693244 1.91501627]. \t  -4.5200529916615935 \t -0.7006529536113199\n",
            "26     \t [1.2648268  1.95189606]. \t  -12.468224069265048 \t -0.7006529536113199\n",
            "27     \t [-1.64046227  1.09604527]. \t  -261.3972480403634 \t -0.7006529536113199\n",
            "28     \t [1.30135346 2.00336669]. \t  -9.691258954825049 \t -0.7006529536113199\n",
            "29     \t [-1.16042226  1.34441447]. \t  -4.6678932398591195 \t -0.7006529536113199\n",
            "30     \t [ 0.70648111 -1.04731814]. \t  -239.23187114732363 \t -0.7006529536113199\n",
            "31     \t [ 0.48119433 -0.05490514]. \t  -8.47469889811514 \t -0.7006529536113199\n",
            "32     \t [-1.74169083  1.27475939]. \t  -316.8291374869721 \t -0.7006529536113199\n",
            "33     \t [1.7123502  0.67173204]. \t  -511.4533086722718 \t -0.7006529536113199\n",
            "34     \t [-1.3263643   1.78344949]. \t  -5.470569826551693 \t -0.7006529536113199\n",
            "35     \t [ 1.72965811 -0.3611323 ]. \t  -1124.692361971671 \t -0.7006529536113199\n",
            "36     \t [1.65026453 1.56159772]. \t  -135.3950316719991 \t -0.7006529536113199\n",
            "37     \t [1.13788871 1.34593417]. \t  \u001b[92m-0.2805786483261245\u001b[0m \t -0.2805786483261245\n",
            "38     \t [-1.43418413  2.04323213]. \t  -5.9438901023915065 \t -0.2805786483261245\n",
            "39     \t [ 1.21764807 -1.67881643]. \t  -999.5450123676623 \t -0.2805786483261245\n",
            "40     \t [0.40598237 0.15515586]. \t  -0.3621997604851612 \t -0.2805786483261245\n",
            "41     \t [0.24322351 0.69817329]. \t  -41.40680608386912 \t -0.2805786483261245\n",
            "42     \t [2.01611511 0.40900302]. \t  -1337.4592511891349 \t -0.2805786483261245\n",
            "43     \t [1.03240123 1.0246802 ]. \t  \u001b[92m-0.1705640268388653\u001b[0m \t -0.1705640268388653\n",
            "44     \t [1.28152177 1.7505609 ]. \t  -1.2513388720204772 \t -0.1705640268388653\n",
            "45     \t [-0.06960103  0.39329284]. \t  -16.233272792355127 \t -0.1705640268388653\n",
            "46     \t [0.67555253 0.41318479]. \t  -0.29177297534344543 \t -0.1705640268388653\n",
            "47     \t [0.90998114 0.83213197]. \t  \u001b[92m-0.009756866569367853\u001b[0m \t -0.009756866569367853\n",
            "48     \t [ 1.09019811 -0.85902057]. \t  -419.2552535068078 \t -0.009756866569367853\n",
            "49     \t [1.4486613  2.00225655]. \t  -1.1298797719141624 \t -0.009756866569367853\n",
            "50     \t [0.60534978 0.36004684]. \t  -0.15984674139762628 \t -0.009756866569367853\n",
            "51     \t [-0.77698697  1.56001318]. \t  -94.60949982977071 \t -0.009756866569367853\n",
            "52     \t [-0.93002234 -0.66266491]. \t  -237.0831376175313 \t -0.009756866569367853\n",
            "53     \t [-0.14395639  0.83083395]. \t  -66.93653951463152 \t -0.009756866569367853\n",
            "54     \t [ 1.64283333 -0.54736037]. \t  -1054.234750181094 \t -0.009756866569367853\n",
            "55     \t [ 1.26456752 -1.16271586]. \t  -762.8498168433456 \t -0.009756866569367853\n",
            "56     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.009756866569367853\n",
            "57     \t [-0.80713641 -0.74818458]. \t  -199.16880468917404 \t -0.009756866569367853\n",
            "58     \t [0.65318575 1.34301448]. \t  -84.09236747648043 \t -0.009756866569367853\n",
            "59     \t [ 1.18645898 -0.35980089]. \t  -312.43536793777577 \t -0.009756866569367853\n",
            "60     \t [-1.34258878 -1.30591657]. \t  -971.7408229441094 \t -0.009756866569367853\n",
            "61     \t [1.41971308 1.58634581]. \t  -18.60080807169307 \t -0.009756866569367853\n",
            "62     \t [1.21421145 1.46308396]. \t  -0.05848770857539719 \t -0.009756866569367853\n",
            "63     \t [1.28816371 1.64764937]. \t  -0.09676569284381095 \t -0.009756866569367853\n",
            "64     \t [1.15668781 1.33464311]. \t  -0.02562925484048015 \t -0.009756866569367853\n",
            "65     \t [0.87352625 0.67162583]. \t  -0.8517988881576821 \t -0.009756866569367853\n",
            "66     \t [1.2121726  1.47672163]. \t  -0.05043301495345337 \t -0.009756866569367853\n",
            "67     \t [0.48556553 0.58101052]. \t  -12.183476168587521 \t -0.009756866569367853\n",
            "68     \t [-0.55155062  0.27051137]. \t  -2.520856175085368 \t -0.009756866569367853\n",
            "69     \t [-1.28137897 -1.19756858]. \t  -811.4810884152644 \t -0.009756866569367853\n",
            "70     \t [1.78871846 1.50721248]. \t  -287.01042744204386 \t -0.009756866569367853\n",
            "71     \t [-1.52580274 -1.09844367]. \t  -1180.4820266939364 \t -0.009756866569367853\n",
            "72     \t [1.74924623 1.41644118]. \t  -270.64469116755384 \t -0.009756866569367853\n",
            "73     \t [ 0.26996934 -0.00881265]. \t  -1.2003699574098554 \t -0.009756866569367853\n",
            "74     \t [ 0.39624454 -1.5080447 ]. \t  -277.60514967002473 \t -0.009756866569367853\n",
            "75     \t [-1.13532691  1.12907122]. \t  -7.11629317883051 \t -0.009756866569367853\n",
            "76     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.009756866569367853\n",
            "77     \t [1.31389352 1.69902415]. \t  -0.17301461096386167 \t -0.009756866569367853\n",
            "78     \t [ 0.45875963 -1.83421845]. \t  -418.36409948972675 \t -0.009756866569367853\n",
            "79     \t [-0.54559481 -2.02115196]. \t  -540.0841043685225 \t -0.009756866569367853\n",
            "80     \t [-0.14442587  1.30848992]. \t  -167.10909260384756 \t -0.009756866569367853\n",
            "81     \t [1.16813263 0.97525035]. \t  -15.182431900590537 \t -0.009756866569367853\n",
            "82     \t [-1.66291297  0.60205752]. \t  -475.0440655718367 \t -0.009756866569367853\n",
            "83     \t [-1.72488578  0.24382631]. \t  -753.4821381841246 \t -0.009756866569367853\n",
            "84     \t [-1.77559241 -0.23396533]. \t  -1154.673348529511 \t -0.009756866569367853\n",
            "85     \t [-0.40389518 -1.25094207]. \t  -201.93127424599834 \t -0.009756866569367853\n",
            "86     \t [1.49181292 1.1552911 ]. \t  -114.77783114989622 \t -0.009756866569367853\n",
            "87     \t [1.82363278 1.78019387]. \t  -239.51767098091653 \t -0.009756866569367853\n",
            "88     \t [-0.74225263  0.62892851]. \t  -3.643681056909558 \t -0.009756866569367853\n",
            "89     \t [1.38559925 1.91310283]. \t  -0.1532869501122807 \t -0.009756866569367853\n",
            "90     \t [-0.80526177 -0.21591912]. \t  -77.97176485445083 \t -0.009756866569367853\n",
            "91     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.009756866569367853\n",
            "92     \t [ 1.55930984 -1.22208842]. \t  -1335.1450532415909 \t -0.009756866569367853\n",
            "93     \t [-0.73903477  0.13573331]. \t  -19.87026636959404 \t -0.009756866569367853\n",
            "94     \t [-0.0111504  1.0522277]. \t  -111.71457518049557 \t -0.009756866569367853\n",
            "95     \t [1.34555748 1.79746949]. \t  -0.13645445664675504 \t -0.009756866569367853\n",
            "96     \t [-1.80615082 -1.30792707]. \t  -2096.4630705644536 \t -0.009756866569367853\n",
            "97     \t [-1.51158698 -1.05923967]. \t  -1124.6318739826365 \t -0.009756866569367853\n",
            "98     \t [-1.21562782 -1.17840815]. \t  -710.4271513891267 \t -0.009756866569367853\n",
            "99     \t [-0.62834832  1.35196072]. \t  -94.26304497966618 \t -0.009756866569367853\n",
            "100    \t [-1.45895949 -1.65195926]. \t  -1435.2811721917542 \t -0.009756866569367853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "af13a136-6a2e-4d3a-eff7-7c5c788719ef"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_7 = d2GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.59759223  0.9308147 ]. \t  -35.46526561710063 \t -2.0077595729598063\n",
            "2      \t [-0.0058376  -0.35079779]. \t  -13.32000895935528 \t -2.0077595729598063\n",
            "3      \t [1.76201881 0.51049049]. \t  -673.5783036707471 \t -2.0077595729598063\n",
            "4      \t [-0.19486974  0.41884208]. \t  -15.933746723274396 \t -2.0077595729598063\n",
            "5      \t [1.06374379 1.49601535]. \t  -13.287499901069403 \t -2.0077595729598063\n",
            "6      \t [-1.67730575  1.8483109 ]. \t  -100.29889331446304 \t -2.0077595729598063\n",
            "7      \t [ 0.02899976 -1.87024276]. \t  -351.03827947822185 \t -2.0077595729598063\n",
            "8      \t [1.95674177 1.98388934]. \t  -341.2990453474088 \t -2.0077595729598063\n",
            "9      \t [-0.96270193  0.96140437]. \t  -3.9719796763875306 \t -2.0077595729598063\n",
            "10     \t [-0.8378277   0.68657241]. \t  -3.4012738508923444 \t -2.0077595729598063\n",
            "11     \t [0.37039334 0.46202325]. \t  -10.947988592367214 \t -2.0077595729598063\n",
            "12     \t [1.44092439 2.00966667]. \t  \u001b[92m-0.6379228425333943\u001b[0m \t -0.6379228425333943\n",
            "13     \t [-0.36977527 -0.14905641]. \t  -10.043886078264244 \t -0.6379228425333943\n",
            "14     \t [1.23080472 1.82915193]. \t  -9.92993893282597 \t -0.6379228425333943\n",
            "15     \t [1.23803088 1.64253822]. \t  -1.262652709411629 \t -0.6379228425333943\n",
            "16     \t [0.55341142 0.85400642]. \t  -30.201594405550928 \t -0.6379228425333943\n",
            "17     \t [-0.86772739  2.00536871]. \t  -160.3434635056507 \t -0.6379228425333943\n",
            "18     \t [0.20572533 0.10250844]. \t  -0.9931020250478781 \t -0.6379228425333943\n",
            "19     \t [-1.69743641 -1.22823506]. \t  -1696.0960855088406 \t -0.6379228425333943\n",
            "20     \t [ 0.53666934 -0.77199514]. \t  -112.57660879140734 \t -0.6379228425333943\n",
            "21     \t [1.12188762 1.33140896]. \t  \u001b[92m-0.5445076651743154\u001b[0m \t -0.5445076651743154\n",
            "22     \t [1.14538573 1.36078149]. \t  \u001b[92m-0.25999411602466876\u001b[0m \t -0.25999411602466876\n",
            "23     \t [ 0.41310572 -1.76305071]. \t  -374.26673998424303 \t -0.25999411602466876\n",
            "24     \t [1.30589016 0.52928269]. \t  -138.40678783944495 \t -0.25999411602466876\n",
            "25     \t [0.51904482 0.02742474]. \t  -6.086884927258632 \t -0.25999411602466876\n",
            "26     \t [ 0.5899706  -1.56765579]. \t  -367.1668559668683 \t -0.25999411602466876\n",
            "27     \t [0.96050284 0.92609199]. \t  \u001b[92m-0.0028034904338554937\u001b[0m \t -0.0028034904338554937\n",
            "28     \t [-1.87469876 -0.93454369]. \t  -1987.6588142819153 \t -0.0028034904338554937\n",
            "29     \t [-0.82434682 -0.60356239]. \t  -167.96538417843664 \t -0.0028034904338554937\n",
            "30     \t [1.38228676 2.03828843]. \t  -1.773598184653324 \t -0.0028034904338554937\n",
            "31     \t [-1.2491973   1.36576703]. \t  -8.850743886580085 \t -0.0028034904338554937\n",
            "32     \t [0.80103762 0.62090104]. \t  -0.08268470279829082 \t -0.0028034904338554937\n",
            "33     \t [-1.8431759  -1.77951983]. \t  -2688.0272945478355 \t -0.0028034904338554937\n",
            "34     \t [1.31530313 1.74904088]. \t  -0.13558661457542254 \t -0.0028034904338554937\n",
            "35     \t [1.38158823 1.95906057]. \t  -0.39836238741930424 \t -0.0028034904338554937\n",
            "36     \t [-0.63864392  1.52983507]. \t  -128.5665997916627 \t -0.0028034904338554937\n",
            "37     \t [ 0.14065466 -0.02402529]. \t  -0.9303974829919991 \t -0.0028034904338554937\n",
            "38     \t [-1.76261228  1.06688063]. \t  -423.75995997652166 \t -0.0028034904338554937\n",
            "39     \t [-1.37290563  1.84347112]. \t  -5.802066792178752 \t -0.0028034904338554937\n",
            "40     \t [0.05564954 0.60649055]. \t  -37.300190512549484 \t -0.0028034904338554937\n",
            "41     \t [-1.10729729  1.56215956]. \t  -15.733814583469387 \t -0.0028034904338554937\n",
            "42     \t [0.71134215 0.42604502]. \t  -0.7227256704006639 \t -0.0028034904338554937\n",
            "43     \t [-0.41984526  0.18900767]. \t  -2.032185281113014 \t -0.0028034904338554937\n",
            "44     \t [-0.19208348  0.01195388]. \t  -1.483274283039819 \t -0.0028034904338554937\n",
            "45     \t [1.34803695 0.03901215]. \t  -316.3176151563217 \t -0.0028034904338554937\n",
            "46     \t [ 1.50666073 -0.10246566]. \t  -563.1286359744842 \t -0.0028034904338554937\n",
            "47     \t [ 0.83852599 -0.34052251]. \t  -108.9462605973516 \t -0.0028034904338554937\n",
            "48     \t [ 1.28403975 -1.80099872]. \t  -1190.162876097512 \t -0.0028034904338554937\n",
            "49     \t [1.35790219 1.8516503 ]. \t  -0.13410324273290322 \t -0.0028034904338554937\n",
            "50     \t [-0.34422207  1.24450668]. \t  -128.59855232461769 \t -0.0028034904338554937\n",
            "51     \t [-1.01553322 -0.07176961]. \t  -125.74033204142796 \t -0.0028034904338554937\n",
            "52     \t [ 0.58352362 -0.23479548]. \t  -33.2699195494349 \t -0.0028034904338554937\n",
            "53     \t [1.27684505 0.33886227]. \t  -166.86638213510824 \t -0.0028034904338554937\n",
            "54     \t [ 1.95242307 -1.48669605]. \t  -2808.478305233238 \t -0.0028034904338554937\n",
            "55     \t [ 0.40544191 -1.72069599]. \t  -355.7058359961106 \t -0.0028034904338554937\n",
            "56     \t [ 0.96780316 -0.78044245]. \t  -294.8392690826092 \t -0.0028034904338554937\n",
            "57     \t [-0.46836524  0.05583377]. \t  -4.830375481641471 \t -0.0028034904338554937\n",
            "58     \t [0.80253598 0.61275263]. \t  -0.1370322543224723 \t -0.0028034904338554937\n",
            "59     \t [-1.23998148 -0.56124367]. \t  -445.5127150577988 \t -0.0028034904338554937\n",
            "60     \t [-0.48624993  1.60285367]. \t  -188.91784469060016 \t -0.0028034904338554937\n",
            "61     \t [ 1.7078212  -0.23497913]. \t  -993.779676556309 \t -0.0028034904338554937\n",
            "62     \t [1.93673741 0.97984521]. \t  -768.7806515093562 \t -0.0028034904338554937\n",
            "63     \t [ 1.97666469 -0.70501524]. \t  -2128.209855573222 \t -0.0028034904338554937\n",
            "64     \t [0.92810711 0.85829953]. \t  -0.00611924929040041 \t -0.0028034904338554937\n",
            "65     \t [0.97108911 0.96030652]. \t  -0.030738761196914842 \t -0.0028034904338554937\n",
            "66     \t [0.9893409 0.9655526]. \t  -0.017650821030332026 \t -0.0028034904338554937\n",
            "67     \t [ 1.29866214 -0.78551704]. \t  -611.1875774241657 \t -0.0028034904338554937\n",
            "68     \t [-0.35571567 -0.50568623]. \t  -41.80816166414239 \t -0.0028034904338554937\n",
            "69     \t [1.36856932 1.90893938]. \t  -0.2651368551133825 \t -0.0028034904338554937\n",
            "70     \t [-1.453335    0.70777001]. \t  -203.2563341481985 \t -0.0028034904338554937\n",
            "71     \t [0.92628316 0.87905167]. \t  -0.0497493904416827 \t -0.0028034904338554937\n",
            "72     \t [-0.12450811 -1.3949929 ]. \t  -200.21418176685694 \t -0.0028034904338554937\n",
            "73     \t [0.61242373 0.36762315]. \t  -0.15575023679245903 \t -0.0028034904338554937\n",
            "74     \t [1.32791237 1.75857392]. \t  -0.10980881641314008 \t -0.0028034904338554937\n",
            "75     \t [-0.62409191 -0.74202643]. \t  -130.6707793379426 \t -0.0028034904338554937\n",
            "76     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.0028034904338554937\n",
            "77     \t [-1.11398796 -0.24648974]. \t  -225.72234792939003 \t -0.0028034904338554937\n",
            "78     \t [-0.09797941  0.996038  ]. \t  -98.51155747480169 \t -0.0028034904338554937\n",
            "79     \t [1.1508533 0.387372 ]. \t  -87.83676818002708 \t -0.0028034904338554937\n",
            "80     \t [ 1.05092279 -1.43042672]. \t  -642.5568651619753 \t -0.0028034904338554937\n",
            "81     \t [1.29468303 1.72357606]. \t  -0.31124794977871817 \t -0.0028034904338554937\n",
            "82     \t [1.0043492  1.01149819]. \t  \u001b[92m-0.0007922403871347229\u001b[0m \t -0.0007922403871347229\n",
            "83     \t [1.84719157 0.07493504]. \t  -1114.3958721029478 \t -0.0007922403871347229\n",
            "84     \t [ 1.10464633 -1.45254347]. \t  -714.3899783273297 \t -0.0007922403871347229\n",
            "85     \t [0.88548117 1.10740968]. \t  -10.467523465249522 \t -0.0007922403871347229\n",
            "86     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.0007922403871347229\n",
            "87     \t [-0.43085748 -1.0387392 ]. \t  -151.95734865395278 \t -0.0007922403871347229\n",
            "88     \t [-0.33939423  0.24049692]. \t  -3.364198389531944 \t -0.0007922403871347229\n",
            "89     \t [1.35936773 0.81886257]. \t  -106.01696195155297 \t -0.0007922403871347229\n",
            "90     \t [-1.9380262   0.13610953]. \t  -1318.9532876497697 \t -0.0007922403871347229\n",
            "91     \t [ 0.9721382  -0.41205174]. \t  -184.17401640929336 \t -0.0007922403871347229\n",
            "92     \t [ 0.14583905 -0.10642548]. \t  -2.3601797106240374 \t -0.0007922403871347229\n",
            "93     \t [0.01802314 2.0015417 ]. \t  -401.4511721215828 \t -0.0007922403871347229\n",
            "94     \t [ 0.15498551 -0.88449689]. \t  -83.25443505500101 \t -0.0007922403871347229\n",
            "95     \t [0.63363781 0.41420535]. \t  -0.15037179969733092 \t -0.0007922403871347229\n",
            "96     \t [0.95429483 0.92879834]. \t  -0.03492137016452186 \t -0.0007922403871347229\n",
            "97     \t [1.36083287 1.87749507]. \t  -0.1958847502149283 \t -0.0007922403871347229\n",
            "98     \t [ 1.15095058 -1.48563159]. \t  -789.8119743499391 \t -0.0007922403871347229\n",
            "99     \t [1.08973792 1.19314612]. \t  -0.011208397544987878 \t -0.0007922403871347229\n",
            "100    \t [-0.54071186  1.67082572]. \t  -192.38799757088222 \t -0.0007922403871347229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "b96582ad-b643-4d2a-d7cb-43ca8cf992a6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_8 = d2GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.61855617 0.33213955]. \t  \u001b[92m-0.4002435137165303\u001b[0m \t -0.4002435137165303\n",
            "2      \t [-1.3524013   0.89573053]. \t  -92.6309820204483 \t -0.4002435137165303\n",
            "3      \t [ 0.67131218 -0.12161085]. \t  -32.857433495954204 \t -0.4002435137165303\n",
            "4      \t [1.64636284 1.02033468]. \t  -286.0872466555437 \t -0.4002435137165303\n",
            "5      \t [0.09846217 1.18140845]. \t  -138.10405818058203 \t -0.4002435137165303\n",
            "6      \t [-0.95718583  0.49937523]. \t  -21.20525766744173 \t -0.4002435137165303\n",
            "7      \t [0.86728265 0.29117733]. \t  -21.269886597021397 \t -0.4002435137165303\n",
            "8      \t [-0.5966981   0.47264577]. \t  -3.9089342320248748 \t -0.4002435137165303\n",
            "9      \t [0.42039843 0.48060161]. \t  -9.569439480848581 \t -0.4002435137165303\n",
            "10     \t [1.23148034 1.44633301]. \t  -0.5465390943467198 \t -0.4002435137165303\n",
            "11     \t [0.76833254 1.58718356]. \t  -99.42439813239193 \t -0.4002435137165303\n",
            "12     \t [1.645183   1.85400241]. \t  -73.11315058379616 \t -0.4002435137165303\n",
            "13     \t [0.67440918 0.48081637]. \t  \u001b[92m-0.1735502508261092\u001b[0m \t -0.1735502508261092\n",
            "14     \t [-1.07680911 -0.66245332]. \t  -336.27103308075306 \t -0.1735502508261092\n",
            "15     \t [-1.39359154  0.69519013]. \t  -161.20705136277712 \t -0.1735502508261092\n",
            "16     \t [-1.28953266  1.92406673]. \t  -12.063054241298978 \t -0.1735502508261092\n",
            "17     \t [-1.31985339  1.72923716]. \t  -5.398041937291334 \t -0.1735502508261092\n",
            "18     \t [-0.78202998  0.90235189]. \t  -11.630990107479219 \t -0.1735502508261092\n",
            "19     \t [-1.63304852  1.74814162]. \t  -91.33498924407327 \t -0.1735502508261092\n",
            "20     \t [-1.15912903  1.5806078 ]. \t  -10.28005066308976 \t -0.1735502508261092\n",
            "21     \t [1.30052045 1.6166225 ]. \t  -0.6487840108141716 \t -0.1735502508261092\n",
            "22     \t [-0.25896759  1.05540738]. \t  -99.26722111254946 \t -0.1735502508261092\n",
            "23     \t [-0.24884643  1.86338313]. \t  -326.08491946804156 \t -0.1735502508261092\n",
            "24     \t [-0.96524174 -1.36581502]. \t  -531.715849495629 \t -0.1735502508261092\n",
            "25     \t [-0.20037066 -1.78243634]. \t  -333.62240331076373 \t -0.1735502508261092\n",
            "26     \t [-0.0924825  -0.87403722]. \t  -79.09007036779633 \t -0.1735502508261092\n",
            "27     \t [-0.48949062  0.73595592]. \t  -26.85539619748126 \t -0.1735502508261092\n",
            "28     \t [1.15731545 1.41963478]. \t  -0.6688465265029917 \t -0.1735502508261092\n",
            "29     \t [-1.49964567  1.78875104]. \t  -27.425351780715324 \t -0.1735502508261092\n",
            "30     \t [1.99728149 1.79335453]. \t  -483.1390356175105 \t -0.1735502508261092\n",
            "31     \t [1.32285773 1.74288326]. \t  \u001b[92m-0.10923463765759268\u001b[0m \t -0.10923463765759268\n",
            "32     \t [ 1.98022044 -0.46929244]. \t  -1928.6673011391758 \t -0.10923463765759268\n",
            "33     \t [-1.25330541  1.95501977]. \t  -19.841831903727883 \t -0.10923463765759268\n",
            "34     \t [ 0.66061502 -0.95589505]. \t  -193.9671294202154 \t -0.10923463765759268\n",
            "35     \t [-1.65658863  0.95732344]. \t  -326.3809421815248 \t -0.10923463765759268\n",
            "36     \t [1.34546321 1.85228452]. \t  -0.2958562599992561 \t -0.10923463765759268\n",
            "37     \t [1.34238537 1.8114368 ]. \t  -0.12613594918616902 \t -0.10923463765759268\n",
            "38     \t [-0.69666159  2.01108644]. \t  -235.66968390762443 \t -0.10923463765759268\n",
            "39     \t [1.20837209 1.48361912]. \t  \u001b[92m-0.0984374179090221\u001b[0m \t -0.0984374179090221\n",
            "40     \t [-1.85366589 -1.66299848]. \t  -2608.200710103936 \t -0.0984374179090221\n",
            "41     \t [2.02060323 1.4606836 ]. \t  -688.6106859946037 \t -0.0984374179090221\n",
            "42     \t [-0.16098447 -0.93811381]. \t  -94.28323298300744 \t -0.0984374179090221\n",
            "43     \t [-0.32534788  0.12380478]. \t  -1.788779968867179 \t -0.0984374179090221\n",
            "44     \t [-0.66904423  1.19340861]. \t  -58.40574725562008 \t -0.0984374179090221\n",
            "45     \t [1.25805159 0.19123451]. \t  -193.68248995741166 \t -0.0984374179090221\n",
            "46     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.0984374179090221\n",
            "47     \t [-0.51424044 -0.06635342]. \t  -13.235566372013212 \t -0.0984374179090221\n",
            "48     \t [-1.11227828  0.93402843]. \t  -13.650774336140383 \t -0.0984374179090221\n",
            "49     \t [0.2536923  0.01175236]. \t  -0.8337292109253478 \t -0.0984374179090221\n",
            "50     \t [-0.39428439  1.1020433 ]. \t  -91.54598937155194 \t -0.0984374179090221\n",
            "51     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.0984374179090221\n",
            "52     \t [-0.56482597 -1.12583464]. \t  -211.21159278875356 \t -0.0984374179090221\n",
            "53     \t [-1.92850496  0.52318824]. \t  -1029.9814077744916 \t -0.0984374179090221\n",
            "54     \t [1.27059944 1.61245402]. \t  \u001b[92m-0.073611717460414\u001b[0m \t -0.073611717460414\n",
            "55     \t [ 1.25588263 -1.705961  ]. \t  -1078.0071343372058 \t -0.073611717460414\n",
            "56     \t [-1.25725437 -0.32600639]. \t  -368.64375370499255 \t -0.073611717460414\n",
            "57     \t [-1.48500591 -1.00094029]. \t  -1034.1360980208867 \t -0.073611717460414\n",
            "58     \t [0.88169297 1.8141501 ]. \t  -107.50270556616614 \t -0.073611717460414\n",
            "59     \t [-1.26352511  0.09110064]. \t  -231.7449777164809 \t -0.073611717460414\n",
            "60     \t [-1.07729279  0.11680071]. \t  -113.25844116075896 \t -0.073611717460414\n",
            "61     \t [1.22988235 1.51959968]. \t  \u001b[92m-0.05773061435510794\u001b[0m \t -0.05773061435510794\n",
            "62     \t [-1.43699318  0.90086473]. \t  -141.448244037027 \t -0.05773061435510794\n",
            "63     \t [-0.07115768 -0.00694404]. \t  -1.161796665807199 \t -0.05773061435510794\n",
            "64     \t [ 1.58178491 -1.94189443]. \t  -1975.1968996050994 \t -0.05773061435510794\n",
            "65     \t [1.85635975 1.58021445]. \t  -348.87561045171907 \t -0.05773061435510794\n",
            "66     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -0.05773061435510794\n",
            "67     \t [-1.37121146  0.53976074]. \t  -185.30597907635834 \t -0.05773061435510794\n",
            "68     \t [-1.10591708 -1.40193983]. \t  -693.4934082066238 \t -0.05773061435510794\n",
            "69     \t [0.88724095 0.75158878]. \t  -0.13950564573343524 \t -0.05773061435510794\n",
            "70     \t [ 0.20673218 -0.15698146]. \t  -4.618067879582819 \t -0.05773061435510794\n",
            "71     \t [ 1.92973463 -0.0818242 ]. \t  -1449.1996114104088 \t -0.05773061435510794\n",
            "72     \t [0.83603421 0.69123358]. \t  \u001b[92m-0.03284404679566778\u001b[0m \t -0.03284404679566778\n",
            "73     \t [ 0.57074712 -0.03149577]. \t  -12.946874619150488 \t -0.03284404679566778\n",
            "74     \t [1.19884127 1.43670789]. \t  -0.039564116197895835 \t -0.03284404679566778\n",
            "75     \t [-0.83864115 -0.48317136]. \t  -144.15653311972133 \t -0.03284404679566778\n",
            "76     \t [ 1.81540232 -0.88185794]. \t  -1745.8518633501558 \t -0.03284404679566778\n",
            "77     \t [1.21315618 1.39356664]. \t  -0.6566667038755232 \t -0.03284404679566778\n",
            "78     \t [ 2.02210299 -1.73191346]. \t  -3389.2322236048617 \t -0.03284404679566778\n",
            "79     \t [-0.12581057 -1.97756492]. \t  -398.6291010451451 \t -0.03284404679566778\n",
            "80     \t [1.4028137  2.04755223]. \t  -0.796925196137526 \t -0.03284404679566778\n",
            "81     \t [1.31792467 1.73789654]. \t  -0.10117040049309 \t -0.03284404679566778\n",
            "82     \t [1.25964485 1.57139516]. \t  -0.09085505731981444 \t -0.03284404679566778\n",
            "83     \t [ 1.45351392 -0.62312236]. \t  -748.6795639055398 \t -0.03284404679566778\n",
            "84     \t [-1.10985455  1.86904196]. \t  -45.0621325530082 \t -0.03284404679566778\n",
            "85     \t [-0.31437303 -1.03790435]. \t  -130.94416630874125 \t -0.03284404679566778\n",
            "86     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.03284404679566778\n",
            "87     \t [0.26139067 0.5017304 ]. \t  -19.329560711259465 \t -0.03284404679566778\n",
            "88     \t [-0.93060338  0.17375074]. \t  -51.651269276420294 \t -0.03284404679566778\n",
            "89     \t [1.2582718  1.55456749]. \t  -0.14896110630318363 \t -0.03284404679566778\n",
            "90     \t [ 0.13619448 -0.65511577]. \t  -46.12857340817518 \t -0.03284404679566778\n",
            "91     \t [0.66499104 0.46347235]. \t  -0.1574266070649768 \t -0.03284404679566778\n",
            "92     \t [-0.08108414  1.84567641]. \t  -339.39827472078963 \t -0.03284404679566778\n",
            "93     \t [0.78207583 0.61807271]. \t  -0.051625580150817384 \t -0.03284404679566778\n",
            "94     \t [0.65952851 1.43503437]. \t  -100.12722412902917 \t -0.03284404679566778\n",
            "95     \t [-0.8577131   1.03479741]. \t  -12.398713195129323 \t -0.03284404679566778\n",
            "96     \t [-2.01567355 -0.66988734]. \t  -2249.0596095799283 \t -0.03284404679566778\n",
            "97     \t [ 2.02019527 -1.57878686]. \t  -3204.5733881370375 \t -0.03284404679566778\n",
            "98     \t [-0.03460129  1.98045929]. \t  -392.81822179015063 \t -0.03284404679566778\n",
            "99     \t [-0.05600739 -1.1276132 ]. \t  -128.97471474266496 \t -0.03284404679566778\n",
            "100    \t [-2.04035406 -0.23039551]. \t  -1939.4754235779617 \t -0.03284404679566778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "920b101b-ff1e-42b8-c0bd-3872262e7047"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_9 = d2GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
            "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
            "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
            "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
            "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
            "1      \t [ 0.44460697 -1.80677204]. \t  -402.0893999775083 \t -29.9831488845538\n",
            "2      \t [-0.58928516  0.82048528]. \t  \u001b[92m-24.92032815745389\u001b[0m \t -24.92032815745389\n",
            "3      \t [-3.26304319 -0.85267124]. \t  -13243.454308372266 \t -24.92032815745389\n",
            "4      \t [-0.28788858  1.19428114]. \t  -125.17994398064035 \t -24.92032815745389\n",
            "5      \t [0.25347767 0.21240218]. \t  \u001b[92m-2.752174917878664\u001b[0m \t -2.752174917878664\n",
            "6      \t [-0.07778183  0.42296554]. \t  -18.543469322798174 \t -2.752174917878664\n",
            "7      \t [-1.69667833  1.90692987]. \t  -101.70916310283783 \t -2.752174917878664\n",
            "8      \t [-0.15828987  1.24297839]. \t  -149.67520799598637 \t -2.752174917878664\n",
            "9      \t [0.47112889 0.16087278]. \t  \u001b[92m-0.652899155062652\u001b[0m \t -0.652899155062652\n",
            "10     \t [-2.01346326 -1.00148458]. \t  -2564.9080532207845 \t -0.652899155062652\n",
            "11     \t [-1.15079099 -0.25076749]. \t  -252.71593273246373 \t -0.652899155062652\n",
            "12     \t [2.02242738 2.02849163]. \t  -426.11464967883586 \t -0.652899155062652\n",
            "13     \t [-1.35528644 -1.63529682]. \t  -1211.0939279799322 \t -0.652899155062652\n",
            "14     \t [-0.99001204  0.74885723]. \t  -9.308572594160779 \t -0.652899155062652\n",
            "15     \t [ 0.3754054  -1.02590489]. \t  -136.54030096647267 \t -0.652899155062652\n",
            "16     \t [-1.07267393  0.84922978]. \t  -13.380148151779716 \t -0.652899155062652\n",
            "17     \t [0.28606875 0.45783772]. \t  -14.647477446197284 \t -0.652899155062652\n",
            "18     \t [-1.80931692 -1.27237471]. \t  -2074.5060733234022 \t -0.652899155062652\n",
            "19     \t [-1.26972815  0.20486679]. \t  -203.21303380884865 \t -0.652899155062652\n",
            "20     \t [-0.42365063  2.06748073]. \t  -358.48151131367814 \t -0.652899155062652\n",
            "21     \t [-0.73422898  1.20642268]. \t  -47.54054729250897 \t -0.652899155062652\n",
            "22     \t [1.24071924 1.92822824]. \t  -15.177911978567126 \t -0.652899155062652\n",
            "23     \t [0.78179429 1.80037644]. \t  -141.4611271589353 \t -0.652899155062652\n",
            "24     \t [ 0.69400485 -2.01494389]. \t  -623.3881102390728 \t -0.652899155062652\n",
            "25     \t [0.39623998 0.18979761]. \t  \u001b[92m-0.4720542992219571\u001b[0m \t -0.4720542992219571\n",
            "26     \t [-1.88587156  0.91344747]. \t  -706.9070146986345 \t -0.4720542992219571\n",
            "27     \t [0.48802176 0.29590318]. \t  -0.5954887436610876 \t -0.4720542992219571\n",
            "28     \t [-0.0795336  -1.23139253]. \t  -154.36000755826518 \t -0.4720542992219571\n",
            "29     \t [-1.40918017  1.68587243]. \t  -14.79912853253838 \t -0.4720542992219571\n",
            "30     \t [0.48490662 0.22623805]. \t  \u001b[92m-0.27323574972374765\u001b[0m \t -0.27323574972374765\n",
            "31     \t [0.58181308 1.3615467 ]. \t  -104.83601437806144 \t -0.27323574972374765\n",
            "32     \t [1.20255604 1.55659725]. \t  -1.2610866779980205 \t -0.27323574972374765\n",
            "33     \t [-1.20712966  1.52928728]. \t  -5.391626693766622 \t -0.27323574972374765\n",
            "34     \t [1.82219672 1.80090893]. \t  -231.56158444838957 \t -0.27323574972374765\n",
            "35     \t [-0.37583209 -1.61138224]. \t  -309.064807839783 \t -0.27323574972374765\n",
            "36     \t [1.2390582  1.70563232]. \t  -2.9596435640601753 \t -0.27323574972374765\n",
            "37     \t [-0.33797863  0.41658592]. \t  -10.932124152684748 \t -0.27323574972374765\n",
            "38     \t [-0.36329081  1.32777155]. \t  -144.85025260403364 \t -0.27323574972374765\n",
            "39     \t [-1.35988747  0.24965379]. \t  -261.4539278191769 \t -0.27323574972374765\n",
            "40     \t [-1.28156234  1.44957011]. \t  -8.923941356608221 \t -0.27323574972374765\n",
            "41     \t [-0.59836164 -1.03523576]. \t  -196.67556195811156 \t -0.27323574972374765\n",
            "42     \t [-0.15119024 -1.71306133]. \t  -302.6670012948018 \t -0.27323574972374765\n",
            "43     \t [-0.08978093 -0.06926149]. \t  -1.7854932328773194 \t -0.27323574972374765\n",
            "44     \t [0.99127909 1.00634457]. \t  \u001b[92m-0.05629401970937036\u001b[0m \t -0.05629401970937036\n",
            "45     \t [-1.2051055   1.08983678]. \t  -17.998945242101303 \t -0.05629401970937036\n",
            "46     \t [-1.87945054 -0.53187245]. \t  -1660.0689019354684 \t -0.05629401970937036\n",
            "47     \t [1.14150822 1.24249683]. \t  -0.38658447479749614 \t -0.05629401970937036\n",
            "48     \t [1.02245441 1.03376632]. \t  \u001b[92m-0.01406873990130508\u001b[0m \t -0.01406873990130508\n",
            "49     \t [-1.72312315 -0.3832916 ]. \t  -1131.3041455848916 \t -0.01406873990130508\n",
            "50     \t [-0.47366259 -0.02495248]. \t  -8.387165767237631 \t -0.01406873990130508\n",
            "51     \t [0.95369749 0.9549156 ]. \t  -0.2080483927142317 \t -0.01406873990130508\n",
            "52     \t [-0.25987559 -0.36331888]. \t  -20.150820827709044 \t -0.01406873990130508\n",
            "53     \t [-1.54536301  0.97857225]. \t  -205.16892502915746 \t -0.01406873990130508\n",
            "54     \t [-1.03742352  0.77025954]. \t  -13.513961478754117 \t -0.01406873990130508\n",
            "55     \t [1.12612472 1.24937664]. \t  -0.051177227773190184 \t -0.01406873990130508\n",
            "56     \t [ 1.63554429 -0.09981155]. \t  -770.3646643128745 \t -0.01406873990130508\n",
            "57     \t [-0.54231303  0.00288144]. \t  -10.859753918600669 \t -0.01406873990130508\n",
            "58     \t [-1.49595357 -0.91197093]. \t  -998.3840314171645 \t -0.01406873990130508\n",
            "59     \t [0.65487724 0.95429416]. \t  -27.726773753029832 \t -0.01406873990130508\n",
            "60     \t [1.04444197 1.05129874]. \t  -0.1584766763447296 \t -0.01406873990130508\n",
            "61     \t [ 1.846403  -1.6901432]. \t  -2601.050623124933 \t -0.01406873990130508\n",
            "62     \t [0.93230244 0.87977952]. \t  -0.01580131273905228 \t -0.01406873990130508\n",
            "63     \t [0.46873837 1.96743036]. \t  -305.7329054062828 \t -0.01406873990130508\n",
            "64     \t [ 0.27674062 -1.32846936]. \t  -197.9409839920002 \t -0.01406873990130508\n",
            "65     \t [1.60269915 1.7340784 ]. \t  -70.01331296577641 \t -0.01406873990130508\n",
            "66     \t [0.00140331 1.36362061]. \t  -186.94277558703737 \t -0.01406873990130508\n",
            "67     \t [0.47213099 1.05764644]. \t  -69.95752629871546 \t -0.01406873990130508\n",
            "68     \t [-1.79204133 -1.16368841]. \t  -1921.9459548040177 \t -0.01406873990130508\n",
            "69     \t [0.73309088 0.47435328]. \t  -0.4690098577340357 \t -0.01406873990130508\n",
            "70     \t [ 1.2980682  -1.25427281]. \t  -864.0101684671865 \t -0.01406873990130508\n",
            "71     \t [-1.84667548 -0.8660456 ]. \t  -1836.7400488056433 \t -0.01406873990130508\n",
            "72     \t [0.35377083 0.55626988]. \t  -19.00371957437456 \t -0.01406873990130508\n",
            "73     \t [0.3678302  0.06744654]. \t  -0.8600350717725931 \t -0.01406873990130508\n",
            "74     \t [ 0.11804468 -0.078564  ]. \t  -1.6334433153865173 \t -0.01406873990130508\n",
            "75     \t [ 0.13847559 -0.18332592]. \t  -4.842906597165351 \t -0.01406873990130508\n",
            "76     \t [-0.88307779  0.82963019]. \t  -3.7940238790094662 \t -0.01406873990130508\n",
            "77     \t [0.81461897 0.64916221]. \t  -0.05522282730712497 \t -0.01406873990130508\n",
            "78     \t [-0.69654673 -1.22160209]. \t  -294.18787606883797 \t -0.01406873990130508\n",
            "79     \t [-0.46708971 -1.31363253]. \t  -236.7951087892674 \t -0.01406873990130508\n",
            "80     \t [ 0.05482189 -1.28791705]. \t  -167.54144919181485 \t -0.01406873990130508\n",
            "81     \t [0.79924822 0.21251998]. \t  -18.211571521285908 \t -0.01406873990130508\n",
            "82     \t [-1.85316016  0.25770396]. \t  -1017.154878475344 \t -0.01406873990130508\n",
            "83     \t [ 1.69336897 -0.61239054]. \t  -1211.4435039774137 \t -0.01406873990130508\n",
            "84     \t [0.9512592  0.89009216]. \t  -0.024285276923990406 \t -0.01406873990130508\n",
            "85     \t [-0.5255028   0.31883678]. \t  -2.509347690056986 \t -0.01406873990130508\n",
            "86     \t [1.78829037 0.77377538]. \t  -588.2993861148012 \t -0.01406873990130508\n",
            "87     \t [-0.2330249  -0.80892747]. \t  -76.03662216236455 \t -0.01406873990130508\n",
            "88     \t [-1.43296824  0.75859945]. \t  -173.569656950802 \t -0.01406873990130508\n",
            "89     \t [-0.93161299 -0.72401088]. \t  -257.1500312220242 \t -0.01406873990130508\n",
            "90     \t [1.14179758 1.48818296]. \t  -3.423439666435232 \t -0.01406873990130508\n",
            "91     \t [-1.10498356 -0.99634577]. \t  -496.0881529291035 \t -0.01406873990130508\n",
            "92     \t [1.25800652 1.57028001]. \t  -0.08169734125134978 \t -0.01406873990130508\n",
            "93     \t [ 0.88961009 -0.45274637]. \t  -154.8037243800122 \t -0.01406873990130508\n",
            "94     \t [-1.25086189 -0.9100251 ]. \t  -617.4707731834394 \t -0.01406873990130508\n",
            "95     \t [-1.19440196 -1.04886507]. \t  -617.6061689884486 \t -0.01406873990130508\n",
            "96     \t [0.84788009 0.70494115]. \t  -0.042627214680075375 \t -0.01406873990130508\n",
            "97     \t [0.94988028 0.92605334]. \t  -0.05906458166288756 \t -0.01406873990130508\n",
            "98     \t [-0.65541128  0.80886722]. \t  -17.12748433920213 \t -0.01406873990130508\n",
            "99     \t [-0.45261262  0.90300121]. \t  -50.85045173697143 \t -0.01406873990130508\n",
            "100    \t [ 1.19856704 -0.13210183]. \t  -246.11034989199715 \t -0.01406873990130508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "cf86cc85-4aea-472e-9c18-78a9549f01cc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_10 = d2GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.5337917   1.15217037]. \t  -77.5624817749036 \t -8.580376531587937\n",
            "3      \t [-1.92496425  1.90332315]. \t  -333.33499540157186 \t -8.580376531587937\n",
            "4      \t [-0.3889227  -2.34816026]. \t  -626.6397038003099 \t -8.580376531587937\n",
            "5      \t [-0.45198894  1.79927878]. \t  -256.50591736736254 \t -8.580376531587937\n",
            "6      \t [-0.89443395  1.32897856]. \t  -31.569432174542456 \t -8.580376531587937\n",
            "7      \t [0.76191163 0.20393692]. \t  -14.237364089085258 \t -8.580376531587937\n",
            "8      \t [0.10682151 0.29528319]. \t  -8.856119010170712 \t -8.580376531587937\n",
            "9      \t [0.65210088 0.83593868]. \t  -16.988738873674894 \t -8.580376531587937\n",
            "10     \t [0.46361979 0.4205371 ]. \t  \u001b[92m-4.514584417459624\u001b[0m \t -4.514584417459624\n",
            "11     \t [1.1895611 1.767425 ]. \t  -12.452351781803818 \t -4.514584417459624\n",
            "12     \t [ 0.22408193 -0.30798824]. \t  -13.432840874741762 \t -4.514584417459624\n",
            "13     \t [1.13036494 1.26651867]. \t  \u001b[92m-0.02955295329598865\u001b[0m \t -0.02955295329598865\n",
            "14     \t [-1.14626072  1.11079591]. \t  -8.732116019164996 \t -0.02955295329598865\n",
            "15     \t [0.48595254 0.28861894]. \t  -0.539545132613225 \t -0.02955295329598865\n",
            "16     \t [1.81963301 1.99652063]. \t  -173.47430262487018 \t -0.02955295329598865\n",
            "17     \t [0.9038167  0.73719996]. \t  -0.6442158428847456 \t -0.02955295329598865\n",
            "18     \t [0.9906713  1.03141413]. \t  -0.24993199240695924 \t -0.02955295329598865\n",
            "19     \t [0.64597902 0.52111529]. \t  -1.2033230965775783 \t -0.02955295329598865\n",
            "20     \t [1.83460611 1.84631973]. \t  -231.57238852383722 \t -0.02955295329598865\n",
            "21     \t [-1.33779574  1.77601685]. \t  -5.484004767419778 \t -0.02955295329598865\n",
            "22     \t [-0.54406621  0.55178079]. \t  -8.926110786722774 \t -0.02955295329598865\n",
            "23     \t [1.40780934 1.88021852]. \t  -1.2007728814974612 \t -0.02955295329598865\n",
            "24     \t [-0.08262942  1.92456551]. \t  -368.94394876270246 \t -0.02955295329598865\n",
            "25     \t [-1.42911955 -1.23177359]. \t  -1077.9105586898488 \t -0.02955295329598865\n",
            "26     \t [1.45733524 0.6290517 ]. \t  -223.64417807412963 \t -0.02955295329598865\n",
            "27     \t [0.49378675 0.21737408]. \t  -0.32621881183629764 \t -0.02955295329598865\n",
            "28     \t [0.93625417 0.89884678]. \t  -0.05368069956471303 \t -0.02955295329598865\n",
            "29     \t [0.72512752 0.5499446 ]. \t  -0.13380318814022046 \t -0.02955295329598865\n",
            "30     \t [0.99194882 0.10029902]. \t  -78.08617211182937 \t -0.02955295329598865\n",
            "31     \t [0.97112853 0.90237227]. \t  -0.16663192538170346 \t -0.02955295329598865\n",
            "32     \t [1.32968178 2.00764926]. \t  -5.849296350186188 \t -0.02955295329598865\n",
            "33     \t [0.83897536 0.68296945]. \t  -0.06965261921905326 \t -0.02955295329598865\n",
            "34     \t [-0.18459878 -0.99157824]. \t  -106.60008293430585 \t -0.02955295329598865\n",
            "35     \t [-1.00179207  1.03819036]. \t  -4.126908331394628 \t -0.02955295329598865\n",
            "36     \t [-1.08562463 -1.84139658]. \t  -916.3761885599844 \t -0.02955295329598865\n",
            "37     \t [-0.49143264  0.98704416]. \t  -57.80707919856127 \t -0.02955295329598865\n",
            "38     \t [-1.9731415   1.30257183]. \t  -680.0202743820246 \t -0.02955295329598865\n",
            "39     \t [ 0.55732215 -1.45954397]. \t  -313.5397560590302 \t -0.02955295329598865\n",
            "40     \t [-0.71931561  1.74910406]. \t  -154.66185238171147 \t -0.02955295329598865\n",
            "41     \t [0.96567381 0.8881451 ]. \t  -0.19814398132903166 \t -0.02955295329598865\n",
            "42     \t [-1.17017422 -0.53387238]. \t  -366.9190964188499 \t -0.02955295329598865\n",
            "43     \t [-0.47502649  0.06830707]. \t  -4.651387965420483 \t -0.02955295329598865\n",
            "44     \t [-0.82119394  0.571139  ]. \t  -4.382194177996784 \t -0.02955295329598865\n",
            "45     \t [1.82708159 0.67678285]. \t  -709.0126391146559 \t -0.02955295329598865\n",
            "46     \t [ 1.89366666 -1.78216789]. \t  -2882.4927331825857 \t -0.02955295329598865\n",
            "47     \t [ 1.1378844  -0.65381764]. \t  -379.7226447456352 \t -0.02955295329598865\n",
            "48     \t [1.02945483 1.02352507]. \t  -0.1322895217153145 \t -0.02955295329598865\n",
            "49     \t [0.99621325 0.9759489 ]. \t  \u001b[92m-0.027212720838110355\u001b[0m \t -0.027212720838110355\n",
            "50     \t [1.28979326 1.71099264]. \t  -0.3089024939637141 \t -0.027212720838110355\n",
            "51     \t [ 0.75641574 -1.40332656]. \t  -390.315934888768 \t -0.027212720838110355\n",
            "52     \t [-0.84502623 -1.83974622]. \t  -655.6015088704874 \t -0.027212720838110355\n",
            "53     \t [0.84464057 1.90600766]. \t  -142.25122111186738 \t -0.027212720838110355\n",
            "54     \t [1.90861306 1.11266196]. \t  -640.9873500056517 \t -0.027212720838110355\n",
            "55     \t [-1.32153967  1.93040522]. \t  -8.77286998621879 \t -0.027212720838110355\n",
            "56     \t [-0.77159888  0.08083053]. \t  -29.613117616473627 \t -0.027212720838110355\n",
            "57     \t [ 1.38455217 -0.42026351]. \t  -546.4208092931456 \t -0.027212720838110355\n",
            "58     \t [-0.86907313  0.16615038]. \t  -38.20175993050177 \t -0.027212720838110355\n",
            "59     \t [-0.11304351  0.00638566]. \t  -1.2429531174429294 \t -0.027212720838110355\n",
            "60     \t [ 1.95664623 -1.40562122]. \t  -2740.4804751994247 \t -0.027212720838110355\n",
            "61     \t [-1.66270093 -1.55400635]. \t  -1872.1039474825202 \t -0.027212720838110355\n",
            "62     \t [ 1.8351678  -0.07827489]. \t  -1188.26887670644 \t -0.027212720838110355\n",
            "63     \t [ 0.08413107 -0.03090426]. \t  -0.9830813786435222 \t -0.027212720838110355\n",
            "64     \t [0.47241056 1.13269949]. \t  -83.00242487243595 \t -0.027212720838110355\n",
            "65     \t [ 1.2739886  -1.98423576]. \t  -1301.3239220270361 \t -0.027212720838110355\n",
            "66     \t [-1.4064444  -0.22099504]. \t  -489.3866516394009 \t -0.027212720838110355\n",
            "67     \t [-1.0497881   1.62391789]. \t  -31.43571377729516 \t -0.027212720838110355\n",
            "68     \t [-0.34965343 -1.95956872]. \t  -435.221610964645 \t -0.027212720838110355\n",
            "69     \t [ 1.51152057 -1.89587699]. \t  -1747.9794032094064 \t -0.027212720838110355\n",
            "70     \t [-1.40558332 -1.91251359]. \t  -1517.579699438475 \t -0.027212720838110355\n",
            "71     \t [ 1.40517301 -0.85657528]. \t  -801.669225988052 \t -0.027212720838110355\n",
            "72     \t [1.01377087 0.96647928]. \t  -0.375371654981389 \t -0.027212720838110355\n",
            "73     \t [ 1.63415265 -1.63483115]. \t  -1853.9509454567055 \t -0.027212720838110355\n",
            "74     \t [2.03618554 0.66993058]. \t  -1209.415371467293 \t -0.027212720838110355\n",
            "75     \t [-1.73400181  1.72905703]. \t  -170.72783899706164 \t -0.027212720838110355\n",
            "76     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.027212720838110355\n",
            "77     \t [0.62080722 0.40307486]. \t  -0.17502154320294153 \t -0.027212720838110355\n",
            "78     \t [ 0.29383177 -1.78799774]. \t  -351.8117852221128 \t -0.027212720838110355\n",
            "79     \t [0.35188336 2.00297952]. \t  -353.54339364049775 \t -0.027212720838110355\n",
            "80     \t [-1.91804923  0.41488613]. \t  -1073.902053974256 \t -0.027212720838110355\n",
            "81     \t [0.59590602 0.32792707]. \t  -0.2371504419933906 \t -0.027212720838110355\n",
            "82     \t [1.01320442 1.0250001 ]. \t  \u001b[92m-0.00042497435016257694\u001b[0m \t -0.00042497435016257694\n",
            "83     \t [-1.16956642  1.40716474]. \t  -4.861303510122351 \t -0.00042497435016257694\n",
            "84     \t [-2.01561484 -0.16401453]. \t  -1795.6082127740456 \t -0.00042497435016257694\n",
            "85     \t [ 1.66561392 -1.2937774 ]. \t  -1655.3437923316073 \t -0.00042497435016257694\n",
            "86     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.00042497435016257694\n",
            "87     \t [-1.034505    0.06794657]. \t  -104.5905260015422 \t -0.00042497435016257694\n",
            "88     \t [ 1.45792617 -0.2476256 ]. \t  -563.4053281955346 \t -0.00042497435016257694\n",
            "89     \t [-1.67514889 -0.19090665]. \t  -905.3755728724991 \t -0.00042497435016257694\n",
            "90     \t [1.03077554 1.06672029]. \t  -0.0027297319301101903 \t -0.00042497435016257694\n",
            "91     \t [ 0.46183315 -0.73357907]. \t  -89.94570007824322 \t -0.00042497435016257694\n",
            "92     \t [0.78705217 0.06078357]. \t  -31.25628888559174 \t -0.00042497435016257694\n",
            "93     \t [1.19064195 1.63726879]. \t  -4.860540991330615 \t -0.00042497435016257694\n",
            "94     \t [ 0.45046954 -1.93188098]. \t  -456.04070283784597 \t -0.00042497435016257694\n",
            "95     \t [-0.29073196 -0.0322581 ]. \t  -3.0298199791996803 \t -0.00042497435016257694\n",
            "96     \t [-0.81057464  1.07467568]. \t  -20.720868440719848 \t -0.00042497435016257694\n",
            "97     \t [-0.79374399  1.80956559]. \t  -142.348050764057 \t -0.00042497435016257694\n",
            "98     \t [-0.65314311 -0.35198627]. \t  -63.351906424072624 \t -0.00042497435016257694\n",
            "99     \t [ 1.97285981 -1.2448696 ]. \t  -2639.870017628281 \t -0.00042497435016257694\n",
            "100    \t [2.01579377 0.71686644]. \t  -1120.9769394817645 \t -0.00042497435016257694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "d67f83ed-be6b-460d-d4f5-d19d52d66cb2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_11 = d2GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.48099284  0.29072557]. \t  \u001b[92m-2.5458368185197586\u001b[0m \t -2.5458368185197586\n",
            "3      \t [-1.51479317  0.91582766]. \t  -196.42504923820073 \t -2.5458368185197586\n",
            "4      \t [ 1.26456845 -1.05274284]. \t  -703.3147299387667 \t -2.5458368185197586\n",
            "5      \t [-0.84568122  0.65790028]. \t  -3.7345983417829682 \t -2.5458368185197586\n",
            "6      \t [-0.17040653  1.91058643]. \t  -355.39215614461466 \t -2.5458368185197586\n",
            "7      \t [0.86108268 0.02839222]. \t  -50.86634547015357 \t -2.5458368185197586\n",
            "8      \t [-0.78387951  0.47324305]. \t  -5.17664881882583 \t -2.5458368185197586\n",
            "9      \t [-0.45946827 -0.23688737]. \t  -22.200309190469696 \t -2.5458368185197586\n",
            "10     \t [0.32671337 0.01372152]. \t  \u001b[92m-1.3185890249791758\u001b[0m \t -1.3185890249791758\n",
            "11     \t [-0.81760807  1.43209638]. \t  -61.614244889445445 \t -1.3185890249791758\n",
            "12     \t [-0.81485775  0.95757903]. \t  -11.912975214664602 \t -1.3185890249791758\n",
            "13     \t [-0.14923856 -0.0982244 ]. \t  -2.772691186909121 \t -1.3185890249791758\n",
            "14     \t [ 0.27032535 -0.00483403]. \t  \u001b[92m-1.1394191469838941\u001b[0m \t -1.1394191469838941\n",
            "15     \t [-1.2386734  -1.13925684]. \t  -719.8085800301743 \t -1.1394191469838941\n",
            "16     \t [-0.01948117  0.07956673]. \t  -1.6664033719738844 \t -1.1394191469838941\n",
            "17     \t [ 0.13710783 -0.01311928]. \t  \u001b[92m-0.8464577480844927\u001b[0m \t -0.8464577480844927\n",
            "18     \t [1.81337382 0.56913604]. \t  -740.0602343264216 \t -0.8464577480844927\n",
            "19     \t [3.89366938 3.94728782]. \t  -12582.34767320607 \t -0.8464577480844927\n",
            "20     \t [ 1.0137647  -0.07230145]. \t  -121.00465961248214 \t -0.8464577480844927\n",
            "21     \t [-0.1928319   0.04383959]. \t  -1.4272774432293853 \t -0.8464577480844927\n",
            "22     \t [ 0.2436819  -0.27884161]. \t  -12.011461671266463 \t -0.8464577480844927\n",
            "23     \t [ 1.21602431 -0.70188661]. \t  -475.54905943904737 \t -0.8464577480844927\n",
            "24     \t [0.17726688 1.06658703]. \t  -107.8332336585574 \t -0.8464577480844927\n",
            "25     \t [-1.2198352   0.34836495]. \t  -134.80399984543754 \t -0.8464577480844927\n",
            "26     \t [0.55658795 0.34293503]. \t  \u001b[92m-0.3064725823616601\u001b[0m \t -0.3064725823616601\n",
            "27     \t [0.54573972 0.32954028]. \t  -0.30689495819183915 \t -0.3064725823616601\n",
            "28     \t [1.27910866 0.64469621]. \t  -98.36981129039162 \t -0.3064725823616601\n",
            "29     \t [0.77226364 1.28101089]. \t  -46.922285148688744 \t -0.3064725823616601\n",
            "30     \t [0.89954013 0.6742527 ]. \t  -1.8304259874003503 \t -0.3064725823616601\n",
            "31     \t [1.35280498 1.82261692]. \t  \u001b[92m-0.13004304400713795\u001b[0m \t -0.13004304400713795\n",
            "32     \t [1.10669906 1.55994632]. \t  -11.24484215136535 \t -0.13004304400713795\n",
            "33     \t [1.21873984 2.03453278]. \t  -30.2105698008516 \t -0.13004304400713795\n",
            "34     \t [-1.92237694  1.06979801]. \t  -697.9887655770525 \t -0.13004304400713795\n",
            "35     \t [-1.20891536  1.22662808]. \t  -10.394677304004466 \t -0.13004304400713795\n",
            "36     \t [0.76016295 0.51002373]. \t  -0.5175309792898023 \t -0.13004304400713795\n",
            "37     \t [1.46992957 2.00469881]. \t  -2.654250596513754 \t -0.13004304400713795\n",
            "38     \t [-0.27563309  1.9377205 ]. \t  -348.23739126600947 \t -0.13004304400713795\n",
            "39     \t [ 1.93024108 -0.94086061]. \t  -2178.6660545401273 \t -0.13004304400713795\n",
            "40     \t [-1.98053855 -1.49949419]. \t  -2948.7214246399426 \t -0.13004304400713795\n",
            "41     \t [-0.40959781 -1.59677702]. \t  -313.34971456531264 \t -0.13004304400713795\n",
            "42     \t [ 1.6604497  -1.82892649]. \t  -2103.593856771169 \t -0.13004304400713795\n",
            "43     \t [1.38153412 1.9502998 ]. \t  -0.3191510875761674 \t -0.13004304400713795\n",
            "44     \t [0.93920879 0.30816011]. \t  -32.94590397346381 \t -0.13004304400713795\n",
            "45     \t [1.19656676 1.4986728 ]. \t  -0.4862101514195416 \t -0.13004304400713795\n",
            "46     \t [-0.0069477   0.10223989]. \t  -2.058256476849774 \t -0.13004304400713795\n",
            "47     \t [1.92883444 0.33470046]. \t  -1147.160415538827 \t -0.13004304400713795\n",
            "48     \t [-0.57896128 -1.17887706]. \t  -231.73488884239327 \t -0.13004304400713795\n",
            "49     \t [-0.5681301   0.61439423]. \t  -10.963395698427673 \t -0.13004304400713795\n",
            "50     \t [-0.69152467  1.25049012]. \t  -62.50347520244296 \t -0.13004304400713795\n",
            "51     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.13004304400713795\n",
            "52     \t [-1.88092602 -1.71952514]. \t  -2772.333433014948 \t -0.13004304400713795\n",
            "53     \t [0.98115619 0.95299302]. \t  \u001b[92m-0.009714587400323622\u001b[0m \t -0.009714587400323622\n",
            "54     \t [ 0.9735238  -1.26120847]. \t  -487.9498313854103 \t -0.009714587400323622\n",
            "55     \t [ 1.13312633 -1.69041578]. \t  -884.7179340438174 \t -0.009714587400323622\n",
            "56     \t [-0.04710502 -1.72545809]. \t  -299.5832021814439 \t -0.009714587400323622\n",
            "57     \t [-0.18157075  0.43033067]. \t  -17.18582347230089 \t -0.009714587400323622\n",
            "58     \t [ 0.47994735 -1.40300951]. \t  -267.056607350391 \t -0.009714587400323622\n",
            "59     \t [1.71424921 1.48059655]. \t  -213.10223813541185 \t -0.009714587400323622\n",
            "60     \t [ 1.86779441 -1.8241196 ]. \t  -2823.3114968565624 \t -0.009714587400323622\n",
            "61     \t [1.10977032 1.22502324]. \t  -0.01636197460653358 \t -0.009714587400323622\n",
            "62     \t [-1.64117344 -1.89599003]. \t  -2113.2720115420593 \t -0.009714587400323622\n",
            "63     \t [ 1.31580711 -1.11887593]. \t  -812.477579666318 \t -0.009714587400323622\n",
            "64     \t [0.8764226 1.4176972]. \t  -42.210768926149 \t -0.009714587400323622\n",
            "65     \t [1.38530645 1.88614241]. \t  -0.2569098273430356 \t -0.009714587400323622\n",
            "66     \t [0.40434128 0.5194397 ]. \t  -13.024695036665513 \t -0.009714587400323622\n",
            "67     \t [-0.59792941 -0.37098712]. \t  -55.62557857024103 \t -0.009714587400323622\n",
            "68     \t [-1.22447442 -1.35309648]. \t  -818.5863114414681 \t -0.009714587400323622\n",
            "69     \t [1.38875036 1.90440087]. \t  -0.20982013604078686 \t -0.009714587400323622\n",
            "70     \t [ 1.96677042 -1.23243807]. \t  -2602.5710986536355 \t -0.009714587400323622\n",
            "71     \t [-1.67254638  0.24145778]. \t  -660.4323974788158 \t -0.009714587400323622\n",
            "72     \t [1.23528053 1.53957237]. \t  -0.07400113586258704 \t -0.009714587400323622\n",
            "73     \t [ 0.47673951 -1.2253542 ]. \t  -211.28857552365994 \t -0.009714587400323622\n",
            "74     \t [0.94928522 0.07194122]. \t  -68.76003629130035 \t -0.009714587400323622\n",
            "75     \t [1.13027469 1.28507003]. \t  -0.022670470621979785 \t -0.009714587400323622\n",
            "76     \t [-1.30501148  0.16778885]. \t  -241.01728153385264 \t -0.009714587400323622\n",
            "77     \t [ 1.98216118 -0.56707881]. \t  -2022.4037849155825 \t -0.009714587400323622\n",
            "78     \t [1.22070398 1.48277296]. \t  -0.054105523682131784 \t -0.009714587400323622\n",
            "79     \t [-0.14297143 -1.50855436]. \t  -235.0890121843255 \t -0.009714587400323622\n",
            "80     \t [1.56533633 0.93464884]. \t  -230.0327265035141 \t -0.009714587400323622\n",
            "81     \t [ 0.28557859 -0.87338187]. \t  -91.7008658277363 \t -0.009714587400323622\n",
            "82     \t [0.56217192 1.41420373]. \t  -120.78865068275417 \t -0.009714587400323622\n",
            "83     \t [1.79282834 0.64597849]. \t  -660.2219442582725 \t -0.009714587400323622\n",
            "84     \t [ 1.70460458 -1.09192798]. \t  -1598.5808533328623 \t -0.009714587400323622\n",
            "85     \t [0.70858878 0.52242116]. \t  -0.12622330354170108 \t -0.009714587400323622\n",
            "86     \t [1.30620573 1.69903537]. \t  -0.09885712638163657 \t -0.009714587400323622\n",
            "87     \t [-2.00994707  0.85300458]. \t  -1024.6818743389156 \t -0.009714587400323622\n",
            "88     \t [1.28083494 1.58337541]. \t  -0.4056261549357417 \t -0.009714587400323622\n",
            "89     \t [-1.78620936  1.42392272]. \t  -319.8579928979332 \t -0.009714587400323622\n",
            "90     \t [-0.42532623  1.76570948]. \t  -253.19290341592327 \t -0.009714587400323622\n",
            "91     \t [-0.23251562 -0.87987128]. \t  -88.74251487636296 \t -0.009714587400323622\n",
            "92     \t [-0.28023732  0.8644397 ]. \t  -63.40394859145026 \t -0.009714587400323622\n",
            "93     \t [ 0.63138666 -1.65560771]. \t  -422.13298671981994 \t -0.009714587400323622\n",
            "94     \t [1.55955816 0.71295826]. \t  -295.89976870463903 \t -0.009714587400323622\n",
            "95     \t [1.18209282 0.49641175]. \t  -81.20094879602708 \t -0.009714587400323622\n",
            "96     \t [0.11221832 0.17690602]. \t  -3.488034753685005 \t -0.009714587400323622\n",
            "97     \t [ 1.53115004 -0.15748759]. \t  -626.2364977558283 \t -0.009714587400323622\n",
            "98     \t [-0.04377054  1.07377422]. \t  -115.97749058569023 \t -0.009714587400323622\n",
            "99     \t [-1.07275224  1.30988671]. \t  -6.8272437195548115 \t -0.009714587400323622\n",
            "100    \t [ 1.99729274 -0.68967908]. \t  -2190.1652376019097 \t -0.009714587400323622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "3f16faf9-d061-43e4-a3dc-ed671f42e322"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_12 = d2GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.5807869   1.84651715]. \t  -49.21913193836258 \t -19.52113145175031\n",
            "2      \t [-5.31961505 -5.318402  ]. \t  -113048.23167412907 \t -19.52113145175031\n",
            "3      \t [-0.99001754  1.70314883]. \t  -56.23510823872594 \t -19.52113145175031\n",
            "4      \t [-1.41428192  1.35769893]. \t  -47.10866587145217 \t -19.52113145175031\n",
            "5      \t [-1.26900693 -2.01275233]. \t  -1317.8561656117965 \t -19.52113145175031\n",
            "6      \t [-1.19370865 -1.3207978 ]. \t  -758.720153086171 \t -19.52113145175031\n",
            "7      \t [0.49818184 1.06984657]. \t  -67.76457057592293 \t -19.52113145175031\n",
            "8      \t [0.5651675  0.63849688]. \t  \u001b[92m-10.370448682858372\u001b[0m \t -10.370448682858372\n",
            "9      \t [0.76618385 0.84890775]. \t  \u001b[92m-6.9122626838359436\u001b[0m \t -6.9122626838359436\n",
            "10     \t [0.9267065  0.70950159]. \t  \u001b[92m-2.2339237174316997\u001b[0m \t -2.2339237174316997\n",
            "11     \t [0.87996977 0.69771017]. \t  \u001b[92m-0.6017243805315178\u001b[0m \t -0.6017243805315178\n",
            "12     \t [-0.45669532 -0.58668045]. \t  -65.36438753782245 \t -0.6017243805315178\n",
            "13     \t [0.58359327 1.09024576]. \t  -56.37310446471709 \t -0.6017243805315178\n",
            "14     \t [-0.44674014 -0.00499089]. \t  -6.277848991175647 \t -0.6017243805315178\n",
            "15     \t [ 0.13180762 -0.02910733]. \t  -0.969802387030947 \t -0.6017243805315178\n",
            "16     \t [-0.00782162 -0.23369184]. \t  -6.479751595050169 \t -0.6017243805315178\n",
            "17     \t [-0.17118402  0.10503113]. \t  -1.945132304995946 \t -0.6017243805315178\n",
            "18     \t [0.31574566 0.10316433]. \t  \u001b[92m-0.46940741028878447\u001b[0m \t -0.46940741028878447\n",
            "19     \t [1.37233369 0.60017456]. \t  -164.77966097529503 \t -0.46940741028878447\n",
            "20     \t [-0.16822463  0.21355451]. \t  -4.7966896499975835 \t -0.46940741028878447\n",
            "21     \t [-1.8898996  -1.52770608]. \t  -2608.766667606483 \t -0.46940741028878447\n",
            "22     \t [-0.67528109  0.37605484]. \t  -3.4457624042398445 \t -0.46940741028878447\n",
            "23     \t [-0.19226789  1.6783132 ]. \t  -270.82325797463244 \t -0.46940741028878447\n",
            "24     \t [0.73562834 0.65201884]. \t  -1.2991033098095381 \t -0.46940741028878447\n",
            "25     \t [-1.90380966  1.58542849]. \t  -424.20978872368084 \t -0.46940741028878447\n",
            "26     \t [-1.62219888 -0.9583691 ]. \t  -1295.6129145410832 \t -0.46940741028878447\n",
            "27     \t [0.7011432  0.21686683]. \t  -7.637245058596269 \t -0.46940741028878447\n",
            "28     \t [-1.25298003  1.75584366]. \t  -8.531231134607253 \t -0.46940741028878447\n",
            "29     \t [-0.8380545  -1.56912158]. \t  -519.3301011515124 \t -0.46940741028878447\n",
            "30     \t [-1.1409442  1.1314158]. \t  -7.485141243225406 \t -0.46940741028878447\n",
            "31     \t [ 0.17420856 -0.07665063]. \t  -1.8268154575155213 \t -0.46940741028878447\n",
            "32     \t [1.28092455 1.27436466]. \t  -13.504038134636817 \t -0.46940741028878447\n",
            "33     \t [0.65461909 0.42844223]. \t  \u001b[92m-0.1192886759589269\u001b[0m \t -0.1192886759589269\n",
            "34     \t [2.03542152 1.75779376]. \t  -569.9647250113087 \t -0.1192886759589269\n",
            "35     \t [-1.25457954 -0.90390996]. \t  -619.0719516289206 \t -0.1192886759589269\n",
            "36     \t [1.09105898 1.26725517]. \t  -0.5988144925947695 \t -0.1192886759589269\n",
            "37     \t [1.16298671 0.3173189 ]. \t  -107.1944404400957 \t -0.1192886759589269\n",
            "38     \t [-0.87070096  1.76451424]. \t  -104.78242689426114 \t -0.1192886759589269\n",
            "39     \t [1.17835732 0.73878845]. \t  -42.24769636950319 \t -0.1192886759589269\n",
            "40     \t [1.03056976 1.15819075]. \t  -0.924777087160749 \t -0.1192886759589269\n",
            "41     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -0.1192886759589269\n",
            "42     \t [1.0078514  1.01446131]. \t  \u001b[92m-0.00023145942915070469\u001b[0m \t -0.00023145942915070469\n",
            "43     \t [1.03757793 1.11179844]. \t  -0.12553074575784126 \t -0.00023145942915070469\n",
            "44     \t [ 1.97086788 -1.38085597]. \t  -2773.1505780850493 \t -0.00023145942915070469\n",
            "45     \t [-1.1721999   1.78049686]. \t  -21.238145501831017 \t -0.00023145942915070469\n",
            "46     \t [1.03391126 1.09866605]. \t  -0.0893207759809499 \t -0.00023145942915070469\n",
            "47     \t [ 0.74708299 -1.63196796]. \t  -479.71818826441717 \t -0.00023145942915070469\n",
            "48     \t [1.00275562 1.00025021]. \t  -0.0027834374847643477 \t -0.00023145942915070469\n",
            "49     \t [2.0291388  0.88476863]. \t  -1046.0524466450936 \t -0.00023145942915070469\n",
            "50     \t [1.44500644 1.98654748]. \t  -1.2281773013661643 \t -0.00023145942915070469\n",
            "51     \t [0.85234672 0.72202859]. \t  -0.023796306809060305 \t -0.00023145942915070469\n",
            "52     \t [-0.79672646  0.54017111]. \t  -4.1231787832939615 \t -0.00023145942915070469\n",
            "53     \t [1.29966181 1.67801877]. \t  -0.10212273357979632 \t -0.00023145942915070469\n",
            "54     \t [-1.97029281 -0.58851854]. \t  -2007.4243074353817 \t -0.00023145942915070469\n",
            "55     \t [ 0.7991025  -1.46293189]. \t  -441.6691954364644 \t -0.00023145942915070469\n",
            "56     \t [-1.4730128  1.9926884]. \t  -9.251464609188657 \t -0.00023145942915070469\n",
            "57     \t [1.35082463 1.92992551]. \t  -1.229746562256964 \t -0.00023145942915070469\n",
            "58     \t [-0.6690598   0.62857839]. \t  -6.059594112791341 \t -0.00023145942915070469\n",
            "59     \t [-1.05187429  0.9227578 ]. \t  -7.584085831378985 \t -0.00023145942915070469\n",
            "60     \t [ 0.34967597 -0.49509189]. \t  -38.53689694008928 \t -0.00023145942915070469\n",
            "61     \t [-0.64286013  0.47116103]. \t  -3.0341363514341926 \t -0.00023145942915070469\n",
            "62     \t [0.99416961 0.99303535]. \t  -0.00220754470493691 \t -0.00023145942915070469\n",
            "63     \t [ 1.38770185 -0.97071159]. \t  -839.0798349707751 \t -0.00023145942915070469\n",
            "64     \t [1.01970208 1.04122363]. \t  -0.0005930351006262985 \t -0.00023145942915070469\n",
            "65     \t [ 1.40147933 -0.0424511 ]. \t  -402.80370032728825 \t -0.00023145942915070469\n",
            "66     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.00023145942915070469\n",
            "67     \t [-0.99635422  1.42286402]. \t  -22.48766865522903 \t -0.00023145942915070469\n",
            "68     \t [0.4887883 0.2856681]. \t  -0.4799319764233977 \t -0.00023145942915070469\n",
            "69     \t [0.97717995 0.9783093 ]. \t  -0.0554109114786312 \t -0.00023145942915070469\n",
            "70     \t [0.67177029 0.74464743]. \t  -8.714454155806434 \t -0.00023145942915070469\n",
            "71     \t [-1.47018171  0.40299074]. \t  -315.3141561232518 \t -0.00023145942915070469\n",
            "72     \t [1.38469128 2.0101149 ]. \t  -1.0081499667484584 \t -0.00023145942915070469\n",
            "73     \t [-0.73846938  1.78295618]. \t  -156.1923914126747 \t -0.00023145942915070469\n",
            "74     \t [1.29200634 1.66625638]. \t  -0.08618215770273947 \t -0.00023145942915070469\n",
            "75     \t [0.51526031 0.24503145]. \t  -0.27684083081721195 \t -0.00023145942915070469\n",
            "76     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.00023145942915070469\n",
            "77     \t [ 0.41220436 -1.23097664]. \t  -196.59452422588546 \t -0.00023145942915070469\n",
            "78     \t [ 0.94679312 -1.0564852 ]. \t  -381.3856099237539 \t -0.00023145942915070469\n",
            "79     \t [1.33923561 1.73694865]. \t  -0.4354748039706085 \t -0.00023145942915070469\n",
            "80     \t [1.28286425 0.77135349]. \t  -76.53531097115662 \t -0.00023145942915070469\n",
            "81     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.00023145942915070469\n",
            "82     \t [-0.0245695   0.50057882]. \t  -26.047258765573183 \t -0.00023145942915070469\n",
            "83     \t [0.73172183 0.5141069 ]. \t  -0.11738453814473468 \t -0.00023145942915070469\n",
            "84     \t [ 0.24020323 -1.74935592]. \t  -327.12153009436537 \t -0.00023145942915070469\n",
            "85     \t [-0.37725487  1.1371482 ]. \t  -100.86489912110035 \t -0.00023145942915070469\n",
            "86     \t [0.59075068 0.3705354 ]. \t  -0.21392106764824198 \t -0.00023145942915070469\n",
            "87     \t [0.95115313 0.90638088]. \t  -0.0026711546260692227 \t -0.00023145942915070469\n",
            "88     \t [-1.95437521 -0.72886382]. \t  -2077.5646770921367 \t -0.00023145942915070469\n",
            "89     \t [ 1.22901738 -1.40143556]. \t  -847.9798365211404 \t -0.00023145942915070469\n",
            "90     \t [-1.56558783  0.93830891]. \t  -235.42541967516422 \t -0.00023145942915070469\n",
            "91     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.00023145942915070469\n",
            "92     \t [ 1.23184003 -0.68497411]. \t  -485.1120682204871 \t -0.00023145942915070469\n",
            "93     \t [0.51114574 0.29936629]. \t  -0.3841114838909807 \t -0.00023145942915070469\n",
            "94     \t [-1.93926247  0.49980756]. \t  -1072.006603036883 \t -0.00023145942915070469\n",
            "95     \t [1.38918894 1.95261974]. \t  -0.2033328123713055 \t -0.00023145942915070469\n",
            "96     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.00023145942915070469\n",
            "97     \t [1.00421244 1.90356606]. \t  -80.12461242201168 \t -0.00023145942915070469\n",
            "98     \t [-1.25873717  1.22698148]. \t  -17.87807061042809 \t -0.00023145942915070469\n",
            "99     \t [-1.00773153 -1.93180522]. \t  -872.7052511651193 \t -0.00023145942915070469\n",
            "100    \t [-0.43163006 -0.55778265]. \t  -57.41613508491781 \t -0.00023145942915070469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "d0cb86ab-2ed7-4dc6-9409-54c24885aa1e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_13 = d2GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.36934199 0.61085505]. \t  \u001b[92m-22.907207745006197\u001b[0m \t -22.907207745006197\n",
            "2      \t [1.58116795 1.67002464]. \t  -69.2389530876244 \t -22.907207745006197\n",
            "3      \t [0.58510916 0.28783105]. \t  \u001b[92m-0.4693957948480285\u001b[0m \t -0.4693957948480285\n",
            "4      \t [1.11519895 1.11681829]. \t  -1.622373386589119 \t -0.4693957948480285\n",
            "5      \t [-1.03995817  1.90893915]. \t  -72.6248326358745 \t -0.4693957948480285\n",
            "6      \t [ 0.26215918 -0.48522484]. \t  -31.23072174035374 \t -0.4693957948480285\n",
            "7      \t [0.96834878 0.89091393]. \t  \u001b[92m-0.2198895305428826\u001b[0m \t -0.2198895305428826\n",
            "8      \t [0.42458555 0.10540022]. \t  -0.8916934884883143 \t -0.2198895305428826\n",
            "9      \t [1.40743808 1.2187828 ]. \t  -58.245517311846605 \t -0.2198895305428826\n",
            "10     \t [0.08947238 1.87820275]. \t  -350.59290856368915 \t -0.2198895305428826\n",
            "11     \t [-1.31024486  1.19688966]. \t  -32.361833761233704 \t -0.2198895305428826\n",
            "12     \t [-1.87962516  1.97145617]. \t  -252.13125797805603 \t -0.2198895305428826\n",
            "13     \t [-0.64347717  0.96261537]. \t  -32.7920024037792 \t -0.2198895305428826\n",
            "14     \t [1.15108523 1.35160252]. \t  \u001b[92m-0.09361100209794326\u001b[0m \t -0.09361100209794326\n",
            "15     \t [1.35004866 1.88894992]. \t  -0.5623488284966747 \t -0.09361100209794326\n",
            "16     \t [ 0.85956863 -0.72063444]. \t  -213.0316067957737 \t -0.09361100209794326\n",
            "17     \t [-1.79116532  0.39368635]. \t  -799.9805187663303 \t -0.09361100209794326\n",
            "18     \t [-1.05510715  1.4330144 ]. \t  -14.448322109568188 \t -0.09361100209794326\n",
            "19     \t [1.26268328 1.78436356]. \t  -3.678793307145136 \t -0.09361100209794326\n",
            "20     \t [-6.44024618e-01 -5.96988615e-04]. \t  -19.955600150313476 \t -0.09361100209794326\n",
            "21     \t [-0.39759935  0.36022957]. \t  -6.039516721292988 \t -0.09361100209794326\n",
            "22     \t [-0.34487985 -0.20253144]. \t  -12.143226262234158 \t -0.09361100209794326\n",
            "23     \t [1.68411474 0.68570093]. \t  -462.9508923919746 \t -0.09361100209794326\n",
            "24     \t [-0.43480277  0.18302108]. \t  -2.062297952971337 \t -0.09361100209794326\n",
            "25     \t [-0.95011608 -1.76342322]. \t  -714.6352160143671 \t -0.09361100209794326\n",
            "26     \t [ 1.74525021 -2.03394314]. \t  -2581.034292136097 \t -0.09361100209794326\n",
            "27     \t [-1.21394464  0.86163408]. \t  -42.359319193002385 \t -0.09361100209794326\n",
            "28     \t [-0.96229044  0.97768811]. \t  -4.117719902706644 \t -0.09361100209794326\n",
            "29     \t [ 1.97957486 -1.68024005]. \t  -3135.791165885233 \t -0.09361100209794326\n",
            "30     \t [-2.03682352 -1.95950133]. \t  -3740.1736451495244 \t -0.09361100209794326\n",
            "31     \t [ 0.59201426 -1.62687485]. \t  -391.16002009217027 \t -0.09361100209794326\n",
            "32     \t [0.09857805 0.21412616]. \t  -4.99084636952099 \t -0.09361100209794326\n",
            "33     \t [-1.83727401  1.86692274]. \t  -235.65352583664207 \t -0.09361100209794326\n",
            "34     \t [-1.30842607 -1.26558062]. \t  -891.9148312871883 \t -0.09361100209794326\n",
            "35     \t [-0.68082076 -0.94634829]. \t  -201.59714479346763 \t -0.09361100209794326\n",
            "36     \t [-0.14916624 -1.03404296]. \t  -112.89618517418847 \t -0.09361100209794326\n",
            "37     \t [-1.54445384 -0.55743512]. \t  -872.4654088084312 \t -0.09361100209794326\n",
            "38     \t [1.21933868 1.40765359]. \t  -0.6743160546485591 \t -0.09361100209794326\n",
            "39     \t [-1.31877234 -0.92973758]. \t  -717.6783954777645 \t -0.09361100209794326\n",
            "40     \t [1.38830377 1.98958105]. \t  -0.5375853615386967 \t -0.09361100209794326\n",
            "41     \t [ 0.13353934 -0.00260272]. \t  -0.792514946262475 \t -0.09361100209794326\n",
            "42     \t [ 0.4781391  -1.74497403]. \t  -389.7784949090759 \t -0.09361100209794326\n",
            "43     \t [0.67799951 0.41469865]. \t  -0.306046521367792 \t -0.09361100209794326\n",
            "44     \t [ 1.22585933 -1.0603278 ]. \t  -656.9781034524161 \t -0.09361100209794326\n",
            "45     \t [0.05342914 0.77434006]. \t  -60.41496633178125 \t -0.09361100209794326\n",
            "46     \t [-1.54062542  0.92282972]. \t  -216.90694503923066 \t -0.09361100209794326\n",
            "47     \t [-1.92765507 -0.28267287]. \t  -1607.3929445715162 \t -0.09361100209794326\n",
            "48     \t [ 0.88818791 -1.32839832]. \t  -448.2983037969398 \t -0.09361100209794326\n",
            "49     \t [-0.65987705 -0.13774205]. \t  -35.608697485650005 \t -0.09361100209794326\n",
            "50     \t [1.9362214  1.29276233]. \t  -604.1639204282061 \t -0.09361100209794326\n",
            "51     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.09361100209794326\n",
            "52     \t [-0.21907144 -1.96703807]. \t  -407.5208723607068 \t -0.09361100209794326\n",
            "53     \t [-1.70191383  1.46957766]. \t  -210.91412597964643 \t -0.09361100209794326\n",
            "54     \t [-1.15543014 -0.80314546]. \t  -461.8205277732385 \t -0.09361100209794326\n",
            "55     \t [-0.6445735   0.19937118]. \t  -7.374707665560582 \t -0.09361100209794326\n",
            "56     \t [0.8318862  1.35426881]. \t  -43.88367000284221 \t -0.09361100209794326\n",
            "57     \t [-0.49534133 -0.87140828]. \t  -126.95386347910244 \t -0.09361100209794326\n",
            "58     \t [0.74548014 1.42810719]. \t  -76.16712000504089 \t -0.09361100209794326\n",
            "59     \t [0.46896872 0.98691183]. \t  -59.10785224430413 \t -0.09361100209794326\n",
            "60     \t [1.02667399 1.0636183 ]. \t  \u001b[92m-0.009848584950261956\u001b[0m \t -0.009848584950261956\n",
            "61     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.009848584950261956\n",
            "62     \t [ 0.98149748 -1.26261417]. \t  -495.4863362533916 \t -0.009848584950261956\n",
            "63     \t [0.27225107 1.76041364]. \t  -284.8880240298879 \t -0.009848584950261956\n",
            "64     \t [-0.83953917  0.4039758 ]. \t  -12.434989412544088 \t -0.009848584950261956\n",
            "65     \t [-0.44755408 -1.96706802]. \t  -471.8458404542322 \t -0.009848584950261956\n",
            "66     \t [1.31828968 1.71317367]. \t  -0.1623865275504108 \t -0.009848584950261956\n",
            "67     \t [0.83759372 0.14908133]. \t  -30.550001785808465 \t -0.009848584950261956\n",
            "68     \t [0.98456094 0.25075815]. \t  -51.63913704506476 \t -0.009848584950261956\n",
            "69     \t [ 0.73246608 -1.77368981]. \t  -533.772297993229 \t -0.009848584950261956\n",
            "70     \t [-1.58563016  1.17593792]. \t  -185.78617810527686 \t -0.009848584950261956\n",
            "71     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.009848584950261956\n",
            "72     \t [-0.81686469  0.1363013 ]. \t  -31.49355347048604 \t -0.009848584950261956\n",
            "73     \t [1.49237833 1.5409657 ]. \t  -47.333238345706455 \t -0.009848584950261956\n",
            "74     \t [0.77181157 0.30847436]. \t  -8.301530181457267 \t -0.009848584950261956\n",
            "75     \t [1.16901453 1.36296943]. \t  -0.0298803724616033 \t -0.009848584950261956\n",
            "76     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.009848584950261956\n",
            "77     \t [0.14351039 0.89506344]. \t  -77.20303940540292 \t -0.009848584950261956\n",
            "78     \t [ 0.82063841 -0.86387592]. \t  -236.36846866714586 \t -0.009848584950261956\n",
            "79     \t [ 0.29947585 -1.35759449]. \t  -209.95275381756917 \t -0.009848584950261956\n",
            "80     \t [0.41354938 1.88425379]. \t  -293.8598650763438 \t -0.009848584950261956\n",
            "81     \t [ 1.16142564 -0.78263878]. \t  -454.37587569895425 \t -0.009848584950261956\n",
            "82     \t [0.4382337  0.96718236]. \t  -60.39878767252786 \t -0.009848584950261956\n",
            "83     \t [-1.55106656  1.40530115]. \t  -106.60922967669718 \t -0.009848584950261956\n",
            "84     \t [-0.66643241 -1.82568483]. \t  -517.9839136256584 \t -0.009848584950261956\n",
            "85     \t [-1.30003343  0.94185481]. \t  -61.27528154653576 \t -0.009848584950261956\n",
            "86     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.009848584950261956\n",
            "87     \t [-1.17277124 -0.21195015]. \t  -256.686568077995 \t -0.009848584950261956\n",
            "88     \t [-0.37738586  2.03176769]. \t  -358.8606279912727 \t -0.009848584950261956\n",
            "89     \t [ 1.09585673 -0.96871735]. \t  -470.7339851117273 \t -0.009848584950261956\n",
            "90     \t [1.21564778 1.42834496]. \t  -0.29107943655372714 \t -0.009848584950261956\n",
            "91     \t [1.26013997 2.01043038]. \t  -17.91640790402721 \t -0.009848584950261956\n",
            "92     \t [1.06460311 1.12424786]. \t  -0.012512767571803815 \t -0.009848584950261956\n",
            "93     \t [-0.60869851  1.45333029]. \t  -119.83704824326959 \t -0.009848584950261956\n",
            "94     \t [-0.37949809 -1.97403528]. \t  -450.51832415832763 \t -0.009848584950261956\n",
            "95     \t [-0.03532646  0.20508262]. \t  -5.226757806966602 \t -0.009848584950261956\n",
            "96     \t [-1.44592539  0.65227005]. \t  -212.89069487972304 \t -0.009848584950261956\n",
            "97     \t [0.55016118 0.30517732]. \t  -0.20297995737585128 \t -0.009848584950261956\n",
            "98     \t [-1.10398428  1.61088889]. \t  -19.801586554812506 \t -0.009848584950261956\n",
            "99     \t [ 1.22811738 -0.14253543]. \t  -272.5686505446685 \t -0.009848584950261956\n",
            "100    \t [2.0161086 1.2859591]. \t  -773.1691858866905 \t -0.009848584950261956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "98caf20f-59ed-49e7-f3fa-051c931f46cd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_14 = d2GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.24800705 0.56011416]. \t  -99.54366772238504 \t -4.306489127802793\n",
            "2      \t [0.49471651 0.39956896]. \t  \u001b[92m-2.652375168352414\u001b[0m \t -2.652375168352414\n",
            "3      \t [1.3279018  1.78259635]. \t  \u001b[92m-0.14466504609048203\u001b[0m \t -0.14466504609048203\n",
            "4      \t [-1.10253403 -0.55489366]. \t  -317.8788037567552 \t -0.14466504609048203\n",
            "5      \t [0.58599126 0.35218709]. \t  -0.1791495767026406 \t -0.14466504609048203\n",
            "6      \t [1.22709133 1.64386416]. \t  -1.9590361316735203 \t -0.14466504609048203\n",
            "7      \t [1.95579534 1.99686802]. \t  -335.1697046308149 \t -0.14466504609048203\n",
            "8      \t [ 0.06241567 -1.28637043]. \t  -167.35773773045298 \t -0.14466504609048203\n",
            "9      \t [0.99272777 1.61062043]. \t  -39.07655438276457 \t -0.14466504609048203\n",
            "10     \t [-1.9081828   1.84188939]. \t  -332.1955764905027 \t -0.14466504609048203\n",
            "11     \t [-1.51846679 -1.34386354]. \t  -1338.30428218148 \t -0.14466504609048203\n",
            "12     \t [ 0.28703135 -0.42608967]. \t  -26.363176805245892 \t -0.14466504609048203\n",
            "13     \t [-0.53461138 -0.21466716]. \t  -27.402704124229643 \t -0.14466504609048203\n",
            "14     \t [-0.08636038 -0.2377112 ]. \t  -7.190978374761219 \t -0.14466504609048203\n",
            "15     \t [-0.26571277 -0.52794389]. \t  -37.42790049501683 \t -0.14466504609048203\n",
            "16     \t [1.27578827 1.74664617]. \t  -1.4924083048599641 \t -0.14466504609048203\n",
            "17     \t [ 0.92289904 -1.64478012]. \t  -623.2685286112684 \t -0.14466504609048203\n",
            "18     \t [-0.78956188 -0.12524497]. \t  -59.250652875611806 \t -0.14466504609048203\n",
            "19     \t [-1.75256083  1.7006648 ]. \t  -195.48713309869547 \t -0.14466504609048203\n",
            "20     \t [-0.89976709  1.37398796]. \t  -35.46465657299879 \t -0.14466504609048203\n",
            "21     \t [1.07664943 1.24512533]. \t  -0.7446382737429118 \t -0.14466504609048203\n",
            "22     \t [-1.1497589   1.36939796]. \t  -4.846636539284939 \t -0.14466504609048203\n",
            "23     \t [-0.18188964  0.04843406]. \t  -1.4204260516855303 \t -0.14466504609048203\n",
            "24     \t [-1.71316048  0.26234352]. \t  -721.6271162631949 \t -0.14466504609048203\n",
            "25     \t [-0.17347511 -0.56901051]. \t  -37.26961907705688 \t -0.14466504609048203\n",
            "26     \t [1.35949135 1.98432447]. \t  -1.981765697419812 \t -0.14466504609048203\n",
            "27     \t [0.26172214 0.04461656]. \t  -0.6020888236094376 \t -0.14466504609048203\n",
            "28     \t [-0.57630773  0.31491141]. \t  -2.514396113286633 \t -0.14466504609048203\n",
            "29     \t [ 0.08063829 -0.60628709]. \t  -38.39633775902093 \t -0.14466504609048203\n",
            "30     \t [-1.39992158  0.49737864]. \t  -219.62152666592806 \t -0.14466504609048203\n",
            "31     \t [-1.13725457  1.08965637]. \t  -8.716883501670221 \t -0.14466504609048203\n",
            "32     \t [1.62780113 0.05607663]. \t  -673.101292312616 \t -0.14466504609048203\n",
            "33     \t [-1.16085581  0.21366414]. \t  -133.24722419089474 \t -0.14466504609048203\n",
            "34     \t [1.1384122  1.23342003]. \t  -0.41056217056391336 \t -0.14466504609048203\n",
            "35     \t [-1.2649414   1.63658342]. \t  -5.263233236510421 \t -0.14466504609048203\n",
            "36     \t [ 0.77205549 -1.24169292]. \t  -337.7890947457284 \t -0.14466504609048203\n",
            "37     \t [0.90652924 0.73259302]. \t  -0.8044409244475154 \t -0.14466504609048203\n",
            "38     \t [ 1.47847777 -1.31991067]. \t  -1229.297341811999 \t -0.14466504609048203\n",
            "39     \t [0.24445474 0.67058581]. \t  -37.88189592221282 \t -0.14466504609048203\n",
            "40     \t [1.05254916 1.11553231]. \t  \u001b[92m-0.008648262422754968\u001b[0m \t -0.008648262422754968\n",
            "41     \t [0.45014774 1.91013468]. \t  -291.8585425470606 \t -0.008648262422754968\n",
            "42     \t [1.43367697 0.32061112]. \t  -301.14760728437517 \t -0.008648262422754968\n",
            "43     \t [-0.01326109 -0.49478701]. \t  -25.525522378701645 \t -0.008648262422754968\n",
            "44     \t [-1.5182108   0.80987152]. \t  -229.87155168550814 \t -0.008648262422754968\n",
            "45     \t [ 0.32514034 -1.28652307]. \t  -194.28846499360978 \t -0.008648262422754968\n",
            "46     \t [-1.9703253   1.59067058]. \t  -533.9251830288297 \t -0.008648262422754968\n",
            "47     \t [ 1.91370402 -1.69217447]. \t  -2867.835007825963 \t -0.008648262422754968\n",
            "48     \t [-1.73379435 -0.73116528]. \t  -1404.1460948150188 \t -0.008648262422754968\n",
            "49     \t [ 1.23819025 -0.23669483]. \t  -313.27944944341465 \t -0.008648262422754968\n",
            "50     \t [-0.05142272  2.04169095]. \t  -416.87661347692347 \t -0.008648262422754968\n",
            "51     \t [-2.00496167  0.62367447]. \t  -1162.4450734371903 \t -0.008648262422754968\n",
            "52     \t [1.17828682 1.31491136]. \t  -0.5712539261312275 \t -0.008648262422754968\n",
            "53     \t [-1.65210435 -0.0990571 ]. \t  -807.0782056667098 \t -0.008648262422754968\n",
            "54     \t [0.05155122 0.37314437]. \t  -14.625605019530663 \t -0.008648262422754968\n",
            "55     \t [1.19272477 1.42495248]. \t  -0.03769984449128316 \t -0.008648262422754968\n",
            "56     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.008648262422754968\n",
            "57     \t [-1.58088158 -0.22387269]. \t  -748.1661172920212 \t -0.008648262422754968\n",
            "58     \t [-0.87484484  1.09494647]. \t  -14.37819674086421 \t -0.008648262422754968\n",
            "59     \t [1.11981129 2.01294109]. \t  -57.61695416628044 \t -0.008648262422754968\n",
            "60     \t [-1.32088165  0.18350019]. \t  -249.12981988210257 \t -0.008648262422754968\n",
            "61     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.008648262422754968\n",
            "62     \t [-0.83360606 -0.39821277]. \t  -122.85145702179399 \t -0.008648262422754968\n",
            "63     \t [-1.66722611  0.57154307]. \t  -494.6845860274701 \t -0.008648262422754968\n",
            "64     \t [ 1.43432849 -0.71886116]. \t  -770.8947222516093 \t -0.008648262422754968\n",
            "65     \t [1.74145888 0.47871748]. \t  -652.8217275890095 \t -0.008648262422754968\n",
            "66     \t [-1.38319376 -0.62574996]. \t  -650.3189806290884 \t -0.008648262422754968\n",
            "67     \t [ 1.33553529 -1.94373514]. \t  -1389.4559372659976 \t -0.008648262422754968\n",
            "68     \t [ 2.03794533 -1.12526205]. \t  -2787.3158341879744 \t -0.008648262422754968\n",
            "69     \t [-1.70860006 -0.65519372]. \t  -1285.0471706975995 \t -0.008648262422754968\n",
            "70     \t [ 1.67728183 -0.22584165]. \t  -924.0813148512133 \t -0.008648262422754968\n",
            "71     \t [-0.55951318  0.93751851]. \t  -41.427548758187484 \t -0.008648262422754968\n",
            "72     \t [-1.19208035 -1.95536668]. \t  -1144.827932674559 \t -0.008648262422754968\n",
            "73     \t [-2.02108705  1.69906427]. \t  -578.2970575781673 \t -0.008648262422754968\n",
            "74     \t [1.0360411  1.08228603]. \t  -0.009228619843965592 \t -0.008648262422754968\n",
            "75     \t [-0.67274238 -1.56222325]. \t  -408.7422158525488 \t -0.008648262422754968\n",
            "76     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.008648262422754968\n",
            "77     \t [-1.53121491  2.03244255]. \t  -16.15246815647346 \t -0.008648262422754968\n",
            "78     \t [-1.69721438 -1.92572466]. \t  -2317.2897352832306 \t -0.008648262422754968\n",
            "79     \t [ 1.38138552 -1.7334792 ]. \t  -1326.3470913776823 \t -0.008648262422754968\n",
            "80     \t [-1.18472947  0.67165453]. \t  -58.34510613785976 \t -0.008648262422754968\n",
            "81     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.008648262422754968\n",
            "82     \t [-1.06147932 -1.21766938]. \t  -553.8744545919657 \t -0.008648262422754968\n",
            "83     \t [-0.48247986 -1.08389492]. \t  -175.56282593029763 \t -0.008648262422754968\n",
            "84     \t [ 1.39245159 -1.35409851]. \t  -1084.5520546572018 \t -0.008648262422754968\n",
            "85     \t [ 0.68835944 -1.54346285]. \t  -407.0476822785868 \t -0.008648262422754968\n",
            "86     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.008648262422754968\n",
            "87     \t [1.3625676  0.35762958]. \t  -224.81982638961267 \t -0.008648262422754968\n",
            "88     \t [1.67259036 0.60769277]. \t  -480.00357033406283 \t -0.008648262422754968\n",
            "89     \t [-0.10675584  1.4505127 ]. \t  -208.3303635018325 \t -0.008648262422754968\n",
            "90     \t [ 1.66960399 -0.90521397]. \t  -1364.119242511162 \t -0.008648262422754968\n",
            "91     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
            "92     \t [-0.32989008  0.52002008]. \t  -18.676544542455904 \t -0.00598628680283637\n",
            "93     \t [ 1.00089176 -0.74819103]. \t  -306.24137151272816 \t -0.00598628680283637\n",
            "94     \t [-0.31594662  0.30611815]. \t  -5.987514954531987 \t -0.00598628680283637\n",
            "95     \t [1.2145813  1.50049977]. \t  -0.11001379594607463 \t -0.00598628680283637\n",
            "96     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.00598628680283637\n",
            "97     \t [1.06233966 1.11046425]. \t  -0.0366519316149771 \t -0.00598628680283637\n",
            "98     \t [ 1.98684688 -0.24356647]. \t  -1757.5284212307308 \t -0.00598628680283637\n",
            "99     \t [ 1.8417731  -0.26493655]. \t  -1338.1207981665664 \t -0.00598628680283637\n",
            "100    \t [-0.67578045 -1.95670776]. \t  -585.2519126932482 \t -0.00598628680283637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "32206216-0d87-4065-8e53-1295b60e48be"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_15 = d2GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.85976836  0.74899772]. \t  -742.4477063352382 \t -6.867717811955245\n",
            "2      \t [-0.20803264  0.28777365]. \t  -7.4371755439131615 \t -6.867717811955245\n",
            "3      \t [-0.71353464  0.78042254]. \t  -10.29607442865114 \t -6.867717811955245\n",
            "4      \t [1.51801528 1.92878838]. \t  -14.374524553155442 \t -6.867717811955245\n",
            "5      \t [1.08082053 0.08053379]. \t  -118.30244166824869 \t -6.867717811955245\n",
            "6      \t [-0.48569941 -0.11972766]. \t  -14.854684363022324 \t -6.867717811955245\n",
            "7      \t [ 0.26334297 -0.06630184]. \t  \u001b[92m-2.382792622423571\u001b[0m \t -2.382792622423571\n",
            "8      \t [-0.06035571 -0.06680376]. \t  \u001b[92m-1.620626221345426\u001b[0m \t -1.620626221345426\n",
            "9      \t [0.01508638 0.00130221]. \t  \u001b[92m-0.9701703091599679\u001b[0m \t -0.9701703091599679\n",
            "10     \t [-0.2868408   0.20632703]. \t  -3.1947842012406973 \t -0.9701703091599679\n",
            "11     \t [-1.00523999  1.421984  ]. \t  -20.95228258386988 \t -0.9701703091599679\n",
            "12     \t [0.20502039 1.38416858]. \t  -180.76468746907915 \t -0.9701703091599679\n",
            "13     \t [ 0.81769938 -1.39697574]. \t  -426.7068816963656 \t -0.9701703091599679\n",
            "14     \t [1.90871707 1.92396936]. \t  -296.40145816577973 \t -0.9701703091599679\n",
            "15     \t [1.28242201 2.00768231]. \t  -13.26218732394694 \t -0.9701703091599679\n",
            "16     \t [1.28770054 1.22973133]. \t  -18.43897116299225 \t -0.9701703091599679\n",
            "17     \t [1.32339482 1.60912997]. \t  -2.127916248328541 \t -0.9701703091599679\n",
            "18     \t [-0.50267092  0.30773983]. \t  -2.561199829167795 \t -0.9701703091599679\n",
            "19     \t [-1.05075576  1.10436209]. \t  -4.205606711652041 \t -0.9701703091599679\n",
            "20     \t [-1.9191221   1.39569548]. \t  -531.7110285934393 \t -0.9701703091599679\n",
            "21     \t [ 1.81284984 -0.20690402]. \t  -1220.995160811488 \t -0.9701703091599679\n",
            "22     \t [0.76357062 0.56575029]. \t  \u001b[92m-0.08579258876773765\u001b[0m \t -0.08579258876773765\n",
            "23     \t [-1.28718132  1.3649696 ]. \t  -13.749783661926397 \t -0.08579258876773765\n",
            "24     \t [0.72818365 1.90109329]. \t  -187.99462602351747 \t -0.08579258876773765\n",
            "25     \t [-1.21888986  1.87781057]. \t  -20.299131433609716 \t -0.08579258876773765\n",
            "26     \t [ 0.22401697 -1.35838773]. \t  -199.00946979239325 \t -0.08579258876773765\n",
            "27     \t [ 1.42645813 -1.38249747]. \t  -1167.962308583028 \t -0.08579258876773765\n",
            "28     \t [0.96043146 0.7552118 ]. \t  -2.7977111976752664 \t -0.08579258876773765\n",
            "29     \t [0.64518464 0.34878588]. \t  -0.5812130216948354 \t -0.08579258876773765\n",
            "30     \t [ 0.97642433 -0.04462397]. \t  -99.60663494896336 \t -0.08579258876773765\n",
            "31     \t [0.20243607 1.33620308]. \t  -168.3962972454647 \t -0.08579258876773765\n",
            "32     \t [ 1.86202408 -0.41052955]. \t  -1504.3702930072584 \t -0.08579258876773765\n",
            "33     \t [-1.87844345  1.70426946]. \t  -341.08531238943704 \t -0.08579258876773765\n",
            "34     \t [-0.69688178  0.87300274]. \t  -17.884070497534147 \t -0.08579258876773765\n",
            "35     \t [-0.5765119   0.70890288]. \t  -16.66339463956982 \t -0.08579258876773765\n",
            "36     \t [0.68769956 0.08772452]. \t  -14.935911093023686 \t -0.08579258876773765\n",
            "37     \t [ 0.94346704 -0.1780317 ]. \t  -114.10015004857898 \t -0.08579258876773765\n",
            "38     \t [0.75464442 0.56752197]. \t  \u001b[92m-0.060585966473464356\u001b[0m \t -0.060585966473464356\n",
            "39     \t [-0.9926861   0.51774171]. \t  -25.843628897374533 \t -0.060585966473464356\n",
            "40     \t [0.74866456 0.57680296]. \t  -0.08975261194468892 \t -0.060585966473464356\n",
            "41     \t [1.32391133 1.70660611]. \t  -0.31776342534804597 \t -0.060585966473464356\n",
            "42     \t [-0.45892964  0.92975848]. \t  -53.84500608764505 \t -0.060585966473464356\n",
            "43     \t [1.79425229 1.17518273]. \t  -418.4892525149357 \t -0.060585966473464356\n",
            "44     \t [0.17414949 0.71603838]. \t  -47.70189533400468 \t -0.060585966473464356\n",
            "45     \t [ 1.68741734 -1.4058258 ]. \t  -1809.4462006025503 \t -0.060585966473464356\n",
            "46     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.060585966473464356\n",
            "47     \t [0.99116959 1.02702693]. \t  -0.19908113031364252 \t -0.060585966473464356\n",
            "48     \t [1.18553489 1.4628777 ]. \t  -0.36372384737361596 \t -0.060585966473464356\n",
            "49     \t [-1.12522902 -1.26760128]. \t  -646.5012649272198 \t -0.060585966473464356\n",
            "50     \t [1.44025579 2.04683577]. \t  -0.2694555683212846 \t -0.060585966473464356\n",
            "51     \t [1.82117592 0.02005112]. \t  -1087.4516611715662 \t -0.060585966473464356\n",
            "52     \t [-0.97041722 -0.56868483]. \t  -232.01166970174543 \t -0.060585966473464356\n",
            "53     \t [1.09887081 1.2615793 ]. \t  -0.3020479380411238 \t -0.060585966473464356\n",
            "54     \t [0.96021954 0.95337528]. \t  -0.09988798143770664 \t -0.060585966473464356\n",
            "55     \t [ 0.13604281 -1.96151768]. \t  -392.79644976636814 \t -0.060585966473464356\n",
            "56     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.060585966473464356\n",
            "57     \t [-1.72252026 -1.78299641]. \t  -2263.730965109315 \t -0.060585966473464356\n",
            "58     \t [0.95246595 0.91919823]. \t  \u001b[92m-0.01667593589089989\u001b[0m \t -0.01667593589089989\n",
            "59     \t [-0.89795156 -0.02881667]. \t  -73.34704401720438 \t -0.01667593589089989\n",
            "60     \t [ 2.02435328 -0.77141525]. \t  -2372.1758244197367 \t -0.01667593589089989\n",
            "61     \t [0.49676181 0.3065502 ]. \t  -0.6105884926302754 \t -0.01667593589089989\n",
            "62     \t [ 0.74638391 -0.94930964]. \t  -226.98799123775052 \t -0.01667593589089989\n",
            "63     \t [-1.85923427  0.72504786]. \t  -754.3960043276296 \t -0.01667593589089989\n",
            "64     \t [1.29422912 1.65370397]. \t  -0.13204656406780957 \t -0.01667593589089989\n",
            "65     \t [-1.0754649   0.43522195]. \t  -56.34975499227283 \t -0.01667593589089989\n",
            "66     \t [0.68320998 0.47494266]. \t  -0.10702555738314529 \t -0.01667593589089989\n",
            "67     \t [0.71328651 0.49137061]. \t  -0.11250513742056896 \t -0.01667593589089989\n",
            "68     \t [0.96582491 0.94273216]. \t  \u001b[92m-0.010997489813224338\u001b[0m \t -0.010997489813224338\n",
            "69     \t [0.99943212 1.00428487]. \t  \u001b[92m-0.0029382945008131363\u001b[0m \t -0.0029382945008131363\n",
            "70     \t [1.14525522 1.327836  ]. \t  -0.04742892953935504 \t -0.0029382945008131363\n",
            "71     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.0029382945008131363\n",
            "72     \t [1.01021726 0.99828821]. \t  -0.049613736625678376 \t -0.0029382945008131363\n",
            "73     \t [-0.88726298  1.60797499]. \t  -70.92307559057677 \t -0.0029382945008131363\n",
            "74     \t [1.41104204 1.9901646 ]. \t  -0.1690321238893527 \t -0.0029382945008131363\n",
            "75     \t [ 0.56475636 -1.4631005 ]. \t  -317.7597443470307 \t -0.0029382945008131363\n",
            "76     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.0029382945008131363\n",
            "77     \t [-0.50112241  0.72246393]. \t  -24.46953284373991 \t -0.0029382945008131363\n",
            "78     \t [ 2.0139713  -0.14158256]. \t  -1763.065573072827 \t -0.0029382945008131363\n",
            "79     \t [0.43465746 0.19616264]. \t  -0.32484748480555864 \t -0.0029382945008131363\n",
            "80     \t [0.6547521  0.44081045]. \t  -0.1338616449430302 \t -0.0029382945008131363\n",
            "81     \t [1.58678507 2.0096641 ]. \t  -26.173355388676914 \t -0.0029382945008131363\n",
            "82     \t [0.10639442 1.08194119]. \t  -115.42155236530361 \t -0.0029382945008131363\n",
            "83     \t [-0.06708015 -1.62779617]. \t  -267.5776570316121 \t -0.0029382945008131363\n",
            "84     \t [0.68517674 0.17783462]. \t  -8.604067714590572 \t -0.0029382945008131363\n",
            "85     \t [0.39495926 1.89434764]. \t  -302.55382616645153 \t -0.0029382945008131363\n",
            "86     \t [-1.00569849  0.65747462]. \t  -16.551228310140736 \t -0.0029382945008131363\n",
            "87     \t [-1.69201713 -0.00320477]. \t  -828.7152142879839 \t -0.0029382945008131363\n",
            "88     \t [0.6276886  0.41282151]. \t  -0.17406713455960235 \t -0.0029382945008131363\n",
            "89     \t [-0.40366291 -0.83154994]. \t  -100.87203805527996 \t -0.0029382945008131363\n",
            "90     \t [-0.59352026 -0.2073231 ]. \t  -33.85333716244417 \t -0.0029382945008131363\n",
            "91     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.0029382945008131363\n",
            "92     \t [1.1832348  1.38873823]. \t  -0.04635836716497785 \t -0.0029382945008131363\n",
            "93     \t [-1.85018417  1.29794549]. \t  -459.7863391163911 \t -0.0029382945008131363\n",
            "94     \t [1.53159452 0.88600749]. \t  -213.37668811417538 \t -0.0029382945008131363\n",
            "95     \t [1.06770387 1.46676907]. \t  -10.682938992362306 \t -0.0029382945008131363\n",
            "96     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.0029382945008131363\n",
            "97     \t [1.10675821 0.2211871 ]. \t  -100.75811175632232 \t -0.0029382945008131363\n",
            "98     \t [ 0.83029518 -0.30582201]. \t  -99.07350907469653 \t -0.0029382945008131363\n",
            "99     \t [-0.87332185 -0.51619905]. \t  -167.06532525331025 \t -0.0029382945008131363\n",
            "100    \t [-1.01396943 -1.98510142]. \t  -912.0148419647545 \t -0.0029382945008131363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "035a52d7-b830-44cf-a1a2-f5b702e79fa5"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_16 = d2GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.0020416  -1.10535141]. \t  -123.1770158866703 \t -21.690996320546372\n",
            "2      \t [0.24240804 0.47502699]. \t  \u001b[92m-17.901628692275246\u001b[0m \t -17.901628692275246\n",
            "3      \t [ 0.32151913 -0.27077225]. \t  \u001b[92m-14.458919207940461\u001b[0m \t -14.458919207940461\n",
            "4      \t [-2.20708547  0.65144214]. \t  -1790.943210340659 \t -14.458919207940461\n",
            "5      \t [-0.89103455 -0.34877301]. \t  -134.15590261488182 \t -14.458919207940461\n",
            "6      \t [ 0.40793858 -2.14025479]. \t  -532.4225733960579 \t -14.458919207940461\n",
            "7      \t [-0.54173567  0.52236823]. \t  \u001b[92m-7.616043900087673\u001b[0m \t -7.616043900087673\n",
            "8      \t [0.50463874 0.2764911 ]. \t  \u001b[92m-0.293041351368684\u001b[0m \t -0.293041351368684\n",
            "9      \t [0.53307902 1.03665218]. \t  -56.840470204497755 \t -0.293041351368684\n",
            "10     \t [-1.58662742 -0.14000879]. \t  -712.8656417917898 \t -0.293041351368684\n",
            "11     \t [-0.73557801  0.08090682]. \t  -24.187707027594676 \t -0.293041351368684\n",
            "12     \t [0.41482158 0.12694349]. \t  -0.5461366341189933 \t -0.293041351368684\n",
            "13     \t [1.9363175  1.74529415]. \t  -402.49083744223543 \t -0.293041351368684\n",
            "14     \t [-0.81581675  0.35348286]. \t  -13.036215038051493 \t -0.293041351368684\n",
            "15     \t [-0.44192674  1.54812946]. \t  -185.09411335156437 \t -0.293041351368684\n",
            "16     \t [-1.52930951 -1.01038496]. \t  -1128.0930833336422 \t -0.293041351368684\n",
            "17     \t [-0.48561958 -1.32420417]. \t  -245.5765963866809 \t -0.293041351368684\n",
            "18     \t [1.28959433 1.43567933]. \t  -5.2537672055258255 \t -0.293041351368684\n",
            "19     \t [1.27826058 1.87512765]. \t  -5.894089219687638 \t -0.293041351368684\n",
            "20     \t [1.08800789 1.40419291]. \t  -4.866760439273172 \t -0.293041351368684\n",
            "21     \t [0.68986958 1.53523052]. \t  -112.31005072817484 \t -0.293041351368684\n",
            "22     \t [-1.09066884  1.80179203]. \t  -41.85388301514736 \t -0.293041351368684\n",
            "23     \t [-1.35518327  2.01101538]. \t  -8.591692801280274 \t -0.293041351368684\n",
            "24     \t [1.18631505 1.52298232]. \t  -1.3719492584656896 \t -0.293041351368684\n",
            "25     \t [1.24661803 1.99563496]. \t  -19.559972994477278 \t -0.293041351368684\n",
            "26     \t [1.12976238 1.31710681]. \t  \u001b[92m-0.18284384364625939\u001b[0m \t -0.18284384364625939\n",
            "27     \t [ 0.67906291 -1.46483459]. \t  -371.0355872844689 \t -0.18284384364625939\n",
            "28     \t [-1.99274381  0.30019054]. \t  -1356.4611941865257 \t -0.18284384364625939\n",
            "29     \t [0.44299567 0.19271193]. \t  -0.3115021985034623 \t -0.18284384364625939\n",
            "30     \t [ 0.9332122  -1.70126987]. \t  -661.6025332165561 \t -0.18284384364625939\n",
            "31     \t [ 0.79193639 -1.45821261]. \t  -434.9225345066252 \t -0.18284384364625939\n",
            "32     \t [-0.40342255 -1.92282347]. \t  -436.9311602155533 \t -0.18284384364625939\n",
            "33     \t [0.44858559 0.14463035]. \t  -0.6243989373758893 \t -0.18284384364625939\n",
            "34     \t [1.17350919 1.36012331]. \t  \u001b[92m-0.05900721600123292\u001b[0m \t -0.05900721600123292\n",
            "35     \t [-0.63146643 -1.64020932]. \t  -418.3971345771591 \t -0.05900721600123292\n",
            "36     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.05900721600123292\n",
            "37     \t [-0.29731575  0.14691306]. \t  -2.0254451751424076 \t -0.05900721600123292\n",
            "38     \t [-0.96407838  1.00210741]. \t  -4.385555592058041 \t -0.05900721600123292\n",
            "39     \t [-1.69585202 -1.39566601]. \t  -1831.9072668479507 \t -0.05900721600123292\n",
            "40     \t [1.1621361  1.34597005]. \t  \u001b[92m-0.028395174620842813\u001b[0m \t -0.028395174620842813\n",
            "41     \t [-0.33921788  2.02645588]. \t  -367.13357261498413 \t -0.028395174620842813\n",
            "42     \t [1.51433978 2.00771194]. \t  -8.416314764337956 \t -0.028395174620842813\n",
            "43     \t [ 1.27618886 -1.3273556 ]. \t  -873.8779230470094 \t -0.028395174620842813\n",
            "44     \t [1.15197951 1.32160423]. \t  \u001b[92m-0.026070818032635418\u001b[0m \t -0.026070818032635418\n",
            "45     \t [-0.61286995 -1.65116907]. \t  -413.3845153643779 \t -0.026070818032635418\n",
            "46     \t [0.67686122 1.11927547]. \t  -43.81428231245156 \t -0.026070818032635418\n",
            "47     \t [-0.22066691 -0.38020652]. \t  -19.885583106297887 \t -0.026070818032635418\n",
            "48     \t [-0.01985748  0.00080942]. \t  -1.0401265016967576 \t -0.026070818032635418\n",
            "49     \t [0.95730076 0.22697202]. \t  -47.53633027926535 \t -0.026070818032635418\n",
            "50     \t [0.79284307 0.60278649]. \t  -0.1095484382638309 \t -0.026070818032635418\n",
            "51     \t [1.12478789 1.27913747]. \t  -0.035143115742366546 \t -0.026070818032635418\n",
            "52     \t [-0.67517123 -1.00998659]. \t  -217.67570332419444 \t -0.026070818032635418\n",
            "53     \t [-1.27228905 -1.86451717]. \t  -1218.457017748896 \t -0.026070818032635418\n",
            "54     \t [0.68674822 0.44958093]. \t  -0.14671246635983115 \t -0.026070818032635418\n",
            "55     \t [-0.64063064 -1.4676662 ]. \t  -355.40779388843447 \t -0.026070818032635418\n",
            "56     \t [ 1.44253993 -0.83813951]. \t  -852.2875350360233 \t -0.026070818032635418\n",
            "57     \t [-1.20340356  0.65695278]. \t  -67.45905943685975 \t -0.026070818032635418\n",
            "58     \t [ 1.74567009 -1.89656171]. \t  -2444.796244157968 \t -0.026070818032635418\n",
            "59     \t [0.02346288 1.9066815 ]. \t  -364.2871597262516 \t -0.026070818032635418\n",
            "60     \t [0.00200424 1.56567155]. \t  -246.12747811354427 \t -0.026070818032635418\n",
            "61     \t [-0.78139034  0.53479172]. \t  -3.747599573318423 \t -0.026070818032635418\n",
            "62     \t [ 1.35482936 -0.40326941]. \t  -501.3627715286027 \t -0.026070818032635418\n",
            "63     \t [-0.19401555 -0.5657402 ]. \t  -37.83268513239098 \t -0.026070818032635418\n",
            "64     \t [-1.88164839 -0.66326923]. \t  -1775.5560972745068 \t -0.026070818032635418\n",
            "65     \t [-0.35956237  1.98789795]. \t  -347.292581304873 \t -0.026070818032635418\n",
            "66     \t [0.52248363 0.2877346 ]. \t  -0.24976471362434333 \t -0.026070818032635418\n",
            "67     \t [-0.51937017 -1.56237223]. \t  -337.97397823813054 \t -0.026070818032635418\n",
            "68     \t [ 1.47032429 -0.11759852]. \t  -519.8113640209032 \t -0.026070818032635418\n",
            "69     \t [1.29801797 1.061399  ]. \t  -38.95801006777799 \t -0.026070818032635418\n",
            "70     \t [ 1.27704013 -0.05560941]. \t  -284.48504812707824 \t -0.026070818032635418\n",
            "71     \t [-0.01455274  1.61486514]. \t  -261.7398640546982 \t -0.026070818032635418\n",
            "72     \t [-0.20521007 -0.20636756]. \t  -7.626699547511159 \t -0.026070818032635418\n",
            "73     \t [0.70388361 0.47529786]. \t  -0.12830441405776385 \t -0.026070818032635418\n",
            "74     \t [1.17001987 1.3447501 ]. \t  -0.08745329084247008 \t -0.026070818032635418\n",
            "75     \t [-1.33548973  0.16839956]. \t  -266.3200555030319 \t -0.026070818032635418\n",
            "76     \t [-1.13398124  1.03745487]. \t  -10.727043227510059 \t -0.026070818032635418\n",
            "77     \t [-1.27488388  0.72116571]. \t  -86.92620525398058 \t -0.026070818032635418\n",
            "78     \t [0.57716626 0.36447815]. \t  -0.2771160892930471 \t -0.026070818032635418\n",
            "79     \t [ 1.95609107 -0.37593049]. \t  -1766.7817341777504 \t -0.026070818032635418\n",
            "80     \t [1.91253661 0.98388592]. \t  -715.8123979341622 \t -0.026070818032635418\n",
            "81     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.026070818032635418\n",
            "82     \t [ 1.40973898 -1.13316608]. \t  -973.9386714775475 \t -0.026070818032635418\n",
            "83     \t [ 1.67192921 -1.86254559]. \t  -2170.0480973974777 \t -0.026070818032635418\n",
            "84     \t [ 0.51738178 -0.67274971]. \t  -88.67445975258771 \t -0.026070818032635418\n",
            "85     \t [-0.72281118 -1.29658803]. \t  -333.8601983413895 \t -0.026070818032635418\n",
            "86     \t [0.23713683 1.18773581]. \t  -128.61162224204764 \t -0.026070818032635418\n",
            "87     \t [ 0.52141901 -1.73419679]. \t  -402.66255897447763 \t -0.026070818032635418\n",
            "88     \t [-1.66027152 -0.81384423]. \t  -1281.8139136822583 \t -0.026070818032635418\n",
            "89     \t [0.12518397 0.9027846 ]. \t  -79.46235216551702 \t -0.026070818032635418\n",
            "90     \t [ 0.27307719 -1.38925742]. \t  -214.80782634268718 \t -0.026070818032635418\n",
            "91     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.026070818032635418\n",
            "92     \t [-0.07028959 -1.58607329]. \t  -254.27804647879904 \t -0.026070818032635418\n",
            "93     \t [-1.5765528  -1.26771701]. \t  -1415.3164776664685 \t -0.026070818032635418\n",
            "94     \t [-0.93985372  1.36091761]. \t  -26.572501212386467 \t -0.026070818032635418\n",
            "95     \t [-0.6762613   1.75609806]. \t  -171.48987023474416 \t -0.026070818032635418\n",
            "96     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.026070818032635418\n",
            "97     \t [-1.16529491  1.41904107]. \t  -5.062175677754481 \t -0.026070818032635418\n",
            "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.026070818032635418\n",
            "99     \t [-1.45038161  1.64952399]. \t  -26.623491401680013 \t -0.026070818032635418\n",
            "100    \t [-1.31115942  1.94872672]. \t  -10.612508665163395 \t -0.026070818032635418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "c3f26154-d99a-4291-c9ab-8a73449718f3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_17 = d2GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.38985203 0.67340752]. \t  -158.47912633425426 \t -31.22188590191926\n",
            "2      \t [-5.29387775 -5.02762889]. \t  -109288.1768954753 \t -31.22188590191926\n",
            "3      \t [0.48520605 0.70400813]. \t  \u001b[92m-22.22203554670755\u001b[0m \t -22.22203554670755\n",
            "4      \t [-0.20838344 -4.13873903]. \t  -1750.5086688467193 \t -22.22203554670755\n",
            "5      \t [0.4329683  0.42971303]. \t  \u001b[92m-6.19010294534742\u001b[0m \t -6.19010294534742\n",
            "6      \t [0.29225687 0.02326178]. \t  \u001b[92m-0.8871911182600547\u001b[0m \t -0.8871911182600547\n",
            "7      \t [ 0.70483753 -0.42345124]. \t  -84.77260802886116 \t -0.8871911182600547\n",
            "8      \t [ 0.68301883 -1.76299303]. \t  -497.17095596989674 \t -0.8871911182600547\n",
            "9      \t [-0.66906229 -0.37411569]. \t  -70.31472418109783 \t -0.8871911182600547\n",
            "10     \t [-2.01216355 -1.52072272]. \t  -3111.033869252535 \t -0.8871911182600547\n",
            "11     \t [-0.14818959 -0.3550115 ]. \t  -15.52910245168684 \t -0.8871911182600547\n",
            "12     \t [ 1.93938365 -0.78303923]. \t  -2065.9015925754293 \t -0.8871911182600547\n",
            "13     \t [1.61708335 1.87446851]. \t  -55.21334351374121 \t -0.8871911182600547\n",
            "14     \t [0.55212146 0.22901582]. \t  \u001b[92m-0.775497196500416\u001b[0m \t -0.775497196500416\n",
            "15     \t [0.41858064 0.04854635]. \t  -1.9424102508801775 \t -0.775497196500416\n",
            "16     \t [-0.56000551  0.6035436 ]. \t  -10.83998864340854 \t -0.775497196500416\n",
            "17     \t [-0.17237737  0.18732588]. \t  -3.8586204313464103 \t -0.775497196500416\n",
            "18     \t [-0.4716724   0.16871679]. \t  -2.454812657624317 \t -0.775497196500416\n",
            "19     \t [-1.41099775  1.57008845]. \t  -23.522378127692644 \t -0.775497196500416\n",
            "20     \t [0.89603548 0.98240498]. \t  -3.2337451427249 \t -0.775497196500416\n",
            "21     \t [0.86012806 0.73355477]. \t  \u001b[92m-0.02348982399751418\u001b[0m \t -0.02348982399751418\n",
            "22     \t [2.03191803 1.35777373]. \t  -768.8630380181667 \t -0.02348982399751418\n",
            "23     \t [-1.07904441  1.57141038]. \t  -20.89331233314179 \t -0.02348982399751418\n",
            "24     \t [1.32580573 2.00071421]. \t  -6.0087842005780745 \t -0.02348982399751418\n",
            "25     \t [0.85774365 0.02614855]. \t  -50.36999313130531 \t -0.02348982399751418\n",
            "26     \t [1.4520594  2.00503465]. \t  -1.2743793660612428 \t -0.02348982399751418\n",
            "27     \t [1.27722413 1.80831407]. \t  -3.210199275762105 \t -0.02348982399751418\n",
            "28     \t [-1.213083    1.36238646]. \t  -6.08984908867052 \t -0.02348982399751418\n",
            "29     \t [0.26288311 0.93788352]. \t  -76.02051291904768 \t -0.02348982399751418\n",
            "30     \t [-1.31910352  1.21186882]. \t  -33.27409734432689 \t -0.02348982399751418\n",
            "31     \t [-0.61373384 -0.38423968]. \t  -60.50237428910913 \t -0.02348982399751418\n",
            "32     \t [-1.40875573  1.6289175 ]. \t  -18.452590617769218 \t -0.02348982399751418\n",
            "33     \t [-1.43068679  1.93400494]. \t  -7.181970491335563 \t -0.02348982399751418\n",
            "34     \t [1.74080353 1.54362102]. \t  -221.59905021424652 \t -0.02348982399751418\n",
            "35     \t [1.38712502 1.91055307]. \t  -0.16826063219756351 \t -0.02348982399751418\n",
            "36     \t [1.22350578 1.47875791]. \t  -0.08310972546276509 \t -0.02348982399751418\n",
            "37     \t [-1.5044638  -1.07565267]. \t  -1121.2071802953471 \t -0.02348982399751418\n",
            "38     \t [0.95571311 0.85753591]. \t  -0.31390186154294075 \t -0.02348982399751418\n",
            "39     \t [0.82190017 0.70090886]. \t  -0.09617957494150906 \t -0.02348982399751418\n",
            "40     \t [0.77914003 0.58874034]. \t  -0.08233711756889409 \t -0.02348982399751418\n",
            "41     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -0.02348982399751418\n",
            "42     \t [-1.44728885  0.73940159]. \t  -189.65769942715474 \t -0.02348982399751418\n",
            "43     \t [1.01095279 0.99570127]. \t  -0.06941669001508997 \t -0.02348982399751418\n",
            "44     \t [0.81131344 0.59368202]. \t  -0.45224034378805245 \t -0.02348982399751418\n",
            "45     \t [ 0.36166798 -0.62458116]. \t  -57.46810040818697 \t -0.02348982399751418\n",
            "46     \t [1.75456992 1.83954575]. \t  -154.07400851032006 \t -0.02348982399751418\n",
            "47     \t [ 0.15483644 -1.32420254]. \t  -182.47238793527598 \t -0.02348982399751418\n",
            "48     \t [ 1.47443898 -1.28631293]. \t  -1197.5811107866384 \t -0.02348982399751418\n",
            "49     \t [-0.3415817  1.4835984]. \t  -188.6469634120826 \t -0.02348982399751418\n",
            "50     \t [-1.02978198  1.05996761]. \t  -4.120038261661534 \t -0.02348982399751418\n",
            "51     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -0.02348982399751418\n",
            "52     \t [-1.81682755 -1.25327162]. \t  -2081.9481213711797 \t -0.02348982399751418\n",
            "53     \t [-0.16982153  1.3650376 ]. \t  -179.91105836582113 \t -0.02348982399751418\n",
            "54     \t [-0.5832036  -0.39844562]. \t  -57.055402148986836 \t -0.02348982399751418\n",
            "55     \t [-1.9677794  0.3980393]. \t  -1215.756244906381 \t -0.02348982399751418\n",
            "56     \t [1.08285742 1.13062686]. \t  -0.18287360058649244 \t -0.02348982399751418\n",
            "57     \t [-1.9069261   0.97514391]. \t  -716.6611417020337 \t -0.02348982399751418\n",
            "58     \t [-1.39981897  1.24181038]. \t  -57.265984360090215 \t -0.02348982399751418\n",
            "59     \t [1.28493309 1.59084518]. \t  -0.4436854658574313 \t -0.02348982399751418\n",
            "60     \t [-0.89741927  0.70989727]. \t  -4.511538640930511 \t -0.02348982399751418\n",
            "61     \t [0.73263739 0.51349456]. \t  -0.12559940352862 \t -0.02348982399751418\n",
            "62     \t [-1.8409285   0.12421796]. \t  -1073.9626300129419 \t -0.02348982399751418\n",
            "63     \t [0.95663178 1.56151027]. \t  -41.78076889114262 \t -0.02348982399751418\n",
            "64     \t [ 0.50275513 -1.72798608]. \t  -392.5838341975225 \t -0.02348982399751418\n",
            "65     \t [-1.34141841 -0.96070529]. \t  -767.3022143375099 \t -0.02348982399751418\n",
            "66     \t [1.13036921 1.22851349]. \t  -0.259267473751217 \t -0.02348982399751418\n",
            "67     \t [-1.90526878  1.91562327]. \t  -302.36618322749564 \t -0.02348982399751418\n",
            "68     \t [-1.8590975  -0.03748674]. \t  -1228.789541338643 \t -0.02348982399751418\n",
            "69     \t [1.50847337 0.24772284]. \t  -411.4432843590957 \t -0.02348982399751418\n",
            "70     \t [0.78560143 0.61215899]. \t  -0.04847736861870728 \t -0.02348982399751418\n",
            "71     \t [1.16794486 1.37444498]. \t  -0.0389172856546883 \t -0.02348982399751418\n",
            "72     \t [1.65108357 1.7624896 ]. \t  -93.27396602248619 \t -0.02348982399751418\n",
            "73     \t [-0.8309511   0.38541964]. \t  -12.65854777429919 \t -0.02348982399751418\n",
            "74     \t [-0.62039625 -0.28322156]. \t  -47.26318975428057 \t -0.02348982399751418\n",
            "75     \t [1.58093254 0.269114  ]. \t  -497.73172264831635 \t -0.02348982399751418\n",
            "76     \t [-1.88671546  1.04551085]. \t  -640.4454340967029 \t -0.02348982399751418\n",
            "77     \t [-1.10915018  0.20551825]. \t  -109.44867834043086 \t -0.02348982399751418\n",
            "78     \t [-1.01927465  1.69726372]. \t  -47.41900715611397 \t -0.02348982399751418\n",
            "79     \t [1.92040529 0.19413672]. \t  -1221.52480784025 \t -0.02348982399751418\n",
            "80     \t [1.00947053 0.26619503]. \t  -56.67625089578919 \t -0.02348982399751418\n",
            "81     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.02348982399751418\n",
            "82     \t [1.94725409 1.95571794]. \t  -338.01646995656824 \t -0.02348982399751418\n",
            "83     \t [ 0.75218183 -1.40652874]. \t  -389.0606101266476 \t -0.02348982399751418\n",
            "84     \t [-1.60862375  1.17313899]. \t  -206.89481918097397 \t -0.02348982399751418\n",
            "85     \t [1.942115   1.69760421]. \t  -431.12082590076807 \t -0.02348982399751418\n",
            "86     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.02348982399751418\n",
            "87     \t [-0.45545945  1.64796546]. \t  -209.62876938362476 \t -0.02348982399751418\n",
            "88     \t [-0.56896611 -0.73121945]. \t  -113.7518930266233 \t -0.02348982399751418\n",
            "89     \t [1.27427815 0.38180957]. \t  -154.3254793068761 \t -0.02348982399751418\n",
            "90     \t [0.46906583 0.20677824]. \t  -0.2994328043313553 \t -0.02348982399751418\n",
            "91     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.02348982399751418\n",
            "92     \t [-1.00700917 -1.26368828]. \t  -522.8452122917694 \t -0.02348982399751418\n",
            "93     \t [0.16015336 0.24262124]. \t  -5.413033502014389 \t -0.02348982399751418\n",
            "94     \t [-0.29640351  0.64054732]. \t  -32.22753711685563 \t -0.02348982399751418\n",
            "95     \t [ 0.76057997 -0.8169352 ]. \t  -194.77620593589174 \t -0.02348982399751418\n",
            "96     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.02348982399751418\n",
            "97     \t [-0.32671966 -0.20386439]. \t  -11.40804985655089 \t -0.02348982399751418\n",
            "98     \t [1.39696587 0.04097297]. \t  -365.1741489099554 \t -0.02348982399751418\n",
            "99     \t [1.676243   1.94336126]. \t  -75.52728652939464 \t -0.02348982399751418\n",
            "100    \t [0.53826131 1.57679642]. \t  -165.8684252955217 \t -0.02348982399751418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "941706ab-dd85-4c00-9ffa-01520ca94867"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_18 = d2GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-1.08079362  0.64247109]. \t  -31.959837779690332 \t -1.7663579664225912\n",
            "2      \t [-0.41546317  0.04330888]. \t  -3.6754048035136746 \t -1.7663579664225912\n",
            "3      \t [-1.97293368  1.55987045]. \t  -552.9391476990983 \t -1.7663579664225912\n",
            "4      \t [ 0.24908556 -0.62651991]. \t  -47.97584494549423 \t -1.7663579664225912\n",
            "5      \t [0.85796143 2.03679451]. \t  -169.2013635345611 \t -1.7663579664225912\n",
            "6      \t [1.97666027 1.64579726]. \t  -512.341691851289 \t -1.7663579664225912\n",
            "7      \t [-1.14361294  0.0682015 ]. \t  -158.2680579779561 \t -1.7663579664225912\n",
            "8      \t [1.22146106 2.00252107]. \t  -26.11557814556912 \t -1.7663579664225912\n",
            "9      \t [1.57867612 2.0443506 ]. \t  -20.393412020944396 \t -1.7663579664225912\n",
            "10     \t [-0.30628522  1.08965752]. \t  -100.87748297998274 \t -1.7663579664225912\n",
            "11     \t [1.41219034 1.79442438]. \t  -4.164189735537804 \t -1.7663579664225912\n",
            "12     \t [-0.28297738  0.09629176]. \t  \u001b[92m-1.672325417523482\u001b[0m \t -1.672325417523482\n",
            "13     \t [-0.34249005  0.05093614]. \t  -2.242688169963358 \t -1.672325417523482\n",
            "14     \t [1.27191506 1.86101208]. \t  -5.990709744348142 \t -1.672325417523482\n",
            "15     \t [-0.56039474  0.48032627]. \t  -5.19986866301503 \t -1.672325417523482\n",
            "16     \t [-0.23186364  0.04475835]. \t  \u001b[92m-1.5255923412022363\u001b[0m \t -1.5255923412022363\n",
            "17     \t [0.56823728 0.55824759]. \t  -5.725569024386187 \t -1.5255923412022363\n",
            "18     \t [0.00842168 1.21627265]. \t  -148.89789227870298 \t -1.5255923412022363\n",
            "19     \t [0.81909326 1.16483162]. \t  -24.428211483480897 \t -1.5255923412022363\n",
            "20     \t [-0.00131156 -0.06090081]. \t  \u001b[92m-1.373536673082386\u001b[0m \t -1.373536673082386\n",
            "21     \t [1.36465468 1.79040632]. \t  \u001b[92m-0.6495901307152054\u001b[0m \t -0.6495901307152054\n",
            "22     \t [ 1.45926384 -0.17412019]. \t  -530.8549191918929 \t -0.6495901307152054\n",
            "23     \t [ 1.78433219 -1.76828436]. \t  -2452.970093434316 \t -0.6495901307152054\n",
            "24     \t [1.42817515 1.96297074]. \t  -0.7718304984467063 \t -0.6495901307152054\n",
            "25     \t [-0.01906735 -0.21467012]. \t  -5.662446732853291 \t -0.6495901307152054\n",
            "26     \t [-1.02894882 -0.67536524]. \t  -304.8272344430442 \t -0.6495901307152054\n",
            "27     \t [ 1.87622405 -1.27367586]. \t  -2298.9083574649876 \t -0.6495901307152054\n",
            "28     \t [0.46668987 0.24522742]. \t  \u001b[92m-0.3596491578126802\u001b[0m \t -0.3596491578126802\n",
            "29     \t [0.35708244 0.14021974]. \t  -0.4295021353157243 \t -0.3596491578126802\n",
            "30     \t [-1.03958881  1.97004239]. \t  -83.24492519007167 \t -0.3596491578126802\n",
            "31     \t [1.32346668 1.7576601 ]. \t  \u001b[92m-0.10834685842696845\u001b[0m \t -0.10834685842696845\n",
            "32     \t [-0.51379973 -1.63690855]. \t  -363.6331805481531 \t -0.10834685842696845\n",
            "33     \t [-0.3086102 -1.5223783]. \t  -263.3814376804697 \t -0.10834685842696845\n",
            "34     \t [1.33718159 1.80549516]. \t  -0.14410867860741455 \t -0.10834685842696845\n",
            "35     \t [0.54120142 1.47148362]. \t  -139.1166704926515 \t -0.10834685842696845\n",
            "36     \t [-0.74895875  1.57222134]. \t  -105.32801185836587 \t -0.10834685842696845\n",
            "37     \t [-1.83696341 -1.25620347]. \t  -2152.32922596957 \t -0.10834685842696845\n",
            "38     \t [1.8550285  0.74680791]. \t  -726.6686225385389 \t -0.10834685842696845\n",
            "39     \t [-1.88536862  2.00180992]. \t  -249.44566312940998 \t -0.10834685842696845\n",
            "40     \t [-1.44206756 -1.21484429]. \t  -1091.2728928664756 \t -0.10834685842696845\n",
            "41     \t [1.068591   0.91240867]. \t  -5.270722615947526 \t -0.10834685842696845\n",
            "42     \t [0.92244644 0.96845054]. \t  -1.3876526296779692 \t -0.10834685842696845\n",
            "43     \t [0.51472498 0.82805609]. \t  -31.945262563754127 \t -0.10834685842696845\n",
            "44     \t [0.67987989 0.4117727 ]. \t  -0.35713806866751835 \t -0.10834685842696845\n",
            "45     \t [ 0.34533537 -0.98658915]. \t  -122.71805110077798 \t -0.10834685842696845\n",
            "46     \t [0.97731604 1.02479671]. \t  -0.4856276460123181 \t -0.10834685842696845\n",
            "47     \t [0.8041428  0.64008424]. \t  \u001b[92m-0.04266524311005875\u001b[0m \t -0.04266524311005875\n",
            "48     \t [-0.33344666  1.70336935]. \t  -255.28264766624903 \t -0.04266524311005875\n",
            "49     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.04266524311005875\n",
            "50     \t [0.97561873 0.96841336]. \t  \u001b[92m-0.02808890586959634\u001b[0m \t -0.02808890586959634\n",
            "51     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.02808890586959634\n",
            "52     \t [1.61404428 1.32600028]. \t  -163.99662342852554 \t -0.02808890586959634\n",
            "53     \t [-1.94029947 -0.86847316]. \t  -2155.3322056729285 \t -0.02808890586959634\n",
            "54     \t [0.07464172 1.83765711]. \t  -336.51009897136794 \t -0.02808890586959634\n",
            "55     \t [0.51311545 0.0998633 ]. \t  -2.9078022004534447 \t -0.02808890586959634\n",
            "56     \t [1.10942903 1.2371227 ]. \t  \u001b[92m-0.01593102077182717\u001b[0m \t -0.01593102077182717\n",
            "57     \t [0.99204615 0.98705732]. \t  \u001b[92m-0.000905281389271085\u001b[0m \t -0.000905281389271085\n",
            "58     \t [1.00713096 0.99768186]. \t  -0.02770957987774812 \t -0.000905281389271085\n",
            "59     \t [-0.6656559   1.80874007]. \t  -189.27229594052525 \t -0.000905281389271085\n",
            "60     \t [ 0.66227292 -0.70944906]. \t  -131.9169686429098 \t -0.000905281389271085\n",
            "61     \t [ 1.91918506 -0.9411309 ]. \t  -2139.354445757398 \t -0.000905281389271085\n",
            "62     \t [ 0.61530074 -0.04220243]. \t  -17.855040930280474 \t -0.000905281389271085\n",
            "63     \t [-1.26278418  1.72912074]. \t  -6.929132761748157 \t -0.000905281389271085\n",
            "64     \t [1.08652946 1.22040262]. \t  -0.16634022798350892 \t -0.000905281389271085\n",
            "65     \t [-1.3258731  -1.69927707]. \t  -1200.6443181943291 \t -0.000905281389271085\n",
            "66     \t [1.20136095 1.46171366]. \t  -0.07456999375202833 \t -0.000905281389271085\n",
            "67     \t [ 1.94377297 -1.45285214]. \t  -2737.3371698894134 \t -0.000905281389271085\n",
            "68     \t [1.25395039 1.49810155]. \t  -0.6163916927328872 \t -0.000905281389271085\n",
            "69     \t [-1.26602996  1.28388001]. \t  -15.30791956683049 \t -0.000905281389271085\n",
            "70     \t [-1.92633866 -2.01001904]. \t  -3281.3183646001717 \t -0.000905281389271085\n",
            "71     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.000905281389271085\n",
            "72     \t [-0.84373096  1.94458006]. \t  -155.35381258533093 \t -0.000905281389271085\n",
            "73     \t [-1.84375537 -1.86727236]. \t  -2781.9064022820107 \t -0.000905281389271085\n",
            "74     \t [-0.78666941 -1.10787842]. \t  -301.3508649798184 \t -0.000905281389271085\n",
            "75     \t [0.33148238 0.90152338]. \t  -63.11674969328898 \t -0.000905281389271085\n",
            "76     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.000905281389271085\n",
            "77     \t [ 0.11668496 -1.96729657]. \t  -393.1814597329686 \t -0.000905281389271085\n",
            "78     \t [-0.88247338  2.04742146]. \t  -164.49408042071573 \t -0.000905281389271085\n",
            "79     \t [-0.43549238 -2.01846151]. \t  -489.6378795741759 \t -0.000905281389271085\n",
            "80     \t [-0.01091106  0.37740414]. \t  -15.256345137726763 \t -0.000905281389271085\n",
            "81     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.000905281389271085\n",
            "82     \t [1.05681225 1.10630803]. \t  -0.014345462707764456 \t -0.000905281389271085\n",
            "83     \t [0.03857045 1.98165671]. \t  -393.0312840335908 \t -0.000905281389271085\n",
            "84     \t [1.25627707 1.55911194]. \t  -0.1022358848862952 \t -0.000905281389271085\n",
            "85     \t [1.12817242 1.258006  ]. \t  -0.03823466048577656 \t -0.000905281389271085\n",
            "86     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.000905281389271085\n",
            "87     \t [-0.11353625  0.93397012]. \t  -86.07873293752077 \t -0.000905281389271085\n",
            "88     \t [-0.92679023 -1.14150193]. \t  -403.8893634877983 \t -0.000905281389271085\n",
            "89     \t [1.04779559 1.08256261]. \t  -0.025733207977007115 \t -0.000905281389271085\n",
            "90     \t [-1.89079297 -1.36673699]. \t  -2450.53005067672 \t -0.000905281389271085\n",
            "91     \t [1.24161161 1.48911063]. \t  -0.3338831906168633 \t -0.000905281389271085\n",
            "92     \t [-1.14294969 -0.48032313]. \t  -323.8066030564311 \t -0.000905281389271085\n",
            "93     \t [-0.87056293 -0.71276528]. \t  -219.7787060538371 \t -0.000905281389271085\n",
            "94     \t [1.58263691 1.77828318]. \t  -53.11335976600227 \t -0.000905281389271085\n",
            "95     \t [-0.3000354 -0.4367719]. \t  -29.441193255529072 \t -0.000905281389271085\n",
            "96     \t [ 1.6314697  -1.10691842]. \t  -1420.642250098064 \t -0.000905281389271085\n",
            "97     \t [1.03110902 1.07090098]. \t  -0.0069201721416625875 \t -0.000905281389271085\n",
            "98     \t [-0.97394672  1.79359404]. \t  -75.30265313975646 \t -0.000905281389271085\n",
            "99     \t [ 2.00177263 -1.26549273]. \t  -2781.0202770904502 \t -0.000905281389271085\n",
            "100    \t [-0.20113591  1.12016858]. \t  -118.02072811771028 \t -0.000905281389271085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "de361c8a-2621-49e3-d1b0-e93d9048b7d2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_19 = d2GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.51626155 -0.56228938]. \t  -68.92749304221446 \t -4.219752052396591\n",
            "2      \t [-2.02960879  1.52561624]. \t  -681.9042235334397 \t -4.219752052396591\n",
            "3      \t [-0.2348982   0.43181231]. \t  -15.710376934861005 \t -4.219752052396591\n",
            "4      \t [1.4674717  1.42866499]. \t  -52.75322074972573 \t -4.219752052396591\n",
            "5      \t [-0.8101096   0.90290421]. \t  -9.358966735498882 \t -4.219752052396591\n",
            "6      \t [ 0.25517628 -0.09768572]. \t  \u001b[92m-3.205167585453764\u001b[0m \t -3.205167585453764\n",
            "7      \t [-0.11292085 -0.00022941]. \t  \u001b[92m-1.2554422302544317\u001b[0m \t -1.2554422302544317\n",
            "8      \t [0.95568556 0.67455006]. \t  -5.703783578614714 \t -1.2554422302544317\n",
            "9      \t [-0.4873257  -1.64043458]. \t  -354.87083187620107 \t -1.2554422302544317\n",
            "10     \t [0.74722887 0.14699418]. \t  -16.985335493250744 \t -1.2554422302544317\n",
            "11     \t [0.2655869  0.28459397]. \t  -5.121427107891311 \t -1.2554422302544317\n",
            "12     \t [ 0.16672824 -0.05280299]. \t  -1.3439987200828647 \t -1.2554422302544317\n",
            "13     \t [-0.0040308  -0.14780086]. \t  -3.1930675883076995 \t -1.2554422302544317\n",
            "14     \t [-1.02326689  0.13551371]. \t  -87.18803054460024 \t -1.2554422302544317\n",
            "15     \t [-0.91779786  0.09679596]. \t  -59.26346567837915 \t -1.2554422302544317\n",
            "16     \t [-0.22515032  0.07446674]. \t  -1.5575139454934113 \t -1.2554422302544317\n",
            "17     \t [-1.12928901  1.45469743]. \t  -7.752442263337384 \t -1.2554422302544317\n",
            "18     \t [1.75812626 0.86484883]. \t  -496.15319703486256 \t -1.2554422302544317\n",
            "19     \t [1.35868812 1.94183978]. \t  \u001b[92m-1.0465433314849117\u001b[0m \t -1.0465433314849117\n",
            "20     \t [0.95675012 1.15751997]. \t  -5.865492293871632 \t -1.0465433314849117\n",
            "21     \t [0.48422123 0.41768505]. \t  -3.6227960347836814 \t -1.0465433314849117\n",
            "22     \t [-1.41872827 -1.02009759]. \t  -925.6909054350544 \t -1.0465433314849117\n",
            "23     \t [1.45933798 1.02393232]. \t  -122.47598556447716 \t -1.0465433314849117\n",
            "24     \t [0.9769391  0.85198936]. \t  -1.0495308747064602 \t -1.0465433314849117\n",
            "25     \t [1.26940536 1.87734232]. \t  -7.14564450404931 \t -1.0465433314849117\n",
            "26     \t [ 0.45401777 -1.9141188 ]. \t  -449.84449635407304 \t -1.0465433314849117\n",
            "27     \t [0.4395653  0.24236322]. \t  \u001b[92m-0.5556157751403172\u001b[0m \t -0.5556157751403172\n",
            "28     \t [-1.63856669  0.69331682]. \t  -403.60271179563733 \t -0.5556157751403172\n",
            "29     \t [1.09688313 1.23777462]. \t  \u001b[92m-0.12925473585939035\u001b[0m \t -0.12925473585939035\n",
            "30     \t [-0.20962842  0.03663851]. \t  -1.468538038697957 \t -0.12925473585939035\n",
            "31     \t [-1.53636148  0.48205596]. \t  -359.2532426805832 \t -0.12925473585939035\n",
            "32     \t [-0.62571956  1.4256426 ]. \t  -109.58289262533435 \t -0.12925473585939035\n",
            "33     \t [1.04132517 1.14173883]. \t  -0.33096236508594307 \t -0.12925473585939035\n",
            "34     \t [-0.5868938   1.12710118]. \t  -63.77340517813315 \t -0.12925473585939035\n",
            "35     \t [2.0381793 0.72216  ]. \t  -1178.9504173057226 \t -0.12925473585939035\n",
            "36     \t [1.43484231 1.87875507]. \t  -3.4297135567163557 \t -0.12925473585939035\n",
            "37     \t [ 1.33652762 -1.41097198]. \t  -1022.3719497808307 \t -0.12925473585939035\n",
            "38     \t [-0.34728615  1.95003961]. \t  -336.4973021582914 \t -0.12925473585939035\n",
            "39     \t [1.17092552 1.57273923]. \t  -4.096401225250451 \t -0.12925473585939035\n",
            "40     \t [ 0.959215  -1.3861733]. \t  -531.888274532088 \t -0.12925473585939035\n",
            "41     \t [ 0.963913   -1.67233212]. \t  -676.7609160489471 \t -0.12925473585939035\n",
            "42     \t [1.38714819 2.01076668]. \t  -0.8996071631917575 \t -0.12925473585939035\n",
            "43     \t [-0.58202633 -0.52425901]. \t  -76.98206422017245 \t -0.12925473585939035\n",
            "44     \t [-0.93957858 -0.85981068]. \t  -307.43391881515123 \t -0.12925473585939035\n",
            "45     \t [ 0.42345259 -0.53876914]. \t  -51.896472941900235 \t -0.12925473585939035\n",
            "46     \t [ 1.79677526 -1.32817717]. \t  -2076.875614620295 \t -0.12925473585939035\n",
            "47     \t [0.73850107 0.59346067]. \t  -0.29952000738086426 \t -0.12925473585939035\n",
            "48     \t [-0.65978578 -1.76653712]. \t  -487.5711645016507 \t -0.12925473585939035\n",
            "49     \t [-0.24266514 -0.76140332]. \t  -68.8317344229052 \t -0.12925473585939035\n",
            "50     \t [ 1.02841419 -1.36419327]. \t  -586.5263844771054 \t -0.12925473585939035\n",
            "51     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.12925473585939035\n",
            "52     \t [-0.99195143 -0.35693566]. \t  -183.77003696304735 \t -0.12925473585939035\n",
            "53     \t [1.42875409 0.43741702]. \t  -257.4401579992734 \t -0.12925473585939035\n",
            "54     \t [1.50216865 0.12571883]. \t  -454.2795524743432 \t -0.12925473585939035\n",
            "55     \t [0.92795105 0.89015785]. \t  \u001b[92m-0.08966670291830654\u001b[0m \t -0.08966670291830654\n",
            "56     \t [-2.02404919  0.73184435]. \t  -1141.4207830370153 \t -0.08966670291830654\n",
            "57     \t [-0.02668429  1.56088139]. \t  -244.46691702906102 \t -0.08966670291830654\n",
            "58     \t [-1.47234921  2.01440936]. \t  -8.465754086896485 \t -0.08966670291830654\n",
            "59     \t [0.76101456 0.58943797]. \t  \u001b[92m-0.06771235104415185\u001b[0m \t -0.06771235104415185\n",
            "60     \t [1.4199054  2.03499426]. \t  -0.2119014890051892 \t -0.06771235104415185\n",
            "61     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.06771235104415185\n",
            "62     \t [-1.0803744   0.18974846]. \t  -99.8708387620765 \t -0.06771235104415185\n",
            "63     \t [-1.07883665  0.83854152]. \t  -14.906629231210427 \t -0.06771235104415185\n",
            "64     \t [-1.78051982  0.0386201 ]. \t  -988.4423965795083 \t -0.06771235104415185\n",
            "65     \t [-1.74403802 -1.30179646]. \t  -1894.098633935269 \t -0.06771235104415185\n",
            "66     \t [-0.02586991 -1.69586234]. \t  -288.8743529540324 \t -0.06771235104415185\n",
            "67     \t [ 1.84141782 -0.56448018]. \t  -1565.1476106907987 \t -0.06771235104415185\n",
            "68     \t [0.29070172 0.83551013]. \t  -56.90360093054751 \t -0.06771235104415185\n",
            "69     \t [ 0.77466228 -0.26976528]. \t  -75.71762526666582 \t -0.06771235104415185\n",
            "70     \t [0.7284284  0.49569926]. \t  -0.19561273230144782 \t -0.06771235104415185\n",
            "71     \t [0.65205829 0.43189422]. \t  -0.12557148555833916 \t -0.06771235104415185\n",
            "72     \t [0.47591668 1.90706049]. \t  -282.7041319858106 \t -0.06771235104415185\n",
            "73     \t [-0.41998961 -1.70756351]. \t  -356.9449335463213 \t -0.06771235104415185\n",
            "74     \t [0.56254087 0.32709241]. \t  -0.2026918237776151 \t -0.06771235104415185\n",
            "75     \t [ 1.54651295 -0.94071756]. \t  -1110.8008886824762 \t -0.06771235104415185\n",
            "76     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.06771235104415185\n",
            "77     \t [-1.2738401   0.34097693]. \t  -169.44370171654418 \t -0.06771235104415185\n",
            "78     \t [-0.25579785 -1.91213514]. \t  -392.65441879290506 \t -0.06771235104415185\n",
            "79     \t [-1.01193613  1.27793013]. \t  -10.4951895211278 \t -0.06771235104415185\n",
            "80     \t [-0.04833811  0.43081048]. \t  -19.4580015708361 \t -0.06771235104415185\n",
            "81     \t [0.14672588 0.87963436]. \t  -74.36264551014955 \t -0.06771235104415185\n",
            "82     \t [ 1.59712897 -0.86236332]. \t  -1165.3392525757588 \t -0.06771235104415185\n",
            "83     \t [-0.98486741  0.68231356]. \t  -12.213965271784032 \t -0.06771235104415185\n",
            "84     \t [-1.81862779  1.1758283 ]. \t  -462.3074627004391 \t -0.06771235104415185\n",
            "85     \t [-0.24366068  1.84452526]. \t  -320.2244345913622 \t -0.06771235104415185\n",
            "86     \t [-1.22222237  1.39634206]. \t  -5.88861363497911 \t -0.06771235104415185\n",
            "87     \t [-0.92145058 -1.64561721]. \t  -626.0389838876489 \t -0.06771235104415185\n",
            "88     \t [-0.14002292 -1.35041328]. \t  -188.99504911871696 \t -0.06771235104415185\n",
            "89     \t [-0.57263001  0.90454645]. \t  -35.72468675050905 \t -0.06771235104415185\n",
            "90     \t [-0.52357218  0.25964062]. \t  -2.342260135583863 \t -0.06771235104415185\n",
            "91     \t [0.01824425 1.73847894]. \t  -303.07902757254914 \t -0.06771235104415185\n",
            "92     \t [1.81914489 1.51624507]. \t  -322.17134307118727 \t -0.06771235104415185\n",
            "93     \t [ 0.88411369 -0.31217853]. \t  -119.66104849291484 \t -0.06771235104415185\n",
            "94     \t [0.88798542 0.19356752]. \t  -35.40916619948873 \t -0.06771235104415185\n",
            "95     \t [0.5619075  0.34878087]. \t  -0.30109469868374783 \t -0.06771235104415185\n",
            "96     \t [-0.87974617 -1.97633035]. \t  -759.9394729222546 \t -0.06771235104415185\n",
            "97     \t [1.22201364 0.65736029]. \t  -69.93170963058354 \t -0.06771235104415185\n",
            "98     \t [-1.51314065  0.48297648]. \t  -332.70279227581244 \t -0.06771235104415185\n",
            "99     \t [-2.02472409  1.55710344]. \t  -655.530863928991 \t -0.06771235104415185\n",
            "100    \t [-0.20069779  1.79982045]. \t  -311.04007298950785 \t -0.06771235104415185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "cc7c5504-4dc9-42cc-9faa-c8327725e503"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_20 = d2GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.04243452  0.03655484]. \t  \u001b[92m-1.2074547980027717\u001b[0m \t -1.2074547980027717\n",
            "2      \t [-0.17521923  1.07986149]. \t  -111.45475044450576 \t -1.2074547980027717\n",
            "3      \t [1.52145555 0.45818482]. \t  -344.9839327128462 \t -1.2074547980027717\n",
            "4      \t [ 0.64617018 -1.58680744]. \t  -401.86441762239986 \t -1.2074547980027717\n",
            "5      \t [0.44599456 0.72907409]. \t  -28.414196275862338 \t -1.2074547980027717\n",
            "6      \t [-0.23993779  0.25579499]. \t  -5.4667548036756735 \t -1.2074547980027717\n",
            "7      \t [-0.33309491  0.13382094]. \t  -1.8294398994854826 \t -1.2074547980027717\n",
            "8      \t [ 0.47022636 -0.27507135]. \t  -24.900533503791205 \t -1.2074547980027717\n",
            "9      \t [1.3771472 1.6997598]. \t  -4.0142649271370585 \t -1.2074547980027717\n",
            "10     \t [1.63284947 2.02301574]. \t  -41.76876094589841 \t -1.2074547980027717\n",
            "11     \t [1.01261893 1.61594319]. \t  -34.874628198103295 \t -1.2074547980027717\n",
            "12     \t [0.09161453 0.02906792]. \t  \u001b[92m-0.8679084958481715\u001b[0m \t -0.8679084958481715\n",
            "13     \t [ 0.02223601 -0.08365887]. \t  -1.6642003900212567 \t -0.8679084958481715\n",
            "14     \t [0.12188557 0.01894414]. \t  \u001b[92m-0.7727561604554114\u001b[0m \t -0.7727561604554114\n",
            "15     \t [-0.3983729  -1.64194288]. \t  -326.1872740695626 \t -0.7727561604554114\n",
            "16     \t [ 0.44378849 -1.53929824]. \t  -301.76455004759686 \t -0.7727561604554114\n",
            "17     \t [-0.39119219  0.81005883]. \t  -45.10392970029373 \t -0.7727561604554114\n",
            "18     \t [ 0.25799003 -0.24867382]. \t  -10.4877429368878 \t -0.7727561604554114\n",
            "19     \t [ 0.20646555 -1.89909653]. \t  -377.6591207061545 \t -0.7727561604554114\n",
            "20     \t [-0.82503668 -1.59352367]. \t  -520.5335091103622 \t -0.7727561604554114\n",
            "21     \t [-0.48316149  0.04533448]. \t  -5.738325805710129 \t -0.7727561604554114\n",
            "22     \t [-0.19580929  0.08150855]. \t  -1.616301184436927 \t -0.7727561604554114\n",
            "23     \t [ 1.66764852 -1.71888722]. \t  -2025.3906879916842 \t -0.7727561604554114\n",
            "24     \t [-1.69537024 -1.39278119]. \t  -1828.0463414122016 \t -0.7727561604554114\n",
            "25     \t [0.23677012 0.4709882 ]. \t  -17.799052926002027 \t -0.7727561604554114\n",
            "26     \t [1.46577427 1.59884175]. \t  -30.42872926576542 \t -0.7727561604554114\n",
            "27     \t [ 0.14364922 -0.89507096]. \t  -84.58509496095981 \t -0.7727561604554114\n",
            "28     \t [1.35669588 1.73299358]. \t  -1.2856565986941313 \t -0.7727561604554114\n",
            "29     \t [-0.67300137 -1.88677436]. \t  -550.2209802598624 \t -0.7727561604554114\n",
            "30     \t [-1.1342216  -0.37006358]. \t  -278.9614916305373 \t -0.7727561604554114\n",
            "31     \t [0.43348374 0.24789285]. \t  \u001b[92m-0.6807571136465769\u001b[0m \t -0.6807571136465769\n",
            "32     \t [1.20459609 1.79563618]. \t  -11.915702706226021 \t -0.6807571136465769\n",
            "33     \t [-0.47830041 -1.52576977]. \t  -310.02680065959424 \t -0.6807571136465769\n",
            "34     \t [-1.20550988 -0.05889056]. \t  -233.52241158111184 \t -0.6807571136465769\n",
            "35     \t [0.53335797 0.42740699]. \t  -2.26083224628383 \t -0.6807571136465769\n",
            "36     \t [-0.82508405  1.2406861 ]. \t  -34.68224222505055 \t -0.6807571136465769\n",
            "37     \t [-1.10682554  1.81019491]. \t  -38.67667557500352 \t -0.6807571136465769\n",
            "38     \t [-0.3040362   0.76544083]. \t  -46.9937903703847 \t -0.6807571136465769\n",
            "39     \t [-0.14460198  1.34721041]. \t  -177.21746209505548 \t -0.6807571136465769\n",
            "40     \t [ 1.45681723 -0.84409684]. \t  -880.1694540625086 \t -0.6807571136465769\n",
            "41     \t [ 0.53483739 -1.11396349]. \t  -196.22044177322152 \t -0.6807571136465769\n",
            "42     \t [-0.4068628  -1.28073464]. \t  -211.14952512344544 \t -0.6807571136465769\n",
            "43     \t [0.17529204 1.15916279]. \t  -128.0168083834562 \t -0.6807571136465769\n",
            "44     \t [0.60187974 0.34939177]. \t  \u001b[92m-0.1750568811306341\u001b[0m \t -0.1750568811306341\n",
            "45     \t [0.47964708 1.26569985]. \t  -107.52548316444184 \t -0.1750568811306341\n",
            "46     \t [-1.50941603 -1.10529415]. \t  -1151.1929674840562 \t -0.1750568811306341\n",
            "47     \t [0.50026608 0.2283606 ]. \t  -0.29771932837876525 \t -0.1750568811306341\n",
            "48     \t [1.51949793 0.87348176]. \t  -206.30495661161856 \t -0.1750568811306341\n",
            "49     \t [-1.86573362  0.37119973]. \t  -975.2745320400953 \t -0.1750568811306341\n",
            "50     \t [-1.21636438  1.16785896]. \t  -14.626921436764537 \t -0.1750568811306341\n",
            "51     \t [-1.03283697  0.86426186]. \t  -8.232659855306 \t -0.1750568811306341\n",
            "52     \t [-1.42473421  1.72963498]. \t  -14.893297052662595 \t -0.1750568811306341\n",
            "53     \t [ 1.71219045 -1.9877464 ]. \t  -2420.500308270193 \t -0.1750568811306341\n",
            "54     \t [0.46888773 1.61398374]. \t  -194.64137800239578 \t -0.1750568811306341\n",
            "55     \t [ 1.15212581 -1.08914448]. \t  -583.9889106648615 \t -0.1750568811306341\n",
            "56     \t [0.39304684 0.1292966 ]. \t  -0.43184182513176 \t -0.1750568811306341\n",
            "57     \t [-0.34825048  1.50945315]. \t  -194.52069416797514 \t -0.1750568811306341\n",
            "58     \t [-1.66676799  0.66994422]. \t  -451.5502786295779 \t -0.1750568811306341\n",
            "59     \t [-0.99523387  0.24252604]. \t  -59.926034914967154 \t -0.1750568811306341\n",
            "60     \t [-0.64931251 -0.54001115]. \t  -95.19112707327969 \t -0.1750568811306341\n",
            "61     \t [ 1.9065061  -0.75279122]. \t  -1925.8871723897323 \t -0.1750568811306341\n",
            "62     \t [0.58319821 0.28775523]. \t  -0.4479322263932979 \t -0.1750568811306341\n",
            "63     \t [ 0.57187407 -1.66012158]. \t  -395.0643876039098 \t -0.1750568811306341\n",
            "64     \t [0.44374421 1.12386373]. \t  -86.23394152062177 \t -0.1750568811306341\n",
            "65     \t [1.18674958 1.09267993]. \t  -10.00118618641607 \t -0.1750568811306341\n",
            "66     \t [1.1735376 1.4572848]. \t  -0.6716248154015088 \t -0.1750568811306341\n",
            "67     \t [0.80679763 0.65749411]. \t  \u001b[92m-0.04164587689877476\u001b[0m \t -0.04164587689877476\n",
            "68     \t [1.15112364 0.41629861]. \t  -82.61222519369333 \t -0.04164587689877476\n",
            "69     \t [0.87819343 0.84765241]. \t  -0.5989716097113141 \t -0.04164587689877476\n",
            "70     \t [0.00420307 0.2388898 ]. \t  -6.697601156350904 \t -0.04164587689877476\n",
            "71     \t [1.11036593 1.21933424]. \t  \u001b[92m-0.03061754712837163\u001b[0m \t -0.03061754712837163\n",
            "72     \t [2.02236401 0.80552465]. \t  -1079.7942702821092 \t -0.03061754712837163\n",
            "73     \t [-0.62886151 -1.17512401]. \t  -249.32873851716164 \t -0.03061754712837163\n",
            "74     \t [1.52137197 0.73026457]. \t  -251.27504800338738 \t -0.03061754712837163\n",
            "75     \t [0.38581887 1.73789702]. \t  -252.88229059851358 \t -0.03061754712837163\n",
            "76     \t [-1.96944417 -0.5409523 ]. \t  -1962.1593894476853 \t -0.03061754712837163\n",
            "77     \t [ 1.154773   -0.95401997]. \t  -523.2990278949036 \t -0.03061754712837163\n",
            "78     \t [-0.51051861  1.68444812]. \t  -205.0076833315466 \t -0.03061754712837163\n",
            "79     \t [-1.06095289 -0.39755859]. \t  -236.25514667039812 \t -0.03061754712837163\n",
            "80     \t [ 0.36031694 -0.00897675]. \t  -2.335878478975027 \t -0.03061754712837163\n",
            "81     \t [-1.75119222 -1.53266927]. \t  -2122.965094391892 \t -0.03061754712837163\n",
            "82     \t [-0.82949027  1.47402342]. \t  -65.12181013587157 \t -0.03061754712837163\n",
            "83     \t [1.30753342 1.71402681]. \t  -0.09649802203556279 \t -0.03061754712837163\n",
            "84     \t [1.78402586 0.47746595]. \t  -732.4699348470266 \t -0.03061754712837163\n",
            "85     \t [-1.05457877  1.81289119]. \t  -53.32702467490017 \t -0.03061754712837163\n",
            "86     \t [-0.85440648 -1.43697053]. \t  -473.019468218265 \t -0.03061754712837163\n",
            "87     \t [1.27325474 1.63730028]. \t  -0.10066213531364071 \t -0.03061754712837163\n",
            "88     \t [-1.42980548  1.22539506]. \t  -72.97164514750482 \t -0.03061754712837163\n",
            "89     \t [-1.88312686  1.82949245]. \t  -303.00949927111895 \t -0.03061754712837163\n",
            "90     \t [-1.4660533   1.66046065]. \t  -29.979010274014193 \t -0.03061754712837163\n",
            "91     \t [-0.0655655   0.64578957]. \t  -42.28646587945757 \t -0.03061754712837163\n",
            "92     \t [1.59988677 0.83088525]. \t  -299.2183568062418 \t -0.03061754712837163\n",
            "93     \t [-0.18625368  0.67477663]. \t  -42.37823194204345 \t -0.03061754712837163\n",
            "94     \t [1.19295637 0.88925704]. \t  -28.540856130884436 \t -0.03061754712837163\n",
            "95     \t [ 1.25493264 -0.61556837]. \t  -479.8608501814786 \t -0.03061754712837163\n",
            "96     \t [0.5585229  0.33265303]. \t  -0.23777258014034008 \t -0.03061754712837163\n",
            "97     \t [0.6027097  1.82522772]. \t  -213.89309756821692 \t -0.03061754712837163\n",
            "98     \t [-1.99286282 -1.79067893]. \t  -3329.230399486352 \t -0.03061754712837163\n",
            "99     \t [ 1.5240691  -0.14424668]. \t  -608.8999803784616 \t -0.03061754712837163\n",
            "100    \t [-0.71917525  1.86097463]. \t  -183.5250858961645 \t -0.03061754712837163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "20c9d54e-ec36-43a2-ddc0-26c97351e470"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1612266050.9629154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "d8d7e162-dc77-4382-af03-5fd01d4b8705"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.45795185 -0.30236194]. \t  \u001b[92m-28.3484050352548\u001b[0m \t -28.3484050352548\n",
            "2      \t [ 1.77122139 -1.9380348 ]. \t  -2576.421199974247 \t -28.3484050352548\n",
            "3      \t [-0.13693262 -0.7618248 ]. \t  -62.22240251785367 \t -28.3484050352548\n",
            "4      \t [1.88493603 1.44424789]. \t  -445.45983372037045 \t -28.3484050352548\n",
            "5      \t [-0.50551879  1.86210364]. \t  -260.36828725301234 \t -28.3484050352548\n",
            "6      \t [ 1.77169217 -0.26658693]. \t  -1160.3249553860608 \t -28.3484050352548\n",
            "7      \t [ 1.78803192 -0.60648493]. \t  -1447.3149830671293 \t -28.3484050352548\n",
            "8      \t [-0.54544849 -0.38504307]. \t  -48.97683286981722 \t -28.3484050352548\n",
            "9      \t [-0.26214603  0.09980271]. \t  \u001b[92m-1.6896226896377502\u001b[0m \t -1.6896226896377502\n",
            "10     \t [-0.08863664  0.34933897]. \t  -12.846160356286827 \t -1.6896226896377502\n",
            "11     \t [ 0.36142632 -1.18833087]. \t  -174.3732864880468 \t -1.6896226896377502\n",
            "12     \t [ 0.05654304 -1.76204174]. \t  -312.49693174379473 \t -1.6896226896377502\n",
            "13     \t [-0.94458782 -1.75105902]. \t  -702.4876461284873 \t -1.6896226896377502\n",
            "14     \t [0.9684561 1.7270646]. \t  -62.27793206271763 \t -1.6896226896377502\n",
            "15     \t [-0.13798151  1.2120092 ]. \t  -143.61281532895606 \t -1.6896226896377502\n",
            "16     \t [-0.82321339  1.2832978 ]. \t  -40.00136365302549 \t -1.6896226896377502\n",
            "17     \t [-1.87871605  2.00157264]. \t  -241.76582640057916 \t -1.6896226896377502\n",
            "18     \t [ 0.74775298 -0.63960254]. \t  -143.7606812354837 \t -1.6896226896377502\n",
            "19     \t [ 0.27733762 -0.15449891]. \t  -5.877534230816795 \t -1.6896226896377502\n",
            "20     \t [1.58184171 1.11098514]. \t  -193.89287342355146 \t -1.6896226896377502\n",
            "21     \t [0.98684155 1.01835701]. \t  \u001b[92m-0.1982049182695292\u001b[0m \t -0.1982049182695292\n",
            "22     \t [ 0.02452927 -0.15219243]. \t  -3.2861471837298675 \t -0.1982049182695292\n",
            "23     \t [1.06089113 1.82174536]. \t  -48.480860367576014 \t -0.1982049182695292\n",
            "24     \t [0.7482042  0.48643691]. \t  -0.6017551125263971 \t -0.1982049182695292\n",
            "25     \t [-1.49960569 -1.47265252]. \t  -1391.181726837824 \t -0.1982049182695292\n",
            "26     \t [0.77364958 0.56590163]. \t  \u001b[92m-0.15771949233931992\u001b[0m \t -0.15771949233931992\n",
            "27     \t [ 1.160157   -0.46970718]. \t  -329.69192664755565 \t -0.15771949233931992\n",
            "28     \t [0.90467553 0.75692879]. \t  -0.3874227225876973 \t -0.15771949233931992\n",
            "29     \t [0.4996943  0.24604992]. \t  -0.251634011491688 \t -0.15771949233931992\n",
            "30     \t [0.94538626 0.88448055]. \t  \u001b[92m-0.011584527057748276\u001b[0m \t -0.011584527057748276\n",
            "31     \t [0.94980596 0.89421683]. \t  \u001b[92m-0.008783433047088498\u001b[0m \t -0.008783433047088498\n",
            "32     \t [-0.06879558 -0.02313788]. \t  -1.2200016473609514 \t -0.008783433047088498\n",
            "33     \t [0.58501611 0.26185152]. \t  -0.8185042352196917 \t -0.008783433047088498\n",
            "34     \t [-0.85840066  1.22147162]. \t  -26.939300528904813 \t -0.008783433047088498\n",
            "35     \t [ 0.81411133 -0.95859836]. \t  -262.92044163522456 \t -0.008783433047088498\n",
            "36     \t [-0.52288829  0.31344504]. \t  -2.4794518201901314 \t -0.008783433047088498\n",
            "37     \t [-1.22067096  1.44149904]. \t  -5.166978555590179 \t -0.008783433047088498\n",
            "38     \t [-0.26683513 -0.66227277]. \t  -55.40324609566223 \t -0.008783433047088498\n",
            "39     \t [-1.6856219   0.18648594]. \t  -712.0275913623143 \t -0.008783433047088498\n",
            "40     \t [-1.53740048  1.2231408 ]. \t  -136.50317180697115 \t -0.008783433047088498\n",
            "41     \t [1.9426309 1.9633803]. \t  -328.65586656807943 \t -0.008783433047088498\n",
            "42     \t [0.08183052 1.23046565]. \t  -150.60419279831945 \t -0.008783433047088498\n",
            "43     \t [0.91512751 0.79373124]. \t  -0.19840948810967465 \t -0.008783433047088498\n",
            "44     \t [-0.68352765  0.39545188]. \t  -3.3491886795146724 \t -0.008783433047088498\n",
            "45     \t [0.74327672 1.96714473]. \t  -200.19911334585981 \t -0.008783433047088498\n",
            "46     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.008783433047088498\n",
            "47     \t [-1.21082569  1.68390588]. \t  -9.631740102570523 \t -0.008783433047088498\n",
            "48     \t [-1.33437787  0.68815167]. \t  -124.78585917988336 \t -0.008783433047088498\n",
            "49     \t [-0.23094754 -1.61276938]. \t  -279.10620252253784 \t -0.008783433047088498\n",
            "50     \t [0.66014216 1.64749383]. \t  -146.93868689891644 \t -0.008783433047088498\n",
            "51     \t [-0.43226986  1.33958364]. \t  -134.92921491596502 \t -0.008783433047088498\n",
            "52     \t [1.22158747 1.08415614]. \t  -16.70527922550893 \t -0.008783433047088498\n",
            "53     \t [ 0.16555488 -0.53612718]. \t  -32.4535352858786 \t -0.008783433047088498\n",
            "54     \t [ 0.42173736 -1.26611057]. \t  -208.84018173094518 \t -0.008783433047088498\n",
            "55     \t [-0.5189422  -1.68782909]. \t  -385.34301020541506 \t -0.008783433047088498\n",
            "56     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.008783433047088498\n",
            "57     \t [ 0.54384183 -0.89280069]. \t  -141.4766651669655 \t -0.008783433047088498\n",
            "58     \t [0.8051323  0.49947153]. \t  -2.2511201711580875 \t -0.008783433047088498\n",
            "59     \t [0.40937437 1.18707475]. \t  -104.28428965658428 \t -0.008783433047088498\n",
            "60     \t [1.02250523 1.72285278]. \t  -45.878888676329034 \t -0.008783433047088498\n",
            "61     \t [-0.54492688 -0.24007982]. \t  -31.226397664342578 \t -0.008783433047088498\n",
            "62     \t [0.98075065 1.02514641]. \t  -0.4007377904572951 \t -0.008783433047088498\n",
            "63     \t [-1.12854391  1.29562523]. \t  -4.57916022324821 \t -0.008783433047088498\n",
            "64     \t [-1.00705448  1.16548517]. \t  -6.318237027567069 \t -0.008783433047088498\n",
            "65     \t [-0.21515472  1.99157176]. \t  -379.8881065908075 \t -0.008783433047088498\n",
            "66     \t [0.97493963 0.91709419]. \t  -0.11227143431038482 \t -0.008783433047088498\n",
            "67     \t [ 1.46348283 -1.48988797]. \t  -1319.1174901038878 \t -0.008783433047088498\n",
            "68     \t [-1.95064235 -0.88388403]. \t  -2207.274882433743 \t -0.008783433047088498\n",
            "69     \t [0.16198391 0.10840563]. \t  -1.3774100518971923 \t -0.008783433047088498\n",
            "70     \t [-1.55333206 -1.38547729]. \t  -1449.24130480916 \t -0.008783433047088498\n",
            "71     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.008783433047088498\n",
            "72     \t [-0.28009762  1.23351318]. \t  -135.0546635396757 \t -0.008783433047088498\n",
            "73     \t [0.5994469  0.37866452]. \t  -0.19779968761968295 \t -0.008783433047088498\n",
            "74     \t [-0.27488168 -1.9321867 ]. \t  -404.7299793637241 \t -0.008783433047088498\n",
            "75     \t [-0.24921281  0.39865629]. \t  -12.887073322359871 \t -0.008783433047088498\n",
            "76     \t [-0.52180996 -0.0714736 ]. \t  -14.132946809213406 \t -0.008783433047088498\n",
            "77     \t [ 0.215878   -0.57694162]. \t  -39.495675767076335 \t -0.008783433047088498\n",
            "78     \t [1.02999374 2.00653251]. \t  -89.42542473466041 \t -0.008783433047088498\n",
            "79     \t [-0.4332399 -0.1926707]. \t  -16.522120495331716 \t -0.008783433047088498\n",
            "80     \t [1.0573334  1.06879388]. \t  -0.2449581118713727 \t -0.008783433047088498\n",
            "81     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.008783433047088498\n",
            "82     \t [1.06649139 1.1029999 ]. \t  -0.12278460417748603 \t -0.008783433047088498\n",
            "83     \t [0.80777678 0.66365278]. \t  -0.049380795879166925 \t -0.008783433047088498\n",
            "84     \t [1.18993284 1.65277456]. \t  -5.6451275735506075 \t -0.008783433047088498\n",
            "85     \t [-1.82086888  0.66929264]. \t  -708.232246310807 \t -0.008783433047088498\n",
            "86     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.008783433047088498\n",
            "87     \t [-0.39927098  1.30131624]. \t  -132.3512741366938 \t -0.008783433047088498\n",
            "88     \t [-1.08176543  0.14264375]. \t  -109.92430978318076 \t -0.008783433047088498\n",
            "89     \t [0.88647147 0.72135748]. \t  -0.42858072349503895 \t -0.008783433047088498\n",
            "90     \t [ 1.21960029 -0.11987661]. \t  -258.3900302193461 \t -0.008783433047088498\n",
            "91     \t [0.26039215 0.32972883]. \t  -7.407477600251035 \t -0.008783433047088498\n",
            "92     \t [ 1.46806628 -0.6833924 ]. \t  -805.9903261107626 \t -0.008783433047088498\n",
            "93     \t [1.10093847 1.23767273]. \t  -0.07576147281820803 \t -0.008783433047088498\n",
            "94     \t [-1.17118342  0.76949043]. \t  -40.976133562082566 \t -0.008783433047088498\n",
            "95     \t [ 1.65361628 -1.69356049]. \t  -1961.1520765663745 \t -0.008783433047088498\n",
            "96     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.008783433047088498\n",
            "97     \t [1.05498416 1.12829335]. \t  -0.026437674680498402 \t -0.008783433047088498\n",
            "98     \t [ 1.72114382 -0.85529876]. \t  -1457.9536047382583 \t -0.008783433047088498\n",
            "99     \t [0.34268958 0.38226986]. \t  -7.445746616971624 \t -0.008783433047088498\n",
            "100    \t [-1.0954881   0.33887054]. \t  -78.56168510444935 \t -0.008783433047088498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "1ee719f7-1c18-449f-a664-4da98df7855c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.32202196 -0.27765003]. \t  -15.002297070009188 \t -1.3013277264983028\n",
            "2      \t [ 0.58837969 -0.00300811]. \t  -12.363409394431228 \t -1.3013277264983028\n",
            "3      \t [0.95051201 1.84796455]. \t  -89.20886136012885 \t -1.3013277264983028\n",
            "4      \t [ 0.25260485 -0.8513971 ]. \t  -84.318858901537 \t -1.3013277264983028\n",
            "5      \t [ 0.48855765 -0.24792703]. \t  -23.941048904425948 \t -1.3013277264983028\n",
            "6      \t [-0.72815629 -0.88892487]. \t  -204.38135379909082 \t -1.3013277264983028\n",
            "7      \t [-0.28484403 -0.10151184]. \t  -4.986852000800813 \t -1.3013277264983028\n",
            "8      \t [-0.05333159 -0.64126195]. \t  -42.596788177665594 \t -1.3013277264983028\n",
            "9      \t [-0.21150412  0.53697288]. \t  -25.697654751649207 \t -1.3013277264983028\n",
            "10     \t [-0.13241827 -0.06648672]. \t  -1.9883292841916196 \t -1.3013277264983028\n",
            "11     \t [-0.39971556  1.91803232]. \t  -311.1069541386683 \t -1.3013277264983028\n",
            "12     \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -1.3013277264983028\n",
            "13     \t [0.38927775 0.37355461]. \t  -5.3021562821946615 \t -1.3013277264983028\n",
            "14     \t [ 0.30627648 -0.14910958]. \t  -6.382015315679451 \t -1.3013277264983028\n",
            "15     \t [0.16667004 0.053798  ]. \t  \u001b[92m-0.7621381764908968\u001b[0m \t -0.7621381764908968\n",
            "16     \t [0.03802576 0.01623518]. \t  -0.9472665483479287 \t -0.7621381764908968\n",
            "17     \t [-0.61156446  0.86155167]. \t  -26.366721503073624 \t -0.7621381764908968\n",
            "18     \t [-1.42323969  0.13891168]. \t  -361.83560479418685 \t -0.7621381764908968\n",
            "19     \t [2.00013178 1.35546904]. \t  -700.6334995177767 \t -0.7621381764908968\n",
            "20     \t [-0.3413371  -0.26077155]. \t  -16.033398685166116 \t -0.7621381764908968\n",
            "21     \t [ 1.78173395 -1.38890908]. \t  -2083.1506130796793 \t -0.7621381764908968\n",
            "22     \t [0.81906356 1.08153833]. \t  -16.89798661381232 \t -0.7621381764908968\n",
            "23     \t [1.25096243 0.99435341]. \t  -32.616121404735196 \t -0.7621381764908968\n",
            "24     \t [0.93081782 0.10040177]. \t  -58.68345579354755 \t -0.7621381764908968\n",
            "25     \t [1.1025212  1.23366077]. \t  \u001b[92m-0.043299717652430215\u001b[0m \t -0.043299717652430215\n",
            "26     \t [-0.32829337 -1.80462139]. \t  -367.49094444375294 \t -0.043299717652430215\n",
            "27     \t [0.94927903 0.86880936]. \t  -0.10703940702953608 \t -0.043299717652430215\n",
            "28     \t [1.02089201 1.03090536]. \t  \u001b[92m-0.013239697604951903\u001b[0m \t -0.013239697604951903\n",
            "29     \t [2.03504526 0.92232824]. \t  -1037.3195379050223 \t -0.013239697604951903\n",
            "30     \t [-0.46905516 -0.16980238]. \t  -17.353706206974486 \t -0.013239697604951903\n",
            "31     \t [0.9960668  1.00835707]. \t  -0.02628542354851871 \t -0.013239697604951903\n",
            "32     \t [1.04632362 1.10818383]. \t  -0.020077011697160443 \t -0.013239697604951903\n",
            "33     \t [-0.24541195 -1.78732495]. \t  -342.8958826553927 \t -0.013239697604951903\n",
            "34     \t [-0.08853253 -0.47931179]. \t  -24.91639580189656 \t -0.013239697604951903\n",
            "35     \t [-0.68739673 -0.15970921]. \t  -42.81796004380665 \t -0.013239697604951903\n",
            "36     \t [ 1.74293882 -0.80536341]. \t  -1477.5699245440094 \t -0.013239697604951903\n",
            "37     \t [1.19934537 0.68577621]. \t  -56.6884084518539 \t -0.013239697604951903\n",
            "38     \t [0.94574699 0.87768505]. \t  -0.031007412811015662 \t -0.013239697604951903\n",
            "39     \t [0.85410208 0.71472118]. \t  -0.043099087689889945 \t -0.013239697604951903\n",
            "40     \t [-0.80967392 -0.64047878]. \t  -171.24964427357978 \t -0.013239697604951903\n",
            "41     \t [1.03165866 0.66417548]. \t  -16.01253240286429 \t -0.013239697604951903\n",
            "42     \t [-0.34286574  1.94133637]. \t  -334.4204397773866 \t -0.013239697604951903\n",
            "43     \t [-0.75738889  0.35083364]. \t  -8.05259127383539 \t -0.013239697604951903\n",
            "44     \t [-1.55338422  1.07956766]. \t  -184.32462558936453 \t -0.013239697604951903\n",
            "45     \t [-1.4043808   1.54930755]. \t  -23.67207670394118 \t -0.013239697604951903\n",
            "46     \t [-1.75491246  0.43709123]. \t  -705.9370292132911 \t -0.013239697604951903\n",
            "47     \t [1.4153351  2.01332853]. \t  -0.18281581734714947 \t -0.013239697604951903\n",
            "48     \t [1.33541614 1.93273641]. \t  -2.3445442225035347 \t -0.013239697604951903\n",
            "49     \t [-1.13730456  1.12167406]. \t  -7.519168498705018 \t -0.013239697604951903\n",
            "50     \t [-1.63190752  2.03189777]. \t  -46.77135926762252 \t -0.013239697604951903\n",
            "51     \t [1.05914411 1.12345495]. \t  \u001b[92m-0.0037764840180860245\u001b[0m \t -0.0037764840180860245\n",
            "52     \t [-1.51848669  1.61216493]. \t  -54.45599160830821 \t -0.0037764840180860245\n",
            "53     \t [2.00029358 1.68210447]. \t  -538.8091297035712 \t -0.0037764840180860245\n",
            "54     \t [-0.57362638  0.37193407]. \t  -2.6602281661743064 \t -0.0037764840180860245\n",
            "55     \t [-0.84109744  0.58123478]. \t  -4.982539243900186 \t -0.0037764840180860245\n",
            "56     \t [1.31403595 0.46630098]. \t  -158.95678900240353 \t -0.0037764840180860245\n",
            "57     \t [1.96646943 0.26738163]. \t  -1296.6607706834602 \t -0.0037764840180860245\n",
            "58     \t [-1.93116689 -0.85024414]. \t  -2105.910875856383 \t -0.0037764840180860245\n",
            "59     \t [-1.32427512  0.34521736]. \t  -203.78588019736992 \t -0.0037764840180860245\n",
            "60     \t [0.28483277 1.61744199]. \t  -236.53700601606423 \t -0.0037764840180860245\n",
            "61     \t [1.57259458 0.00595459]. \t  -608.9856696190132 \t -0.0037764840180860245\n",
            "62     \t [1.85709005 1.58285552]. \t  -348.9033138942158 \t -0.0037764840180860245\n",
            "63     \t [1.3197972  1.72661041]. \t  -0.12553946874587352 \t -0.0037764840180860245\n",
            "64     \t [0.88530007 0.79860164]. \t  -0.0351947349558261 \t -0.0037764840180860245\n",
            "65     \t [-0.11136909  1.32217248]. \t  -172.78473047588355 \t -0.0037764840180860245\n",
            "66     \t [ 0.08406173 -2.02016692]. \t  -411.80642514590784 \t -0.0037764840180860245\n",
            "67     \t [ 0.60840368 -1.00349936]. \t  -188.84598804941893 \t -0.0037764840180860245\n",
            "68     \t [-0.04636727 -0.07615456]. \t  -1.7080436883233414 \t -0.0037764840180860245\n",
            "69     \t [-1.14585149 -1.04130742]. \t  -558.8695561563712 \t -0.0037764840180860245\n",
            "70     \t [ 0.27362341 -0.21059029]. \t  -8.676367778288038 \t -0.0037764840180860245\n",
            "71     \t [0.74365306 0.54086756]. \t  -0.08048163691127136 \t -0.0037764840180860245\n",
            "72     \t [ 1.57804743 -0.83791126]. \t  -1107.9890290920496 \t -0.0037764840180860245\n",
            "73     \t [0.95314975 0.73193607]. \t  -3.1194807181052044 \t -0.0037764840180860245\n",
            "74     \t [-1.3715832   1.97165084]. \t  -6.441810433383501 \t -0.0037764840180860245\n",
            "75     \t [0.84361066 0.69447202]. \t  -0.05406546866753627 \t -0.0037764840180860245\n",
            "76     \t [1.76927873 1.12586904]. \t  -402.38507314741815 \t -0.0037764840180860245\n",
            "77     \t [0.6980859  0.51290021]. \t  -0.15656680592815847 \t -0.0037764840180860245\n",
            "78     \t [ 0.59733697 -0.66741213]. \t  -105.06553138860436 \t -0.0037764840180860245\n",
            "79     \t [-1.03351079  1.23317937]. \t  -6.858815070908965 \t -0.0037764840180860245\n",
            "80     \t [0.64054837 0.43043475]. \t  -0.16973737976498024 \t -0.0037764840180860245\n",
            "81     \t [0.54678719 1.70335149]. \t  -197.43238918791414 \t -0.0037764840180860245\n",
            "82     \t [-1.06598285  1.8813477 ]. \t  -59.77499610171581 \t -0.0037764840180860245\n",
            "83     \t [2.010771   1.52827972]. \t  -633.504066675221 \t -0.0037764840180860245\n",
            "84     \t [ 1.63095484 -0.77756562]. \t  -1182.0932557353174 \t -0.0037764840180860245\n",
            "85     \t [-0.08660451 -0.70069306]. \t  -51.33449912311258 \t -0.0037764840180860245\n",
            "86     \t [0.64459737 0.41077377]. \t  -0.12855020945016554 \t -0.0037764840180860245\n",
            "87     \t [0.68762104 0.41937062]. \t  -0.38329296543179847 \t -0.0037764840180860245\n",
            "88     \t [-0.34497038 -1.43593559]. \t  -243.5928354559929 \t -0.0037764840180860245\n",
            "89     \t [0.74091969 0.55119627]. \t  -0.06762180313179203 \t -0.0037764840180860245\n",
            "90     \t [1.67435419 1.56277203]. \t  -154.38590161338132 \t -0.0037764840180860245\n",
            "91     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.0037764840180860245\n",
            "92     \t [0.62877442 0.36371166]. \t  -0.23795293688178648 \t -0.0037764840180860245\n",
            "93     \t [-0.35843956  0.40250036]. \t  -9.35413291051952 \t -0.0037764840180860245\n",
            "94     \t [-1.60692968  1.75495421]. \t  -75.2334457526766 \t -0.0037764840180860245\n",
            "95     \t [ 1.29687332 -1.13775409]. \t  -795.1219996279622 \t -0.0037764840180860245\n",
            "96     \t [1.17085781 0.86407361]. \t  -25.717303713322334 \t -0.0037764840180860245\n",
            "97     \t [-0.54378274  1.59506367]. \t  -171.2179454991696 \t -0.0037764840180860245\n",
            "98     \t [-1.20891657  0.77681768]. \t  -51.75546165606691 \t -0.0037764840180860245\n",
            "99     \t [ 1.04183685 -0.83097723]. \t  -367.26112762398185 \t -0.0037764840180860245\n",
            "100    \t [-0.67481668  0.98829967]. \t  -31.20560964966402 \t -0.0037764840180860245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "afed56c2-cddd-4d06-a3d1-12901ca8d252"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.55708359 -1.00893837]. \t  -176.4746104200411 \t -1.118465165857483\n",
            "2      \t [ 0.05363572 -0.17185532]. \t  -3.9487362871437206 \t -1.118465165857483\n",
            "3      \t [-0.51674409  1.56487519]. \t  -170.7421666487869 \t -1.118465165857483\n",
            "4      \t [-0.12273722  0.34885148]. \t  -12.40191874801925 \t -1.118465165857483\n",
            "5      \t [-0.69177303  0.26381369]. \t  -7.473260886051463 \t -1.118465165857483\n",
            "6      \t [1.78622843 1.94317708]. \t  -156.22753952620175 \t -1.118465165857483\n",
            "7      \t [-0.42725094  0.25300168]. \t  -2.533482665488056 \t -1.118465165857483\n",
            "8      \t [-1.66456866  0.53875491]. \t  -505.29746991579844 \t -1.118465165857483\n",
            "9      \t [-0.65807027 -0.0382027 ]. \t  -24.95771835833803 \t -1.118465165857483\n",
            "10     \t [0.18075198 0.11651342]. \t  -1.3741178489357697 \t -1.118465165857483\n",
            "11     \t [-0.04771009  1.83272856]. \t  -336.15326133551326 \t -1.118465165857483\n",
            "12     \t [-0.50264089 -1.77309514]. \t  -412.6214003285959 \t -1.118465165857483\n",
            "13     \t [0.08977973 0.11215719]. \t  -1.9121150452959301 \t -1.118465165857483\n",
            "14     \t [1.15123811 0.62558476]. \t  -48.98989773457063 \t -1.118465165857483\n",
            "15     \t [0.17293491 1.45901441]. \t  -204.91898369264595 \t -1.118465165857483\n",
            "16     \t [0.51432943 0.47149936]. \t  -4.5193106024589484 \t -1.118465165857483\n",
            "17     \t [0.8575366  1.97026682]. \t  -152.51755456713286 \t -1.118465165857483\n",
            "18     \t [-0.81755149 -1.84721376]. \t  -636.1299433662065 \t -1.118465165857483\n",
            "19     \t [-0.1388746  -1.42389238]. \t  -209.57346413413353 \t -1.118465165857483\n",
            "20     \t [-0.88073865  0.81059516]. \t  -3.6589411256290556 \t -1.118465165857483\n",
            "21     \t [-1.93971307  0.59809255]. \t  -1009.9810140546972 \t -1.118465165857483\n",
            "22     \t [0.7502442  0.55465127]. \t  \u001b[92m-0.06912672416332427\u001b[0m \t -0.06912672416332427\n",
            "23     \t [-0.84638391  0.30055093]. \t  -20.699327559089415 \t -0.06912672416332427\n",
            "24     \t [-1.23772274  1.88910827]. \t  -17.763064208883094 \t -0.06912672416332427\n",
            "25     \t [-1.10498823  1.21305413]. \t  -4.437287522857704 \t -0.06912672416332427\n",
            "26     \t [0.88597414 0.88376159]. \t  -0.9893716758140745 \t -0.06912672416332427\n",
            "27     \t [0.34338132 0.05568098]. \t  -0.8184022919127651 \t -0.06912672416332427\n",
            "28     \t [0.75826192 0.65280305]. \t  -0.6643735494397385 \t -0.06912672416332427\n",
            "29     \t [1.83979201 0.74160758]. \t  -699.3701751977119 \t -0.06912672416332427\n",
            "30     \t [-1.88548814  0.78108077]. \t  -777.8251909301468 \t -0.06912672416332427\n",
            "31     \t [1.36835161 1.87515751]. \t  -0.13645096684008431 \t -0.06912672416332427\n",
            "32     \t [-0.98261309  0.04318056]. \t  -89.00332324256519 \t -0.06912672416332427\n",
            "33     \t [1.28392702 1.87493244]. \t  -5.209202096967919 \t -0.06912672416332427\n",
            "34     \t [-1.28509462  0.25780746]. \t  -199.45068044631728 \t -0.06912672416332427\n",
            "35     \t [0.90666372 0.75533849]. \t  -0.45360882874570363 \t -0.06912672416332427\n",
            "36     \t [1.09375911 1.15291888]. \t  -0.19706082631901312 \t -0.06912672416332427\n",
            "37     \t [-0.42003048 -1.28318347]. \t  -215.0623511402162 \t -0.06912672416332427\n",
            "38     \t [1.22598613 1.54916559]. \t  -0.26380840921578896 \t -0.06912672416332427\n",
            "39     \t [0.79596617 0.65001395]. \t  \u001b[92m-0.06869599827619491\u001b[0m \t -0.06869599827619491\n",
            "40     \t [-1.10201953  0.97528675]. \t  -10.138250229804926 \t -0.06869599827619491\n",
            "41     \t [-1.5684247   1.89050768]. \t  -39.023947112409836 \t -0.06869599827619491\n",
            "42     \t [0.49967729 1.38096823]. \t  -128.23221812997554 \t -0.06869599827619491\n",
            "43     \t [-0.3711829 -0.3645665]. \t  -27.115015768712222 \t -0.06869599827619491\n",
            "44     \t [-0.54203671 -1.29930436]. \t  -256.1772363256532 \t -0.06869599827619491\n",
            "45     \t [-1.83494523 -0.90244545]. \t  -1830.873844658998 \t -0.06869599827619491\n",
            "46     \t [ 0.34057347 -0.16994831]. \t  -8.610931318744916 \t -0.06869599827619491\n",
            "47     \t [0.82226705 0.67906304]. \t  \u001b[92m-0.032453324653580806\u001b[0m \t -0.032453324653580806\n",
            "48     \t [1.62675981 0.68831337]. \t  -383.7825862242523 \t -0.032453324653580806\n",
            "49     \t [0.92631707 0.87581349]. \t  -0.03693602741183688 \t -0.032453324653580806\n",
            "50     \t [-1.43196859  1.68221133]. \t  -19.48063398134607 \t -0.032453324653580806\n",
            "51     \t [-1.54677484 -1.42960587]. \t  -1467.344875545267 \t -0.032453324653580806\n",
            "52     \t [ 0.85535277 -0.34463053]. \t  -115.85424275582528 \t -0.032453324653580806\n",
            "53     \t [1.04500878 1.17619008]. \t  -0.710092914498775 \t -0.032453324653580806\n",
            "54     \t [1.38281945 0.84828167]. \t  -113.33656656331196 \t -0.032453324653580806\n",
            "55     \t [-1.58578633  0.99741321]. \t  -236.90775479420387 \t -0.032453324653580806\n",
            "56     \t [-0.93135105 -0.28698016]. \t  -136.99288509973206 \t -0.032453324653580806\n",
            "57     \t [-1.39071243  0.62388476]. \t  -177.3769387689481 \t -0.032453324653580806\n",
            "58     \t [0.89558766 0.78814397]. \t  \u001b[92m-0.03031559161773625\u001b[0m \t -0.03031559161773625\n",
            "59     \t [-1.1281658   0.19057678]. \t  -121.6407261095112 \t -0.03031559161773625\n",
            "60     \t [-0.94243415  0.15022679]. \t  -58.23085882279816 \t -0.03031559161773625\n",
            "61     \t [-0.53104766 -1.92376744]. \t  -488.89022912611455 \t -0.03031559161773625\n",
            "62     \t [-0.40346979 -0.06644562]. \t  -7.22452655935794 \t -0.03031559161773625\n",
            "63     \t [-1.72920729  0.0166434 ]. \t  -891.627389019488 \t -0.03031559161773625\n",
            "64     \t [1.28565025 1.6387425 ]. \t  -0.10162987270937565 \t -0.03031559161773625\n",
            "65     \t [-1.21261842 -1.9313326 ]. \t  -1162.1036949545671 \t -0.03031559161773625\n",
            "66     \t [1.66604392 0.39622405]. \t  -566.6353090913572 \t -0.03031559161773625\n",
            "67     \t [-1.29162818  0.20955702]. \t  -218.04564925677252 \t -0.03031559161773625\n",
            "68     \t [ 0.19994297 -1.39459727]. \t  -206.44047973584387 \t -0.03031559161773625\n",
            "69     \t [0.97054361 0.76680745]. \t  -3.0685303410528384 \t -0.03031559161773625\n",
            "70     \t [1.76609451 0.29053984]. \t  -800.6563950055495 \t -0.03031559161773625\n",
            "71     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.03031559161773625\n",
            "72     \t [ 1.73459313 -1.33251473]. \t  -1885.252561455859 \t -0.03031559161773625\n",
            "73     \t [ 1.07843284 -1.26873019]. \t  -591.3457791951057 \t -0.03031559161773625\n",
            "74     \t [ 0.05185248 -0.61927705]. \t  -39.58312041762906 \t -0.03031559161773625\n",
            "75     \t [1.0747425  1.15764072]. \t  \u001b[92m-0.006246566659851051\u001b[0m \t -0.006246566659851051\n",
            "76     \t [1.06176258 1.1392823 ]. \t  -0.018077009257239162 \t -0.006246566659851051\n",
            "77     \t [-1.13297187 -0.62810777]. \t  -370.021887695414 \t -0.006246566659851051\n",
            "78     \t [-0.16329517 -0.12542853]. \t  -3.6665093913833675 \t -0.006246566659851051\n",
            "79     \t [1.12645294 0.53893632]. \t  -53.30013520835561 \t -0.006246566659851051\n",
            "80     \t [ 1.59495137 -0.17072792]. \t  -737.2580887986887 \t -0.006246566659851051\n",
            "81     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.006246566659851051\n",
            "82     \t [0.68399128 0.74996957]. \t  -8.059340982336861 \t -0.006246566659851051\n",
            "83     \t [-1.37415569  1.40566405]. \t  -28.9307325057905 \t -0.006246566659851051\n",
            "84     \t [-0.71861415  0.91425582]. \t  -18.78205873832965 \t -0.006246566659851051\n",
            "85     \t [-1.7717308   2.02961981]. \t  -130.761591605953 \t -0.006246566659851051\n",
            "86     \t [0.37447845 0.96941186]. \t  -69.14485198697025 \t -0.006246566659851051\n",
            "87     \t [ 0.13406096 -0.30988621]. \t  -11.49897327350787 \t -0.006246566659851051\n",
            "88     \t [-1.97757266  0.61735958]. \t  -1093.5367051180658 \t -0.006246566659851051\n",
            "89     \t [-0.04372883 -0.81460824]. \t  -67.75993521587327 \t -0.006246566659851051\n",
            "90     \t [-0.9563154  -0.45374215]. \t  -191.04654219354157 \t -0.006246566659851051\n",
            "91     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.006246566659851051\n",
            "92     \t [ 0.56971754 -0.26063979]. \t  -34.43313749931704 \t -0.006246566659851051\n",
            "93     \t [ 0.71186019 -0.97858099]. \t  -220.70233264226943 \t -0.006246566659851051\n",
            "94     \t [-0.68390632 -0.72930392]. \t  -146.12404581908285 \t -0.006246566659851051\n",
            "95     \t [0.76579775 1.19033201]. \t  -36.5226583551558 \t -0.006246566659851051\n",
            "96     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.006246566659851051\n",
            "97     \t [-1.25274348  1.20739645]. \t  -18.17706518631196 \t -0.006246566659851051\n",
            "98     \t [-1.8188858  -2.01500327]. \t  -2841.750389796985 \t -0.006246566659851051\n",
            "99     \t [0.28598232 1.17276022]. \t  -119.53232007701105 \t -0.006246566659851051\n",
            "100    \t [ 1.94508247 -0.03896131]. \t  -1461.8963614633797 \t -0.006246566659851051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "a6371602-3941-45eb-955f-9a2136070499"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [ 0.0784139  -1.99165818]. \t  -399.9725697606536 \t -12.122423820878506\n",
            "2      \t [0.36848897 1.24803051]. \t  -124.10800925982912 \t -12.122423820878506\n",
            "3      \t [1.30511175 0.39567731]. \t  -171.08516983793774 \t -12.122423820878506\n",
            "4      \t [2.04167073 2.00984239]. \t  -467.03053841253114 \t -12.122423820878506\n",
            "5      \t [0.79646458 0.97983386]. \t  \u001b[92m-11.976933688542712\u001b[0m \t -11.976933688542712\n",
            "6      \t [0.74756226 0.81859824]. \t  \u001b[92m-6.810674116298312\u001b[0m \t -6.810674116298312\n",
            "7      \t [ 1.73111495 -0.26146777]. \t  -1062.138680699351 \t -6.810674116298312\n",
            "8      \t [0.91633332 0.64286051]. \t  \u001b[92m-3.8802697106202237\u001b[0m \t -3.8802697106202237\n",
            "9      \t [ 1.56120513 -1.00487715]. \t  -1185.2156248763604 \t -3.8802697106202237\n",
            "10     \t [0.98381338 0.94938182]. \t  \u001b[92m-0.03451268464424739\u001b[0m \t -0.03451268464424739\n",
            "11     \t [-0.68497996  0.16259641]. \t  -12.239583504391522 \t -0.03451268464424739\n",
            "12     \t [-1.55488042  0.60867409]. \t  -333.76793250443313 \t -0.03451268464424739\n",
            "13     \t [0.88777518 1.86497353]. \t  -115.96861256553636 \t -0.03451268464424739\n",
            "14     \t [-0.67002381 -1.75175791]. \t  -487.092544410677 \t -0.03451268464424739\n",
            "15     \t [-0.17325487  1.62640431]. \t  -256.22169244829337 \t -0.03451268464424739\n",
            "16     \t [-0.18308617 -1.34190572]. \t  -190.579434904392 \t -0.03451268464424739\n",
            "17     \t [-0.61716653  0.41143276]. \t  -2.70848594564955 \t -0.03451268464424739\n",
            "18     \t [1.71579144 1.97745606]. \t  -93.92152626136267 \t -0.03451268464424739\n",
            "19     \t [ 1.12210002 -1.56110415]. \t  -795.3748184470774 \t -0.03451268464424739\n",
            "20     \t [1.24089949 1.47395796]. \t  -0.4919652926139556 \t -0.03451268464424739\n",
            "21     \t [-0.13107399 -0.84726098]. \t  -76.00521578930851 \t -0.03451268464424739\n",
            "22     \t [1.35328057 1.97031429]. \t  -2.0554055417217794 \t -0.03451268464424739\n",
            "23     \t [-0.20416152  0.1225642 ]. \t  -2.1041992225439206 \t -0.03451268464424739\n",
            "24     \t [ 0.24588503 -0.23879994]. \t  -9.524307609272594 \t -0.03451268464424739\n",
            "25     \t [1.36793403 0.01419889]. \t  -344.99684787182787 \t -0.03451268464424739\n",
            "26     \t [-0.41135607  0.2374338 ]. \t  -2.4573225357148383 \t -0.03451268464424739\n",
            "27     \t [1.40046922 1.97908209]. \t  -0.19194594668516246 \t -0.03451268464424739\n",
            "28     \t [1.28348958 1.6588105 ]. \t  -0.09351095984627762 \t -0.03451268464424739\n",
            "29     \t [1.36343772 1.78131999]. \t  -0.7349215090536451 \t -0.03451268464424739\n",
            "30     \t [-1.92875429 -0.65237303]. \t  -1920.4236213733727 \t -0.03451268464424739\n",
            "31     \t [0.41102916 0.07789441]. \t  -1.175907177166759 \t -0.03451268464424739\n",
            "32     \t [-0.55885019 -1.36792651]. \t  -284.7506778298577 \t -0.03451268464424739\n",
            "33     \t [0.24363327 1.74337258]. \t  -284.16287928828496 \t -0.03451268464424739\n",
            "34     \t [-1.14939003  1.53856303]. \t  -9.34900532106429 \t -0.03451268464424739\n",
            "35     \t [-1.70714828 -1.19774021]. \t  -1698.2615538946 \t -0.03451268464424739\n",
            "36     \t [-1.21007254  1.24115724]. \t  -9.862598833170638 \t -0.03451268464424739\n",
            "37     \t [0.70937683 0.41913137]. \t  -0.7914756696310384 \t -0.03451268464424739\n",
            "38     \t [ 0.37926631 -1.66568356]. \t  -327.8239238447716 \t -0.03451268464424739\n",
            "39     \t [-1.12292196  1.80136345]. \t  -33.71106413908456 \t -0.03451268464424739\n",
            "40     \t [1.12919081 1.3146048 ]. \t  -0.17297540644221462 \t -0.03451268464424739\n",
            "41     \t [-1.0674548   1.24275854]. \t  -5.341433134534333 \t -0.03451268464424739\n",
            "42     \t [1.19079437 0.37061068]. \t  -109.7370036372121 \t -0.03451268464424739\n",
            "43     \t [1.03370032 1.06483527]. \t  \u001b[92m-0.0025055071534173033\u001b[0m \t -0.0025055071534173033\n",
            "44     \t [0.64018237 0.42059213]. \t  -0.1410436020062543 \t -0.0025055071534173033\n",
            "45     \t [-1.37429839  1.73310574]. \t  -8.058127826750235 \t -0.0025055071534173033\n",
            "46     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.0025055071534173033\n",
            "47     \t [0.1361602  0.91973458]. \t  -81.9614576900326 \t -0.0025055071534173033\n",
            "48     \t [0.26502029 1.06660612]. \t  -99.81558628550876 \t -0.0025055071534173033\n",
            "49     \t [-0.85724841 -1.36641014]. \t  -444.9892310789567 \t -0.0025055071534173033\n",
            "50     \t [-1.91655642 -0.80527284]. \t  -2014.1679084374393 \t -0.0025055071534173033\n",
            "51     \t [1.80553109 0.05877185]. \t  -1025.398245789344 \t -0.0025055071534173033\n",
            "52     \t [1.96827608 0.51385253]. \t  -1130.0710754655745 \t -0.0025055071534173033\n",
            "53     \t [1.88164587 0.89465385]. \t  -700.875734542644 \t -0.0025055071534173033\n",
            "54     \t [0.33385595 0.11906401]. \t  -0.449530294453345 \t -0.0025055071534173033\n",
            "55     \t [ 0.78927822 -1.02652579]. \t  -272.12477832565867 \t -0.0025055071534173033\n",
            "56     \t [-1.09340419 -0.90661056]. \t  -446.28298085426337 \t -0.0025055071534173033\n",
            "57     \t [0.0025451  1.09913131]. \t  -121.80245700293212 \t -0.0025055071534173033\n",
            "58     \t [-1.13628881 -0.58089985]. \t  -355.02164384040896 \t -0.0025055071534173033\n",
            "59     \t [-1.90671123  0.45571025]. \t  -1019.5855922763678 \t -0.0025055071534173033\n",
            "60     \t [ 1.69350482 -0.70147587]. \t  -1274.5671653469997 \t -0.0025055071534173033\n",
            "61     \t [1.39519021 1.91911382]. \t  -0.23148105388436957 \t -0.0025055071534173033\n",
            "62     \t [ 1.0450378  -1.71499994]. \t  -787.9852810986221 \t -0.0025055071534173033\n",
            "63     \t [-0.68900583 -0.93015724]. \t  -200.2232850113495 \t -0.0025055071534173033\n",
            "64     \t [-1.9511115   1.59976625]. \t  -495.82478171662814 \t -0.0025055071534173033\n",
            "65     \t [0.1433744  0.57671401]. \t  -31.66495620399632 \t -0.0025055071534173033\n",
            "66     \t [-0.75099975 -0.36212237]. \t  -88.83637904884068 \t -0.0025055071534173033\n",
            "67     \t [ 0.76788049 -1.61894204]. \t  -487.8375366203857 \t -0.0025055071534173033\n",
            "68     \t [0.1155818 0.9244811]. \t  -83.79651624949977 \t -0.0025055071534173033\n",
            "69     \t [-1.98924876 -0.98517663]. \t  -2451.555942394714 \t -0.0025055071534173033\n",
            "70     \t [-1.47252835  0.50797933]. \t  -281.7930650445806 \t -0.0025055071534173033\n",
            "71     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.0025055071534173033\n",
            "72     \t [0.13545758 1.00238521]. \t  -97.58020718432583 \t -0.0025055071534173033\n",
            "73     \t [-0.29352304 -0.09093165]. \t  -4.8091973629883595 \t -0.0025055071534173033\n",
            "74     \t [-0.84245666 -1.64991733]. \t  -560.1897177737782 \t -0.0025055071534173033\n",
            "75     \t [ 1.60969256 -1.00479884]. \t  -1293.4278517665257 \t -0.0025055071534173033\n",
            "76     \t [0.33941695 0.41635123]. \t  -9.50534310900239 \t -0.0025055071534173033\n",
            "77     \t [1.43984719 1.69352848]. \t  -14.605468984402297 \t -0.0025055071534173033\n",
            "78     \t [1.43507517 1.99915639]. \t  -0.552710690365004 \t -0.0025055071534173033\n",
            "79     \t [ 0.43481695 -1.72276409]. \t  -365.8287801332857 \t -0.0025055071534173033\n",
            "80     \t [-1.6419876  -1.70374082]. \t  -1942.8604978327307 \t -0.0025055071534173033\n",
            "81     \t [0.77102632 0.77151141]. \t  -3.1863844894260773 \t -0.0025055071534173033\n",
            "82     \t [-1.88207283  1.49017923]. \t  -429.3845006603239 \t -0.0025055071534173033\n",
            "83     \t [0.20823721 0.03705188]. \t  -0.6308710012557555 \t -0.0025055071534173033\n",
            "84     \t [1.48120889 1.90768085]. \t  -8.428269162591466 \t -0.0025055071534173033\n",
            "85     \t [1.78889777 0.96399803]. \t  -500.6622668173832 \t -0.0025055071534173033\n",
            "86     \t [ 0.62086675 -1.99945441]. \t  -568.9328188065471 \t -0.0025055071534173033\n",
            "87     \t [-1.99508571  1.16711341]. \t  -800.410112024754 \t -0.0025055071534173033\n",
            "88     \t [ 2.00197735 -0.98261503]. \t  -2491.5412798331467 \t -0.0025055071534173033\n",
            "89     \t [-0.75829718 -1.78029594]. \t  -557.8403897443384 \t -0.0025055071534173033\n",
            "90     \t [ 1.26053213 -0.50401093]. \t  -438.1127623760935 \t -0.0025055071534173033\n",
            "91     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.0025055071534173033\n",
            "92     \t [ 1.1015313  -0.52844185]. \t  -303.40158362456754 \t -0.0025055071534173033\n",
            "93     \t [1.11703424 1.1815487 ]. \t  -0.4521633044595778 \t -0.0025055071534173033\n",
            "94     \t [0.01671397 0.36214482]. \t  -14.061512984456144 \t -0.0025055071534173033\n",
            "95     \t [-0.26471549 -0.51119417]. \t  -35.38680766315234 \t -0.0025055071534173033\n",
            "96     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.0025055071534173033\n",
            "97     \t [0.79248782 0.00551502]. \t  -38.79641589678887 \t -0.0025055071534173033\n",
            "98     \t [-1.22520577  1.49539825]. \t  -4.954825042362605 \t -0.0025055071534173033\n",
            "99     \t [-0.26276442  1.13854958]. \t  -115.97854910800217 \t -0.0025055071534173033\n",
            "100    \t [ 1.28981407 -0.97803313]. \t  -697.9172874987738 \t -0.0025055071534173033\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "cbcaba42-302e-473c-e604-185bddfe9efd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.2928771   0.48228046]. \t  -17.393031094420916 \t -1.9278091788796494\n",
            "2      \t [-2.0143131  -1.31251332]. \t  -2892.7444960501225 \t -1.9278091788796494\n",
            "3      \t [ 0.99390592 -1.25402384]. \t  -502.5994088777894 \t -1.9278091788796494\n",
            "4      \t [-0.07196585  0.07270944]. \t  \u001b[92m-1.6051456622453808\u001b[0m \t -1.6051456622453808\n",
            "5      \t [-0.39446258  1.16212925]. \t  -103.25449275171425 \t -1.6051456622453808\n",
            "6      \t [ 0.19547418 -0.20209885]. \t  -6.422103437492146 \t -1.6051456622453808\n",
            "7      \t [-0.36629948 -0.2342708 ]. \t  -15.44202785243899 \t -1.6051456622453808\n",
            "8      \t [0.60017377 0.59201184]. \t  -5.533137087582245 \t -1.6051456622453808\n",
            "9      \t [0.47362224 0.3657527 ]. \t  -2.277450254338673 \t -1.6051456622453808\n",
            "10     \t [0.3866629  0.46929783]. \t  -10.602723505091621 \t -1.6051456622453808\n",
            "11     \t [-0.81527029  0.48580487]. \t  -6.4943241166310575 \t -1.6051456622453808\n",
            "12     \t [-0.09326502 -0.11280755]. \t  -2.671597172877685 \t -1.6051456622453808\n",
            "13     \t [-0.31301738  0.11992448]. \t  -1.7721712045869713 \t -1.6051456622453808\n",
            "14     \t [0.45641775 0.21128945]. \t  \u001b[92m-0.29636510612109956\u001b[0m \t -0.29636510612109956\n",
            "15     \t [-2.0223997  -1.18506995]. \t  -2791.877273939035 \t -0.29636510612109956\n",
            "16     \t [-1.20400563  1.61743151]. \t  -7.673390676574341 \t -0.29636510612109956\n",
            "17     \t [-1.0316335   1.99846325]. \t  -91.39967031340389 \t -0.29636510612109956\n",
            "18     \t [-1.07468618  1.27741972]. \t  -5.804196584519788 \t -0.29636510612109956\n",
            "19     \t [-0.17517263  0.69141474]. \t  -45.03734983140083 \t -0.29636510612109956\n",
            "20     \t [-0.73274214 -0.48359735]. \t  -107.14613451292593 \t -0.29636510612109956\n",
            "21     \t [ 0.43346255 -0.75667882]. \t  -89.54194982904822 \t -0.29636510612109956\n",
            "22     \t [-0.03741183 -1.94505909]. \t  -379.9463841882155 \t -0.29636510612109956\n",
            "23     \t [0.59587717 0.26728552]. \t  -0.9339197665094424 \t -0.29636510612109956\n",
            "24     \t [ 0.81500145 -0.12707556]. \t  -62.650256265456946 \t -0.29636510612109956\n",
            "25     \t [-1.33177649 -1.0812734 ]. \t  -820.4837430337147 \t -0.29636510612109956\n",
            "26     \t [-1.51315853  1.92252334]. \t  -19.7940722326532 \t -0.29636510612109956\n",
            "27     \t [-0.86111168  0.7568192 ]. \t  -3.4871636605690095 \t -0.29636510612109956\n",
            "28     \t [ 0.87329433 -0.30051235]. \t  -113.04598083597291 \t -0.29636510612109956\n",
            "29     \t [-0.64761208 -0.63941521]. \t  -114.82388800391946 \t -0.29636510612109956\n",
            "30     \t [-1.47691244 -1.57602495]. \t  -1417.861890443338 \t -0.29636510612109956\n",
            "31     \t [ 0.4327339  -1.35970691]. \t  -239.6320259130426 \t -0.29636510612109956\n",
            "32     \t [-1.18032486 -1.07964242]. \t  -616.2323504683599 \t -0.29636510612109956\n",
            "33     \t [2.04191633 1.94083269]. \t  -497.7467544769756 \t -0.29636510612109956\n",
            "34     \t [1.09830144 1.56154684]. \t  -12.632106730370626 \t -0.29636510612109956\n",
            "35     \t [-1.67606772 -0.89015035]. \t  -1375.682863433497 \t -0.29636510612109956\n",
            "36     \t [0.97862103 1.11305307]. \t  -2.413942043792792 \t -0.29636510612109956\n",
            "37     \t [-1.67958539  0.41857656]. \t  -584.3474178954291 \t -0.29636510612109956\n",
            "38     \t [-0.75365745 -1.90976445]. \t  -617.0067622668932 \t -0.29636510612109956\n",
            "39     \t [-0.82794695 -1.66106782]. \t  -553.9776385316743 \t -0.29636510612109956\n",
            "40     \t [ 1.09158889 -1.85527902]. \t  -928.3350305129967 \t -0.29636510612109956\n",
            "41     \t [-1.90053733 -1.91620189]. \t  -3064.5613349294117 \t -0.29636510612109956\n",
            "42     \t [0.3659369  0.16727404]. \t  -0.5133531334691585 \t -0.29636510612109956\n",
            "43     \t [-1.06064946  0.22065185]. \t  -86.02672492343893 \t -0.29636510612109956\n",
            "44     \t [-1.4610998   1.76453335]. \t  -19.767685852850942 \t -0.29636510612109956\n",
            "45     \t [-0.25640987  0.21172911]. \t  -3.7096718553017576 \t -0.29636510612109956\n",
            "46     \t [ 0.15915991 -0.78903105]. \t  -67.02570961865136 \t -0.29636510612109956\n",
            "47     \t [-1.5604199  -1.26951857]. \t  -1378.8350388257602 \t -0.29636510612109956\n",
            "48     \t [0.38545522 0.15383549]. \t  -0.3804318050463579 \t -0.29636510612109956\n",
            "49     \t [-0.12274723  1.13054738]. \t  -125.69023654816054 \t -0.29636510612109956\n",
            "50     \t [1.12805075 1.41851326]. \t  -2.1484280874112036 \t -0.29636510612109956\n",
            "51     \t [-0.15643341 -0.63785765]. \t  -45.20531701437525 \t -0.29636510612109956\n",
            "52     \t [-1.26879899 -0.26761964]. \t  -357.63700430421267 \t -0.29636510612109956\n",
            "53     \t [-0.57842583 -1.50280606]. \t  -340.0888766860873 \t -0.29636510612109956\n",
            "54     \t [0.49963123 0.23086254]. \t  \u001b[92m-0.28559581496417397\u001b[0m \t -0.28559581496417397\n",
            "55     \t [ 0.82248618 -1.04625839]. \t  -296.8154796675672 \t -0.28559581496417397\n",
            "56     \t [-0.64689927  1.63209789]. \t  -149.9994387123553 \t -0.28559581496417397\n",
            "57     \t [0.39651546 0.12863071]. \t  -0.44595410758247106 \t -0.28559581496417397\n",
            "58     \t [-1.45316867 -0.38311089]. \t  -628.4257640568197 \t -0.28559581496417397\n",
            "59     \t [1.58170114 0.41906204]. \t  -434.1091590560216 \t -0.28559581496417397\n",
            "60     \t [ 1.54664472 -0.85143791]. \t  -1052.3590489943044 \t -0.28559581496417397\n",
            "61     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.28559581496417397\n",
            "62     \t [-0.97376053 -1.4571699 ]. \t  -582.480767503922 \t -0.28559581496417397\n",
            "63     \t [-0.61116527 -1.07885359]. \t  -213.53562625344438 \t -0.28559581496417397\n",
            "64     \t [ 0.53230149 -1.91970845]. \t  -485.56313408956726 \t -0.28559581496417397\n",
            "65     \t [1.39449448 0.02403067]. \t  -369.01998822142326 \t -0.28559581496417397\n",
            "66     \t [0.76000026 0.50018348]. \t  -0.6569378013316715 \t -0.28559581496417397\n",
            "67     \t [0.71829433 0.51397338]. \t  \u001b[92m-0.07974750030707736\u001b[0m \t -0.07974750030707736\n",
            "68     \t [1.18565431 0.24203744]. \t  -135.46324582697912 \t -0.07974750030707736\n",
            "69     \t [1.07956816 1.14131118]. \t  \u001b[92m-0.06468351988062572\u001b[0m \t -0.06468351988062572\n",
            "70     \t [-1.63445622 -1.84554485]. \t  -2047.2620122828723 \t -0.06468351988062572\n",
            "71     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.06468351988062572\n",
            "72     \t [ 0.51006207 -0.81553677]. \t  -115.9531049663765 \t -0.06468351988062572\n",
            "73     \t [-1.70649306  0.19121674]. \t  -747.6557790250947 \t -0.06468351988062572\n",
            "74     \t [1.12341822 1.31161188]. \t  -0.26068684247621443 \t -0.06468351988062572\n",
            "75     \t [-1.52326452 -0.02977024]. \t  -558.6662390049898 \t -0.06468351988062572\n",
            "76     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.06468351988062572\n",
            "77     \t [1.95511732 1.8879273 ]. \t  -375.16310905375974 \t -0.06468351988062572\n",
            "78     \t [0.83614707 0.76350855]. \t  -0.44115406300436916 \t -0.06468351988062572\n",
            "79     \t [-1.16481664  1.87316636]. \t  -31.35007917044794 \t -0.06468351988062572\n",
            "80     \t [1.28442412 1.84086309]. \t  -3.7334975399839903 \t -0.06468351988062572\n",
            "81     \t [ 0.92531161 -0.60711948]. \t  -214.1364263200481 \t -0.06468351988062572\n",
            "82     \t [-1.48490288  1.04258879]. \t  -141.27997684480601 \t -0.06468351988062572\n",
            "83     \t [0.39968827 1.86533819]. \t  -291.263238083348 \t -0.06468351988062572\n",
            "84     \t [-0.14592709  1.77647586]. \t  -309.37923450719995 \t -0.06468351988062572\n",
            "85     \t [-0.93470523  0.42585779]. \t  -23.79700799426778 \t -0.06468351988062572\n",
            "86     \t [0.80808532 0.58624571]. \t  -0.4824698690405001 \t -0.06468351988062572\n",
            "87     \t [-1.57565981  0.39355628]. \t  -443.08777565153105 \t -0.06468351988062572\n",
            "88     \t [ 0.32296706 -1.50958795]. \t  -260.9242971549489 \t -0.06468351988062572\n",
            "89     \t [ 1.48541484 -0.86606207]. \t  -944.273132645884 \t -0.06468351988062572\n",
            "90     \t [-1.46338831 -1.42839435]. \t  -1280.4866590917438 \t -0.06468351988062572\n",
            "91     \t [ 1.87252561 -0.1611803 ]. \t  -1345.8407490325253 \t -0.06468351988062572\n",
            "92     \t [ 1.77148003 -1.40361085]. \t  -2063.34662094588 \t -0.06468351988062572\n",
            "93     \t [1.28066055 1.67255676]. \t  -0.184170020423924 \t -0.06468351988062572\n",
            "94     \t [-1.09556612  1.43234486]. \t  -9.777497992412677 \t -0.06468351988062572\n",
            "95     \t [ 1.7290815  -1.47455581]. \t  -1993.509957799454 \t -0.06468351988062572\n",
            "96     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.06468351988062572\n",
            "97     \t [ 1.8800602  -0.87843687]. \t  -1948.2872100578213 \t -0.06468351988062572\n",
            "98     \t [-1.81468614  1.5420402 ]. \t  -314.5385193068393 \t -0.06468351988062572\n",
            "99     \t [-0.74573766  0.83727171]. \t  -10.951966768460675 \t -0.06468351988062572\n",
            "100    \t [-0.72720043 -1.80539374]. \t  -547.8388160624896 \t -0.06468351988062572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "334144ce-7c7c-475e-f65a-a079ef67bba7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.59148658 -0.75888088]. \t  -125.46266108053989 \t -3.0269049669752817\n",
            "3      \t [-0.27699108 -0.00478117]. \t  \u001b[92m-2.2950165224549486\u001b[0m \t -2.2950165224549486\n",
            "4      \t [-0.71666554  1.43891421]. \t  -88.56582144505617 \t -2.2950165224549486\n",
            "5      \t [-0.46293894  0.38231551]. \t  -4.962692708816377 \t -2.2950165224549486\n",
            "6      \t [-0.71455683  0.59063208]. \t  -3.5803551187038183 \t -2.2950165224549486\n",
            "7      \t [1.07446951 1.28490972]. \t  \u001b[92m-1.706613580355841\u001b[0m \t -1.706613580355841\n",
            "8      \t [ 0.00064236 -0.08816777]. \t  -1.7760786152829726 \t -1.706613580355841\n",
            "9      \t [0.64475552 1.28959921]. \t  -76.49448969358191 \t -1.706613580355841\n",
            "10     \t [0.99884159 0.89926659]. \t  \u001b[92m-0.9686102734955419\u001b[0m \t -0.9686102734955419\n",
            "11     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.9686102734955419\n",
            "12     \t [-0.47153314  1.08256636]. \t  -76.16374716643861 \t -0.9686102734955419\n",
            "13     \t [-0.44235007 -0.34607322]. \t  -31.429332920183068 \t -0.9686102734955419\n",
            "14     \t [1.0788473  1.03807371]. \t  -1.5897320913288764 \t -0.9686102734955419\n",
            "15     \t [0.72919399 0.93640474]. \t  -16.44999659960347 \t -0.9686102734955419\n",
            "16     \t [1.0599992  1.69357134]. \t  -32.49052720243163 \t -0.9686102734955419\n",
            "17     \t [-0.13572112 -0.44297594]. \t  -22.578504583068362 \t -0.9686102734955419\n",
            "18     \t [0.42885755 0.33271266]. \t  -2.5401650169361023 \t -0.9686102734955419\n",
            "19     \t [-0.23267667 -1.00058447]. \t  -112.76353295205944 \t -0.9686102734955419\n",
            "20     \t [0.34600884 1.45306496]. \t  -178.20801948167232 \t -0.9686102734955419\n",
            "21     \t [-0.28573539  1.35129447]. \t  -162.85416529025727 \t -0.9686102734955419\n",
            "22     \t [1.37411082 0.79543521]. \t  -119.54919310897355 \t -0.9686102734955419\n",
            "23     \t [0.01844167 0.44839901]. \t  -21.039136270192785 \t -0.9686102734955419\n",
            "24     \t [ 1.22131312 -1.63164916]. \t  -975.5210941657798 \t -0.9686102734955419\n",
            "25     \t [1.45693244 1.91501627]. \t  -4.5200529916615935 \t -0.9686102734955419\n",
            "26     \t [-1.99106531  2.03266648]. \t  -382.08314633047803 \t -0.9686102734955419\n",
            "27     \t [1.04270967 1.10992823]. \t  \u001b[92m-0.05328398677051592\u001b[0m \t -0.05328398677051592\n",
            "28     \t [1.30135346 2.00336669]. \t  -9.691258954825049 \t -0.05328398677051592\n",
            "29     \t [1.18398914 1.5772563 ]. \t  -3.1112808777046888 \t -0.05328398677051592\n",
            "30     \t [ 0.70648111 -1.04731814]. \t  -239.23187114732363 \t -0.05328398677051592\n",
            "31     \t [ 0.48119433 -0.05490514]. \t  -8.47469889811514 \t -0.05328398677051592\n",
            "32     \t [-1.74169083  1.27475939]. \t  -316.8291374869721 \t -0.05328398677051592\n",
            "33     \t [1.7123502  0.67173204]. \t  -511.4533086722718 \t -0.05328398677051592\n",
            "34     \t [ 1.01819213 -0.91463989]. \t  -380.7790056557882 \t -0.05328398677051592\n",
            "35     \t [ 1.72965811 -0.3611323 ]. \t  -1124.692361971671 \t -0.05328398677051592\n",
            "36     \t [1.65026453 1.56159772]. \t  -135.3950316719991 \t -0.05328398677051592\n",
            "37     \t [-1.95473651 -1.41815147]. \t  -2753.5958428807457 \t -0.05328398677051592\n",
            "38     \t [ 1.92546661 -0.45590711]. \t  -1734.187145917912 \t -0.05328398677051592\n",
            "39     \t [ 1.21764807 -1.67881643]. \t  -999.5450123676623 \t -0.05328398677051592\n",
            "40     \t [ 0.32029479 -1.97746247]. \t  -433.1233074488358 \t -0.05328398677051592\n",
            "41     \t [0.24322351 0.69817329]. \t  -41.40680608386912 \t -0.05328398677051592\n",
            "42     \t [2.01611511 0.40900302]. \t  -1337.4592511891349 \t -0.05328398677051592\n",
            "43     \t [-0.42277262  0.56208339]. \t  -16.719750907684208 \t -0.05328398677051592\n",
            "44     \t [1.28152177 1.7505609 ]. \t  -1.2513388720204772 \t -0.05328398677051592\n",
            "45     \t [-0.06960103  0.39329284]. \t  -16.233272792355127 \t -0.05328398677051592\n",
            "46     \t [ 1.48293339 -1.80569008]. \t  -1604.0607230964827 \t -0.05328398677051592\n",
            "47     \t [1.1860111  0.69200666]. \t  -51.10215552529505 \t -0.05328398677051592\n",
            "48     \t [ 1.09019811 -0.85902057]. \t  -419.2552535068078 \t -0.05328398677051592\n",
            "49     \t [0.07221105 0.65215451]. \t  -42.71393786596149 \t -0.05328398677051592\n",
            "50     \t [-1.06352219 -0.92286626]. \t  -426.12742213244945 \t -0.05328398677051592\n",
            "51     \t [-0.77698697  1.56001318]. \t  -94.60949982977071 \t -0.05328398677051592\n",
            "52     \t [-0.93002234 -0.66266491]. \t  -237.0831376175313 \t -0.05328398677051592\n",
            "53     \t [-0.14395639  0.83083395]. \t  -66.93653951463152 \t -0.05328398677051592\n",
            "54     \t [ 1.64283333 -0.54736037]. \t  -1054.234750181094 \t -0.05328398677051592\n",
            "55     \t [1.07076024 1.13215882]. \t  \u001b[92m-0.025652885426238975\u001b[0m \t -0.025652885426238975\n",
            "56     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.025652885426238975\n",
            "57     \t [-0.80713641 -0.74818458]. \t  -199.16880468917404 \t -0.025652885426238975\n",
            "58     \t [1.30312057 1.69586611]. \t  -0.09239152927304696 \t -0.025652885426238975\n",
            "59     \t [ 1.18645898 -0.35980089]. \t  -312.43536793777577 \t -0.025652885426238975\n",
            "60     \t [-1.34258878 -1.30591657]. \t  -971.7408229441094 \t -0.025652885426238975\n",
            "61     \t [0.34484399 0.13474933]. \t  -0.454294484790764 \t -0.025652885426238975\n",
            "62     \t [-0.00653096  1.4474278 ]. \t  -210.50548033462462 \t -0.025652885426238975\n",
            "63     \t [ 0.89735443 -0.11846241]. \t  -85.33406945143894 \t -0.025652885426238975\n",
            "64     \t [1.15668781 1.33464311]. \t  \u001b[92m-0.02562925484048015\u001b[0m \t -0.02562925484048015\n",
            "65     \t [-1.91902188 -1.03299139]. \t  -2232.243308095427 \t -0.02562925484048015\n",
            "66     \t [1.47166778 0.25534753]. \t  -365.20765281558084 \t -0.02562925484048015\n",
            "67     \t [0.48556553 0.58101052]. \t  -12.183476168587521 \t -0.02562925484048015\n",
            "68     \t [0.64682454 0.3298723 ]. \t  -0.908129289980764 \t -0.02562925484048015\n",
            "69     \t [-1.28137897 -1.19756858]. \t  -811.4810884152644 \t -0.02562925484048015\n",
            "70     \t [1.78871846 1.50721248]. \t  -287.01042744204386 \t -0.02562925484048015\n",
            "71     \t [0.41070401 0.14851583]. \t  -0.3879201971187658 \t -0.02562925484048015\n",
            "72     \t [1.74924623 1.41644118]. \t  -270.64469116755384 \t -0.02562925484048015\n",
            "73     \t [ 0.26996934 -0.00881265]. \t  -1.2003699574098554 \t -0.02562925484048015\n",
            "74     \t [0.48343844 0.21728772]. \t  -0.29381391000716506 \t -0.02562925484048015\n",
            "75     \t [-1.13532691  1.12907122]. \t  -7.11629317883051 \t -0.02562925484048015\n",
            "76     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.02562925484048015\n",
            "77     \t [1.30618096 1.69042806]. \t  -0.11833503448230637 \t -0.02562925484048015\n",
            "78     \t [ 0.45875963 -1.83421845]. \t  -418.36409948972675 \t -0.02562925484048015\n",
            "79     \t [-0.54559481 -2.02115196]. \t  -540.0841043685225 \t -0.02562925484048015\n",
            "80     \t [-0.14442587  1.30848992]. \t  -167.10909260384756 \t -0.02562925484048015\n",
            "81     \t [1.16813263 0.97525035]. \t  -15.182431900590537 \t -0.02562925484048015\n",
            "82     \t [-1.66291297  0.60205752]. \t  -475.0440655718367 \t -0.02562925484048015\n",
            "83     \t [0.45476723 0.18628484]. \t  -0.33942026029901723 \t -0.02562925484048015\n",
            "84     \t [-1.77559241 -0.23396533]. \t  -1154.673348529511 \t -0.02562925484048015\n",
            "85     \t [-0.40389518 -1.25094207]. \t  -201.93127424599834 \t -0.02562925484048015\n",
            "86     \t [1.49181292 1.1552911 ]. \t  -114.77783114989622 \t -0.02562925484048015\n",
            "87     \t [1.82363278 1.78019387]. \t  -239.51767098091653 \t -0.02562925484048015\n",
            "88     \t [1.0304445  1.05936354]. \t  \u001b[92m-0.0015282576420380743\u001b[0m \t -0.0015282576420380743\n",
            "89     \t [1.36004372 1.8836212 ]. \t  -0.2445679772728797 \t -0.0015282576420380743\n",
            "90     \t [-0.80526177 -0.21591912]. \t  -77.97176485445083 \t -0.0015282576420380743\n",
            "91     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.0015282576420380743\n",
            "92     \t [ 1.55930984 -1.22208842]. \t  -1335.1450532415909 \t -0.0015282576420380743\n",
            "93     \t [-0.73903477  0.13573331]. \t  -19.87026636959404 \t -0.0015282576420380743\n",
            "94     \t [-0.0111504  1.0522277]. \t  -111.71457518049557 \t -0.0015282576420380743\n",
            "95     \t [1.34555748 1.79746949]. \t  -0.13645445664675504 \t -0.0015282576420380743\n",
            "96     \t [-1.80615082 -1.30792707]. \t  -2096.4630705644536 \t -0.0015282576420380743\n",
            "97     \t [-1.51158698 -1.05923967]. \t  -1124.6318739826365 \t -0.0015282576420380743\n",
            "98     \t [-1.21562782 -1.17840815]. \t  -710.4271513891267 \t -0.0015282576420380743\n",
            "99     \t [0.98490993 0.96779459]. \t  \u001b[92m-0.0007352989387427473\u001b[0m \t -0.0007352989387427473\n",
            "100    \t [-1.45895949 -1.65195926]. \t  -1435.2811721917542 \t -0.0007352989387427473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "0d2ab88b-bbc0-4132-dda4-bf532ca892c3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.59759223  0.9308147 ]. \t  -35.46526561710063 \t -2.0077595729598063\n",
            "2      \t [-0.11367113 -0.06337105]. \t  \u001b[92m-1.8223129462422212\u001b[0m \t -1.8223129462422212\n",
            "3      \t [1.76909035 0.53584624]. \t  -673.3892119741232 \t -1.8223129462422212\n",
            "4      \t [0.69183795 1.68021784]. \t  -144.47395429281357 \t -1.8223129462422212\n",
            "5      \t [-0.37340181  0.39409835]. \t  -8.371884591714196 \t -1.8223129462422212\n",
            "6      \t [-1.48155002  1.42463652]. \t  -65.50261012313227 \t -1.8223129462422212\n",
            "7      \t [-0.11090012 -1.61136087]. \t  -264.8611846556586 \t -1.8223129462422212\n",
            "8      \t [1.12390779 1.39223173]. \t  \u001b[92m-1.6810792503155203\u001b[0m \t -1.6810792503155203\n",
            "9      \t [-0.95676296  0.8547522 ]. \t  -4.19668062005095 \t -1.6810792503155203\n",
            "10     \t [-0.66300988  0.4649159 ]. \t  -2.8297820076230247 \t -1.6810792503155203\n",
            "11     \t [1.88722903 1.85431757]. \t  -292.2799171678451 \t -1.6810792503155203\n",
            "12     \t [ 0.38122742 -0.29137227]. \t  -19.45414665793747 \t -1.6810792503155203\n",
            "13     \t [-2.02876466  2.03982622]. \t  -440.1758591245962 \t -1.6810792503155203\n",
            "14     \t [1.23080472 1.82915193]. \t  -9.92993893282597 \t -1.6810792503155203\n",
            "15     \t [0.46658997 0.54605972]. \t  -11.066129675434231 \t -1.6810792503155203\n",
            "16     \t [0.55341142 0.85400642]. \t  -30.201594405550928 \t -1.6810792503155203\n",
            "17     \t [0.02408074 0.01928061]. \t  \u001b[92m-0.9873901135796085\u001b[0m \t -0.9873901135796085\n",
            "18     \t [0.47256492 1.92336581]. \t  -289.29457645503635 \t -0.9873901135796085\n",
            "19     \t [0.61737102 0.19052322]. \t  -3.780146920927054 \t -0.9873901135796085\n",
            "20     \t [1.59781627 0.18748029]. \t  -559.9336991280086 \t -0.9873901135796085\n",
            "21     \t [1.24282245 1.62102112]. \t  \u001b[92m-0.6428648231883373\u001b[0m \t -0.6428648231883373\n",
            "22     \t [0.44155207 0.14434972]. \t  \u001b[92m-0.5680875190206944\u001b[0m \t -0.5680875190206944\n",
            "23     \t [0.40852863 0.23667563]. \t  -0.8367630283618335 \t -0.5680875190206944\n",
            "24     \t [1.30589016 0.52928269]. \t  -138.40678783944495 \t -0.5680875190206944\n",
            "25     \t [0.66066592 0.60944707]. \t  -3.106926998837532 \t -0.5680875190206944\n",
            "26     \t [1.24775957 1.63437628]. \t  -0.6615810608294141 \t -0.5680875190206944\n",
            "27     \t [1.00578142 0.97156419]. \t  \u001b[92m-0.16029005955807224\u001b[0m \t -0.16029005955807224\n",
            "28     \t [1.48891242 2.00694041]. \t  -4.645666649964053 \t -0.16029005955807224\n",
            "29     \t [-0.82434682 -0.60356239]. \t  -167.96538417843664 \t -0.16029005955807224\n",
            "30     \t [-1.93949334  1.69699813]. \t  -434.9129180923196 \t -0.16029005955807224\n",
            "31     \t [-0.91197259  1.69217746]. \t  -77.69881546101477 \t -0.16029005955807224\n",
            "32     \t [0.84832143 0.66973804]. \t  -0.27211923119678194 \t -0.16029005955807224\n",
            "33     \t [0.45018974 0.20489109]. \t  -0.3027842836324918 \t -0.16029005955807224\n",
            "34     \t [1.31530313 1.74904088]. \t  \u001b[92m-0.13558661457542254\u001b[0m \t -0.13558661457542254\n",
            "35     \t [1.38158823 1.95906057]. \t  -0.39836238741930424 \t -0.13558661457542254\n",
            "36     \t [-0.63864392  1.52983507]. \t  -128.5665997916627 \t -0.13558661457542254\n",
            "37     \t [-1.22042366  1.49123976]. \t  -4.9306073327210544 \t -0.13558661457542254\n",
            "38     \t [-1.76261228  1.06688063]. \t  -423.75995997652166 \t -0.13558661457542254\n",
            "39     \t [-1.72517692 -0.55026939]. \t  -1251.0501918294053 \t -0.13558661457542254\n",
            "40     \t [0.05564954 0.60649055]. \t  -37.300190512549484 \t -0.13558661457542254\n",
            "41     \t [-1.82254402  1.35506658]. \t  -394.71835903777065 \t -0.13558661457542254\n",
            "42     \t [0.76022488 0.54072842]. \t  -0.195976197303345 \t -0.13558661457542254\n",
            "43     \t [0.8199031  1.16600161]. \t  -24.41237951303199 \t -0.13558661457542254\n",
            "44     \t [-1.34496471  1.77718712]. \t  -5.599620944981216 \t -0.13558661457542254\n",
            "45     \t [1.34803695 0.03901215]. \t  -316.3176151563217 \t -0.13558661457542254\n",
            "46     \t [ 1.50666073 -0.10246566]. \t  -563.1286359744842 \t -0.13558661457542254\n",
            "47     \t [ 0.83852599 -0.34052251]. \t  -108.9462605973516 \t -0.13558661457542254\n",
            "48     \t [ 1.28403975 -1.80099872]. \t  -1190.162876097512 \t -0.13558661457542254\n",
            "49     \t [1.35790219 1.8516503 ]. \t  \u001b[92m-0.13410324273290322\u001b[0m \t -0.13410324273290322\n",
            "50     \t [0.77878971 0.54527248]. \t  -0.42397925707006573 \t -0.13410324273290322\n",
            "51     \t [-1.01553322 -0.07176961]. \t  -125.74033204142796 \t -0.13410324273290322\n",
            "52     \t [-1.41605369  2.04337698]. \t  -5.983002071749153 \t -0.13410324273290322\n",
            "53     \t [-0.32754511  0.09782025]. \t  -1.7713356679080614 \t -0.13410324273290322\n",
            "54     \t [ 1.95242307 -1.48669605]. \t  -2808.478305233238 \t -0.13410324273290322\n",
            "55     \t [ 0.40544191 -1.72069599]. \t  -355.7058359961106 \t -0.13410324273290322\n",
            "56     \t [ 0.96780316 -0.78044245]. \t  -294.8392690826092 \t -0.13410324273290322\n",
            "57     \t [-0.46836524  0.05583377]. \t  -4.830375481641471 \t -0.13410324273290322\n",
            "58     \t [0.80496816 0.6151848 ]. \t  -0.14554881441016712 \t -0.13410324273290322\n",
            "59     \t [-1.23998148 -0.56124367]. \t  -445.5127150577988 \t -0.13410324273290322\n",
            "60     \t [-0.48624993  1.60285367]. \t  -188.91784469060016 \t -0.13410324273290322\n",
            "61     \t [ 1.7078212  -0.23497913]. \t  -993.779676556309 \t -0.13410324273290322\n",
            "62     \t [1.93673741 0.97984521]. \t  -768.7806515093562 \t -0.13410324273290322\n",
            "63     \t [ 1.97666469 -0.70501524]. \t  -2128.209855573222 \t -0.13410324273290322\n",
            "64     \t [0.9422807  0.87247312]. \t  \u001b[92m-0.02710853180809069\u001b[0m \t -0.02710853180809069\n",
            "65     \t [0.97108911 0.96030652]. \t  -0.030738761196914842 \t -0.02710853180809069\n",
            "66     \t [0.9893409 0.9655526]. \t  \u001b[92m-0.017650821030332026\u001b[0m \t -0.017650821030332026\n",
            "67     \t [ 1.29866214 -0.78551704]. \t  -611.1875774241657 \t -0.017650821030332026\n",
            "68     \t [1.2181309  1.49227172]. \t  -0.05468561363209745 \t -0.017650821030332026\n",
            "69     \t [-0.00265492 -1.91488594]. \t  -367.68683254968056 \t -0.017650821030332026\n",
            "70     \t [0.23958518 0.02401745]. \t  -0.6896772001263887 \t -0.017650821030332026\n",
            "71     \t [0.92628316 0.87905167]. \t  -0.0497493904416827 \t -0.017650821030332026\n",
            "72     \t [-0.12450811 -1.3949929 ]. \t  -200.21418176685694 \t -0.017650821030332026\n",
            "73     \t [-0.93108572  1.6070963 ]. \t  -58.51509767547627 \t -0.017650821030332026\n",
            "74     \t [-1.41877428 -0.24425245]. \t  -515.3334162173542 \t -0.017650821030332026\n",
            "75     \t [-0.62409191 -0.74202643]. \t  -130.6707793379426 \t -0.017650821030332026\n",
            "76     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.017650821030332026\n",
            "77     \t [-1.11398796 -0.24648974]. \t  -225.72234792939003 \t -0.017650821030332026\n",
            "78     \t [-0.09797941  0.996038  ]. \t  -98.51155747480169 \t -0.017650821030332026\n",
            "79     \t [1.1508533 0.387372 ]. \t  -87.83676818002708 \t -0.017650821030332026\n",
            "80     \t [ 1.05092279 -1.43042672]. \t  -642.5568651619753 \t -0.017650821030332026\n",
            "81     \t [0.20377698 0.76973986]. \t  -53.66365112401365 \t -0.017650821030332026\n",
            "82     \t [-1.40529617 -1.22887157]. \t  -1032.1733284565323 \t -0.017650821030332026\n",
            "83     \t [1.84719157 0.07493504]. \t  -1114.3958721029478 \t -0.017650821030332026\n",
            "84     \t [ 1.10464633 -1.45254347]. \t  -714.3899783273297 \t -0.017650821030332026\n",
            "85     \t [0.88548117 1.10740968]. \t  -10.467523465249522 \t -0.017650821030332026\n",
            "86     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.017650821030332026\n",
            "87     \t [-0.43085748 -1.0387392 ]. \t  -151.95734865395278 \t -0.017650821030332026\n",
            "88     \t [-0.33939423  0.24049692]. \t  -3.364198389531944 \t -0.017650821030332026\n",
            "89     \t [1.35936773 0.81886257]. \t  -106.01696195155297 \t -0.017650821030332026\n",
            "90     \t [-1.9380262   0.13610953]. \t  -1318.9532876497697 \t -0.017650821030332026\n",
            "91     \t [ 0.9721382  -0.41205174]. \t  -184.17401640929336 \t -0.017650821030332026\n",
            "92     \t [ 0.14583905 -0.10642548]. \t  -2.3601797106240374 \t -0.017650821030332026\n",
            "93     \t [0.01802314 2.0015417 ]. \t  -401.4511721215828 \t -0.017650821030332026\n",
            "94     \t [ 0.15498551 -0.88449689]. \t  -83.25443505500101 \t -0.017650821030332026\n",
            "95     \t [-0.52232349 -0.34714472]. \t  -40.7533218818055 \t -0.017650821030332026\n",
            "96     \t [0.95429483 0.92879834]. \t  -0.03492137016452186 \t -0.017650821030332026\n",
            "97     \t [1.36083287 1.87749507]. \t  -0.1958847502149283 \t -0.017650821030332026\n",
            "98     \t [0.63313443 0.41442596]. \t  -0.15299603144374702 \t -0.017650821030332026\n",
            "99     \t [1.08973792 1.19314612]. \t  \u001b[92m-0.011208397544987878\u001b[0m \t -0.011208397544987878\n",
            "100    \t [-0.54071186  1.67082572]. \t  -192.38799757088222 \t -0.011208397544987878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "2d79c9ac-f1a8-490a-adb9-bae4792555f8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.6128464  0.32704307]. \t  \u001b[92m-0.38547814510709133\u001b[0m \t -0.38547814510709133\n",
            "2      \t [ 0.44453963 -0.13867527]. \t  -11.617683512743746 \t -0.38547814510709133\n",
            "3      \t [-1.37708479  0.78503961]. \t  -129.15439497522596 \t -0.38547814510709133\n",
            "4      \t [1.40095679 0.92321819]. \t  -108.20883798382877 \t -0.38547814510709133\n",
            "5      \t [-0.61672302  0.67761438]. \t  -11.450565992272354 \t -0.38547814510709133\n",
            "6      \t [0.6821945  0.16706919]. \t  -9.000491312737527 \t -0.38547814510709133\n",
            "7      \t [0.34252129 1.56336662]. \t  -209.5371192117686 \t -0.38547814510709133\n",
            "8      \t [-0.11066246  0.52058745]. \t  -27.07465614486516 \t -0.38547814510709133\n",
            "9      \t [0.48847748 0.70937412]. \t  -22.42351725266694 \t -0.38547814510709133\n",
            "10     \t [1.23148034 1.44633301]. \t  -0.5465390943467198 \t -0.38547814510709133\n",
            "11     \t [1.7813847  1.85317711]. \t  -174.8913150166666 \t -0.38547814510709133\n",
            "12     \t [0.59105258 0.43472463]. \t  -0.8962377307812004 \t -0.38547814510709133\n",
            "13     \t [1.22365818 1.5305697 ]. \t  \u001b[92m-0.16044863786787977\u001b[0m \t -0.16044863786787977\n",
            "14     \t [-0.75630061  0.39433109]. \t  -6.2408823296758005 \t -0.16044863786787977\n",
            "15     \t [-1.27237762  1.91407969]. \t  -13.87415913587872 \t -0.16044863786787977\n",
            "16     \t [-0.39077275  0.27416163]. \t  -3.4094604987280985 \t -0.16044863786787977\n",
            "17     \t [1.09359119 1.26966407]. \t  -0.5522583829394507 \t -0.16044863786787977\n",
            "18     \t [-1.54848356 -1.82686842]. \t  -1791.2782240225968 \t -0.16044863786787977\n",
            "19     \t [-0.5852862  -1.15131569]. \t  -225.67957067906138 \t -0.16044863786787977\n",
            "20     \t [1.09977589 1.39446747]. \t  -3.4309922701927955 \t -0.16044863786787977\n",
            "21     \t [-0.58524277  0.37755924]. \t  -2.635845872103277 \t -0.16044863786787977\n",
            "22     \t [-0.81458661  1.70623184]. \t  -112.01098499567348 \t -0.16044863786787977\n",
            "23     \t [-0.24884643  1.86338313]. \t  -326.08491946804156 \t -0.16044863786787977\n",
            "24     \t [-0.96524174 -1.36581502]. \t  -531.715849495629 \t -0.16044863786787977\n",
            "25     \t [-1.43055645  1.85200873]. \t  -9.689969906950466 \t -0.16044863786787977\n",
            "26     \t [-1.45841171  2.02404169]. \t  -7.103102879159884 \t -0.16044863786787977\n",
            "27     \t [-0.48949062  0.73595592]. \t  -26.85539619748126 \t -0.16044863786787977\n",
            "28     \t [0.2767406  0.46772662]. \t  -15.82225228302519 \t -0.16044863786787977\n",
            "29     \t [-1.49964567  1.78875104]. \t  -27.425351780715324 \t -0.16044863786787977\n",
            "30     \t [1.99728149 1.79335453]. \t  -483.1390356175105 \t -0.16044863786787977\n",
            "31     \t [1.32285773 1.74288326]. \t  \u001b[92m-0.10923463765759268\u001b[0m \t -0.10923463765759268\n",
            "32     \t [1.34509347 1.73604456]. \t  -0.6553805432372953 \t -0.10923463765759268\n",
            "33     \t [-1.25330541  1.95501977]. \t  -19.841831903727883 \t -0.10923463765759268\n",
            "34     \t [1.14095904 1.30514939]. \t  \u001b[92m-0.02099965628878963\u001b[0m \t -0.02099965628878963\n",
            "35     \t [-1.65658863  0.95732344]. \t  -326.3809421815248 \t -0.02099965628878963\n",
            "36     \t [-0.16732383 -0.03939141]. \t  -1.8167683151227054 \t -0.02099965628878963\n",
            "37     \t [1.4405219  1.90957334]. \t  -2.9340780308759187 \t -0.02099965628878963\n",
            "38     \t [-0.69666159  2.01108644]. \t  -235.66968390762443 \t -0.02099965628878963\n",
            "39     \t [1.20837209 1.48361912]. \t  -0.0984374179090221 \t -0.02099965628878963\n",
            "40     \t [-1.85366589 -1.66299848]. \t  -2608.200710103936 \t -0.02099965628878963\n",
            "41     \t [-1.08534666  1.25525468]. \t  -4.9458488541442565 \t -0.02099965628878963\n",
            "42     \t [-0.48690379 -1.91255078]. \t  -464.30011211061515 \t -0.02099965628878963\n",
            "43     \t [0.85739977 0.72323348]. \t  -0.03449794939885728 \t -0.02099965628878963\n",
            "44     \t [-0.66904423  1.19340861]. \t  -58.40574725562008 \t -0.02099965628878963\n",
            "45     \t [1.25805159 0.19123451]. \t  -193.68248995741166 \t -0.02099965628878963\n",
            "46     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.02099965628878963\n",
            "47     \t [-0.51424044 -0.06635342]. \t  -13.235566372013212 \t -0.02099965628878963\n",
            "48     \t [-1.11227828  0.93402843]. \t  -13.650774336140383 \t -0.02099965628878963\n",
            "49     \t [0.85892397 0.34161258]. \t  -15.7124183297375 \t -0.02099965628878963\n",
            "50     \t [-0.39428439  1.1020433 ]. \t  -91.54598937155194 \t -0.02099965628878963\n",
            "51     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.02099965628878963\n",
            "52     \t [-0.56482597 -1.12583464]. \t  -211.21159278875356 \t -0.02099965628878963\n",
            "53     \t [-1.92850496  0.52318824]. \t  -1029.9814077744916 \t -0.02099965628878963\n",
            "54     \t [1.27059944 1.61245402]. \t  -0.073611717460414 \t -0.02099965628878963\n",
            "55     \t [0.69628977 0.50437927]. \t  -0.1304985736143353 \t -0.02099965628878963\n",
            "56     \t [-1.25725437 -0.32600639]. \t  -368.64375370499255 \t -0.02099965628878963\n",
            "57     \t [-1.48500591 -1.00094029]. \t  -1034.1360980208867 \t -0.02099965628878963\n",
            "58     \t [0.72679583 0.52693   ]. \t  -0.07481008670244552 \t -0.02099965628878963\n",
            "59     \t [-1.26352511  0.09110064]. \t  -231.7449777164809 \t -0.02099965628878963\n",
            "60     \t [-1.07729279  0.11680071]. \t  -113.25844116075896 \t -0.02099965628878963\n",
            "61     \t [1.22988235 1.51959968]. \t  -0.05773061435510794 \t -0.02099965628878963\n",
            "62     \t [-1.43699318  0.90086473]. \t  -141.448244037027 \t -0.02099965628878963\n",
            "63     \t [-0.04835609  0.47621891]. \t  -23.555332262915186 \t -0.02099965628878963\n",
            "64     \t [ 1.58178491 -1.94189443]. \t  -1975.1968996050994 \t -0.02099965628878963\n",
            "65     \t [1.85635975 1.58021445]. \t  -348.87561045171907 \t -0.02099965628878963\n",
            "66     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -0.02099965628878963\n",
            "67     \t [-1.37121146  0.53976074]. \t  -185.30597907635834 \t -0.02099965628878963\n",
            "68     \t [-1.10591708 -1.40193983]. \t  -693.4934082066238 \t -0.02099965628878963\n",
            "69     \t [0.88375916 0.93238652]. \t  -2.3043837843615584 \t -0.02099965628878963\n",
            "70     \t [ 0.20673218 -0.15698146]. \t  -4.618067879582819 \t -0.02099965628878963\n",
            "71     \t [ 1.92973463 -0.0818242 ]. \t  -1449.1996114104088 \t -0.02099965628878963\n",
            "72     \t [-0.45455404  0.97947863]. \t  -61.84687062939467 \t -0.02099965628878963\n",
            "73     \t [ 0.57074712 -0.03149577]. \t  -12.946874619150488 \t -0.02099965628878963\n",
            "74     \t [1.20905882 1.44406329]. \t  -0.07524712163445915 \t -0.02099965628878963\n",
            "75     \t [-0.83864115 -0.48317136]. \t  -144.15653311972133 \t -0.02099965628878963\n",
            "76     \t [ 1.81540232 -0.88185794]. \t  -1745.8518633501558 \t -0.02099965628878963\n",
            "77     \t [-1.21131546e+00 -4.31045776e-04]. \t  -220.30899682730103 \t -0.02099965628878963\n",
            "78     \t [ 2.02210299 -1.73191346]. \t  -3389.2322236048617 \t -0.02099965628878963\n",
            "79     \t [-0.12581057 -1.97756492]. \t  -398.6291010451451 \t -0.02099965628878963\n",
            "80     \t [0.34375493 1.43552062]. \t  -173.97259431303019 \t -0.02099965628878963\n",
            "81     \t [0.48494408 0.24195262]. \t  -0.2698819499293388 \t -0.02099965628878963\n",
            "82     \t [1.26745447 1.57611651]. \t  -0.16348835918113885 \t -0.02099965628878963\n",
            "83     \t [ 1.45351392 -0.62312236]. \t  -748.6795639055398 \t -0.02099965628878963\n",
            "84     \t [-1.10985455  1.86904196]. \t  -45.0621325530082 \t -0.02099965628878963\n",
            "85     \t [-0.31437303 -1.03790435]. \t  -130.94416630874125 \t -0.02099965628878963\n",
            "86     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.02099965628878963\n",
            "87     \t [0.26139067 0.5017304 ]. \t  -19.329560711259465 \t -0.02099965628878963\n",
            "88     \t [0.40212975 0.20098648]. \t  -0.5117260973859651 \t -0.02099965628878963\n",
            "89     \t [1.27145657 1.56726578]. \t  -0.3170929899630388 \t -0.02099965628878963\n",
            "90     \t [ 0.13619448 -0.65511577]. \t  -46.12857340817518 \t -0.02099965628878963\n",
            "91     \t [-1.33951156  1.72886835]. \t  -5.901329482203336 \t -0.02099965628878963\n",
            "92     \t [-0.08108414  1.84567641]. \t  -339.39827472078963 \t -0.02099965628878963\n",
            "93     \t [ 1.55037833 -1.34548242]. \t  -1405.9195318452576 \t -0.02099965628878963\n",
            "94     \t [0.65952851 1.43503437]. \t  -100.12722412902917 \t -0.02099965628878963\n",
            "95     \t [-0.8577131   1.03479741]. \t  -12.398713195129323 \t -0.02099965628878963\n",
            "96     \t [-2.01567355 -0.66988734]. \t  -2249.0596095799283 \t -0.02099965628878963\n",
            "97     \t [ 2.02019527 -1.57878686]. \t  -3204.5733881370375 \t -0.02099965628878963\n",
            "98     \t [-0.03460129  1.98045929]. \t  -392.81822179015063 \t -0.02099965628878963\n",
            "99     \t [-0.05600739 -1.1276132 ]. \t  -128.97471474266496 \t -0.02099965628878963\n",
            "100    \t [-2.04035406 -0.23039551]. \t  -1939.4754235779617 \t -0.02099965628878963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "3188e8b7-1927-4a66-983f-15273bc229b0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
            "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
            "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
            "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
            "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
            "1      \t [ 0.43264237 -1.68184172]. \t  -349.6458987787885 \t -29.9831488845538\n",
            "2      \t [-0.34113362  0.6975074 ]. \t  -35.57045714765444 \t -29.9831488845538\n",
            "3      \t [-1.8510361   2.03742245]. \t  -201.03611268756 \t -29.9831488845538\n",
            "4      \t [0.25249225 0.34338989]. \t  \u001b[92m-8.378484021899524\u001b[0m \t -8.378484021899524\n",
            "5      \t [1.26149788 0.47545735]. \t  -124.5960224346698 \t -8.378484021899524\n",
            "6      \t [1.30152373 1.78649899]. \t  \u001b[92m-0.9471884050646143\u001b[0m \t -0.9471884050646143\n",
            "7      \t [0.95197225 1.24419972]. \t  -11.423229087093857 \t -0.9471884050646143\n",
            "8      \t [1.71538352 1.71101803]. \t  -152.1765656934985 \t -0.9471884050646143\n",
            "9      \t [1.07230747 2.01506979]. \t  -74.86691334483989 \t -0.9471884050646143\n",
            "10     \t [-0.97515692  0.15717293]. \t  -66.9064354193267 \t -0.9471884050646143\n",
            "11     \t [1.23176749 1.31112268]. \t  -4.302610558044586 \t -0.9471884050646143\n",
            "12     \t [-0.97728222  0.64889156]. \t  -13.284813401119356 \t -0.9471884050646143\n",
            "13     \t [0.50445284 0.32892102]. \t  \u001b[92m-0.799822646944145\u001b[0m \t -0.799822646944145\n",
            "14     \t [0.43730765 0.13402628]. \t  \u001b[92m-0.6439405193178424\u001b[0m \t -0.6439405193178424\n",
            "15     \t [1.14319003 1.40963935]. \t  -1.0763811401198151 \t -0.6439405193178424\n",
            "16     \t [0.94208226 0.99476639]. \t  -1.1535548404064502 \t -0.6439405193178424\n",
            "17     \t [ 1.63480771 -0.56566643]. \t  -1049.0374887651844 \t -0.6439405193178424\n",
            "18     \t [1.37172553 1.92846565]. \t  \u001b[92m-0.3575290663498065\u001b[0m \t -0.3575290663498065\n",
            "19     \t [1.83310149 1.59724977]. \t  -311.51493996050334 \t -0.3575290663498065\n",
            "20     \t [-1.64171145  0.55023419]. \t  -467.0735487536142 \t -0.3575290663498065\n",
            "21     \t [-0.64296224  0.36309178]. \t  -2.95242118758337 \t -0.3575290663498065\n",
            "22     \t [-0.97301566  1.8732736 ]. \t  -89.73563414535613 \t -0.3575290663498065\n",
            "23     \t [-0.65218932  0.42609412]. \t  -2.729784781104334 \t -0.3575290663498065\n",
            "24     \t [-1.03314949  1.19574439]. \t  -5.780979965007911 \t -0.3575290663498065\n",
            "25     \t [-0.96396233  1.01400372]. \t  -4.575918684287094 \t -0.3575290663498065\n",
            "26     \t [-1.88587156  0.91344747]. \t  -706.9070146986345 \t -0.3575290663498065\n",
            "27     \t [0.48802176 0.29590318]. \t  -0.5954887436610876 \t -0.3575290663498065\n",
            "28     \t [1.04184248 1.06729908]. \t  \u001b[92m-0.03464467191970977\u001b[0m \t -0.03464467191970977\n",
            "29     \t [1.07749438 1.18565378]. \t  -0.06681510959401103 \t -0.03464467191970977\n",
            "30     \t [0.48490662 0.22623805]. \t  -0.27323574972374765 \t -0.03464467191970977\n",
            "31     \t [1.12513032 1.28498992]. \t  -0.05203056208387297 \t -0.03464467191970977\n",
            "32     \t [1.24554173 1.59958294]. \t  -0.29269901063787895 \t -0.03464467191970977\n",
            "33     \t [-1.20712966  1.52928728]. \t  -5.391626693766622 \t -0.03464467191970977\n",
            "34     \t [1.82219672 1.80090893]. \t  -231.56158444838957 \t -0.03464467191970977\n",
            "35     \t [-0.37583209 -1.61138224]. \t  -309.064807839783 \t -0.03464467191970977\n",
            "36     \t [-0.35552445 -0.55804901]. \t  -48.68416637688337 \t -0.03464467191970977\n",
            "37     \t [-0.33797863  0.41658592]. \t  -10.932124152684748 \t -0.03464467191970977\n",
            "38     \t [-0.36329081  1.32777155]. \t  -144.85025260403364 \t -0.03464467191970977\n",
            "39     \t [-1.35988747  0.24965379]. \t  -261.4539278191769 \t -0.03464467191970977\n",
            "40     \t [-1.56920135 -0.12707265]. \t  -677.1339649108769 \t -0.03464467191970977\n",
            "41     \t [1.3450071  1.77448101]. \t  -0.2384905664074295 \t -0.03464467191970977\n",
            "42     \t [-0.2103321   0.04243011]. \t  -1.4652312261130258 \t -0.03464467191970977\n",
            "43     \t [ 1.33246773 -1.56053987]. \t  -1113.0068904358352 \t -0.03464467191970977\n",
            "44     \t [0.99127909 1.00634457]. \t  -0.05629401970937036 \t -0.03464467191970977\n",
            "45     \t [-1.2051055   1.08983678]. \t  -17.998945242101303 \t -0.03464467191970977\n",
            "46     \t [-1.87945054 -0.53187245]. \t  -1660.0689019354684 \t -0.03464467191970977\n",
            "47     \t [-0.94126862 -1.11145889]. \t  -402.7473815286371 \t -0.03464467191970977\n",
            "48     \t [0.64058855 1.94804941]. \t  -236.57998974621958 \t -0.03464467191970977\n",
            "49     \t [-1.72312315 -0.3832916 ]. \t  -1131.3041455848916 \t -0.03464467191970977\n",
            "50     \t [0.79821937 0.66418828]. \t  -0.11379970586295174 \t -0.03464467191970977\n",
            "51     \t [ 0.88625641 -1.55731416]. \t  -548.8675271403135 \t -0.03464467191970977\n",
            "52     \t [0.58361384 0.36856312]. \t  -0.2515424397765883 \t -0.03464467191970977\n",
            "53     \t [-1.54536301  0.97857225]. \t  -205.16892502915746 \t -0.03464467191970977\n",
            "54     \t [-1.03742352  0.77025954]. \t  -13.513961478754117 \t -0.03464467191970977\n",
            "55     \t [1.12612472 1.24937664]. \t  -0.051177227773190184 \t -0.03464467191970977\n",
            "56     \t [ 1.63554429 -0.09981155]. \t  -770.3646643128745 \t -0.03464467191970977\n",
            "57     \t [-0.54231303  0.00288144]. \t  -10.859753918600669 \t -0.03464467191970977\n",
            "58     \t [0.66145163 0.44939786]. \t  -0.12872749483460355 \t -0.03464467191970977\n",
            "59     \t [0.65487724 0.95429416]. \t  -27.726773753029832 \t -0.03464467191970977\n",
            "60     \t [0.73099845 0.47728471]. \t  -0.3981063127539933 \t -0.03464467191970977\n",
            "61     \t [ 1.846403  -1.6901432]. \t  -2601.050623124933 \t -0.03464467191970977\n",
            "62     \t [1.29697119 1.64870456]. \t  -0.1999465141451819 \t -0.03464467191970977\n",
            "63     \t [1.06211672 1.1484952 ]. \t  -0.045487836985732354 \t -0.03464467191970977\n",
            "64     \t [ 0.27674062 -1.32846936]. \t  -197.9409839920002 \t -0.03464467191970977\n",
            "65     \t [1.35688466 1.83509698]. \t  -0.1310135984322546 \t -0.03464467191970977\n",
            "66     \t [1.05127864 1.10628871]. \t  \u001b[92m-0.002750924686980043\u001b[0m \t -0.002750924686980043\n",
            "67     \t [0.47213099 1.05764644]. \t  -69.95752629871546 \t -0.002750924686980043\n",
            "68     \t [-1.79204133 -1.16368841]. \t  -1921.9459548040177 \t -0.002750924686980043\n",
            "69     \t [-0.55761127 -1.33865854]. \t  -274.54049687577555 \t -0.002750924686980043\n",
            "70     \t [ 1.2980682  -1.25427281]. \t  -864.0101684671865 \t -0.002750924686980043\n",
            "71     \t [-1.84667548 -0.8660456 ]. \t  -1836.7400488056433 \t -0.002750924686980043\n",
            "72     \t [0.35377083 0.55626988]. \t  -19.00371957437456 \t -0.002750924686980043\n",
            "73     \t [ 0.23754772 -0.61027856]. \t  -45.03121945417927 \t -0.002750924686980043\n",
            "74     \t [-1.66305092 -0.65620777]. \t  -1178.0633699721684 \t -0.002750924686980043\n",
            "75     \t [ 0.13847559 -0.18332592]. \t  -4.842906597165351 \t -0.002750924686980043\n",
            "76     \t [-0.88307779  0.82963019]. \t  -3.7940238790094662 \t -0.002750924686980043\n",
            "77     \t [0.10426182 1.8456642 ]. \t  -337.44912878751745 \t -0.002750924686980043\n",
            "78     \t [1.34644937 1.77572466]. \t  -0.2584205148782829 \t -0.002750924686980043\n",
            "79     \t [-0.46708971 -1.31363253]. \t  -236.7951087892674 \t -0.002750924686980043\n",
            "80     \t [ 0.05482189 -1.28791705]. \t  -167.54144919181485 \t -0.002750924686980043\n",
            "81     \t [0.79924822 0.21251998]. \t  -18.211571521285908 \t -0.002750924686980043\n",
            "82     \t [-1.85316016  0.25770396]. \t  -1017.154878475344 \t -0.002750924686980043\n",
            "83     \t [ 1.69336897 -0.61239054]. \t  -1211.4435039774137 \t -0.002750924686980043\n",
            "84     \t [0.9512592  0.89009216]. \t  -0.024285276923990406 \t -0.002750924686980043\n",
            "85     \t [-0.5255028   0.31883678]. \t  -2.509347690056986 \t -0.002750924686980043\n",
            "86     \t [1.78829037 0.77377538]. \t  -588.2993861148012 \t -0.002750924686980043\n",
            "87     \t [-0.2330249  -0.80892747]. \t  -76.03662216236455 \t -0.002750924686980043\n",
            "88     \t [-1.43296824  0.75859945]. \t  -173.569656950802 \t -0.002750924686980043\n",
            "89     \t [-0.93161299 -0.72401088]. \t  -257.1500312220242 \t -0.002750924686980043\n",
            "90     \t [1.14179758 1.48818296]. \t  -3.423439666435232 \t -0.002750924686980043\n",
            "91     \t [-1.10498356 -0.99634577]. \t  -496.0881529291035 \t -0.002750924686980043\n",
            "92     \t [1.28316479 1.59543828]. \t  -0.3410335449016928 \t -0.002750924686980043\n",
            "93     \t [ 0.88961009 -0.45274637]. \t  -154.8037243800122 \t -0.002750924686980043\n",
            "94     \t [-1.25086189 -0.9100251 ]. \t  -617.4707731834394 \t -0.002750924686980043\n",
            "95     \t [-1.19440196 -1.04886507]. \t  -617.6061689884486 \t -0.002750924686980043\n",
            "96     \t [ 1.98571538 -1.49840793]. \t  -2961.9350309063184 \t -0.002750924686980043\n",
            "97     \t [-1.03840028 -0.95345358]. \t  -416.9472377976473 \t -0.002750924686980043\n",
            "98     \t [-0.65541128  0.80886722]. \t  -17.12748433920213 \t -0.002750924686980043\n",
            "99     \t [1.36246271 1.84170477]. \t  -0.15269482018227526 \t -0.002750924686980043\n",
            "100    \t [1.01960606 1.03705218]. \t  \u001b[92m-0.0010317679226575015\u001b[0m \t -0.0010317679226575015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "6aa64abe-7a5c-44ed-dac8-dbadd39afc9d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.5337917   1.15217037]. \t  -77.5624817749036 \t -8.580376531587937\n",
            "3      \t [-1.92938963  1.76120076]. \t  -393.268196122621 \t -8.580376531587937\n",
            "4      \t [-0.09123912 -1.98420962]. \t  -398.21005692392646 \t -8.580376531587937\n",
            "5      \t [-0.45198894  1.79927878]. \t  -256.50591736736254 \t -8.580376531587937\n",
            "6      \t [-0.93594459  1.20745926]. \t  -14.734916780544422 \t -8.580376531587937\n",
            "7      \t [0.76191163 0.20393692]. \t  -14.237364089085258 \t -8.580376531587937\n",
            "8      \t [0.27871169 0.21334427]. \t  \u001b[92m-2.3607306871102907\u001b[0m \t -2.3607306871102907\n",
            "9      \t [0.38694116 0.55272541]. \t  -16.61689865584352 \t -2.3607306871102907\n",
            "10     \t [0.51338761 0.25200019]. \t  \u001b[92m-0.25017035355267603\u001b[0m \t -0.25017035355267603\n",
            "11     \t [1.27697198 1.33106713]. \t  -9.052148821885917 \t -0.25017035355267603\n",
            "12     \t [0.76637781 0.86446689]. \t  -7.734790222532369 \t -0.25017035355267603\n",
            "13     \t [0.93348386 0.61128471]. \t  -6.77001097697758 \t -0.25017035355267603\n",
            "14     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.25017035355267603\n",
            "15     \t [1.04639229 1.26278699]. \t  -2.819519722305211 \t -0.25017035355267603\n",
            "16     \t [0.98226372 1.2697215 ]. \t  -9.295464012565606 \t -0.25017035355267603\n",
            "17     \t [1.04736012 0.98604106]. \t  -1.2326156865877245 \t -0.25017035355267603\n",
            "18     \t [-0.1804168   0.21361237]. \t  -4.671734039145899 \t -0.25017035355267603\n",
            "19     \t [0.64597902 0.52111529]. \t  -1.2033230965775783 \t -0.25017035355267603\n",
            "20     \t [1.83460611 1.84631973]. \t  -231.57238852383722 \t -0.25017035355267603\n",
            "21     \t [-0.22558261  0.39247068]. \t  -13.169958643967346 \t -0.25017035355267603\n",
            "22     \t [ 0.10599217 -0.09893432]. \t  -2.0129634147956326 \t -0.25017035355267603\n",
            "23     \t [0.86323706 0.75080476]. \t  \u001b[92m-0.02186989381092025\u001b[0m \t -0.02186989381092025\n",
            "24     \t [-0.08262942  1.92456551]. \t  -368.94394876270246 \t -0.02186989381092025\n",
            "25     \t [-1.42911955 -1.23177359]. \t  -1077.9105586898488 \t -0.02186989381092025\n",
            "26     \t [-0.06475983  0.0667749 ]. \t  -1.5253524584437668 \t -0.02186989381092025\n",
            "27     \t [-0.24544502  1.24586148]. \t  -142.12018927681916 \t -0.02186989381092025\n",
            "28     \t [-1.66538701 -1.53751163]. \t  -1865.5984088211412 \t -0.02186989381092025\n",
            "29     \t [-0.28616373 -0.64715834]. \t  -54.805318842856664 \t -0.02186989381092025\n",
            "30     \t [0.99194882 0.10029902]. \t  -78.08617211182937 \t -0.02186989381092025\n",
            "31     \t [0.89944854 0.79724127]. \t  -0.023955412745170483 \t -0.02186989381092025\n",
            "32     \t [-0.31793576  0.82467337]. \t  -54.09523653176086 \t -0.02186989381092025\n",
            "33     \t [0.84780549 0.69293897]. \t  -0.08990881459543673 \t -0.02186989381092025\n",
            "34     \t [-0.18459878 -0.99157824]. \t  -106.60008293430585 \t -0.02186989381092025\n",
            "35     \t [ 2.04545002 -1.72379955]. \t  -3491.1439590465025 \t -0.02186989381092025\n",
            "36     \t [ 0.63529127 -0.74700897]. \t  -132.52196273265068 \t -0.02186989381092025\n",
            "37     \t [0.17910044 0.05209302]. \t  -0.7139403211994563 \t -0.02186989381092025\n",
            "38     \t [-1.9731415   1.30257183]. \t  -680.0202743820246 \t -0.02186989381092025\n",
            "39     \t [ 0.55732215 -1.45954397]. \t  -313.5397560590302 \t -0.02186989381092025\n",
            "40     \t [-0.71931561  1.74910406]. \t  -154.66185238171147 \t -0.02186989381092025\n",
            "41     \t [0.81476492 0.65637019]. \t  -0.03989465175433457 \t -0.02186989381092025\n",
            "42     \t [-1.17017422 -0.53387238]. \t  -366.9190964188499 \t -0.02186989381092025\n",
            "43     \t [ 1.09423273 -1.03254202]. \t  -497.2486072284271 \t -0.02186989381092025\n",
            "44     \t [-1.87395402  0.49278705]. \t  -919.6453670223667 \t -0.02186989381092025\n",
            "45     \t [1.82708159 0.67678285]. \t  -709.0126391146559 \t -0.02186989381092025\n",
            "46     \t [ 1.89366666 -1.78216789]. \t  -2882.4927331825857 \t -0.02186989381092025\n",
            "47     \t [ 1.1378844  -0.65381764]. \t  -379.7226447456352 \t -0.02186989381092025\n",
            "48     \t [0.98059323 0.97475763]. \t  \u001b[92m-0.01778621223579117\u001b[0m \t -0.01778621223579117\n",
            "49     \t [1.09142071 1.23296213]. \t  -0.18277223570379217 \t -0.01778621223579117\n",
            "50     \t [1.1227541  0.79666545]. \t  -21.536438712554748 \t -0.01778621223579117\n",
            "51     \t [ 0.75641574 -1.40332656]. \t  -390.315934888768 \t -0.01778621223579117\n",
            "52     \t [-0.84502623 -1.83974622]. \t  -655.6015088704874 \t -0.01778621223579117\n",
            "53     \t [0.84464057 1.90600766]. \t  -142.25122111186738 \t -0.01778621223579117\n",
            "54     \t [1.90861306 1.11266196]. \t  -640.9873500056517 \t -0.01778621223579117\n",
            "55     \t [1.45436127 1.89285631]. \t  -5.1486351153812775 \t -0.01778621223579117\n",
            "56     \t [1.23370866 1.47267027]. \t  -0.2983277812589271 \t -0.01778621223579117\n",
            "57     \t [0.87464874 0.772021  ]. \t  -0.020627754926764767 \t -0.01778621223579117\n",
            "58     \t [1.18092966 1.41135911]. \t  -0.060839586759303836 \t -0.01778621223579117\n",
            "59     \t [1.31417954 1.98169538]. \t  -6.582225985731442 \t -0.01778621223579117\n",
            "60     \t [1.31674681 1.6930376 ]. \t  -0.2666665902912155 \t -0.01778621223579117\n",
            "61     \t [-1.66270093 -1.55400635]. \t  -1872.1039474825202 \t -0.01778621223579117\n",
            "62     \t [ 1.8351678  -0.07827489]. \t  -1188.26887670644 \t -0.01778621223579117\n",
            "63     \t [-1.81823939  1.2990678 ]. \t  -410.71794193666307 \t -0.01778621223579117\n",
            "64     \t [0.91388574 0.87655893]. \t  -0.17857812384788313 \t -0.01778621223579117\n",
            "65     \t [ 1.2739886  -1.98423576]. \t  -1301.3239220270361 \t -0.01778621223579117\n",
            "66     \t [-1.4064444  -0.22099504]. \t  -489.3866516394009 \t -0.01778621223579117\n",
            "67     \t [-0.72584082  0.34536652]. \t  -6.271966454024923 \t -0.01778621223579117\n",
            "68     \t [-0.34965343 -1.95956872]. \t  -435.221610964645 \t -0.01778621223579117\n",
            "69     \t [ 1.51152057 -1.89587699]. \t  -1747.9794032094064 \t -0.01778621223579117\n",
            "70     \t [-1.40558332 -1.91251359]. \t  -1517.579699438475 \t -0.01778621223579117\n",
            "71     \t [ 1.40517301 -0.85657528]. \t  -801.669225988052 \t -0.01778621223579117\n",
            "72     \t [1.01377087 0.96647928]. \t  -0.375371654981389 \t -0.01778621223579117\n",
            "73     \t [-0.85885753  0.65020349]. \t  -4.2198003999352425 \t -0.01778621223579117\n",
            "74     \t [2.03618554 0.66993058]. \t  -1209.415371467293 \t -0.01778621223579117\n",
            "75     \t [-1.73400181  1.72905703]. \t  -170.72783899706164 \t -0.01778621223579117\n",
            "76     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.01778621223579117\n",
            "77     \t [1.31661832 1.71476842]. \t  -0.13527370122654758 \t -0.01778621223579117\n",
            "78     \t [ 0.29383177 -1.78799774]. \t  -351.8117852221128 \t -0.01778621223579117\n",
            "79     \t [0.35188336 2.00297952]. \t  -353.54339364049775 \t -0.01778621223579117\n",
            "80     \t [-1.91804923  0.41488613]. \t  -1073.902053974256 \t -0.01778621223579117\n",
            "81     \t [ 2.02131551 -1.66183259]. \t  -3304.4750197844164 \t -0.01778621223579117\n",
            "82     \t [1.39795819 1.13389889]. \t  -67.46205269700383 \t -0.01778621223579117\n",
            "83     \t [-1.16956642  1.40716474]. \t  -4.861303510122351 \t -0.01778621223579117\n",
            "84     \t [-2.01561484 -0.16401453]. \t  -1795.6082127740456 \t -0.01778621223579117\n",
            "85     \t [ 1.66561392 -1.2937774 ]. \t  -1655.3437923316073 \t -0.01778621223579117\n",
            "86     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.01778621223579117\n",
            "87     \t [1.31844919 1.77939599]. \t  -0.2702299989797614 \t -0.01778621223579117\n",
            "88     \t [ 1.45792617 -0.2476256 ]. \t  -563.4053281955346 \t -0.01778621223579117\n",
            "89     \t [-1.67514889 -0.19090665]. \t  -905.3755728724991 \t -0.01778621223579117\n",
            "90     \t [0.54829616 0.29275421]. \t  -0.21023708943098843 \t -0.01778621223579117\n",
            "91     \t [1.35391355 1.78523813]. \t  -0.35415734201857874 \t -0.01778621223579117\n",
            "92     \t [0.78705217 0.06078357]. \t  -31.25628888559174 \t -0.01778621223579117\n",
            "93     \t [1.19064195 1.63726879]. \t  -4.860540991330615 \t -0.01778621223579117\n",
            "94     \t [ 0.45046954 -1.93188098]. \t  -456.04070283784597 \t -0.01778621223579117\n",
            "95     \t [-0.29073196 -0.0322581 ]. \t  -3.0298199791996803 \t -0.01778621223579117\n",
            "96     \t [-0.81057464  1.07467568]. \t  -20.720868440719848 \t -0.01778621223579117\n",
            "97     \t [-0.79374399  1.80956559]. \t  -142.348050764057 \t -0.01778621223579117\n",
            "98     \t [-0.65314311 -0.35198627]. \t  -63.351906424072624 \t -0.01778621223579117\n",
            "99     \t [ 1.97285981 -1.2448696 ]. \t  -2639.870017628281 \t -0.01778621223579117\n",
            "100    \t [0.33426477 0.14073541]. \t  -0.5273177277113946 \t -0.01778621223579117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "41e2d5ff-0d82-4118-aa69-e4581159aedb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.41807988 -0.02190259]. \t  \u001b[92m-5.879779069542418\u001b[0m \t -5.879779069542418\n",
            "3      \t [-0.73360589  0.27454655]. \t  -9.955522490248732 \t -5.879779069542418\n",
            "4      \t [-1.46520598  1.64692458]. \t  -31.06763830678938 \t -5.879779069542418\n",
            "5      \t [-0.27344202  1.75207414]. \t  -282.95639222124265 \t -5.879779069542418\n",
            "6      \t [-1.29966264  1.00641867]. \t  -51.89696542040336 \t -5.879779069542418\n",
            "7      \t [ 0.27407879 -0.83166825]. \t  -82.75330704725901 \t -5.879779069542418\n",
            "8      \t [-1.67956625  2.04705779]. \t  -67.06987500669742 \t -5.879779069542418\n",
            "9      \t [-0.53027258 -0.13405383]. \t  -19.584395060342768 \t -5.879779069542418\n",
            "10     \t [0.12226844 0.34155575]. \t  -11.437571930007199 \t -5.879779069542418\n",
            "11     \t [ 1.97454869 -1.97798822]. \t  -3454.6637160634355 \t -5.879779069542418\n",
            "12     \t [ 0.1354058  -0.07164913]. \t  \u001b[92m-1.5572326916850323\u001b[0m \t -1.5572326916850323\n",
            "13     \t [ 0.20209807 -0.25336336]. \t  -9.292423072901487 \t -1.5572326916850323\n",
            "14     \t [0.32525293 0.06749037]. \t  \u001b[92m-0.6019656776973432\u001b[0m \t -0.6019656776973432\n",
            "15     \t [-1.2386734  -1.13925684]. \t  -719.8085800301743 \t -0.6019656776973432\n",
            "16     \t [1.12681131 0.78354499]. \t  -23.651112251327692 \t -0.6019656776973432\n",
            "17     \t [1.08760319 0.18389203]. \t  -99.8055130292928 \t -0.6019656776973432\n",
            "18     \t [-0.45513762  0.24162594]. \t  -2.236282752693371 \t -0.6019656776973432\n",
            "19     \t [ 1.98042259 -0.98573901]. \t  -2409.6237150913776 \t -0.6019656776973432\n",
            "20     \t [0.6766157  0.54836566]. \t  -0.9246316876056652 \t -0.6019656776973432\n",
            "21     \t [-0.9989334   1.20383564]. \t  -8.238004697692737 \t -0.6019656776973432\n",
            "22     \t [ 0.2436819  -0.27884161]. \t  -12.011461671266463 \t -0.6019656776973432\n",
            "23     \t [0.83610828 0.82304648]. \t  -1.563702370890848 \t -0.6019656776973432\n",
            "24     \t [0.17726688 1.06658703]. \t  -107.8332336585574 \t -0.6019656776973432\n",
            "25     \t [-1.2198352   0.34836495]. \t  -134.80399984543754 \t -0.6019656776973432\n",
            "26     \t [ 1.29236218 -0.97473783]. \t  -699.6550875729368 \t -0.6019656776973432\n",
            "27     \t [0.210671   0.00196463]. \t  -0.8029658529589887 \t -0.6019656776973432\n",
            "28     \t [1.27910866 0.64469621]. \t  -98.36981129039162 \t -0.6019656776973432\n",
            "29     \t [-1.47304488 -0.75853993]. \t  -863.669276188972 \t -0.6019656776973432\n",
            "30     \t [0.79332293 0.57615393]. \t  \u001b[92m-0.32581747868790434\u001b[0m \t -0.32581747868790434\n",
            "31     \t [0.86317568 0.75284298]. \t  \u001b[92m-0.024759326461026014\u001b[0m \t -0.024759326461026014\n",
            "32     \t [0.91506195 0.92284786]. \t  -0.7384015270770122 \t -0.024759326461026014\n",
            "33     \t [-1.94199151 -1.42106607]. \t  -2704.754071546131 \t -0.024759326461026014\n",
            "34     \t [-1.92237694  1.06979801]. \t  -697.9887655770525 \t -0.024759326461026014\n",
            "35     \t [-1.27274788  1.79649869]. \t  -8.284545705673716 \t -0.024759326461026014\n",
            "36     \t [0.73699862 0.48845673]. \t  -0.36849074213296074 \t -0.024759326461026014\n",
            "37     \t [-0.18752053 -0.79313575]. \t  -70.01824450778345 \t -0.024759326461026014\n",
            "38     \t [-0.27563309  1.9377205 ]. \t  -348.23739126600947 \t -0.024759326461026014\n",
            "39     \t [ 1.93024108 -0.94086061]. \t  -2178.6660545401273 \t -0.024759326461026014\n",
            "40     \t [-1.98053855 -1.49949419]. \t  -2948.7214246399426 \t -0.024759326461026014\n",
            "41     \t [ 1.22778986 -1.53194722]. \t  -923.8563456372658 \t -0.024759326461026014\n",
            "42     \t [ 1.6604497  -1.82892649]. \t  -2103.593856771169 \t -0.024759326461026014\n",
            "43     \t [1.71365695 1.0271433 ]. \t  -365.1194820522272 \t -0.024759326461026014\n",
            "44     \t [0.93920879 0.30816011]. \t  -32.94590397346381 \t -0.024759326461026014\n",
            "45     \t [-0.08912908  0.32218779]. \t  -11.061118648132027 \t -0.024759326461026014\n",
            "46     \t [-0.0069477   0.10223989]. \t  -2.058256476849774 \t -0.024759326461026014\n",
            "47     \t [1.92883444 0.33470046]. \t  -1147.160415538827 \t -0.024759326461026014\n",
            "48     \t [-0.34700231 -2.0205318 ]. \t  -460.1778499671315 \t -0.024759326461026014\n",
            "49     \t [-0.5681301   0.61439423]. \t  -10.963395698427673 \t -0.024759326461026014\n",
            "50     \t [-0.69152467  1.25049012]. \t  -62.50347520244296 \t -0.024759326461026014\n",
            "51     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.024759326461026014\n",
            "52     \t [0.53762205 0.279527  ]. \t  -0.22283827093694472 \t -0.024759326461026014\n",
            "53     \t [ 1.77670957 -0.01402205]. \t  -1005.9491539234891 \t -0.024759326461026014\n",
            "54     \t [0.89798956 0.82313406]. \t  -0.03845841432218488 \t -0.024759326461026014\n",
            "55     \t [ 1.13312633 -1.69041578]. \t  -884.7179340438174 \t -0.024759326461026014\n",
            "56     \t [-0.04710502 -1.72545809]. \t  -299.5832021814439 \t -0.024759326461026014\n",
            "57     \t [-0.18157075  0.43033067]. \t  -17.18582347230089 \t -0.024759326461026014\n",
            "58     \t [ 0.47994735 -1.40300951]. \t  -267.056607350391 \t -0.024759326461026014\n",
            "59     \t [1.71424921 1.48059655]. \t  -213.10223813541185 \t -0.024759326461026014\n",
            "60     \t [ 1.86779441 -1.8241196 ]. \t  -2823.3114968565624 \t -0.024759326461026014\n",
            "61     \t [1.10977032 1.22502324]. \t  \u001b[92m-0.01636197460653358\u001b[0m \t -0.01636197460653358\n",
            "62     \t [-1.64117344 -1.89599003]. \t  -2113.2720115420593 \t -0.01636197460653358\n",
            "63     \t [1.18982945 1.68161573]. \t  -7.107465166863216 \t -0.01636197460653358\n",
            "64     \t [0.8764226 1.4176972]. \t  -42.210768926149 \t -0.01636197460653358\n",
            "65     \t [1.36940639 1.86792291]. \t  -0.1418647124525429 \t -0.01636197460653358\n",
            "66     \t [0.40434128 0.5194397 ]. \t  -13.024695036665513 \t -0.01636197460653358\n",
            "67     \t [-0.59792941 -0.37098712]. \t  -55.62557857024103 \t -0.01636197460653358\n",
            "68     \t [-1.22447442 -1.35309648]. \t  -818.5863114414681 \t -0.01636197460653358\n",
            "69     \t [1.01755555 1.01941297]. \t  -0.02592844178555785 \t -0.01636197460653358\n",
            "70     \t [ 1.96677042 -1.23243807]. \t  -2602.5710986536355 \t -0.01636197460653358\n",
            "71     \t [-1.67254638  0.24145778]. \t  -660.4323974788158 \t -0.01636197460653358\n",
            "72     \t [1.23528053 1.53957237]. \t  -0.07400113586258704 \t -0.01636197460653358\n",
            "73     \t [ 0.47673951 -1.2253542 ]. \t  -211.28857552365994 \t -0.01636197460653358\n",
            "74     \t [0.94928522 0.07194122]. \t  -68.76003629130035 \t -0.01636197460653358\n",
            "75     \t [-0.80793361 -1.44559157]. \t  -443.5751761740207 \t -0.01636197460653358\n",
            "76     \t [-1.30501148  0.16778885]. \t  -241.01728153385264 \t -0.01636197460653358\n",
            "77     \t [ 1.98216118 -0.56707881]. \t  -2022.4037849155825 \t -0.01636197460653358\n",
            "78     \t [1.22529861 1.48499827]. \t  -0.07751925704129456 \t -0.01636197460653358\n",
            "79     \t [-0.14297143 -1.50855436]. \t  -235.0890121843255 \t -0.01636197460653358\n",
            "80     \t [1.56533633 0.93464884]. \t  -230.0327265035141 \t -0.01636197460653358\n",
            "81     \t [ 0.28557859 -0.87338187]. \t  -91.7008658277363 \t -0.01636197460653358\n",
            "82     \t [0.56217192 1.41420373]. \t  -120.78865068275417 \t -0.01636197460653358\n",
            "83     \t [0.40401244 0.16482405]. \t  -0.3554565374062942 \t -0.01636197460653358\n",
            "84     \t [ 1.70460458 -1.09192798]. \t  -1598.5808533328623 \t -0.01636197460653358\n",
            "85     \t [0.70858878 0.52242116]. \t  -0.12622330354170108 \t -0.01636197460653358\n",
            "86     \t [1.31681837 1.7085673 ]. \t  -0.165110144313325 \t -0.01636197460653358\n",
            "87     \t [-2.00994707  0.85300458]. \t  -1024.6818743389156 \t -0.01636197460653358\n",
            "88     \t [1.23979598 1.53860663]. \t  -0.05773089511488847 \t -0.01636197460653358\n",
            "89     \t [-1.78620936  1.42392272]. \t  -319.8579928979332 \t -0.01636197460653358\n",
            "90     \t [-0.42532623  1.76570948]. \t  -253.19290341592327 \t -0.01636197460653358\n",
            "91     \t [-0.23251562 -0.87987128]. \t  -88.74251487636296 \t -0.01636197460653358\n",
            "92     \t [1.22678574 1.49375434]. \t  -0.06408556684166777 \t -0.01636197460653358\n",
            "93     \t [ 0.63138666 -1.65560771]. \t  -422.13298671981994 \t -0.01636197460653358\n",
            "94     \t [1.55955816 0.71295826]. \t  -295.89976870463903 \t -0.01636197460653358\n",
            "95     \t [1.18209282 0.49641175]. \t  -81.20094879602708 \t -0.01636197460653358\n",
            "96     \t [0.11221832 0.17690602]. \t  -3.488034753685005 \t -0.01636197460653358\n",
            "97     \t [ 1.53115004 -0.15748759]. \t  -626.2364977558283 \t -0.01636197460653358\n",
            "98     \t [-0.04377054  1.07377422]. \t  -115.97749058569023 \t -0.01636197460653358\n",
            "99     \t [-1.07275224  1.30988671]. \t  -6.8272437195548115 \t -0.01636197460653358\n",
            "100    \t [0.5492922  0.28118103]. \t  -0.24533033766320328 \t -0.01636197460653358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98Nt7Tguvna",
        "outputId": "c69161cf-615d-4f97-c0ef-65a210b2b71a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.5807869   1.84651715]. \t  -49.21913193836258 \t -19.52113145175031\n",
            "2      \t [-0.98024834  1.63749586]. \t  -49.70136551447148 \t -19.52113145175031\n",
            "3      \t [-1.37595391  1.31980915]. \t  -38.52850112850759 \t -19.52113145175031\n",
            "4      \t [-1.67837885 -1.78584282]. \t  -2125.7490199053195 \t -19.52113145175031\n",
            "5      \t [-0.8308906   0.89287063]. \t  \u001b[92m-7.452438960329609\u001b[0m \t -7.452438960329609\n",
            "6      \t [-1.19370865 -1.3207978 ]. \t  -758.720153086171 \t -7.452438960329609\n",
            "7      \t [-0.54061806 -0.56283587]. \t  -75.49374829245981 \t -7.452438960329609\n",
            "8      \t [ 0.00770506 -0.6820323 ]. \t  -47.50955294104664 \t -7.452438960329609\n",
            "9      \t [-0.15176604 -1.39356013]. \t  -202.00015456765243 \t -7.452438960329609\n",
            "10     \t [-1.77437392 -0.11040737]. \t  -1069.6815217935523 \t -7.452438960329609\n",
            "11     \t [ 1.05910733 -0.24097216]. \t  -185.69330959029529 \t -7.452438960329609\n",
            "12     \t [-0.90941846  0.79475898]. \t  \u001b[92m-3.7500977410634975\u001b[0m \t -3.7500977410634975\n",
            "13     \t [0.58359327 1.09024576]. \t  -56.37310446471709 \t -3.7500977410634975\n",
            "14     \t [0.5710447  0.21285256]. \t  \u001b[92m-1.4663208669145469\u001b[0m \t -1.4663208669145469\n",
            "15     \t [0.66195105 0.41028878]. \t  \u001b[92m-0.19206455993152932\u001b[0m \t -0.19206455993152932\n",
            "16     \t [0.60970803 0.36049508]. \t  \u001b[92m-0.16498137031265003\u001b[0m \t -0.16498137031265003\n",
            "17     \t [ 0.53875338 -0.81057991]. \t  -121.39654403776179 \t -0.16498137031265003\n",
            "18     \t [0.31574566 0.10316433]. \t  -0.46940741028878447 \t -0.16498137031265003\n",
            "19     \t [0.38647154 0.05165694]. \t  -1.3310108936303933 \t -0.16498137031265003\n",
            "20     \t [-0.16822463  0.21355451]. \t  -4.7966896499975835 \t -0.16498137031265003\n",
            "21     \t [-1.8898996  -1.52770608]. \t  -2608.766667606483 \t -0.16498137031265003\n",
            "22     \t [ 0.26672259 -0.82867455]. \t  -81.50448778891567 \t -0.16498137031265003\n",
            "23     \t [-0.19226789  1.6783132 ]. \t  -270.82325797463244 \t -0.16498137031265003\n",
            "24     \t [ 0.19522319 -1.99656291]. \t  -414.63790159038905 \t -0.16498137031265003\n",
            "25     \t [-1.90380966  1.58542849]. \t  -424.20978872368084 \t -0.16498137031265003\n",
            "26     \t [-1.62219888 -0.9583691 ]. \t  -1295.6129145410832 \t -0.16498137031265003\n",
            "27     \t [0.7011432  0.21686683]. \t  -7.637245058596269 \t -0.16498137031265003\n",
            "28     \t [-1.25298003  1.75584366]. \t  -8.531231134607253 \t -0.16498137031265003\n",
            "29     \t [-0.8380545  -1.56912158]. \t  -519.3301011515124 \t -0.16498137031265003\n",
            "30     \t [-1.63060351  2.03515271]. \t  -45.82212679869235 \t -0.16498137031265003\n",
            "31     \t [-0.25784332 -1.26042859]. \t  -177.651652670226 \t -0.16498137031265003\n",
            "32     \t [1.28092455 1.27436466]. \t  -13.504038134636817 \t -0.16498137031265003\n",
            "33     \t [0.90524991 0.71975603]. \t  -1.0034126782456492 \t -0.16498137031265003\n",
            "34     \t [1.13676642 1.71329304]. \t  -17.74744904508429 \t -0.16498137031265003\n",
            "35     \t [0.90034699 0.86128677]. \t  -0.2665951716148701 \t -0.16498137031265003\n",
            "36     \t [1.09456076 1.26843736]. \t  -0.5041931947634943 \t -0.16498137031265003\n",
            "37     \t [1.16298671 0.3173189 ]. \t  -107.1944404400957 \t -0.16498137031265003\n",
            "38     \t [-1.03947414  1.01610122]. \t  -4.5742586881150995 \t -0.16498137031265003\n",
            "39     \t [1.17835732 0.73878845]. \t  -42.24769636950319 \t -0.16498137031265003\n",
            "40     \t [-1.67712848 -0.15291956]. \t  -886.6925160063921 \t -0.16498137031265003\n",
            "41     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -0.16498137031265003\n",
            "42     \t [1.9716731  0.02544809]. \t  -1492.4846375196191 \t -0.16498137031265003\n",
            "43     \t [ 0.1722737  -1.09339274]. \t  -126.81397133518486 \t -0.16498137031265003\n",
            "44     \t [0.52730367 0.33409076]. \t  -0.5375079495673369 \t -0.16498137031265003\n",
            "45     \t [-1.1721999   1.78049686]. \t  -21.238145501831017 \t -0.16498137031265003\n",
            "46     \t [1.03391126 1.09866605]. \t  \u001b[92m-0.0893207759809499\u001b[0m \t -0.0893207759809499\n",
            "47     \t [ 0.74708299 -1.63196796]. \t  -479.71818826441717 \t -0.0893207759809499\n",
            "48     \t [1.00275562 1.00025021]. \t  \u001b[92m-0.0027834374847643477\u001b[0m \t -0.0027834374847643477\n",
            "49     \t [2.0291388  0.88476863]. \t  -1046.0524466450936 \t -0.0027834374847643477\n",
            "50     \t [1.44500644 1.98654748]. \t  -1.2281773013661643 \t -0.0027834374847643477\n",
            "51     \t [0.8585401  0.72889701]. \t  -0.026725216873273203 \t -0.0027834374847643477\n",
            "52     \t [-0.79672646  0.54017111]. \t  -4.1231787832939615 \t -0.0027834374847643477\n",
            "53     \t [ 0.21272891 -0.7209774 ]. \t  -59.330788597808315 \t -0.0027834374847643477\n",
            "54     \t [-1.97029281 -0.58851854]. \t  -2007.4243074353817 \t -0.0027834374847643477\n",
            "55     \t [ 0.7991025  -1.46293189]. \t  -441.6691954364644 \t -0.0027834374847643477\n",
            "56     \t [ 1.81666571 -1.39798178]. \t  -2208.027975628221 \t -0.0027834374847643477\n",
            "57     \t [1.35674281 1.92569824]. \t  -0.8488677055490179 \t -0.0027834374847643477\n",
            "58     \t [-0.6690598   0.62857839]. \t  -6.059594112791341 \t -0.0027834374847643477\n",
            "59     \t [-1.05187429  0.9227578 ]. \t  -7.584085831378985 \t -0.0027834374847643477\n",
            "60     \t [1.14782618 1.37577057]. \t  -0.3613411085898156 \t -0.0027834374847643477\n",
            "61     \t [1.17391862 1.40292686]. \t  -0.09195989196567633 \t -0.0027834374847643477\n",
            "62     \t [1.03254942 1.0350331 ]. \t  -0.09793729919090209 \t -0.0027834374847643477\n",
            "63     \t [ 1.38770185 -0.97071159]. \t  -839.0798349707751 \t -0.0027834374847643477\n",
            "64     \t [1.01970208 1.04122363]. \t  \u001b[92m-0.0005930351006262985\u001b[0m \t -0.0005930351006262985\n",
            "65     \t [ 1.40147933 -0.0424511 ]. \t  -402.80370032728825 \t -0.0005930351006262985\n",
            "66     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.0005930351006262985\n",
            "67     \t [-0.99635422  1.42286402]. \t  -22.48766865522903 \t -0.0005930351006262985\n",
            "68     \t [-0.78083258 -1.02298409]. \t  -269.7369418380132 \t -0.0005930351006262985\n",
            "69     \t [0.79604865 0.60945204]. \t  -0.10036078097624004 \t -0.0005930351006262985\n",
            "70     \t [0.67177029 0.74464743]. \t  -8.714454155806434 \t -0.0005930351006262985\n",
            "71     \t [-1.47018171  0.40299074]. \t  -315.3141561232518 \t -0.0005930351006262985\n",
            "72     \t [1.38469128 2.0101149 ]. \t  -1.0081499667484584 \t -0.0005930351006262985\n",
            "73     \t [-0.73846938  1.78295618]. \t  -156.1923914126747 \t -0.0005930351006262985\n",
            "74     \t [ 0.1920062  -1.81452282]. \t  -343.41704935735197 \t -0.0005930351006262985\n",
            "75     \t [0.7026122  1.41582394]. \t  -85.12635272420111 \t -0.0005930351006262985\n",
            "76     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.0005930351006262985\n",
            "77     \t [ 0.41220436 -1.23097664]. \t  -196.59452422588546 \t -0.0005930351006262985\n",
            "78     \t [ 0.94679312 -1.0564852 ]. \t  -381.3856099237539 \t -0.0005930351006262985\n",
            "79     \t [0.3264778  1.96547712]. \t  -346.00059964084403 \t -0.0005930351006262985\n",
            "80     \t [1.28286425 0.77135349]. \t  -76.53531097115662 \t -0.0005930351006262985\n",
            "81     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.0005930351006262985\n",
            "82     \t [-0.0245695   0.50057882]. \t  -26.047258765573183 \t -0.0005930351006262985\n",
            "83     \t [1.52288195 0.32830641]. \t  -396.62696437497647 \t -0.0005930351006262985\n",
            "84     \t [ 0.24020323 -1.74935592]. \t  -327.12153009436537 \t -0.0005930351006262985\n",
            "85     \t [-0.37725487  1.1371482 ]. \t  -100.86489912110035 \t -0.0005930351006262985\n",
            "86     \t [1.89917358 1.78193519]. \t  -333.84367620804505 \t -0.0005930351006262985\n",
            "87     \t [0.81180189 1.42532162]. \t  -58.756882950286126 \t -0.0005930351006262985\n",
            "88     \t [-1.95437521 -0.72886382]. \t  -2077.5646770921367 \t -0.0005930351006262985\n",
            "89     \t [ 1.22901738 -1.40143556]. \t  -847.9798365211404 \t -0.0005930351006262985\n",
            "90     \t [-1.56558783  0.93830891]. \t  -235.42541967516422 \t -0.0005930351006262985\n",
            "91     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.0005930351006262985\n",
            "92     \t [ 1.23184003 -0.68497411]. \t  -485.1120682204871 \t -0.0005930351006262985\n",
            "93     \t [-0.5821073  -1.55784162]. \t  -362.24656034397196 \t -0.0005930351006262985\n",
            "94     \t [-1.93926247  0.49980756]. \t  -1072.006603036883 \t -0.0005930351006262985\n",
            "95     \t [-1.40258848  2.01904946]. \t  -6.04070376181843 \t -0.0005930351006262985\n",
            "96     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.0005930351006262985\n",
            "97     \t [1.00421244 1.90356606]. \t  -80.12461242201168 \t -0.0005930351006262985\n",
            "98     \t [1.28873441 1.66118257]. \t  -0.08337954378610914 \t -0.0005930351006262985\n",
            "99     \t [-1.00773153 -1.93180522]. \t  -872.7052511651193 \t -0.0005930351006262985\n",
            "100    \t [-0.43163006 -0.55778265]. \t  -57.41613508491781 \t -0.0005930351006262985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bpn-kmNuvqC",
        "outputId": "fe0380d2-f347-4f12-aee0-b724f5171c89"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.36934199 0.61085505]. \t  \u001b[92m-22.907207745006197\u001b[0m \t -22.907207745006197\n",
            "2      \t [1.49534887 1.58210436]. \t  -43.01224517206277 \t -22.907207745006197\n",
            "3      \t [0.46575437 0.05205997]. \t  \u001b[92m-3.0035364204624333\u001b[0m \t -3.0035364204624333\n",
            "4      \t [0.79361487 1.0117675 ]. \t  -14.630635001343814 \t -3.0035364204624333\n",
            "5      \t [0.71366983 0.53765814]. \t  \u001b[92m-0.16226376538664183\u001b[0m \t -0.16226376538664183\n",
            "6      \t [-1.02302747  1.65239515]. \t  -40.793209785114925 \t -0.16226376538664183\n",
            "7      \t [0.96834878 0.89091393]. \t  -0.2198895305428826 \t -0.16226376538664183\n",
            "8      \t [-1.7346245  -0.53834905]. \t  -1265.7914775211254 \t -0.16226376538664183\n",
            "9      \t [-0.83685523  1.99588849]. \t  -171.2220776195162 \t -0.16226376538664183\n",
            "10     \t [-2.02587481  1.68474538]. \t  -594.5168616213833 \t -0.16226376538664183\n",
            "11     \t [-0.15332471  1.03252917]. \t  -103.142435095349 \t -0.16226376538664183\n",
            "12     \t [1.84166833 1.33921337]. \t  -421.99588166520016 \t -0.16226376538664183\n",
            "13     \t [1.50858771 1.90958043]. \t  -13.673039779325311 \t -0.16226376538664183\n",
            "14     \t [1.15108523 1.35160252]. \t  \u001b[92m-0.09361100209794326\u001b[0m \t -0.09361100209794326\n",
            "15     \t [ 0.17650472 -0.40556053]. \t  -19.750095693790605 \t -0.09361100209794326\n",
            "16     \t [ 0.85956863 -0.72063444]. \t  -213.0316067957737 \t -0.09361100209794326\n",
            "17     \t [1.29022776 1.6111248 ]. \t  -0.3711301449495755 \t -0.09361100209794326\n",
            "18     \t [1.56153311 0.02799735]. \t  -581.3124927720667 \t -0.09361100209794326\n",
            "19     \t [0.10645925 0.28727209]. \t  -8.412621548259297 \t -0.09361100209794326\n",
            "20     \t [0.26310583 0.00104361]. \t  -1.0078788304710982 \t -0.09361100209794326\n",
            "21     \t [-0.52856026 -1.00599816]. \t  -167.55515641751728 \t -0.09361100209794326\n",
            "22     \t [0.82291468 0.62847428]. \t  -0.2686673423887861 \t -0.09361100209794326\n",
            "23     \t [0.84479045 0.68157316]. \t  -0.1271165134973996 \t -0.09361100209794326\n",
            "24     \t [-0.56137467 -0.71804184]. \t  -109.18467518192139 \t -0.09361100209794326\n",
            "25     \t [ 0.14402638 -0.06617728]. \t  -1.4882147448082614 \t -0.09361100209794326\n",
            "26     \t [ 1.74525021 -2.03394314]. \t  -2581.034292136097 \t -0.09361100209794326\n",
            "27     \t [-1.21394464  0.86163408]. \t  -42.359319193002385 \t -0.09361100209794326\n",
            "28     \t [-1.07778224  1.2048948 ]. \t  -4.504497020248675 \t -0.09361100209794326\n",
            "29     \t [ 1.97957486 -1.68024005]. \t  -3135.791165885233 \t -0.09361100209794326\n",
            "30     \t [-0.95738451  1.09778495]. \t  -7.114692815922984 \t -0.09361100209794326\n",
            "31     \t [ 0.59201426 -1.62687485]. \t  -391.16002009217027 \t -0.09361100209794326\n",
            "32     \t [0.48741218 0.24622384]. \t  -0.27023407233327595 \t -0.09361100209794326\n",
            "33     \t [-1.83727401  1.86692274]. \t  -235.65352583664207 \t -0.09361100209794326\n",
            "34     \t [-1.41726462  1.984549  ]. \t  -5.901201106476064 \t -0.09361100209794326\n",
            "35     \t [-0.68082076 -0.94634829]. \t  -201.59714479346763 \t -0.09361100209794326\n",
            "36     \t [-0.14916624 -1.03404296]. \t  -112.89618517418847 \t -0.09361100209794326\n",
            "37     \t [-1.54445384 -0.55743512]. \t  -872.4654088084312 \t -0.09361100209794326\n",
            "38     \t [-0.9877016   0.40431711]. \t  -36.582168875359805 \t -0.09361100209794326\n",
            "39     \t [0.38371182 0.1170249 ]. \t  -0.471074699361551 \t -0.09361100209794326\n",
            "40     \t [ 1.19278592 -1.8498044 ]. \t  -1070.9907086052215 \t -0.09361100209794326\n",
            "41     \t [-1.28994606 -1.18169978]. \t  -815.0222927572457 \t -0.09361100209794326\n",
            "42     \t [ 0.4781391  -1.74497403]. \t  -389.7784949090759 \t -0.09361100209794326\n",
            "43     \t [-1.65106047  1.61048312]. \t  -131.46606497698292 \t -0.09361100209794326\n",
            "44     \t [-1.32022487  1.71749973]. \t  -5.44843780226068 \t -0.09361100209794326\n",
            "45     \t [0.05342914 0.77434006]. \t  -60.41496633178125 \t -0.09361100209794326\n",
            "46     \t [-1.54062542  0.92282972]. \t  -216.90694503923066 \t -0.09361100209794326\n",
            "47     \t [0.26431431 0.09573399]. \t  -0.6081691434134396 \t -0.09361100209794326\n",
            "48     \t [ 0.88818791 -1.32839832]. \t  -448.2983037969398 \t -0.09361100209794326\n",
            "49     \t [-0.65987705 -0.13774205]. \t  -35.608697485650005 \t -0.09361100209794326\n",
            "50     \t [1.9362214  1.29276233]. \t  -604.1639204282061 \t -0.09361100209794326\n",
            "51     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.09361100209794326\n",
            "52     \t [1.18610466 2.01301389]. \t  -36.778796332048096 \t -0.09361100209794326\n",
            "53     \t [-1.70191383  1.46957766]. \t  -210.91412597964643 \t -0.09361100209794326\n",
            "54     \t [-1.15543014 -0.80314546]. \t  -461.8205277732385 \t -0.09361100209794326\n",
            "55     \t [-0.6445735   0.19937118]. \t  -7.374707665560582 \t -0.09361100209794326\n",
            "56     \t [0.8318862  1.35426881]. \t  -43.88367000284221 \t -0.09361100209794326\n",
            "57     \t [-0.49534133 -0.87140828]. \t  -126.95386347910244 \t -0.09361100209794326\n",
            "58     \t [0.74548014 1.42810719]. \t  -76.16712000504089 \t -0.09361100209794326\n",
            "59     \t [0.46896872 0.98691183]. \t  -59.10785224430413 \t -0.09361100209794326\n",
            "60     \t [1.13236327 1.30029879]. \t  \u001b[92m-0.05010824705770907\u001b[0m \t -0.05010824705770907\n",
            "61     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.05010824705770907\n",
            "62     \t [ 0.98149748 -1.26261417]. \t  -495.4863362533916 \t -0.05010824705770907\n",
            "63     \t [0.27225107 1.76041364]. \t  -284.8880240298879 \t -0.05010824705770907\n",
            "64     \t [-0.83953917  0.4039758 ]. \t  -12.434989412544088 \t -0.05010824705770907\n",
            "65     \t [-0.44755408 -1.96706802]. \t  -471.8458404542322 \t -0.05010824705770907\n",
            "66     \t [1.16238627 1.35494319]. \t  \u001b[92m-0.0278143197783725\u001b[0m \t -0.0278143197783725\n",
            "67     \t [0.83759372 0.14908133]. \t  -30.550001785808465 \t -0.0278143197783725\n",
            "68     \t [0.98456094 0.25075815]. \t  -51.63913704506476 \t -0.0278143197783725\n",
            "69     \t [ 0.73246608 -1.77368981]. \t  -533.772297993229 \t -0.0278143197783725\n",
            "70     \t [0.83250976 0.71170681]. \t  -0.06277670935187717 \t -0.0278143197783725\n",
            "71     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.0278143197783725\n",
            "72     \t [-0.81686469  0.1363013 ]. \t  -31.49355347048604 \t -0.0278143197783725\n",
            "73     \t [1.49237833 1.5409657 ]. \t  -47.333238345706455 \t -0.0278143197783725\n",
            "74     \t [0.77181157 0.30847436]. \t  -8.301530181457267 \t -0.0278143197783725\n",
            "75     \t [1.16951247 1.36266958]. \t  -0.03132511738982982 \t -0.0278143197783725\n",
            "76     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.0278143197783725\n",
            "77     \t [0.14351039 0.89506344]. \t  -77.20303940540292 \t -0.0278143197783725\n",
            "78     \t [ 0.82063841 -0.86387592]. \t  -236.36846866714586 \t -0.0278143197783725\n",
            "79     \t [ 0.29947585 -1.35759449]. \t  -209.95275381756917 \t -0.0278143197783725\n",
            "80     \t [0.41354938 1.88425379]. \t  -293.8598650763438 \t -0.0278143197783725\n",
            "81     \t [ 1.16142564 -0.78263878]. \t  -454.37587569895425 \t -0.0278143197783725\n",
            "82     \t [1.099743   1.21029986]. \t  \u001b[92m-0.010023522509316448\u001b[0m \t -0.010023522509316448\n",
            "83     \t [-1.55106656  1.40530115]. \t  -106.60922967669718 \t -0.010023522509316448\n",
            "84     \t [-0.66643241 -1.82568483]. \t  -517.9839136256584 \t -0.010023522509316448\n",
            "85     \t [-1.30003343  0.94185481]. \t  -61.27528154653576 \t -0.010023522509316448\n",
            "86     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.010023522509316448\n",
            "87     \t [-1.17277124 -0.21195015]. \t  -256.686568077995 \t -0.010023522509316448\n",
            "88     \t [-0.37738586  2.03176769]. \t  -358.8606279912727 \t -0.010023522509316448\n",
            "89     \t [ 1.09585673 -0.96871735]. \t  -470.7339851117273 \t -0.010023522509316448\n",
            "90     \t [1.137955   1.24208617]. \t  -0.29840097925298426 \t -0.010023522509316448\n",
            "91     \t [1.26013997 2.01043038]. \t  -17.91640790402721 \t -0.010023522509316448\n",
            "92     \t [1.06460311 1.12424786]. \t  -0.012512767571803815 \t -0.010023522509316448\n",
            "93     \t [-0.60869851  1.45333029]. \t  -119.83704824326959 \t -0.010023522509316448\n",
            "94     \t [-0.37949809 -1.97403528]. \t  -450.51832415832763 \t -0.010023522509316448\n",
            "95     \t [-0.03532646  0.20508262]. \t  -5.226757806966602 \t -0.010023522509316448\n",
            "96     \t [-1.44592539  0.65227005]. \t  -212.89069487972304 \t -0.010023522509316448\n",
            "97     \t [-0.85519495  1.45638497]. \t  -56.008100437405474 \t -0.010023522509316448\n",
            "98     \t [-1.10398428  1.61088889]. \t  -19.801586554812506 \t -0.010023522509316448\n",
            "99     \t [ 1.22811738 -0.14253543]. \t  -272.5686505446685 \t -0.010023522509316448\n",
            "100    \t [1.41247491 2.01900794]. \t  -0.22736446030052537 \t -0.010023522509316448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NdFRXtPuvsP",
        "outputId": "1a196cd4-f38b-49e8-9df0-87dd6274f4ec"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.19997326 0.52210526]. \t  -84.2812837079505 \t -4.306489127802793\n",
            "2      \t [0.44822156 0.35481384]. \t  \u001b[92m-2.6733273772345485\u001b[0m \t -2.6733273772345485\n",
            "3      \t [1.71705062 2.02592882]. \t  -85.58416441979601 \t -2.6733273772345485\n",
            "4      \t [0.48926507 0.31888863]. \t  \u001b[92m-0.8930074804761887\u001b[0m \t -0.8930074804761887\n",
            "5      \t [0.33648752 0.08941757]. \t  \u001b[92m-0.4969227313965301\u001b[0m \t -0.4969227313965301\n",
            "6      \t [1.04319355 1.15819176]. \t  \u001b[92m-0.4910117331202918\u001b[0m \t -0.4910117331202918\n",
            "7      \t [0.74112232 1.67535394]. \t  -126.87525800509205 \t -0.4910117331202918\n",
            "8      \t [ 0.06241567 -1.28637043]. \t  -167.35773773045298 \t -0.4910117331202918\n",
            "9      \t [-1.32848133  1.73144046]. \t  -5.533529495668711 \t -0.4910117331202918\n",
            "10     \t [-1.9081828   1.84188939]. \t  -332.1955764905027 \t -0.4910117331202918\n",
            "11     \t [-0.9781977   1.44189927]. \t  -27.438532606827764 \t -0.4910117331202918\n",
            "12     \t [-1.06876846  1.87322166]. \t  -57.709418125013414 \t -0.4910117331202918\n",
            "13     \t [-1.39004614  1.38694615]. \t  -35.44557874541993 \t -0.4910117331202918\n",
            "14     \t [-0.95781488 -1.9929902 ]. \t  -850.8755911581851 \t -0.4910117331202918\n",
            "15     \t [-1.7998584   1.26172869]. \t  -398.993292706719 \t -0.4910117331202918\n",
            "16     \t [1.83037942 1.27120532]. \t  -432.9483560122253 \t -0.4910117331202918\n",
            "17     \t [-1.26339723  1.49819496]. \t  -6.082928219165251 \t -0.4910117331202918\n",
            "18     \t [-0.91672567  0.72737817]. \t  -4.95091342301823 \t -0.4910117331202918\n",
            "19     \t [-1.07626577  1.07740968]. \t  -4.965980679008487 \t -0.4910117331202918\n",
            "20     \t [0.45051441 0.44648882]. \t  -6.232405635222794 \t -0.4910117331202918\n",
            "21     \t [-0.23283034  0.1424338 ]. \t  -2.2982152072696813 \t -0.4910117331202918\n",
            "22     \t [-0.57893972  0.54576884]. \t  -6.928187335543955 \t -0.4910117331202918\n",
            "23     \t [1.05690505 1.35696456]. \t  -5.759220097716466 \t -0.4910117331202918\n",
            "24     \t [-1.71316048  0.26234352]. \t  -721.6271162631949 \t -0.4910117331202918\n",
            "25     \t [-0.30099442 -0.43478283]. \t  -29.295049919992728 \t -0.4910117331202918\n",
            "26     \t [1.35949135 1.98432447]. \t  -1.981765697419812 \t -0.4910117331202918\n",
            "27     \t [-0.39177782  0.14249594]. \t  -1.9491321424654182 \t -0.4910117331202918\n",
            "28     \t [0.08710804 1.04892057]. \t  -109.27076301525734 \t -0.4910117331202918\n",
            "29     \t [ 0.08063829 -0.60628709]. \t  -38.39633775902093 \t -0.4910117331202918\n",
            "30     \t [-1.39992158  0.49737864]. \t  -219.62152666592806 \t -0.4910117331202918\n",
            "31     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.4910117331202918\n",
            "32     \t [0.40066223 0.19502322]. \t  \u001b[92m-0.4781824374086416\u001b[0m \t -0.4781824374086416\n",
            "33     \t [-1.16085581  0.21366414]. \t  -133.24722419089474 \t -0.4781824374086416\n",
            "34     \t [ 1.06406115 -0.38228808]. \t  -229.37943032512587 \t -0.4781824374086416\n",
            "35     \t [0.7476354  1.17273707]. \t  -37.736077303702444 \t -0.4781824374086416\n",
            "36     \t [ 0.77205549 -1.24169292]. \t  -337.7890947457284 \t -0.4781824374086416\n",
            "37     \t [-0.73850866 -1.64347618]. \t  -482.13813546138 \t -0.4781824374086416\n",
            "38     \t [ 1.47847777 -1.31991067]. \t  -1229.297341811999 \t -0.4781824374086416\n",
            "39     \t [0.24445474 0.67058581]. \t  -37.88189592221282 \t -0.4781824374086416\n",
            "40     \t [0.93771903 0.85426414]. \t  \u001b[92m-0.06664343184280311\u001b[0m \t -0.06664343184280311\n",
            "41     \t [0.45014774 1.91013468]. \t  -291.8585425470606 \t -0.06664343184280311\n",
            "42     \t [1.43367697 0.32061112]. \t  -301.14760728437517 \t -0.06664343184280311\n",
            "43     \t [-0.01326109 -0.49478701]. \t  -25.525522378701645 \t -0.06664343184280311\n",
            "44     \t [-1.5182108   0.80987152]. \t  -229.87155168550814 \t -0.06664343184280311\n",
            "45     \t [0.85707407 0.70849945]. \t  -0.08842627839957765 \t -0.06664343184280311\n",
            "46     \t [-1.9703253   1.59067058]. \t  -533.9251830288297 \t -0.06664343184280311\n",
            "47     \t [ 1.91370402 -1.69217447]. \t  -2867.835007825963 \t -0.06664343184280311\n",
            "48     \t [-1.73379435 -0.73116528]. \t  -1404.1460948150188 \t -0.06664343184280311\n",
            "49     \t [ 1.23819025 -0.23669483]. \t  -313.27944944341465 \t -0.06664343184280311\n",
            "50     \t [-0.05142272  2.04169095]. \t  -416.87661347692347 \t -0.06664343184280311\n",
            "51     \t [-2.00496167  0.62367447]. \t  -1162.4450734371903 \t -0.06664343184280311\n",
            "52     \t [ 0.29888104 -1.46818963]. \t  -243.07826902833955 \t -0.06664343184280311\n",
            "53     \t [-1.65210435 -0.0990571 ]. \t  -807.0782056667098 \t -0.06664343184280311\n",
            "54     \t [0.05155122 0.37314437]. \t  -14.625605019530663 \t -0.06664343184280311\n",
            "55     \t [-1.44799721  1.98541929]. \t  -7.230939421018409 \t -0.06664343184280311\n",
            "56     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.06664343184280311\n",
            "57     \t [0.58250368 0.30259678]. \t  -0.3090931506733943 \t -0.06664343184280311\n",
            "58     \t [-0.87484484  1.09494647]. \t  -14.37819674086421 \t -0.06664343184280311\n",
            "59     \t [1.11981129 2.01294109]. \t  -57.61695416628044 \t -0.06664343184280311\n",
            "60     \t [0.61874762 0.36856999]. \t  -0.16574131141700751 \t -0.06664343184280311\n",
            "61     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.06664343184280311\n",
            "62     \t [-0.83360606 -0.39821277]. \t  -122.85145702179399 \t -0.06664343184280311\n",
            "63     \t [-1.66722611  0.57154307]. \t  -494.6845860274701 \t -0.06664343184280311\n",
            "64     \t [ 1.43432849 -0.71886116]. \t  -770.8947222516093 \t -0.06664343184280311\n",
            "65     \t [1.74145888 0.47871748]. \t  -652.8217275890095 \t -0.06664343184280311\n",
            "66     \t [-1.38319376 -0.62574996]. \t  -650.3189806290884 \t -0.06664343184280311\n",
            "67     \t [0.80136755 0.61666005]. \t  -0.10463241327296494 \t -0.06664343184280311\n",
            "68     \t [ 2.03794533 -1.12526205]. \t  -2787.3158341879744 \t -0.06664343184280311\n",
            "69     \t [-1.70860006 -0.65519372]. \t  -1285.0471706975995 \t -0.06664343184280311\n",
            "70     \t [1.20369026 1.53514558]. \t  -0.7858329242978649 \t -0.06664343184280311\n",
            "71     \t [-0.55951318  0.93751851]. \t  -41.427548758187484 \t -0.06664343184280311\n",
            "72     \t [1.31296822 1.75614905]. \t  -0.20204240613079577 \t -0.06664343184280311\n",
            "73     \t [1.28830482 1.68699392]. \t  -0.15745550837657923 \t -0.06664343184280311\n",
            "74     \t [0.84384095 0.0379231 ]. \t  -45.471459668446435 \t -0.06664343184280311\n",
            "75     \t [-0.67274238 -1.56222325]. \t  -408.7422158525488 \t -0.06664343184280311\n",
            "76     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.06664343184280311\n",
            "77     \t [ 0.65265246 -1.88587041]. \t  -534.5744305473129 \t -0.06664343184280311\n",
            "78     \t [-1.69721438 -1.92572466]. \t  -2317.2897352832306 \t -0.06664343184280311\n",
            "79     \t [ 1.38138552 -1.7334792 ]. \t  -1326.3470913776823 \t -0.06664343184280311\n",
            "80     \t [-1.18472947  0.67165453]. \t  -58.34510613785976 \t -0.06664343184280311\n",
            "81     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.06664343184280311\n",
            "82     \t [-1.06147932 -1.21766938]. \t  -553.8744545919657 \t -0.06664343184280311\n",
            "83     \t [-0.48247986 -1.08389492]. \t  -175.56282593029763 \t -0.06664343184280311\n",
            "84     \t [ 1.39245159 -1.35409851]. \t  -1084.5520546572018 \t -0.06664343184280311\n",
            "85     \t [ 0.68835944 -1.54346285]. \t  -407.0476822785868 \t -0.06664343184280311\n",
            "86     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.06664343184280311\n",
            "87     \t [1.3625676  0.35762958]. \t  -224.81982638961267 \t -0.06664343184280311\n",
            "88     \t [1.67259036 0.60769277]. \t  -480.00357033406283 \t -0.06664343184280311\n",
            "89     \t [-0.10675584  1.4505127 ]. \t  -208.3303635018325 \t -0.06664343184280311\n",
            "90     \t [ 1.66960399 -0.90521397]. \t  -1364.119242511162 \t -0.06664343184280311\n",
            "91     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
            "92     \t [-0.32989008  0.52002008]. \t  -18.676544542455904 \t -0.00598628680283637\n",
            "93     \t [ 1.00089176 -0.74819103]. \t  -306.24137151272816 \t -0.00598628680283637\n",
            "94     \t [-0.31594662  0.30611815]. \t  -5.987514954531987 \t -0.00598628680283637\n",
            "95     \t [0.51232447 0.86448493]. \t  -36.479259130122884 \t -0.00598628680283637\n",
            "96     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.00598628680283637\n",
            "97     \t [1.05228889 1.10025798]. \t  -0.007709934357580409 \t -0.00598628680283637\n",
            "98     \t [ 1.98684688 -0.24356647]. \t  -1757.5284212307308 \t -0.00598628680283637\n",
            "99     \t [ 1.8417731  -0.26493655]. \t  -1338.1207981665664 \t -0.00598628680283637\n",
            "100    \t [-0.67578045 -1.95670776]. \t  -585.2519126932482 \t -0.00598628680283637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86panpOuvum",
        "outputId": "09b9d1ed-5e55-4b42-afc9-77f26157b15d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.86234463  0.71071605]. \t  -768.6351218348236 \t -6.867717811955245\n",
            "2      \t [-0.53078728  0.46717892]. \t  \u001b[92m-5.782249185000509\u001b[0m \t -5.782249185000509\n",
            "3      \t [-0.71353464  0.78042254]. \t  -10.29607442865114 \t -5.782249185000509\n",
            "4      \t [1.47009231 1.87534512]. \t  -8.390653815758292 \t -5.782249185000509\n",
            "5      \t [-0.54608447 -0.29218422]. \t  -37.24670394632657 \t -5.782249185000509\n",
            "6      \t [1.15737336 1.72878775]. \t  -15.178242244174687 \t -5.782249185000509\n",
            "7      \t [2.03771621 0.24878097]. \t  -1524.8130473321185 \t -5.782249185000509\n",
            "8      \t [-0.79105972  0.33593867]. \t  -11.608432899853495 \t -5.782249185000509\n",
            "9      \t [0.61495979 1.93934833]. \t  -243.87430069068787 \t -5.782249185000509\n",
            "10     \t [-1.01404492  1.16604698]. \t  -5.954155508063634 \t -5.782249185000509\n",
            "11     \t [-0.88184769  0.75899891]. \t  \u001b[92m-3.5761570307869737\u001b[0m \t -3.5761570307869737\n",
            "12     \t [0.13675222 0.56568853]. \t  -30.664713397962558 \t -3.5761570307869737\n",
            "13     \t [ 0.69964174 -2.00987375]. \t  -624.7764120773959 \t -3.5761570307869737\n",
            "14     \t [-0.16108632  0.06668055]. \t  \u001b[92m-1.5140289294363827\u001b[0m \t -1.5140289294363827\n",
            "15     \t [ 1.58526543 -0.88665865]. \t  -1156.1556382344208 \t -1.5140289294363827\n",
            "16     \t [-0.95922089  0.91937572]. \t  -3.838599635053302 \t -1.5140289294363827\n",
            "17     \t [-1.26730816  1.85696211]. \t  -11.43537285597318 \t -1.5140289294363827\n",
            "18     \t [1.27359219 1.99899866]. \t  -14.284857578721928 \t -1.5140289294363827\n",
            "19     \t [-1.00960544 -1.69868823]. \t  -742.7862207882696 \t -1.5140289294363827\n",
            "20     \t [-1.9191221   1.39569548]. \t  -531.7110285934393 \t -1.5140289294363827\n",
            "21     \t [-0.07342262 -0.59913791]. \t  -37.69774234045507 \t -1.5140289294363827\n",
            "22     \t [0.84887905 0.97957983]. \t  -6.73011840654331 \t -1.5140289294363827\n",
            "23     \t [0.86472166 1.24091844]. \t  -24.340448488795285 \t -1.5140289294363827\n",
            "24     \t [0.72818365 1.90109329]. \t  -187.99462602351747 \t -1.5140289294363827\n",
            "25     \t [-0.42985361  0.16933688]. \t  -2.0683121713527752 \t -1.5140289294363827\n",
            "26     \t [ 0.22401697 -1.35838773]. \t  -199.00946979239325 \t -1.5140289294363827\n",
            "27     \t [ 1.42645813 -1.38249747]. \t  -1167.962308583028 \t -1.5140289294363827\n",
            "28     \t [0.49141424 0.11773257]. \t  -1.7901989695013907 \t -1.5140289294363827\n",
            "29     \t [0.23003468 0.10816533]. \t  \u001b[92m-0.8980959539740795\u001b[0m \t -0.8980959539740795\n",
            "30     \t [ 0.97642433 -0.04462397]. \t  -99.60663494896336 \t -0.8980959539740795\n",
            "31     \t [0.20243607 1.33620308]. \t  -168.3962972454647 \t -0.8980959539740795\n",
            "32     \t [0.08074016 0.0096551 ]. \t  \u001b[92m-0.8460221852003155\u001b[0m \t -0.8460221852003155\n",
            "33     \t [-1.87844345  1.70426946]. \t  -341.08531238943704 \t -0.8460221852003155\n",
            "34     \t [-0.69688178  0.87300274]. \t  -17.884070497534147 \t -0.8460221852003155\n",
            "35     \t [1.18500998 1.27107998]. \t  -1.8076180165147353 \t -0.8460221852003155\n",
            "36     \t [ 0.23967362 -0.04913235]. \t  -1.7139362067512018 \t -0.8460221852003155\n",
            "37     \t [0.22122611 0.04191359]. \t  \u001b[92m-0.6114272047717411\u001b[0m \t -0.6114272047717411\n",
            "38     \t [-0.20019587  0.38612538]. \t  -13.415322219203915 \t -0.6114272047717411\n",
            "39     \t [-0.9926861   0.51774171]. \t  -25.843628897374533 \t -0.6114272047717411\n",
            "40     \t [1.15381854 1.09155764]. \t  -5.77116687300655 \t -0.6114272047717411\n",
            "41     \t [-1.33649111 -0.36559919]. \t  -468.48682209390284 \t -0.6114272047717411\n",
            "42     \t [-0.45892964  0.92975848]. \t  -53.84500608764505 \t -0.6114272047717411\n",
            "43     \t [0.97287637 0.90311601]. \t  \u001b[92m-0.18885231930087196\u001b[0m \t -0.18885231930087196\n",
            "44     \t [1.05297003 1.18489846]. \t  -0.5827272927112714 \t -0.18885231930087196\n",
            "45     \t [ 1.68741734 -1.4058258 ]. \t  -1809.4462006025503 \t -0.18885231930087196\n",
            "46     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.18885231930087196\n",
            "47     \t [0.99116959 1.02702693]. \t  -0.19908113031364252 \t -0.18885231930087196\n",
            "48     \t [1.35710706 1.88281138]. \t  -0.29621477368128035 \t -0.18885231930087196\n",
            "49     \t [-1.12522902 -1.26760128]. \t  -646.5012649272198 \t -0.18885231930087196\n",
            "50     \t [1.03802117 1.07214657]. \t  \u001b[92m-0.00429864301065778\u001b[0m \t -0.00429864301065778\n",
            "51     \t [1.82117592 0.02005112]. \t  -1087.4516611715662 \t -0.00429864301065778\n",
            "52     \t [-0.97041722 -0.56868483]. \t  -232.01166970174543 \t -0.00429864301065778\n",
            "53     \t [1.09887081 1.2615793 ]. \t  -0.3020479380411238 \t -0.00429864301065778\n",
            "54     \t [0.96021954 0.95337528]. \t  -0.09988798143770664 \t -0.00429864301065778\n",
            "55     \t [ 0.13604281 -1.96151768]. \t  -392.79644976636814 \t -0.00429864301065778\n",
            "56     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.00429864301065778\n",
            "57     \t [-1.72252026 -1.78299641]. \t  -2263.730965109315 \t -0.00429864301065778\n",
            "58     \t [0.95246595 0.91919823]. \t  -0.01667593589089989 \t -0.00429864301065778\n",
            "59     \t [0.72228996 0.48106465]. \t  -0.24226861261882907 \t -0.00429864301065778\n",
            "60     \t [ 2.02435328 -0.77141525]. \t  -2372.1758244197367 \t -0.00429864301065778\n",
            "61     \t [0.76160531 0.59981384]. \t  -0.09592200514764478 \t -0.00429864301065778\n",
            "62     \t [ 0.74638391 -0.94930964]. \t  -226.98799123775052 \t -0.00429864301065778\n",
            "63     \t [-1.85923427  0.72504786]. \t  -754.3960043276296 \t -0.00429864301065778\n",
            "64     \t [ 0.59307306 -0.6666248 ]. \t  -103.87139014582638 \t -0.00429864301065778\n",
            "65     \t [-1.0754649   0.43522195]. \t  -56.34975499227283 \t -0.00429864301065778\n",
            "66     \t [0.68320998 0.47494266]. \t  -0.10702555738314529 \t -0.00429864301065778\n",
            "67     \t [0.05975501 0.77168105]. \t  -59.883417596274604 \t -0.00429864301065778\n",
            "68     \t [0.96582491 0.94273216]. \t  -0.010997489813224338 \t -0.00429864301065778\n",
            "69     \t [0.99943212 1.00428487]. \t  \u001b[92m-0.0029382945008131363\u001b[0m \t -0.0029382945008131363\n",
            "70     \t [1.0459632  1.11706143]. \t  -0.055115739225021804 \t -0.0029382945008131363\n",
            "71     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.0029382945008131363\n",
            "72     \t [1.01021726 0.99828821]. \t  -0.049613736625678376 \t -0.0029382945008131363\n",
            "73     \t [-0.88726298  1.60797499]. \t  -70.92307559057677 \t -0.0029382945008131363\n",
            "74     \t [-1.9100255   0.72643737]. \t  -862.1364214861749 \t -0.0029382945008131363\n",
            "75     \t [ 0.56475636 -1.4631005 ]. \t  -317.7597443470307 \t -0.0029382945008131363\n",
            "76     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.0029382945008131363\n",
            "77     \t [-0.50112241  0.72246393]. \t  -24.46953284373991 \t -0.0029382945008131363\n",
            "78     \t [ 2.0139713  -0.14158256]. \t  -1763.065573072827 \t -0.0029382945008131363\n",
            "79     \t [-0.22101996 -1.38284999]. \t  -206.46732600589422 \t -0.0029382945008131363\n",
            "80     \t [ 0.74454531 -1.06963287]. \t  -263.7965541835455 \t -0.0029382945008131363\n",
            "81     \t [1.58678507 2.0096641 ]. \t  -26.173355388676914 \t -0.0029382945008131363\n",
            "82     \t [0.91946644 0.84576379]. \t  -0.006497574702978802 \t -0.0029382945008131363\n",
            "83     \t [-0.06708015 -1.62779617]. \t  -267.5776570316121 \t -0.0029382945008131363\n",
            "84     \t [0.68517674 0.17783462]. \t  -8.604067714590572 \t -0.0029382945008131363\n",
            "85     \t [0.50152512 0.21268759]. \t  -0.3993306669650516 \t -0.0029382945008131363\n",
            "86     \t [-1.00569849  0.65747462]. \t  -16.551228310140736 \t -0.0029382945008131363\n",
            "87     \t [-1.69201713 -0.00320477]. \t  -828.7152142879839 \t -0.0029382945008131363\n",
            "88     \t [0.6276886  0.41282151]. \t  -0.17406713455960235 \t -0.0029382945008131363\n",
            "89     \t [-0.40366291 -0.83154994]. \t  -100.87203805527996 \t -0.0029382945008131363\n",
            "90     \t [-0.59352026 -0.2073231 ]. \t  -33.85333716244417 \t -0.0029382945008131363\n",
            "91     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.0029382945008131363\n",
            "92     \t [-0.83237225  0.07587352]. \t  -41.422790641693126 \t -0.0029382945008131363\n",
            "93     \t [-1.85018417  1.29794549]. \t  -459.7863391163911 \t -0.0029382945008131363\n",
            "94     \t [1.53159452 0.88600749]. \t  -213.37668811417538 \t -0.0029382945008131363\n",
            "95     \t [0.39809301 0.19335561]. \t  -0.48393645311580696 \t -0.0029382945008131363\n",
            "96     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.0029382945008131363\n",
            "97     \t [1.10675821 0.2211871 ]. \t  -100.75811175632232 \t -0.0029382945008131363\n",
            "98     \t [ 0.83029518 -0.30582201]. \t  -99.07350907469653 \t -0.0029382945008131363\n",
            "99     \t [-0.87332185 -0.51619905]. \t  -167.06532525331025 \t -0.0029382945008131363\n",
            "100    \t [-1.01396943 -1.98510142]. \t  -912.0148419647545 \t -0.0029382945008131363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "any0xrgYuvxA",
        "outputId": "ca20a828-511c-426f-d133-fb8a2f2fe1d7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.00524229 -1.02402267]. \t  -105.85741466051513 \t -21.690996320546372\n",
            "2      \t [0.28334438 0.52959337]. \t  \u001b[92m-20.70148287627886\u001b[0m \t -20.70148287627886\n",
            "3      \t [ 0.27674146 -0.25513394]. \t  \u001b[92m-11.526904015043828\u001b[0m \t -11.526904015043828\n",
            "4      \t [-2.02458606  0.19752199]. \t  -1531.2611682245536 \t -11.526904015043828\n",
            "5      \t [-0.55528626  0.46945129]. \t  \u001b[92m-5.014509081029686\u001b[0m \t -5.014509081029686\n",
            "6      \t [-0.82614884 -0.14332823]. \t  -71.53766591248872 \t -5.014509081029686\n",
            "7      \t [ 0.43855385 -1.96319402]. \t  -464.9433774043028 \t -5.014509081029686\n",
            "8      \t [0.78414794 0.79717424]. \t  \u001b[92m-3.369419575417047\u001b[0m \t -3.369419575417047\n",
            "9      \t [-0.88689921  0.4986182 ]. \t  -11.853175802432746 \t -3.369419575417047\n",
            "10     \t [-1.58662742 -0.14000879]. \t  -712.8656417917898 \t -3.369419575417047\n",
            "11     \t [ 0.3745089  -0.16836413]. \t  -9.915933842703954 \t -3.369419575417047\n",
            "12     \t [0.71345933 0.45055636]. \t  \u001b[92m-0.4239546400274934\u001b[0m \t -0.4239546400274934\n",
            "13     \t [ 0.38354624 -0.70581165]. \t  -73.12716070011288 \t -0.4239546400274934\n",
            "14     \t [0.27201405 1.74268868]. \t  -278.98494480655177 \t -0.4239546400274934\n",
            "15     \t [0.64689433 0.54217387]. \t  -1.654892284442969 \t -0.4239546400274934\n",
            "16     \t [0.5698142  0.32125695]. \t  \u001b[92m-0.1862371831330674\u001b[0m \t -0.1862371831330674\n",
            "17     \t [-0.48561958 -1.32420417]. \t  -245.5765963866809 \t -0.1862371831330674\n",
            "18     \t [1.28959433 1.43567933]. \t  -5.2537672055258255 \t -0.1862371831330674\n",
            "19     \t [1.27826058 1.87512765]. \t  -5.894089219687638 \t -0.1862371831330674\n",
            "20     \t [-0.69967795  0.38763968]. \t  -3.927460715735733 \t -0.1862371831330674\n",
            "21     \t [-0.74814622  0.52225416]. \t  -3.196404890376632 \t -0.1862371831330674\n",
            "22     \t [-1.09066884  1.80179203]. \t  -41.85388301514736 \t -0.1862371831330674\n",
            "23     \t [-1.35518327  2.01101538]. \t  -8.591692801280274 \t -0.1862371831330674\n",
            "24     \t [1.27997698 1.61664425]. \t  \u001b[92m-0.12546229776455867\u001b[0m \t -0.12546229776455867\n",
            "25     \t [0.2693997  1.07008029]. \t  -100.03521822356028 \t -0.12546229776455867\n",
            "26     \t [1.12976238 1.31710681]. \t  -0.18284384364625939 \t -0.12546229776455867\n",
            "27     \t [ 0.67906291 -1.46483459]. \t  -371.0355872844689 \t -0.12546229776455867\n",
            "28     \t [-1.99274381  0.30019054]. \t  -1356.4611941865257 \t -0.12546229776455867\n",
            "29     \t [-1.59732332  1.62182361]. \t  -93.16508290623544 \t -0.12546229776455867\n",
            "30     \t [ 0.9332122  -1.70126987]. \t  -661.6025332165561 \t -0.12546229776455867\n",
            "31     \t [ 0.79193639 -1.45821261]. \t  -434.9225345066252 \t -0.12546229776455867\n",
            "32     \t [-0.40342255 -1.92282347]. \t  -436.9311602155533 \t -0.12546229776455867\n",
            "33     \t [0.15850177 0.040358  ]. \t  -0.731330371519204 \t -0.12546229776455867\n",
            "34     \t [-1.22722795 -0.38785209]. \t  -363.66161351077665 \t -0.12546229776455867\n",
            "35     \t [-0.16110143 -0.04701505]. \t  -1.8805999225443046 \t -0.12546229776455867\n",
            "36     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.12546229776455867\n",
            "37     \t [1.52204548 1.99377915]. \t  -10.695309896775651 \t -0.12546229776455867\n",
            "38     \t [-1.12743965  1.91984752]. \t  -46.610718111637404 \t -0.12546229776455867\n",
            "39     \t [-1.69585202 -1.39566601]. \t  -1831.9072668479507 \t -0.12546229776455867\n",
            "40     \t [ 0.07572925 -2.0285615 ]. \t  -414.69046851086 \t -0.12546229776455867\n",
            "41     \t [-0.33921788  2.02645588]. \t  -367.13357261498413 \t -0.12546229776455867\n",
            "42     \t [1.51433978 2.00771194]. \t  -8.416314764337956 \t -0.12546229776455867\n",
            "43     \t [-0.88709547  0.86932047]. \t  -4.239810246533211 \t -0.12546229776455867\n",
            "44     \t [ 1.6790709  -1.97870545]. \t  -2302.5266957759495 \t -0.12546229776455867\n",
            "45     \t [-0.61286995 -1.65116907]. \t  -413.3845153643779 \t -0.12546229776455867\n",
            "46     \t [0.67686122 1.11927547]. \t  -43.81428231245156 \t -0.12546229776455867\n",
            "47     \t [-0.22066691 -0.38020652]. \t  -19.885583106297887 \t -0.12546229776455867\n",
            "48     \t [-1.55276497  2.04434078]. \t  -19.966304591923667 \t -0.12546229776455867\n",
            "49     \t [0.95730076 0.22697202]. \t  -47.53633027926535 \t -0.12546229776455867\n",
            "50     \t [-0.03078867 -0.10215075]. \t  -2.1254592892524284 \t -0.12546229776455867\n",
            "51     \t [1.23539397 1.46466466]. \t  -0.43404877478747406 \t -0.12546229776455867\n",
            "52     \t [-0.67517123 -1.00998659]. \t  -217.67570332419444 \t -0.12546229776455867\n",
            "53     \t [-1.27228905 -1.86451717]. \t  -1218.457017748896 \t -0.12546229776455867\n",
            "54     \t [ 0.82517819 -1.33677215]. \t  -407.1383404308359 \t -0.12546229776455867\n",
            "55     \t [-0.64063064 -1.4676662 ]. \t  -355.40779388843447 \t -0.12546229776455867\n",
            "56     \t [ 1.44253993 -0.83813951]. \t  -852.2875350360233 \t -0.12546229776455867\n",
            "57     \t [-1.20340356  0.65695278]. \t  -67.45905943685975 \t -0.12546229776455867\n",
            "58     \t [ 1.74567009 -1.89656171]. \t  -2444.796244157968 \t -0.12546229776455867\n",
            "59     \t [0.02346288 1.9066815 ]. \t  -364.2871597262516 \t -0.12546229776455867\n",
            "60     \t [0.00200424 1.56567155]. \t  -246.12747811354427 \t -0.12546229776455867\n",
            "61     \t [-0.78139034  0.53479172]. \t  -3.747599573318423 \t -0.12546229776455867\n",
            "62     \t [ 1.35482936 -0.40326941]. \t  -501.3627715286027 \t -0.12546229776455867\n",
            "63     \t [-0.19401555 -0.5657402 ]. \t  -37.83268513239098 \t -0.12546229776455867\n",
            "64     \t [-1.88164839 -0.66326923]. \t  -1775.5560972745068 \t -0.12546229776455867\n",
            "65     \t [-0.35956237  1.98789795]. \t  -347.292581304873 \t -0.12546229776455867\n",
            "66     \t [-1.5604096  -1.72930828]. \t  -1740.6005414601545 \t -0.12546229776455867\n",
            "67     \t [-0.51937017 -1.56237223]. \t  -337.97397823813054 \t -0.12546229776455867\n",
            "68     \t [ 1.47032429 -0.11759852]. \t  -519.8113640209032 \t -0.12546229776455867\n",
            "69     \t [1.29801797 1.061399  ]. \t  -38.95801006777799 \t -0.12546229776455867\n",
            "70     \t [ 1.27704013 -0.05560941]. \t  -284.48504812707824 \t -0.12546229776455867\n",
            "71     \t [-0.01455274  1.61486514]. \t  -261.7398640546982 \t -0.12546229776455867\n",
            "72     \t [-0.20521007 -0.20636756]. \t  -7.626699547511159 \t -0.12546229776455867\n",
            "73     \t [1.16736231 1.36956708]. \t  \u001b[92m-0.03267819678706106\u001b[0m \t -0.03267819678706106\n",
            "74     \t [1.14642933 1.31987865]. \t  \u001b[92m-0.024553460500846197\u001b[0m \t -0.024553460500846197\n",
            "75     \t [-1.33548973  0.16839956]. \t  -266.3200555030319 \t -0.024553460500846197\n",
            "76     \t [-1.13398124  1.03745487]. \t  -10.727043227510059 \t -0.024553460500846197\n",
            "77     \t [-1.27488388  0.72116571]. \t  -86.92620525398058 \t -0.024553460500846197\n",
            "78     \t [-0.0209748   0.02390357]. \t  -1.097443739468415 \t -0.024553460500846197\n",
            "79     \t [ 1.95609107 -0.37593049]. \t  -1766.7817341777504 \t -0.024553460500846197\n",
            "80     \t [0.3871508  0.14582211]. \t  -0.37723545293268795 \t -0.024553460500846197\n",
            "81     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.024553460500846197\n",
            "82     \t [ 1.40973898 -1.13316608]. \t  -973.9386714775475 \t -0.024553460500846197\n",
            "83     \t [ 1.67192921 -1.86254559]. \t  -2170.0480973974777 \t -0.024553460500846197\n",
            "84     \t [ 0.51738178 -0.67274971]. \t  -88.67445975258771 \t -0.024553460500846197\n",
            "85     \t [0.99410052 0.96151285]. \t  -0.07144665294882865 \t -0.024553460500846197\n",
            "86     \t [0.56832542 0.35812604]. \t  -0.3097705252017462 \t -0.024553460500846197\n",
            "87     \t [ 0.52141901 -1.73419679]. \t  -402.66255897447763 \t -0.024553460500846197\n",
            "88     \t [0.49652896 0.24105326]. \t  -0.256494624176258 \t -0.024553460500846197\n",
            "89     \t [0.12518397 0.9027846 ]. \t  -79.46235216551702 \t -0.024553460500846197\n",
            "90     \t [ 0.27307719 -1.38925742]. \t  -214.80782634268718 \t -0.024553460500846197\n",
            "91     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.024553460500846197\n",
            "92     \t [-0.07028959 -1.58607329]. \t  -254.27804647879904 \t -0.024553460500846197\n",
            "93     \t [-1.5765528  -1.26771701]. \t  -1415.3164776664685 \t -0.024553460500846197\n",
            "94     \t [-0.93985372  1.36091761]. \t  -26.572501212386467 \t -0.024553460500846197\n",
            "95     \t [0.93637046 0.87096123]. \t  \u001b[92m-0.007445752656953233\u001b[0m \t -0.007445752656953233\n",
            "96     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.007445752656953233\n",
            "97     \t [0.40249821 0.19455292]. \t  -0.4629463272319345 \t -0.007445752656953233\n",
            "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.007445752656953233\n",
            "99     \t [-1.45038161  1.64952399]. \t  -26.623491401680013 \t -0.007445752656953233\n",
            "100    \t [-1.31115942  1.94872672]. \t  -10.612508665163395 \t -0.007445752656953233\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLyKt6Quvzx",
        "outputId": "38873e2a-2c70-4aa2-8284-9199a3a84ad6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.55887491 0.71742793]. \t  -293.63381312089524 \t -31.22188590191926\n",
            "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -31.22188590191926\n",
            "3      \t [ 0.21488733 -0.51569643]. \t  -32.18652779140714 \t -31.22188590191926\n",
            "4      \t [ 0.61339166 -1.73957726]. \t  -447.82168087726546 \t -31.22188590191926\n",
            "5      \t [-0.38677072 -0.38895042]. \t  \u001b[92m-30.925882543990163\u001b[0m \t -30.925882543990163\n",
            "6      \t [-0.09514311 -0.41971964]. \t  \u001b[92m-19.58386869219287\u001b[0m \t -19.58386869219287\n",
            "7      \t [ 0.06662921 -0.56717313]. \t  -33.545275832149706 \t -19.58386869219287\n",
            "8      \t [ 0.61048488 -0.09264213]. \t  -21.805287136988486 \t -19.58386869219287\n",
            "9      \t [0.18589114 0.17076367]. \t  \u001b[92m-2.5180394115848017\u001b[0m \t -2.5180394115848017\n",
            "10     \t [-0.69124827  1.83963183]. \t  -188.31232989194442 \t -2.5180394115848017\n",
            "11     \t [-1.6839319   1.98420797]. \t  -79.69486468267328 \t -2.5180394115848017\n",
            "12     \t [ 1.93938365 -0.78303923]. \t  -2065.9015925754293 \t -2.5180394115848017\n",
            "13     \t [1.76975895 1.75670827]. \t  -189.74812303318149 \t -2.5180394115848017\n",
            "14     \t [0.44062187 0.15292891]. \t  \u001b[92m-0.48280225476979566\u001b[0m \t -0.48280225476979566\n",
            "15     \t [-0.46093919  0.32837807]. \t  -3.4779288173498664 \t -0.48280225476979566\n",
            "16     \t [ 0.34706976 -0.01589185]. \t  -2.2854301342126764 \t -0.48280225476979566\n",
            "17     \t [-0.17237737  0.18732588]. \t  -3.8586204313464103 \t -0.48280225476979566\n",
            "18     \t [-1.12306843  1.0736965 ]. \t  -8.026278095930593 \t -0.48280225476979566\n",
            "19     \t [ 0.59840127 -1.07659765]. \t  -205.99245030304718 \t -0.48280225476979566\n",
            "20     \t [-1.41432614  1.892902  ]. \t  -6.982799998635764 \t -0.48280225476979566\n",
            "21     \t [0.38686482 0.14356191]. \t  \u001b[92m-0.3796587751786932\u001b[0m \t -0.3796587751786932\n",
            "22     \t [2.03191803 1.35777373]. \t  -768.8630380181667 \t -0.3796587751786932\n",
            "23     \t [-1.98012465  2.04178599]. \t  -361.9857001577987 \t -0.3796587751786932\n",
            "24     \t [1.32580573 2.00071421]. \t  -6.0087842005780745 \t -0.3796587751786932\n",
            "25     \t [-1.08105505  0.86941199]. \t  -13.286925255926931 \t -0.3796587751786932\n",
            "26     \t [-0.64914997  0.7017468 ]. \t  -10.579370618711021 \t -0.3796587751786932\n",
            "27     \t [0.95225019 0.68052872]. \t  -5.121263638499245 \t -0.3796587751786932\n",
            "28     \t [1.68058629 1.72362185]. \t  -121.62790411263364 \t -0.3796587751786932\n",
            "29     \t [1.23913433 1.57968429]. \t  \u001b[92m-0.25281800814439054\u001b[0m \t -0.25281800814439054\n",
            "30     \t [1.06541298 1.14509619]. \t  \u001b[92m-0.01426160082522306\u001b[0m \t -0.01426160082522306\n",
            "31     \t [-1.48321878  2.03037352]. \t  -9.041585083180875 \t -0.01426160082522306\n",
            "32     \t [-1.40875573  1.6289175 ]. \t  -18.452590617769218 \t -0.01426160082522306\n",
            "33     \t [1.4422034  1.79262673]. \t  -8.451047777514885 \t -0.01426160082522306\n",
            "34     \t [1.74080353 1.54362102]. \t  -221.59905021424652 \t -0.01426160082522306\n",
            "35     \t [-1.33422055  1.95925773]. \t  -8.65674183883505 \t -0.01426160082522306\n",
            "36     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -0.01426160082522306\n",
            "37     \t [1.21769661 1.51443398]. \t  -0.14755748320459522 \t -0.01426160082522306\n",
            "38     \t [-0.35326329 -0.03337889]. \t  -4.333218142660941 \t -0.01426160082522306\n",
            "39     \t [-0.93743739 -1.51691736]. \t  -577.6944968849152 \t -0.01426160082522306\n",
            "40     \t [-1.2429361  -1.20925807]. \t  -763.5640063514635 \t -0.01426160082522306\n",
            "41     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -0.01426160082522306\n",
            "42     \t [-1.44728885  0.73940159]. \t  -189.65769942715474 \t -0.01426160082522306\n",
            "43     \t [-0.09976551  1.90814023]. \t  -361.5209006963194 \t -0.01426160082522306\n",
            "44     \t [1.15691345 1.27068825]. \t  -0.4837700167643405 \t -0.01426160082522306\n",
            "45     \t [ 0.36166798 -0.62458116]. \t  -57.46810040818697 \t -0.01426160082522306\n",
            "46     \t [1.75456992 1.83954575]. \t  -154.07400851032006 \t -0.01426160082522306\n",
            "47     \t [ 0.15483644 -1.32420254]. \t  -182.47238793527598 \t -0.01426160082522306\n",
            "48     \t [ 1.47443898 -1.28631293]. \t  -1197.5811107866384 \t -0.01426160082522306\n",
            "49     \t [-0.3415817  1.4835984]. \t  -188.6469634120826 \t -0.01426160082522306\n",
            "50     \t [ 0.80102667 -0.25698711]. \t  -80.79332915429475 \t -0.01426160082522306\n",
            "51     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -0.01426160082522306\n",
            "52     \t [-1.81682755 -1.25327162]. \t  -2081.9481213711797 \t -0.01426160082522306\n",
            "53     \t [-0.16982153  1.3650376 ]. \t  -179.91105836582113 \t -0.01426160082522306\n",
            "54     \t [-0.5832036  -0.39844562]. \t  -57.055402148986836 \t -0.01426160082522306\n",
            "55     \t [-1.9677794  0.3980393]. \t  -1215.756244906381 \t -0.01426160082522306\n",
            "56     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -0.01426160082522306\n",
            "57     \t [-1.9069261   0.97514391]. \t  -716.6611417020337 \t -0.01426160082522306\n",
            "58     \t [-1.39981897  1.24181038]. \t  -57.265984360090215 \t -0.01426160082522306\n",
            "59     \t [1.24381205 1.54594986]. \t  -0.05956942946075335 \t -0.01426160082522306\n",
            "60     \t [-1.12769857  1.00116836]. \t  -11.846057670026436 \t -0.01426160082522306\n",
            "61     \t [1.09944799 1.18010758]. \t  -0.09213443403798739 \t -0.01426160082522306\n",
            "62     \t [-1.8409285   0.12421796]. \t  -1073.9626300129419 \t -0.01426160082522306\n",
            "63     \t [0.95663178 1.56151027]. \t  -41.78076889114262 \t -0.01426160082522306\n",
            "64     \t [ 0.50275513 -1.72798608]. \t  -392.5838341975225 \t -0.01426160082522306\n",
            "65     \t [-1.34141841 -0.96070529]. \t  -767.3022143375099 \t -0.01426160082522306\n",
            "66     \t [1.13036921 1.22851349]. \t  -0.259267473751217 \t -0.01426160082522306\n",
            "67     \t [-1.90526878  1.91562327]. \t  -302.36618322749564 \t -0.01426160082522306\n",
            "68     \t [-1.8590975  -0.03748674]. \t  -1228.789541338643 \t -0.01426160082522306\n",
            "69     \t [1.50847337 0.24772284]. \t  -411.4432843590957 \t -0.01426160082522306\n",
            "70     \t [-0.03744522  1.13452397]. \t  -129.47279988809532 \t -0.01426160082522306\n",
            "71     \t [1.16794486 1.37444498]. \t  -0.0389172856546883 \t -0.01426160082522306\n",
            "72     \t [1.65108357 1.7624896 ]. \t  -93.27396602248619 \t -0.01426160082522306\n",
            "73     \t [-0.8309511   0.38541964]. \t  -12.65854777429919 \t -0.01426160082522306\n",
            "74     \t [-0.62039625 -0.28322156]. \t  -47.26318975428057 \t -0.01426160082522306\n",
            "75     \t [1.58093254 0.269114  ]. \t  -497.73172264831635 \t -0.01426160082522306\n",
            "76     \t [0.68126714 0.44826367]. \t  -0.12674855443165947 \t -0.01426160082522306\n",
            "77     \t [-1.10915018  0.20551825]. \t  -109.44867834043086 \t -0.01426160082522306\n",
            "78     \t [-1.01927465  1.69726372]. \t  -47.41900715611397 \t -0.01426160082522306\n",
            "79     \t [0.58698548 0.34859448]. \t  -0.1722152018197744 \t -0.01426160082522306\n",
            "80     \t [0.59269678 0.35491972]. \t  -0.16721378205504453 \t -0.01426160082522306\n",
            "81     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.01426160082522306\n",
            "82     \t [1.94725409 1.95571794]. \t  -338.01646995656824 \t -0.01426160082522306\n",
            "83     \t [ 0.75218183 -1.40652874]. \t  -389.0606101266476 \t -0.01426160082522306\n",
            "84     \t [-1.60862375  1.17313899]. \t  -206.89481918097397 \t -0.01426160082522306\n",
            "85     \t [1.942115   1.69760421]. \t  -431.12082590076807 \t -0.01426160082522306\n",
            "86     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.01426160082522306\n",
            "87     \t [-0.45545945  1.64796546]. \t  -209.62876938362476 \t -0.01426160082522306\n",
            "88     \t [-0.56896611 -0.73121945]. \t  -113.7518930266233 \t -0.01426160082522306\n",
            "89     \t [1.27427815 0.38180957]. \t  -154.3254793068761 \t -0.01426160082522306\n",
            "90     \t [0.0154251  1.38924189]. \t  -193.90258558929708 \t -0.01426160082522306\n",
            "91     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.01426160082522306\n",
            "92     \t [-1.00700917 -1.26368828]. \t  -522.8452122917694 \t -0.01426160082522306\n",
            "93     \t [0.16015336 0.24262124]. \t  -5.413033502014389 \t -0.01426160082522306\n",
            "94     \t [-0.29640351  0.64054732]. \t  -32.22753711685563 \t -0.01426160082522306\n",
            "95     \t [1.23574539 1.50854881]. \t  -0.08986700376529747 \t -0.01426160082522306\n",
            "96     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.01426160082522306\n",
            "97     \t [-0.32671966 -0.20386439]. \t  -11.40804985655089 \t -0.01426160082522306\n",
            "98     \t [1.39696587 0.04097297]. \t  -365.1741489099554 \t -0.01426160082522306\n",
            "99     \t [1.676243   1.94336126]. \t  -75.52728652939464 \t -0.01426160082522306\n",
            "100    \t [0.53826131 1.57679642]. \t  -165.8684252955217 \t -0.01426160082522306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3FtfYSuv2u",
        "outputId": "d9aedf4d-88e8-41fd-a164-6b1c473f6b3f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-0.58320207  0.31529626]. \t  -2.568173693939569 \t -1.7663579664225912\n",
            "2      \t [-0.41546317  0.04330888]. \t  -3.6754048035136746 \t -1.7663579664225912\n",
            "3      \t [-1.12263293  1.38524995]. \t  -6.066702457109956 \t -1.7663579664225912\n",
            "4      \t [-0.10380112 -0.13722935]. \t  -3.408895895509912 \t -1.7663579664225912\n",
            "5      \t [-1.71804446  0.18584208]. \t  -772.3719250879412 \t -1.7663579664225912\n",
            "6      \t [-0.10811722  1.36311035]. \t  -183.861800057421 \t -1.7663579664225912\n",
            "7      \t [ 0.67485054 -1.07891608]. \t  -235.5254396629436 \t -1.7663579664225912\n",
            "8      \t [1.91020547 2.03270926]. \t  -262.0308528592806 \t -1.7663579664225912\n",
            "9      \t [1.13492645 1.56602775]. \t  -7.744921575890184 \t -1.7663579664225912\n",
            "10     \t [1.16074401 1.88212976]. \t  -28.62727440689314 \t -1.7663579664225912\n",
            "11     \t [-0.68422577  0.8958773 ]. \t  -21.130406016018057 \t -1.7663579664225912\n",
            "12     \t [-1.16442929  2.02863488]. \t  -49.942570774830614 \t -1.7663579664225912\n",
            "13     \t [-1.41801431  1.68903809]. \t  -16.197586074982272 \t -1.7663579664225912\n",
            "14     \t [-0.04916436  0.0070316 ]. \t  \u001b[92m-1.1028751860517767\u001b[0m \t -1.1028751860517767\n",
            "15     \t [1.35490177 1.61598875]. \t  -4.955843025791601 \t -1.1028751860517767\n",
            "16     \t [0.55711908 0.55600097]. \t  -6.229027724007447 \t -1.1028751860517767\n",
            "17     \t [1.31628439 1.95316562]. \t  -4.9647523273726835 \t -1.1028751860517767\n",
            "18     \t [0.00842168 1.21627265]. \t  -148.89789227870298 \t -1.1028751860517767\n",
            "19     \t [1.30300322 1.68223859]. \t  \u001b[92m-0.11608080226882157\u001b[0m \t -0.11608080226882157\n",
            "20     \t [-1.51317726  0.21494298]. \t  -436.7799826043828 \t -0.11608080226882157\n",
            "21     \t [-1.07348565  0.76442272]. \t  -19.349764600408005 \t -0.11608080226882157\n",
            "22     \t [-0.92796928  0.87533646]. \t  -3.737256459457311 \t -0.11608080226882157\n",
            "23     \t [-0.75276073  0.31485135]. \t  -9.412361479466325 \t -0.11608080226882157\n",
            "24     \t [-2.00786669 -1.57681342]. \t  -3154.39735885571 \t -0.11608080226882157\n",
            "25     \t [1.58898668 1.23518348]. \t  -166.6782747254324 \t -0.11608080226882157\n",
            "26     \t [-1.02894882 -0.67536524]. \t  -304.8272344430442 \t -0.11608080226882157\n",
            "27     \t [ 1.87622405 -1.27367586]. \t  -2298.9083574649876 \t -0.11608080226882157\n",
            "28     \t [-0.02242382 -1.69686183]. \t  -289.1500294048444 \t -0.11608080226882157\n",
            "29     \t [ 1.30212754 -1.04704589]. \t  -752.2668928051232 \t -0.11608080226882157\n",
            "30     \t [0.7802465  0.85355589]. \t  -6.039589991700005 \t -0.11608080226882157\n",
            "31     \t [1.32346668 1.7576601 ]. \t  \u001b[92m-0.10834685842696845\u001b[0m \t -0.10834685842696845\n",
            "32     \t [-0.51379973 -1.63690855]. \t  -363.6331805481531 \t -0.10834685842696845\n",
            "33     \t [-0.3086102 -1.5223783]. \t  -263.3814376804697 \t -0.10834685842696845\n",
            "34     \t [1.33718159 1.80549516]. \t  -0.14410867860741455 \t -0.10834685842696845\n",
            "35     \t [0.54120142 1.47148362]. \t  -139.1166704926515 \t -0.10834685842696845\n",
            "36     \t [-0.74895875  1.57222134]. \t  -105.32801185836587 \t -0.10834685842696845\n",
            "37     \t [-1.83696341 -1.25620347]. \t  -2152.32922596957 \t -0.10834685842696845\n",
            "38     \t [ 0.55381456 -0.0075814 ]. \t  -10.077025404458436 \t -0.10834685842696845\n",
            "39     \t [-1.88536862  2.00180992]. \t  -249.44566312940998 \t -0.10834685842696845\n",
            "40     \t [-1.44206756 -1.21484429]. \t  -1091.2728928664756 \t -0.10834685842696845\n",
            "41     \t [1.068591   0.91240867]. \t  -5.270722615947526 \t -0.10834685842696845\n",
            "42     \t [0.98533775 1.08237889]. \t  -1.2431813152003675 \t -0.10834685842696845\n",
            "43     \t [0.51472498 0.82805609]. \t  -31.945262563754127 \t -0.10834685842696845\n",
            "44     \t [1.24580237 1.51411993]. \t  -0.20408720984367934 \t -0.10834685842696845\n",
            "45     \t [ 0.34533537 -0.98658915]. \t  -122.71805110077798 \t -0.10834685842696845\n",
            "46     \t [1.04666315 1.09347922]. \t  \u001b[92m-0.0025873203936246283\u001b[0m \t -0.0025873203936246283\n",
            "47     \t [0.94765762 1.20817214]. \t  -9.620005128772117 \t -0.0025873203936246283\n",
            "48     \t [-0.33344666  1.70336935]. \t  -255.28264766624903 \t -0.0025873203936246283\n",
            "49     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.0025873203936246283\n",
            "50     \t [0.97561873 0.96841336]. \t  -0.02808890586959634 \t -0.0025873203936246283\n",
            "51     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.0025873203936246283\n",
            "52     \t [1.61404428 1.32600028]. \t  -163.99662342852554 \t -0.0025873203936246283\n",
            "53     \t [-1.94029947 -0.86847316]. \t  -2155.3322056729285 \t -0.0025873203936246283\n",
            "54     \t [0.07464172 1.83765711]. \t  -336.51009897136794 \t -0.0025873203936246283\n",
            "55     \t [0.51311545 0.0998633 ]. \t  -2.9078022004534447 \t -0.0025873203936246283\n",
            "56     \t [-0.9724487  -0.53938542]. \t  -224.42549533519733 \t -0.0025873203936246283\n",
            "57     \t [1.03697633 1.0319875 ]. \t  -0.18913702011945493 \t -0.0025873203936246283\n",
            "58     \t [1.00578573 0.99634078]. \t  -0.02333288467085501 \t -0.0025873203936246283\n",
            "59     \t [-0.6656559   1.80874007]. \t  -189.27229594052525 \t -0.0025873203936246283\n",
            "60     \t [ 0.66227292 -0.70944906]. \t  -131.9169686429098 \t -0.0025873203936246283\n",
            "61     \t [ 1.91918506 -0.9411309 ]. \t  -2139.354445757398 \t -0.0025873203936246283\n",
            "62     \t [ 0.61530074 -0.04220243]. \t  -17.855040930280474 \t -0.0025873203936246283\n",
            "63     \t [-1.26278418  1.72912074]. \t  -6.929132761748157 \t -0.0025873203936246283\n",
            "64     \t [ 0.59748633 -1.25152333]. \t  -258.89350497352297 \t -0.0025873203936246283\n",
            "65     \t [-1.3258731  -1.69927707]. \t  -1200.6443181943291 \t -0.0025873203936246283\n",
            "66     \t [0.33361894 0.10301576]. \t  -0.45092921942802 \t -0.0025873203936246283\n",
            "67     \t [0.36434834 0.14779918]. \t  -0.4267016700267161 \t -0.0025873203936246283\n",
            "68     \t [1.25395039 1.49810155]. \t  -0.6163916927328872 \t -0.0025873203936246283\n",
            "69     \t [-1.26602996  1.28388001]. \t  -15.30791956683049 \t -0.0025873203936246283\n",
            "70     \t [-1.92633866 -2.01001904]. \t  -3281.3183646001717 \t -0.0025873203936246283\n",
            "71     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.0025873203936246283\n",
            "72     \t [-0.84373096  1.94458006]. \t  -155.35381258533093 \t -0.0025873203936246283\n",
            "73     \t [0.67009245 0.40201575]. \t  -0.32981548390297916 \t -0.0025873203936246283\n",
            "74     \t [-0.78666941 -1.10787842]. \t  -301.3508649798184 \t -0.0025873203936246283\n",
            "75     \t [0.33148238 0.90152338]. \t  -63.11674969328898 \t -0.0025873203936246283\n",
            "76     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.0025873203936246283\n",
            "77     \t [ 0.11668496 -1.96729657]. \t  -393.1814597329686 \t -0.0025873203936246283\n",
            "78     \t [-0.88247338  2.04742146]. \t  -164.49408042071573 \t -0.0025873203936246283\n",
            "79     \t [-0.43549238 -2.01846151]. \t  -489.6378795741759 \t -0.0025873203936246283\n",
            "80     \t [-0.01091106  0.37740414]. \t  -15.256345137726763 \t -0.0025873203936246283\n",
            "81     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.0025873203936246283\n",
            "82     \t [ 1.26996063 -1.6710591 ]. \t  -1078.4459340400506 \t -0.0025873203936246283\n",
            "83     \t [0.03857045 1.98165671]. \t  -393.0312840335908 \t -0.0025873203936246283\n",
            "84     \t [1.20764837 0.79814724]. \t  -43.63841506633272 \t -0.0025873203936246283\n",
            "85     \t [ 0.38269592 -1.23938671]. \t  -192.4371118418537 \t -0.0025873203936246283\n",
            "86     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.0025873203936246283\n",
            "87     \t [-0.11353625  0.93397012]. \t  -86.07873293752077 \t -0.0025873203936246283\n",
            "88     \t [1.37470161 1.92493544]. \t  -0.2638195169756034 \t -0.0025873203936246283\n",
            "89     \t [-0.52540106 -1.59164868]. \t  -351.1552930777544 \t -0.0025873203936246283\n",
            "90     \t [-1.89079297 -1.36673699]. \t  -2450.53005067672 \t -0.0025873203936246283\n",
            "91     \t [-0.2896775 -0.7808088]. \t  -76.43765662694076 \t -0.0025873203936246283\n",
            "92     \t [-1.14294969 -0.48032313]. \t  -323.8066030564311 \t -0.0025873203936246283\n",
            "93     \t [-0.87056293 -0.71276528]. \t  -219.7787060538371 \t -0.0025873203936246283\n",
            "94     \t [1.58263691 1.77828318]. \t  -53.11335976600227 \t -0.0025873203936246283\n",
            "95     \t [-0.3000354 -0.4367719]. \t  -29.441193255529072 \t -0.0025873203936246283\n",
            "96     \t [ 1.6314697  -1.10691842]. \t  -1420.642250098064 \t -0.0025873203936246283\n",
            "97     \t [-0.58438823 -2.01372533]. \t  -557.2234469458028 \t -0.0025873203936246283\n",
            "98     \t [-0.97394672  1.79359404]. \t  -75.30265313975646 \t -0.0025873203936246283\n",
            "99     \t [ 2.00177263 -1.26549273]. \t  -2781.0202770904502 \t -0.0025873203936246283\n",
            "100    \t [-0.20113591  1.12016858]. \t  -118.02072811771028 \t -0.0025873203936246283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YT-CgKvuv4q",
        "outputId": "b914d9b5-45c8-45ea-de80-28cab8785893"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.44879336 -0.60137985]. \t  -64.7518629389779 \t -4.219752052396591\n",
            "2      \t [-1.93213315  1.53183724]. \t  -493.17013433001347 \t -4.219752052396591\n",
            "3      \t [-0.99094266  0.6472446 ]. \t  -15.167785351738338 \t -4.219752052396591\n",
            "4      \t [1.35520468 1.21728438]. \t  -38.478841098465224 \t -4.219752052396591\n",
            "5      \t [-0.04916283 -0.21086558]. \t  -5.649687854516544 \t -4.219752052396591\n",
            "6      \t [-0.3140428 -1.4615883]. \t  -245.15259928896978 \t -4.219752052396591\n",
            "7      \t [-0.20612682 -0.18611961]. \t  -6.680898035500192 \t -4.219752052396591\n",
            "8      \t [0.91567107 0.6382808 ]. \t  \u001b[92m-4.014023004638996\u001b[0m \t -4.014023004638996\n",
            "9      \t [-1.10758875  0.96394828]. \t  -11.348553886886418 \t -4.014023004638996\n",
            "10     \t [0.86815397 0.92524128]. \t  \u001b[92m-2.9603221357558596\u001b[0m \t -2.9603221357558596\n",
            "11     \t [1.11054359 1.76260628]. \t  -28.027985863830807 \t -2.9603221357558596\n",
            "12     \t [0.81141591 0.67167584]. \t  \u001b[92m-0.05319994865848465\u001b[0m \t -0.05319994865848465\n",
            "13     \t [0.45814857 0.22963014]. \t  -0.33253037189838713 \t -0.05319994865848465\n",
            "14     \t [-0.93491307  0.81555679]. \t  -4.086179869446733 \t -0.05319994865848465\n",
            "15     \t [0.68664822 0.45258843]. \t  -0.13390032835777083 \t -0.05319994865848465\n",
            "16     \t [0.52681058 0.18952188]. \t  -0.9984404217158397 \t -0.05319994865848465\n",
            "17     \t [-1.29377738 -1.90212913]. \t  -1284.0311715010898 \t -0.05319994865848465\n",
            "18     \t [ 1.02583277 -1.1838309 ]. \t  -500.0435106849533 \t -0.05319994865848465\n",
            "19     \t [-0.76618822 -0.19049423]. \t  -63.576049891706525 \t -0.05319994865848465\n",
            "20     \t [-1.58996028  0.63482111]. \t  -365.1105597023613 \t -0.05319994865848465\n",
            "21     \t [1.82696294 0.65763578]. \t  -719.0084573138294 \t -0.05319994865848465\n",
            "22     \t [-1.41872827 -1.02009759]. \t  -925.6909054350544 \t -0.05319994865848465\n",
            "23     \t [1.45933798 1.02393232]. \t  -122.47598556447716 \t -0.05319994865848465\n",
            "24     \t [0.93126114 0.80819819]. \t  -0.3534048774597986 \t -0.05319994865848465\n",
            "25     \t [ 1.38242446 -0.1308308 ]. \t  -417.0933217978137 \t -0.05319994865848465\n",
            "26     \t [-1.72949124  0.56882801]. \t  -594.2096278065685 \t -0.05319994865848465\n",
            "27     \t [-1.2273021   1.94270242]. \t  -24.00816191790067 \t -0.05319994865848465\n",
            "28     \t [-1.63856669  0.69331682]. \t  -403.60271179563733 \t -0.05319994865848465\n",
            "29     \t [-0.11739086  0.21165952]. \t  -5.164168523825192 \t -0.05319994865848465\n",
            "30     \t [-0.20962842  0.03663851]. \t  -1.468538038697957 \t -0.05319994865848465\n",
            "31     \t [-1.53636148  0.48205596]. \t  -359.2532426805832 \t -0.05319994865848465\n",
            "32     \t [-0.62571956  1.4256426 ]. \t  -109.58289262533435 \t -0.05319994865848465\n",
            "33     \t [-0.61375131  0.89657007]. \t  -29.631652197295683 \t -0.05319994865848465\n",
            "34     \t [0.88046013 0.77666543]. \t  \u001b[92m-0.014501597189376462\u001b[0m \t -0.014501597189376462\n",
            "35     \t [2.0381793 0.72216  ]. \t  -1178.9504173057226 \t -0.014501597189376462\n",
            "36     \t [0.55869531 0.78936277]. \t  -22.968863699515406 \t -0.014501597189376462\n",
            "37     \t [ 1.33652762 -1.41097198]. \t  -1022.3719497808307 \t -0.014501597189376462\n",
            "38     \t [0.42390267 0.25762897]. \t  -0.9392823088151316 \t -0.014501597189376462\n",
            "39     \t [1.17092552 1.57273923]. \t  -4.096401225250451 \t -0.014501597189376462\n",
            "40     \t [1.01753333 1.11366353]. \t  -0.6132313974416519 \t -0.014501597189376462\n",
            "41     \t [1.26638686 1.63153333]. \t  -0.14823291444888886 \t -0.014501597189376462\n",
            "42     \t [1.38714819 2.01076668]. \t  -0.8996071631917575 \t -0.014501597189376462\n",
            "43     \t [ 0.25895027 -1.95669107]. \t  -410.1040674264351 \t -0.014501597189376462\n",
            "44     \t [1.07866545 1.15478931]. \t  \u001b[92m-0.013809277528234323\u001b[0m \t -0.013809277528234323\n",
            "45     \t [1.3360381  1.80830428]. \t  -0.16724075310493375 \t -0.013809277528234323\n",
            "46     \t [1.05748504 1.03830707]. \t  -0.6427853423117403 \t -0.013809277528234323\n",
            "47     \t [0.79281754 0.70307785]. \t  -0.5982207655329881 \t -0.013809277528234323\n",
            "48     \t [1.08573381 1.20856769]. \t  -0.09585522343538208 \t -0.013809277528234323\n",
            "49     \t [0.99195661 0.97075698]. \t  -0.017543984861520867 \t -0.013809277528234323\n",
            "50     \t [ 1.02841419 -1.36419327]. \t  -586.5263844771054 \t -0.013809277528234323\n",
            "51     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.013809277528234323\n",
            "52     \t [-0.99195143 -0.35693566]. \t  -183.77003696304735 \t -0.013809277528234323\n",
            "53     \t [1.42875409 0.43741702]. \t  -257.4401579992734 \t -0.013809277528234323\n",
            "54     \t [1.50216865 0.12571883]. \t  -454.2795524743432 \t -0.013809277528234323\n",
            "55     \t [1.0164898  1.00177992]. \t  -0.09931801295078485 \t -0.013809277528234323\n",
            "56     \t [-2.02404919  0.73184435]. \t  -1141.4207830370153 \t -0.013809277528234323\n",
            "57     \t [-0.02668429  1.56088139]. \t  -244.46691702906102 \t -0.013809277528234323\n",
            "58     \t [-0.57675366  0.36187731]. \t  -2.5716061419430685 \t -0.013809277528234323\n",
            "59     \t [-1.92473422 -1.63865087]. \t  -2863.5889997460195 \t -0.013809277528234323\n",
            "60     \t [0.35067458 0.09349077]. \t  -0.5085416832591348 \t -0.013809277528234323\n",
            "61     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.013809277528234323\n",
            "62     \t [-1.0803744   0.18974846]. \t  -99.8708387620765 \t -0.013809277528234323\n",
            "63     \t [-1.07883665  0.83854152]. \t  -14.906629231210427 \t -0.013809277528234323\n",
            "64     \t [-1.78051982  0.0386201 ]. \t  -988.4423965795083 \t -0.013809277528234323\n",
            "65     \t [-1.74403802 -1.30179646]. \t  -1894.098633935269 \t -0.013809277528234323\n",
            "66     \t [-0.02586991 -1.69586234]. \t  -288.8743529540324 \t -0.013809277528234323\n",
            "67     \t [ 1.84141782 -0.56448018]. \t  -1565.1476106907987 \t -0.013809277528234323\n",
            "68     \t [-1.24765867  1.57470105]. \t  -5.084545727884569 \t -0.013809277528234323\n",
            "69     \t [ 0.77466228 -0.26976528]. \t  -75.71762526666582 \t -0.013809277528234323\n",
            "70     \t [-1.77269767 -1.13558288]. \t  -1837.8503994612217 \t -0.013809277528234323\n",
            "71     \t [0.65205829 0.43189422]. \t  -0.12557148555833916 \t -0.013809277528234323\n",
            "72     \t [0.97955475 0.94901259]. \t  \u001b[92m-0.011474348161723396\u001b[0m \t -0.011474348161723396\n",
            "73     \t [-0.41998961 -1.70756351]. \t  -356.9449335463213 \t -0.011474348161723396\n",
            "74     \t [0.56254087 0.32709241]. \t  -0.2026918237776151 \t -0.011474348161723396\n",
            "75     \t [ 1.54651295 -0.94071756]. \t  -1110.8008886824762 \t -0.011474348161723396\n",
            "76     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.011474348161723396\n",
            "77     \t [-1.2738401   0.34097693]. \t  -169.44370171654418 \t -0.011474348161723396\n",
            "78     \t [-0.25579785 -1.91213514]. \t  -392.65441879290506 \t -0.011474348161723396\n",
            "79     \t [-1.01193613  1.27793013]. \t  -10.4951895211278 \t -0.011474348161723396\n",
            "80     \t [-0.04833811  0.43081048]. \t  -19.4580015708361 \t -0.011474348161723396\n",
            "81     \t [0.14672588 0.87963436]. \t  -74.36264551014955 \t -0.011474348161723396\n",
            "82     \t [ 1.59712897 -0.86236332]. \t  -1165.3392525757588 \t -0.011474348161723396\n",
            "83     \t [-0.98486741  0.68231356]. \t  -12.213965271784032 \t -0.011474348161723396\n",
            "84     \t [-1.81862779  1.1758283 ]. \t  -462.3074627004391 \t -0.011474348161723396\n",
            "85     \t [-0.24366068  1.84452526]. \t  -320.2244345913622 \t -0.011474348161723396\n",
            "86     \t [-1.22222237  1.39634206]. \t  -5.88861363497911 \t -0.011474348161723396\n",
            "87     \t [-0.92145058 -1.64561721]. \t  -626.0389838876489 \t -0.011474348161723396\n",
            "88     \t [-0.14002292 -1.35041328]. \t  -188.99504911871696 \t -0.011474348161723396\n",
            "89     \t [-0.57263001  0.90454645]. \t  -35.72468675050905 \t -0.011474348161723396\n",
            "90     \t [-0.52357218  0.25964062]. \t  -2.342260135583863 \t -0.011474348161723396\n",
            "91     \t [0.01824425 1.73847894]. \t  -303.07902757254914 \t -0.011474348161723396\n",
            "92     \t [1.81914489 1.51624507]. \t  -322.17134307118727 \t -0.011474348161723396\n",
            "93     \t [ 0.88411369 -0.31217853]. \t  -119.66104849291484 \t -0.011474348161723396\n",
            "94     \t [0.88798542 0.19356752]. \t  -35.40916619948873 \t -0.011474348161723396\n",
            "95     \t [1.1443104  1.27560912]. \t  -0.1353208221864878 \t -0.011474348161723396\n",
            "96     \t [1.12250257 1.25962573]. \t  -0.015021802800380481 \t -0.011474348161723396\n",
            "97     \t [1.22201364 0.65736029]. \t  -69.93170963058354 \t -0.011474348161723396\n",
            "98     \t [1.38475914 1.86108684]. \t  -0.4669372748868132 \t -0.011474348161723396\n",
            "99     \t [-2.02472409  1.55710344]. \t  -655.530863928991 \t -0.011474348161723396\n",
            "100    \t [1.42397679 1.96615171]. \t  -0.5586974238059069 \t -0.011474348161723396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHz_Jg2_uv7E",
        "outputId": "5fc19031-429a-410e-dc6d-1d979cdc6772"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.11850304  0.1054049 ]. \t  \u001b[92m-2.085749265976485\u001b[0m \t -2.085749265976485\n",
            "2      \t [1.2681139  0.97931402]. \t  -39.6106834842352 \t -2.085749265976485\n",
            "3      \t [ 1.13066201 -0.90422643]. \t  -476.40139579442047 \t -2.085749265976485\n",
            "4      \t [-0.02763345  1.3175444 ]. \t  -174.44719684091402 \t -2.085749265976485\n",
            "5      \t [0.74587515 0.4967668 ]. \t  \u001b[92m-0.41935377607332613\u001b[0m \t -0.41935377607332613\n",
            "6      \t [2.048 2.048]. \t  -461.7603900415999 \t -0.41935377607332613\n",
            "7      \t [0.3591695  0.55704983]. \t  -18.733095689596613 \t -0.41935377607332613\n",
            "8      \t [-0.31361066  0.29113042]. \t  -5.441938578919002 \t -0.41935377607332613\n",
            "9      \t [1.3771472 1.6997598]. \t  -4.0142649271370585 \t -0.41935377607332613\n",
            "10     \t [0.94764389 1.48542769]. \t  -34.50646971735857 \t -0.41935377607332613\n",
            "11     \t [-0.39550202  0.0502172 ]. \t  -3.075368659588362 \t -0.41935377607332613\n",
            "12     \t [-0.40601446  0.50423466]. \t  -13.495224878025038 \t -0.41935377607332613\n",
            "13     \t [1.11214918 1.89996467]. \t  -43.98126255611708 \t -0.41935377607332613\n",
            "14     \t [ 0.00142705 -0.05817853]. \t  -1.3356457811281088 \t -0.41935377607332613\n",
            "15     \t [1.14743635 1.26072963]. \t  \u001b[92m-0.33400092412436705\u001b[0m \t -0.33400092412436705\n",
            "16     \t [1.21964444 1.42368549]. \t  -0.45588842552745246 \t -0.33400092412436705\n",
            "17     \t [0.82030154 0.74175514]. \t  -0.5064686140579225 \t -0.33400092412436705\n",
            "18     \t [1.20289267 1.5749887 ]. \t  -1.6805362330415001 \t -0.33400092412436705\n",
            "19     \t [0.6718327  0.56598835]. \t  -1.421678326691866 \t -0.33400092412436705\n",
            "20     \t [0.88831376 0.92731154]. \t  -1.9226798199039434 \t -0.33400092412436705\n",
            "21     \t [-0.48316149  0.04533448]. \t  -5.738325805710129 \t -0.33400092412436705\n",
            "22     \t [-0.19580929  0.08150855]. \t  -1.616301184436927 \t -0.33400092412436705\n",
            "23     \t [ 1.66764852 -1.71888722]. \t  -2025.3906879916842 \t -0.33400092412436705\n",
            "24     \t [0.44579746 0.08646141]. \t  -1.5676849181376975 \t -0.33400092412436705\n",
            "25     \t [0.6824056  0.48352447]. \t  \u001b[92m-0.13271798563162657\u001b[0m \t -0.13271798563162657\n",
            "26     \t [ 0.8501451  -0.35546572]. \t  -116.27665665103598 \t -0.13271798563162657\n",
            "27     \t [ 0.14364922 -0.89507096]. \t  -84.58509496095981 \t -0.13271798563162657\n",
            "28     \t [1.80318636 1.94411307]. \t  -171.56621188478246 \t -0.13271798563162657\n",
            "29     \t [1.14353916 1.33406946]. \t  \u001b[92m-0.09023427003469331\u001b[0m \t -0.09023427003469331\n",
            "30     \t [-1.1342216  -0.37006358]. \t  -278.9614916305373 \t -0.09023427003469331\n",
            "31     \t [0.43348374 0.24789285]. \t  -0.6807571136465769 \t -0.09023427003469331\n",
            "32     \t [1.20459609 1.79563618]. \t  -11.915702706226021 \t -0.09023427003469331\n",
            "33     \t [0.60221939 0.39081648]. \t  -0.23746200343372795 \t -0.09023427003469331\n",
            "34     \t [1.40758029 1.97051499]. \t  -0.17771510220570325 \t -0.09023427003469331\n",
            "35     \t [-0.52674853  0.54617333]. \t  -9.55143065929903 \t -0.09023427003469331\n",
            "36     \t [-1.16911537  1.74685832]. \t  -19.147156523774903 \t -0.09023427003469331\n",
            "37     \t [1.32115648 1.18152797]. \t  -31.904447673878682 \t -0.09023427003469331\n",
            "38     \t [-0.3040362   0.76544083]. \t  -46.9937903703847 \t -0.09023427003469331\n",
            "39     \t [-0.14460198  1.34721041]. \t  -177.21746209505548 \t -0.09023427003469331\n",
            "40     \t [-1.63983861  1.94188887]. \t  -62.796810792108566 \t -0.09023427003469331\n",
            "41     \t [ 0.53483739 -1.11396349]. \t  -196.22044177322152 \t -0.09023427003469331\n",
            "42     \t [-0.4068628  -1.28073464]. \t  -211.14952512344544 \t -0.09023427003469331\n",
            "43     \t [-1.19413102  1.98814072]. \t  -36.42017551271222 \t -0.09023427003469331\n",
            "44     \t [0.60187974 0.34939177]. \t  -0.1750568811306341 \t -0.09023427003469331\n",
            "45     \t [0.47964708 1.26569985]. \t  -107.52548316444184 \t -0.09023427003469331\n",
            "46     \t [-1.50941603 -1.10529415]. \t  -1151.1929674840562 \t -0.09023427003469331\n",
            "47     \t [ 1.27091858 -1.70898462]. \t  -1105.116363693183 \t -0.09023427003469331\n",
            "48     \t [1.51949793 0.87348176]. \t  -206.30495661161856 \t -0.09023427003469331\n",
            "49     \t [-1.86573362  0.37119973]. \t  -975.2745320400953 \t -0.09023427003469331\n",
            "50     \t [0.67134476 0.46935239]. \t  -0.14279133921161213 \t -0.09023427003469331\n",
            "51     \t [-0.71458979  0.46304692]. \t  -3.1663146194232707 \t -0.09023427003469331\n",
            "52     \t [0.88438536 0.77607483]. \t  \u001b[92m-0.017042291439201326\u001b[0m \t -0.017042291439201326\n",
            "53     \t [1.35782616 1.83538385]. \t  -0.13494191647085724 \t -0.017042291439201326\n",
            "54     \t [0.46888773 1.61398374]. \t  -194.64137800239578 \t -0.017042291439201326\n",
            "55     \t [ 1.15212581 -1.08914448]. \t  -583.9889106648615 \t -0.017042291439201326\n",
            "56     \t [0.87455881 0.77278239]. \t  -0.02202283006239304 \t -0.017042291439201326\n",
            "57     \t [0.97825923 0.93755616]. \t  -0.03824441295259288 \t -0.017042291439201326\n",
            "58     \t [-1.66676799  0.66994422]. \t  -451.5502786295779 \t -0.017042291439201326\n",
            "59     \t [-0.99195854  0.95801465]. \t  -4.03532782928426 \t -0.017042291439201326\n",
            "60     \t [-0.64931251 -0.54001115]. \t  -95.19112707327969 \t -0.017042291439201326\n",
            "61     \t [ 1.9065061  -0.75279122]. \t  -1925.8871723897323 \t -0.017042291439201326\n",
            "62     \t [-0.94001591  2.04528928]. \t  -138.70891112332072 \t -0.017042291439201326\n",
            "63     \t [ 0.57187407 -1.66012158]. \t  -395.0643876039098 \t -0.017042291439201326\n",
            "64     \t [0.44374421 1.12386373]. \t  -86.23394152062177 \t -0.017042291439201326\n",
            "65     \t [1.18674958 1.09267993]. \t  -10.00118618641607 \t -0.017042291439201326\n",
            "66     \t [-0.91082315 -1.26232069]. \t  -441.26396375882774 \t -0.017042291439201326\n",
            "67     \t [0.0821836  0.58251462]. \t  -33.99239946621883 \t -0.017042291439201326\n",
            "68     \t [1.15112364 0.41629861]. \t  -82.61222519369333 \t -0.017042291439201326\n",
            "69     \t [0.9116035  1.07213758]. \t  -5.821536800474615 \t -0.017042291439201326\n",
            "70     \t [0.00420307 0.2388898 ]. \t  -6.697601156350904 \t -0.017042291439201326\n",
            "71     \t [1.09098552 1.19928518]. \t  \u001b[92m-0.016442882530934063\u001b[0m \t -0.016442882530934063\n",
            "72     \t [2.02236401 0.80552465]. \t  -1079.7942702821092 \t -0.016442882530934063\n",
            "73     \t [-0.62886151 -1.17512401]. \t  -249.32873851716164 \t -0.016442882530934063\n",
            "74     \t [1.52137197 0.73026457]. \t  -251.27504800338738 \t -0.016442882530934063\n",
            "75     \t [0.38581887 1.73789702]. \t  -252.88229059851358 \t -0.016442882530934063\n",
            "76     \t [-1.96944417 -0.5409523 ]. \t  -1962.1593894476853 \t -0.016442882530934063\n",
            "77     \t [ 1.154773   -0.95401997]. \t  -523.2990278949036 \t -0.016442882530934063\n",
            "78     \t [-0.51051861  1.68444812]. \t  -205.0076833315466 \t -0.016442882530934063\n",
            "79     \t [-1.06095289 -0.39755859]. \t  -236.25514667039812 \t -0.016442882530934063\n",
            "80     \t [ 0.36031694 -0.00897675]. \t  -2.335878478975027 \t -0.016442882530934063\n",
            "81     \t [0.94159171 0.87789667]. \t  \u001b[92m-0.010977516465638146\u001b[0m \t -0.010977516465638146\n",
            "82     \t [-0.82949027  1.47402342]. \t  -65.12181013587157 \t -0.010977516465638146\n",
            "83     \t [1.30753342 1.71402681]. \t  -0.09649802203556279 \t -0.010977516465638146\n",
            "84     \t [1.78402586 0.47746595]. \t  -732.4699348470266 \t -0.010977516465638146\n",
            "85     \t [0.97203986 0.93988444]. \t  \u001b[92m-0.0032588749019199564\u001b[0m \t -0.0032588749019199564\n",
            "86     \t [-0.85440648 -1.43697053]. \t  -473.019468218265 \t -0.0032588749019199564\n",
            "87     \t [-1.23659192 -0.58198804]. \t  -450.69676966562554 \t -0.0032588749019199564\n",
            "88     \t [-1.42980548  1.22539506]. \t  -72.97164514750482 \t -0.0032588749019199564\n",
            "89     \t [-1.88312686  1.82949245]. \t  -303.00949927111895 \t -0.0032588749019199564\n",
            "90     \t [-1.4660533   1.66046065]. \t  -29.979010274014193 \t -0.0032588749019199564\n",
            "91     \t [-0.0655655   0.64578957]. \t  -42.28646587945757 \t -0.0032588749019199564\n",
            "92     \t [1.59988677 0.83088525]. \t  -299.2183568062418 \t -0.0032588749019199564\n",
            "93     \t [-0.18625368  0.67477663]. \t  -42.37823194204345 \t -0.0032588749019199564\n",
            "94     \t [1.19295637 0.88925704]. \t  -28.540856130884436 \t -0.0032588749019199564\n",
            "95     \t [ 1.25493264 -0.61556837]. \t  -479.8608501814786 \t -0.0032588749019199564\n",
            "96     \t [0.5585229  0.33265303]. \t  -0.23777258014034008 \t -0.0032588749019199564\n",
            "97     \t [0.6027097  1.82522772]. \t  -213.89309756821692 \t -0.0032588749019199564\n",
            "98     \t [-1.99286282 -1.79067893]. \t  -3329.230399486352 \t -0.0032588749019199564\n",
            "99     \t [ 1.5240691  -0.14424668]. \t  -608.8999803784616 \t -0.0032588749019199564\n",
            "100    \t [-0.71917525  1.86097463]. \t  -183.5250858961645 \t -0.0032588749019199564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUnhsKpCuv9o",
        "outputId": "52f2d0d5-1c0a-4e44-ba0d-93b99019d6ce"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1593.6058838367462"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJG0SLpwuwAL",
        "outputId": "48c31805-b160-4af6-f529-48002a0e50e8"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.197782872284006, -4.73488794011762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lA9eZf0uwCx",
        "outputId": "bb5a78fe-9643-4652-8392-f073b2806fe7"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.775298194977734, -5.578961856157758)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glrTGcpAuwFa",
        "outputId": "72518edd-25fe-4ea4-ecb0-db2fa63f1ebd"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.01681100984598, -5.075723300597093)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaRwfbxeuwIR",
        "outputId": "a54f7278-20bb-46af-a42d-566987af4533"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.3242565690018555, -5.989264108482838)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nb5NkfyuwKp",
        "outputId": "eac94770-b077-492b-8282-2c0d79839563"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.7405877266158902, -2.7382488258308473)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Q-WfXbuwNg",
        "outputId": "6fbc4e8a-ce58-4c91-cba0-42bb8525b075"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.629783978324656, -7.2152334220614565)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqS7VLcuwPy",
        "outputId": "2cb96823-6188-45de-d5c7-3ff87edba4b9"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.1406456930929565, -4.491092000823802)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOi5iX8guwSS",
        "outputId": "e8687dff-b683-4239-a5cf-ece77441ca52"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.4159847743507195, -3.863249208593152)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVApeazuwU5",
        "outputId": "e497616a-8c99-4974-cc58-0ba70319748c"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.263799971119879, -6.876481518358155)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g92jk9WJuwXb",
        "outputId": "5756507b-ae6c-40a4-9bed-73d4db0efb24"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.7634817434197885, -4.029331715331525)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmcF1x-NuwZz",
        "outputId": "c2323a7c-06e3-4f16-b9e9-0466987ba19c"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.634126667427902, -4.112795257869981)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8axVhb6uwcc",
        "outputId": "4da41afa-2922-4dcf-8580-de59fe6d02c0"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-8.371105951845212, -7.430256969105658)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzlrL8XFuwfB",
        "outputId": "483264f3-f26a-4802-c3eb-bfafd1928760"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.62042749399009, -4.602820697267908)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZlLJ1quwh6",
        "outputId": "551a728d-37cc-4324-a07b-047e3a16651b"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.118283958431365, -5.118283958431365)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVBcHRiMuwjx",
        "outputId": "a9e95982-14c7-477a-e522-b6f139ab1f95"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.829925967719914, -5.829925967719914)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8nZ_DrKuwnA",
        "outputId": "cff0239e-faf0-42f4-c3bb-131d12d87f00"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.6469386732154208, -4.900111522327611)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg6qzzQFuwpU",
        "outputId": "de21a9b9-30e1-4502-bfd3-ec6388d0372d"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.7511879729952318, -4.250184610463337)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrRtDLMFuwsB",
        "outputId": "7680dca3-ad10-414b-fde1-7bf1578fa869"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.007264735220881, -5.957132535891781)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkmSu4CUuwuh",
        "outputId": "3c177877-9648-4f57-efff-b4edc9d5ff2f"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.692486677828461, -4.467641329727569)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eymecjwkuwxb",
        "outputId": "3d5f530f-ef0c-4433-b31b-5a89a8de4435"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.48618199880745, -5.726373265282305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "84XIzmWD2oba",
        "outputId": "8d063032-f172-43f3-ae98-291530f9092c"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Red')\r\n",
        "plt.plot(median_winner, color = 'Yellow')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Yellow', alpha=0.4, label='GP EI Regret IQR: L-BFGS-B')\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP EI Regret IQR: Newton-CG with GP d$^{2}$EI')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show() "
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5dkR2Id2oiu",
        "outputId": "c04ef110-9bb7-43fb-8b5e-d804b9132b1e"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2332.73432803154, 1593.6058838367462)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    }
  ]
}